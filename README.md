# GPT2

Training a GPT2 model

This project contains code for training a GPT2 model (124 million parameter) from scratch. It is based on the youtube lessons from Andrej Karapathy (https://www.youtube.com/watch?v=l8pRSuU81PU)

# Outcome
Trained the model (124 Million Parameters/10Billion tokens) on 8 GPU cluster. The model weights are available in the log director. 

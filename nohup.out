W0728 23:11:54.906000 140689212964864 torch/distributed/run.py:779] 
W0728 23:11:54.906000 140689212964864 torch/distributed/run.py:779] *****************************************
W0728 23:11:54.906000 140689212964864 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0728 23:11:54.906000 140689212964864 torch/distributed/run.py:779] *****************************************
W0728 23:11:58.416000 140689212964864 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGINT death signal, shutting down workers
W0728 23:11:58.417000 140689212964864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 15298 closing signal SIGINT
W0728 23:11:58.417000 140689212964864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 15299 closing signal SIGINT
W0728 23:11:58.417000 140689212964864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 15300 closing signal SIGINT
W0728 23:11:58.417000 140689212964864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 15301 closing signal SIGINT
Traceback (most recent call last):
  File "<frozen importlib._bootstrap_external>", line 153, in _path_is_mode_type
W0728 23:11:58.417000 140689212964864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 15302 closing signal SIGINT
W0728 23:11:58.417000 140689212964864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 15303 closing signal SIGINT
W0728 23:11:58.417000 140689212964864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 15304 closing signal SIGINT
W0728 23:11:58.418000 140689212964864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 15305 closing signal SIGINT
Traceback (most recent call last):
  File "/home/ubuntu/fs-il/GPT2/train_gpt2.py", line 14, in <module>
    from hellaswag import render_example, iterate_examples  File "<frozen importlib._bootstrap_external>", line 147, in _path_stat

  File "/home/ubuntu/fs-il/GPT2/hellaswag.py", line 38, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/fs-il/GPT2/train_gpt2.py", line 14, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/fs-il/GPT2/train_gpt2.py", line 14, in <module>
  File "/home/ubuntu/fs-il/GPT2/train_gpt2.py", line 14, in <module>
FileNotFoundError:   File "/home/ubuntu/fs-il/GPT2/train_gpt2.py", line 14, in <module>
Traceback (most recent call last):
[Errno 2] No such file or directory: '/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/encodec/__init__.abi3.so'
  File "/home/ubuntu/fs-il/GPT2/train_gpt2.py", line 14, in <module>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/fs-il/GPT2/train_gpt2.py", line 14, in <module>
  File "/home/ubuntu/fs-il/GPT2/train_gpt2.py", line 14, in <module>
    from hellaswag import render_example, iterate_examples    from hellaswag import render_example, iterate_examples
  File "/home/ubuntu/fs-il/GPT2/hellaswag.py", line 38, in <module>

  File "/home/ubuntu/fs-il/GPT2/hellaswag.py", line 38, in <module>
    from hellaswag import render_example, iterate_examples    from transformers import GPT2LMHeadModel    from hellaswag import render_example, iterate_examples
  File "/home/ubuntu/fs-il/GPT2/hellaswag.py", line 38, in <module>

  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
        
  File "/home/ubuntu/fs-il/GPT2/hellaswag.py", line 38, in <module>
    from hellaswag import render_example, iterate_examples    from hellaswag import render_example, iterate_examplesfrom hellaswag import render_example, iterate_examples
from transformers import GPT2LMHeadModel
    from transformers import GPT2LMHeadModel
  File "/home/ubuntu/fs-il/GPT2/hellaswag.py", line 38, in <module>

  File "/home/ubuntu/fs-il/GPT2/hellaswag.py", line 38, in <module>
  File "/home/ubuntu/fs-il/GPT2/hellaswag.py", line 38, in <module>
    from transformers import GPT2LMHeadModel  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist

  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1576, in __getattr__
    from transformers import GPT2LMHeadModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1576, in __getattr__
    
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1576, in __getattr__
    from transformers import GPT2LMHeadModelfrom transformers import GPT2LMHeadModel
    from transformers import GPT2LMHeadModel  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1576, in __getattr__

  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1576, in __getattr__
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist

  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1576, in __getattr__
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1576, in __getattr__
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1576, in __getattr__
    module = self._get_module(self._class_to_module[name])
    module = self._get_module(self._class_to_module[name])    module = self._get_module(self._class_to_module[name])  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1586, in _get_module

  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1586, in _get_module

  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1586, in _get_module
    module = self._get_module(self._class_to_module[name])
        module = self._get_module(self._class_to_module[name])  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1586, in _get_module
module = self._get_module(self._class_to_module[name])

  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1586, in _get_module
    module = self._get_module(self._class_to_module[name])  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1586, in _get_module

  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1586, in _get_module
    return importlib.import_module("." + module_name, self.__name__)    return importlib.import_module("." + module_name, self.__name__)    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module

  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module

  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return importlib.import_module("." + module_name, self.__name__)    return importlib.import_module("." + module_name, self.__name__)    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
        
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)    return _bootstrap._gcd_import(name[level:], package, level)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)return importlib.import_module("." + module_name, self.__name__)
return _bootstrap._gcd_import(name[level:], package, level)
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>

  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>
    
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>

  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>
return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>
    from . import (    from . import (
    
        from . import (    from . import (  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
from . import (
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
    from . import (from . import (

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1002, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1002, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
    module = self._get_module(self._class_to_module[name])
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1586, in _get_module
  File "<frozen importlib._bootstrap>", line 945, in _find_spec
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap>", line 945, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1439, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1439, in find_spec
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
    return importlib.import_module("." + module_name, self.__name__)  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 1411, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 975, in get_code

  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1411, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
KeyboardInterrupt
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
  File "<frozen importlib._bootstrap_external>", line 1563, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1563, in find_spec
    return _bootstrap._gcd_import(name[level:], package, level)  File "<frozen importlib._bootstrap_external>", line 975, in get_code
KeyboardInterrupt

  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/transformers/models/__init__.py", line 15, in <module>
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
  File "<frozen importlib._bootstrap_external>", line 161, in _path_isfile
  File "<frozen importlib._bootstrap_external>", line 161, in _path_isfile
KeyboardInterrupt
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
  File "<frozen importlib._bootstrap_external>", line 153, in _path_is_mode_type
  File "<frozen importlib._bootstrap_external>", line 153, in _path_is_mode_type
KeyboardInterrupt    from . import (  File "<frozen importlib._bootstrap_external>", line 147, in _path_stat
KeyboardInterrupt

  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load

KeyboardInterrupt
KeyboardInterrupt  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked

  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/ubuntu/fs-il/GPT2/.venv/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/ubuntu/fs-il/GPT2/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 15227 got signal: 2
W0728 23:12:15.472000 139847933739008 torch/distributed/run.py:779] 
W0728 23:12:15.472000 139847933739008 torch/distributed/run.py:779] *****************************************
W0728 23:12:15.472000 139847933739008 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0728 23:12:15.472000 139847933739008 torch/distributed/run.py:779] *****************************************
[W728 23:12:24.291466638 socket.cpp:697] [c10d] The client socket has failed to connect to [149-130-213-165]:59475 (errno: 22 - Invalid argument).
using device :cuda:0
524288
Total desired batch size 524288
==> calculated gradient accumlation steps 4
Loaded 99 shards
Loaded 1 shards
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: True
validation loss 10.9956
HellaSwag accuracy: 2479/10042=0.2469
> Hello, I'm a language model,745WritunciationWrit toilets cush rodentsatri patriteness rodents745 publicized Twice NeighOne rodents minimalist effectiveness AwokenanticallyWritSTE innovations
> Hello, I'm a language model,ointmentantically Framesble buddiesperformance nem investigates buddiesble logicaltl sample contributors Egyptian nem WEEK preced loading EgyptianWritWrit WEEKWrit
> Hello, I'm a language model,infikini buddiesanticallyteness glimpse Elephantda684tl 277 philosophicalda harbor Nielsenimproula overlapping CinemBackWrit MasonictenessOfficials
> Hello, I'm a language model,affectedanticallyributionricaTesla minimalist minimalist nem loading Tigers contributors Roose Directions 277 Randallsembly Revivalribution ammo� sniper "[ Yang ordained> 
Hello, I'm a language model,ointment sore cameorint rodentsCompareOnetlroad healsber Kaiseralyses Student flaw nemrorputing gazedagin Effective UFC flaw kingdom
> > Hello, I'm a language model, Yangantically Effectiveaffected ElephantLast param491inf breArray emerging harbor LOOK Patterns rodentsula sins contributorsulaulaished emerging effectiveness
Hello, I'm a language model, Frames unve Elephantantically postageointmentaws loading bre senate contributors reinventunula Elephant Grayson separating Splash porn Student Elephant suicidesaws491
> Hello, I'm a language model,omonwed leanedointment rodents senate emphributionilianTesla publicized contributors CBS haircSum Roose Valueble dissentleased breber unsolved CBS> 
Hello, I'm a language model, theatreunciationun Dim toiletsror hairctl Monster 277 nem contributors TI movedulaListenerantically team Grayson patri491 Eur patri moved
> Hello, I'm a language model, loading infants flaw cuisineror Effective Rooserica nem inviting Roose BermanbleTesla rodentsble491odka porchblindAlthough breulateness
> > Hello, I'm a language model, cuisine491ListenerWrit Parking ArchitectureTesla occasional nem loading491 effectiveness resignedWrit effectiveness unnoticed sample Dim LOOKbleantically supermarkets TI mt
Hello, I'm a language model,inf nowadays unveanticallyantically CBS rodentstenessinf 65 auditionperformanceda Valueanticallyber subsWrit iCloud favoured Swap subs nobles Apostles
> Hello, I'm a language model,antically Broad Elephant Education Effective param Architecture Directions TIanian SealWrittenesshou grandchildren preced Mald breAUDantically preced PCB psychologyAUD
> Hello, I'm a language model,atri fortunaffected unsolved legitimately Yangnpric491 Yangteness upsetting Elephantishedributionatri minimalist491 pitch236 effectiveness reinvent KILL contributors
> > Hello, I'm a language model, Enforcement param theatre senateHTTPula Bened param effectivenessribution nem rodentstier modulesbleber%:antically fingerprint Yang defiance strat buddies discreteHello, I'm a language model, cadTeslatenessOne loading unemploy745 Roose cushinf Effective flawperformance Tigers Superintendent Yang sins gazed Benedber Superintendent Gearsafetysembly

> > Hello, I'm a language model, cad skepticism samplewed heals surroundednp dissent unsolved Effective nemrica deft491ber Yangrica kingdom Nielsenteness——ribution491Compare
Hello, I'm a language model,inf236 theatreanticallyteness745 Studenttl soreroruphem contributors ParkingTesla cynical favouredikini c FramesListener Yang loading cynical moved
> Hello, I'm a language model, Studentinf rodents ElephantWrit 277 Socialism Patun Student iCloudberSum Student philosophical Pacersda Value Directorribution flaw */ CinemNAT> 
Hello, I'm a language model, Broad Cinem paramrica TIHTTPaws Elephant Yang Woodenanian Rayaws ABC rodentsblind beloved beloved Kathrynassanian contributors kingdom patri
> Hello, I'm a language model, sore CinemOne rodents rodents ParkingListenertltl contributorsTesla outskirts Elephanttl definition Sealaccessleased reinventrica dismalOfficials buddiestl> 
Hello, I'm a language model,HTTPatritenessrortl Yangointment Institution Institution favoringWrit TI unnoticedruffCorninfTesla solvesointmentWrit Ps obstprojectula
> Hello, I'm a language model,ulaointment Student precedawszeenp rodentstenessOne deftListenerTeslaTeslaried Splashointmentbyss nemried unemploy GraysonbleTesla
> Hello, I'm a language model, Superintendent buddies heals heals Dunda Berman� fingerprint unve Tigersrica investigates retention cameo unsolvedCompareantically precedatri solves surrounded logicalPassword
> Hello, I'm a language model, sorerint harbor cush heals senate Elephantrint reinvent 277 Effective contributorstl loading minimalistributionberrica UFCrica sniperror Yang minimalist
> Hello, I'm a language model, loadingrica Pat sore TITesla TI Doc Revival491 Seal effectivenessawstlrint Areaulaointment Student Essence contributorsulablindTesla
> Hello, I'm a language model, toilets Cinem buddies Cinem criminalbleListener trim dissent harbor Magnet wicked Frames contributors rodents loading logical Randall Frames wicked PCBRatherRather213
> Hello, I'm a language model,agin nowadays CBS categ fortun nowadaysdiskrintun declarationula bearings Elephantomon reinvent buddies491leased Seal municipalities Doc Elephantrica Seal
> Hello, I'm a language model,——ikini CinemWritula Effective LOOK publicizedperformance flawignment bearings harborPsychperformance Grayson buddies iCloud precedributionPsych preced interpersonalula
> Hello, I'm a language model, Superintendent—— sorerintListener loadingbleributionuphemried Gearstorage Superintendent SERVICE preced PCBaws dismalble Sm effectiveness Areaagin reactions
> Hello, I'm a language model, Swapaffected transition Elephant TI foil ABC745 loadingribution patri unemploy preced sniperurtle PCBCornpie Yang dismalGF PCB YangListener
> Hello, I'm a language model, forgedatri senate classify cynical flaw745181npwed publicizedaws fingerprint minimalist rodentsTweet Grayson gazed flawTweetwedulachanaws
for step    0 | loss 10.995782 | norm 4.6758 | time 11709.5339 ms | tok/sec 44774.4550
for step    1 | loss 10.986292 | norm 4.5554 | time 452.3640 ms | tok/sec 1158995.9352
for step    2 | loss 10.965803 | norm 4.6402 | time 452.4448 ms | tok/sec 1158788.8943
for step    3 | loss 10.926362 | norm 4.7328 | time 453.2230 ms | tok/sec 1156799.2170
for step    4 | loss 10.883096 | norm 4.6234 | time 453.3861 ms | tok/sec 1156383.1282
for step    5 | loss 10.828099 | norm 4.5668 | time 454.2663 ms | tok/sec 1154142.3803
for step    6 | loss 10.762609 | norm 4.3828 | time 453.3036 ms | tok/sec 1156593.5685
for step    7 | loss 10.693769 | norm 4.1457 | time 453.3603 ms | tok/sec 1156448.8065
for step    8 | loss 10.613091 | norm 3.9165 | time 453.8403 ms | tok/sec 1155225.8606
for step    9 | loss 10.549644 | norm 3.5571 | time 452.7538 ms | tok/sec 1157998.0566
for step   10 | loss 10.467800 | norm 3.3266 | time 453.8851 ms | tok/sec 1155111.7781
for step   11 | loss 10.406240 | norm 3.0846 | time 453.3646 ms | tok/sec 1156437.8596
for step   12 | loss 10.337447 | norm 2.8947 | time 452.9748 ms | tok/sec 1157433.0499
for step   13 | loss 10.279009 | norm 2.7909 | time 452.8883 ms | tok/sec 1157654.2328
for step   14 | loss 10.202783 | norm 2.7552 | time 453.2323 ms | tok/sec 1156775.4846
for step   15 | loss 10.152563 | norm 2.6899 | time 454.6540 ms | tok/sec 1153158.2807
for step   16 | loss 10.093962 | norm 2.5969 | time 453.1002 ms | tok/sec 1157112.6979
for step   17 | loss 10.041484 | norm 2.5302 | time 454.1392 ms | tok/sec 1154465.3320
for step   18 | loss 10.025351 | norm 2.3833 | time 453.6889 ms | tok/sec 1155611.3590
for step   19 | loss 9.994934 | norm 2.3980 | time 453.5716 ms | tok/sec 1155910.2213
for step   20 | loss 9.890878 | norm 2.3595 | time 453.4121 ms | tok/sec 1156316.8493
for step   21 | loss 9.834842 | norm 2.3465 | time 454.3214 ms | tok/sec 1154002.4704
for step   22 | loss 9.800251 | norm 2.2878 | time 453.1810 ms | tok/sec 1156906.3292
for step   23 | loss 9.767969 | norm 2.2284 | time 454.3293 ms | tok/sec 1153982.4861
for step   24 | loss 9.709044 | norm 2.2389 | time 453.8701 ms | tok/sec 1155150.0054
for step   25 | loss 9.684989 | norm 2.2092 | time 453.3420 ms | tok/sec 1156495.6373
for step   26 | loss 9.652309 | norm 2.1550 | time 453.6896 ms | tok/sec 1155609.5371
for step   27 | loss 9.612103 | norm 2.1548 | time 453.5336 ms | tok/sec 1156006.8380
for step   28 | loss 9.586628 | norm 2.1363 | time 454.4339 ms | tok/sec 1153716.6995
for step   29 | loss 9.549210 | norm 2.1465 | time 453.6405 ms | tok/sec 1155734.6514
for step   30 | loss 9.562302 | norm 2.0538 | time 454.6604 ms | tok/sec 1153141.9538
for step   31 | loss 9.505091 | norm 2.0986 | time 453.7275 ms | tok/sec 1155512.9869
for step   32 | loss 9.487572 | norm 2.0738 | time 454.5023 ms | tok/sec 1153543.0054
for step   33 | loss 9.456215 | norm 2.1000 | time 454.5720 ms | tok/sec 1153366.3389
for step   34 | loss 9.378660 | norm 2.1682 | time 454.6118 ms | tok/sec 1153265.3246
for step   35 | loss 9.399010 | norm 2.0736 | time 455.0705 ms | tok/sec 1152102.8172
for step   36 | loss 9.389543 | norm 2.0437 | time 453.7232 ms | tok/sec 1155523.9163
for step   37 | loss 9.336438 | norm 2.0739 | time 454.1485 ms | tok/sec 1154441.6953
for step   38 | loss 9.310010 | norm 2.0724 | time 454.6185 ms | tok/sec 1153248.3897
for step   39 | loss 9.300789 | norm 2.0144 | time 454.7076 ms | tok/sec 1153022.2368
for step   40 | loss 9.285555 | norm 1.9944 | time 454.7045 ms | tok/sec 1153030.0962
for step   41 | loss 9.210675 | norm 2.0678 | time 453.8019 ms | tok/sec 1155323.5768
for step   42 | loss 9.194073 | norm 2.0268 | time 454.9956 ms | tok/sec 1152292.3803
for step   43 | loss 9.163891 | norm 2.0166 | time 455.2791 ms | tok/sec 1151574.9050
for step   44 | loss 9.154465 | norm 1.9738 | time 456.0142 ms | tok/sec 1149718.6934
for step   45 | loss 9.132087 | norm 1.9388 | time 455.2143 ms | tok/sec 1151738.9585
for step   46 | loss 9.086048 | norm 1.9679 | time 454.9246 ms | tok/sec 1152472.3418
for step   47 | loss 9.050661 | norm 1.9694 | time 455.6811 ms | tok/sec 1150559.0567
for step   48 | loss 9.031084 | norm 1.9518 | time 456.7473 ms | tok/sec 1147873.2496
for step   49 | loss 8.969256 | norm 1.9650 | time 455.9958 ms | tok/sec 1149764.9807
for step   50 | loss 8.974564 | norm 1.8952 | time 455.8034 ms | tok/sec 1150250.3196
for step   51 | loss 8.917869 | norm 1.8861 | time 455.7478 ms | tok/sec 1150390.5247
for step   52 | loss 8.892042 | norm 1.8685 | time 456.3093 ms | tok/sec 1148975.0016
for step   53 | loss 8.845690 | norm 1.8441 | time 456.7997 ms | tok/sec 1147741.4449
for step   54 | loss 8.839500 | norm 1.7982 | time 456.0578 ms | tok/sec 1149608.7009
for step   55 | loss 8.795114 | norm 1.7834 | time 455.1139 ms | tok/sec 1151992.9716
for step   56 | loss 8.764624 | norm 1.7745 | time 457.7487 ms | tok/sec 1145362.1945
for step   57 | loss 8.738254 | norm 1.7354 | time 456.2571 ms | tok/sec 1149106.4893
for step   58 | loss 8.693784 | norm 1.7294 | time 455.4915 ms | tok/sec 1151037.8365
for step   59 | loss 8.659124 | norm 1.7098 | time 455.6763 ms | tok/sec 1150571.0966
for step   60 | loss 8.634139 | norm 1.6854 | time 456.2359 ms | tok/sec 1149159.9335
for step   61 | loss 8.609465 | norm 1.6541 | time 456.4047 ms | tok/sec 1148734.9191
for step   62 | loss 8.535116 | norm 1.6670 | time 457.3348 ms | tok/sec 1146398.7640
for step   63 | loss 8.493037 | norm 1.6316 | time 455.9205 ms | tok/sec 1149954.9779
for step   64 | loss 8.449917 | norm 1.6024 | time 456.4352 ms | tok/sec 1148658.1138
for step   65 | loss 8.430908 | norm 1.5585 | time 457.2072 ms | tok/sec 1146718.5921
for step   66 | loss 8.375912 | norm 1.5015 | time 456.0051 ms | tok/sec 1149741.5360
for step   67 | loss 8.357441 | norm 1.4187 | time 456.4512 ms | tok/sec 1148617.9152
for step   68 | loss 8.337061 | norm 1.3932 | time 457.8023 ms | tok/sec 1145227.9837
for step   69 | loss 8.255083 | norm 1.4090 | time 457.9334 ms | tok/sec 1144900.0452
for step   70 | loss 8.215183 | norm 1.3751 | time 457.9663 ms | tok/sec 1144817.7919
for step   71 | loss 8.221001 | norm 1.3041 | time 457.9718 ms | tok/sec 1144804.0842
for step   72 | loss 8.148244 | norm 1.3053 | time 458.8230 ms | tok/sec 1142680.3785
for step   73 | loss 8.110095 | norm 1.2865 | time 458.0183 ms | tok/sec 1144687.8796
for step   74 | loss 8.064471 | norm 1.2507 | time 458.0095 ms | tok/sec 1144709.9269
for step   75 | loss 8.061680 | norm 1.1775 | time 458.1535 ms | tok/sec 1144350.1266
for step   76 | loss 7.994137 | norm 1.1388 | time 457.5448 ms | tok/sec 1145872.4825
for step   77 | loss 7.990807 | norm 1.0911 | time 458.3778 ms | tok/sec 1143790.0273
for step   78 | loss 7.948542 | norm 1.0682 | time 457.6776 ms | tok/sec 1145539.9977
for step   79 | loss 7.899899 | norm 1.0454 | time 458.5063 ms | tok/sec 1143469.4522
for step   80 | loss 7.865157 | norm 1.0092 | time 459.7960 ms | tok/sec 1140262.3227
for step   81 | loss 7.849348 | norm 0.9495 | time 459.2547 ms | tok/sec 1141606.0683
for step   82 | loss 7.797409 | norm 0.9341 | time 458.6964 ms | tok/sec 1142995.7584
for step   83 | loss 7.741935 | norm 0.8972 | time 459.0704 ms | tok/sec 1142064.3754
for step   84 | loss 7.723944 | norm 0.8495 | time 460.4573 ms | tok/sec 1138624.5186
for step   85 | loss 7.677679 | norm 0.8545 | time 460.1762 ms | tok/sec 1139320.0396
for step   86 | loss 7.657676 | norm 0.7696 | time 460.2296 ms | tok/sec 1139187.8312
for step   87 | loss 7.650575 | norm 0.7618 | time 459.3368 ms | tok/sec 1141402.2311
for step   88 | loss 7.625824 | norm 0.7515 | time 459.9977 ms | tok/sec 1139762.3346
for step   89 | loss 7.648941 | norm 0.6421 | time 459.3060 ms | tok/sec 1141478.6615
for step   90 | loss 7.569345 | norm 0.7453 | time 459.9130 ms | tok/sec 1139972.0871
for step   91 | loss 7.554889 | norm 0.6308 | time 459.1060 ms | tok/sec 1141976.0055
for step   92 | loss 7.562799 | norm 0.5721 | time 459.1527 ms | tok/sec 1141859.7815
for step   93 | loss 7.610668 | norm 0.5564 | time 459.1546 ms | tok/sec 1141855.0382
for step   94 | loss 7.607631 | norm 0.4848 | time 460.3915 ms | tok/sec 1138787.2616
for step   95 | loss 7.596326 | norm 0.4508 | time 461.2274 ms | tok/sec 1136723.4052
for step   96 | loss 7.550355 | norm 0.4388 | time 460.4428 ms | tok/sec 1138660.4832
for step   97 | loss 7.511258 | norm 0.5262 | time 459.9524 ms | tok/sec 1139874.5869
for step   98 | loss 7.533668 | norm 0.4242 | time 459.7013 ms | tok/sec 1140497.1019
for step   99 | loss 7.480362 | norm 0.4585 | time 460.7661 ms | tok/sec 1137861.5443
for step  100 | loss 7.534130 | norm 0.4464 | time 460.0198 ms | tok/sec 1139707.3981
for step  101 | loss 7.460957 | norm 0.4241 | time 460.7255 ms | tok/sec 1137961.6448
for step  102 | loss 7.446643 | norm 0.3942 | time 460.3159 ms | tok/sec 1138974.2377
for step  103 | loss 7.346804 | norm 0.4510 | time 461.5819 ms | tok/sec 1135850.3191
for step  104 | loss 7.423830 | norm 0.3906 | time 460.7584 ms | tok/sec 1137880.3854
for step  105 | loss 7.436995 | norm 0.4052 | time 461.0376 ms | tok/sec 1137191.3251
for step  106 | loss 7.387697 | norm 0.3679 | time 461.5192 ms | tok/sec 1136004.6409
for step  107 | loss 7.336432 | norm 0.3360 | time 461.0732 ms | tok/sec 1137103.7078
for step  108 | loss 7.394150 | norm 0.3721 | time 461.7178 ms | tok/sec 1135516.0014
for step  109 | loss 7.410496 | norm 0.2865 | time 460.5005 ms | tok/sec 1138517.8175
for step  110 | loss 7.340801 | norm 0.3548 | time 461.4062 ms | tok/sec 1136282.8780
for step  111 | loss 7.320294 | norm 0.3522 | time 461.6721 ms | tok/sec 1135628.5917
for step  112 | loss 7.290961 | norm 0.3023 | time 461.9944 ms | tok/sec 1134836.2429
for step  113 | loss 7.350860 | norm 0.2952 | time 461.1290 ms | tok/sec 1136966.1347
for step  114 | loss 7.339121 | norm 0.4642 | time 461.0183 ms | tok/sec 1137238.9617
for step  115 | loss 7.301930 | norm 0.2839 | time 461.8735 ms | tok/sec 1135133.2441
for step  116 | loss 7.236015 | norm 0.3000 | time 461.5846 ms | tok/sec 1135843.8655
for step  117 | loss 7.153332 | norm 0.3659 | time 461.8979 ms | tok/sec 1135073.4799
for step  118 | loss 7.182671 | norm 0.3326 | time 462.7404 ms | tok/sec 1133006.7050
for step  119 | loss 7.229049 | norm 0.4360 | time 461.6256 ms | tok/sec 1135742.9641
for step  120 | loss 7.184597 | norm 0.4316 | time 462.3494 ms | tok/sec 1133964.8828
for step  121 | loss 7.171874 | norm 0.3282 | time 463.1934 ms | tok/sec 1131898.6437
for step  122 | loss 7.151023 | norm 0.3705 | time 462.9719 ms | tok/sec 1132440.1564
for step  123 | loss 7.138074 | norm 0.3176 | time 462.8997 ms | tok/sec 1132616.8868
for step  124 | loss 7.156838 | norm 0.4366 | time 463.0976 ms | tok/sec 1132132.9053
for step  125 | loss 7.139067 | norm 0.2959 | time 462.7533 ms | tok/sec 1132975.1828
for step  126 | loss 7.127037 | norm 0.3918 | time 461.5295 ms | tok/sec 1135979.4067
for step  127 | loss 7.082011 | norm 0.4808 | time 462.3866 ms | tok/sec 1133873.6693
for step  128 | loss 7.054642 | norm 0.5190 | time 462.0004 ms | tok/sec 1134821.6019
for step  129 | loss 7.030472 | norm 0.4380 | time 462.2703 ms | tok/sec 1134159.0527
for step  130 | loss 7.083501 | norm 0.2780 | time 463.2137 ms | tok/sec 1131849.1231
for step  131 | loss 7.039553 | norm 0.3519 | time 462.3675 ms | tok/sec 1133920.4436
for step  132 | loss 7.036318 | norm 0.2818 | time 461.9138 ms | tok/sec 1135034.2264
for step  133 | loss 6.942128 | norm 0.3310 | time 462.6746 ms | tok/sec 1133167.8460
for step  134 | loss 6.935931 | norm 0.3968 | time 462.1091 ms | tok/sec 1134554.6162
for step  135 | loss 7.008410 | norm 0.3772 | time 462.3857 ms | tok/sec 1133876.0079
for step  136 | loss 6.981113 | norm 0.5012 | time 462.4515 ms | tok/sec 1133714.6655
for step  137 | loss 7.000640 | norm 0.6057 | time 462.3117 ms | tok/sec 1134057.2807
for step  138 | loss 6.928899 | norm 0.7540 | time 463.2387 ms | tok/sec 1131787.9567
for step  139 | loss 7.010198 | norm 0.8098 | time 462.1489 ms | tok/sec 1134456.8700
for step  140 | loss 7.106215 | norm 0.3965 | time 461.8759 ms | tok/sec 1135127.3846
for step  141 | loss 6.992186 | norm 0.7183 | time 462.1422 ms | tok/sec 1134473.2574
for step  142 | loss 7.051231 | norm 0.6697 | time 463.0904 ms | tok/sec 1132150.3914
for step  143 | loss 6.978644 | norm 0.4235 | time 463.2046 ms | tok/sec 1131871.2612
for step  144 | loss 6.985797 | norm 0.6133 | time 463.2251 ms | tok/sec 1131821.1605
for step  145 | loss 6.979038 | norm 0.4891 | time 463.1803 ms | tok/sec 1131930.6887
for step  146 | loss 6.978383 | norm 0.5677 | time 463.6023 ms | tok/sec 1130900.3319
for step  147 | loss 6.963546 | norm 0.3062 | time 463.0592 ms | tok/sec 1132226.7537
for step  148 | loss 6.961339 | norm 0.4957 | time 462.5854 ms | tok/sec 1133386.2768
for step  149 | loss 6.953692 | norm 0.3524 | time 463.5756 ms | tok/sec 1130965.4741
for step  150 | loss 6.935894 | norm 0.4104 | time 463.6712 ms | tok/sec 1130732.2767
for step  151 | loss 6.851144 | norm 0.3965 | time 463.3324 ms | tok/sec 1131559.0782
for step  152 | loss 6.868771 | norm 0.3806 | time 463.4957 ms | tok/sec 1131160.3636
for step  153 | loss 6.934515 | norm 0.3438 | time 463.3911 ms | tok/sec 1131415.8578
for step  154 | loss 6.837663 | norm 0.4965 | time 462.7502 ms | tok/sec 1132982.7713
for step  155 | loss 6.797904 | norm 0.4374 | time 464.3610 ms | tok/sec 1129052.7279
for step  156 | loss 6.863768 | norm 0.3793 | time 463.6173 ms | tok/sec 1130863.6928
for step  157 | loss 6.904159 | norm 0.4033 | time 463.2225 ms | tok/sec 1131827.5685
for step  158 | loss 6.886981 | norm 0.5981 | time 463.9401 ms | tok/sec 1130076.8154
for step  159 | loss 6.810277 | norm 0.5547 | time 463.7296 ms | tok/sec 1130589.8468
for step  160 | loss 6.751907 | norm 0.3259 | time 464.2649 ms | tok/sec 1129286.3928
for step  161 | loss 6.824950 | norm 0.3774 | time 463.6407 ms | tok/sec 1130806.7033
for step  162 | loss 6.727119 | norm 0.5002 | time 463.4488 ms | tok/sec 1131275.0016
for step  163 | loss 6.683974 | norm 0.5006 | time 463.4495 ms | tok/sec 1131273.2557
for step  164 | loss 6.718744 | norm 0.3920 | time 463.7561 ms | tok/sec 1130525.3291
for step  165 | loss 6.745360 | norm 0.5015 | time 463.5427 ms | tok/sec 1131045.7488
for step  166 | loss 6.590526 | norm 0.4454 | time 463.2549 ms | tok/sec 1131748.3477
for step  167 | loss 6.683117 | norm 0.4634 | time 464.2594 ms | tok/sec 1129299.7314
for step  168 | loss 6.652121 | norm 0.6226 | time 462.8623 ms | tok/sec 1132708.4816
for step  169 | loss 6.686865 | norm 0.5165 | time 463.6559 ms | tok/sec 1130769.4888
for step  170 | loss 6.653593 | norm 0.5110 | time 463.5649 ms | tok/sec 1130991.6493
for step  171 | loss 6.680898 | norm 0.7542 | time 463.8584 ms | tok/sec 1130276.0463
for step  172 | loss 6.663336 | norm 0.6290 | time 463.9196 ms | tok/sec 1130126.7618
for step  173 | loss 6.609617 | norm 0.6168 | time 464.0193 ms | tok/sec 1129884.0407
for step  174 | loss 6.583964 | norm 0.5124 | time 463.9049 ms | tok/sec 1130162.7723
for step  175 | loss 6.648487 | norm 0.6199 | time 464.0348 ms | tok/sec 1129846.3063
for step  176 | loss 6.583367 | norm 0.3553 | time 464.1006 ms | tok/sec 1129686.1086
for step  177 | loss 6.616482 | norm 0.5317 | time 464.2143 ms | tok/sec 1129409.3521
for step  178 | loss 6.546075 | norm 0.4040 | time 464.3161 ms | tok/sec 1129161.7208
for step  179 | loss 6.529800 | norm 0.3993 | time 464.1235 ms | tok/sec 1129630.3983
for step  180 | loss 6.521775 | norm 0.4489 | time 464.1764 ms | tok/sec 1129501.5892
for step  181 | loss 6.551325 | norm 0.4617 | time 464.4232 ms | tok/sec 1128901.4481
for step  182 | loss 6.542672 | norm 0.5115 | time 464.0701 ms | tok/sec 1129760.3975
for step  183 | loss 6.534345 | norm 0.5475 | time 463.7461 ms | tok/sec 1130549.7403
for step  184 | loss 6.551102 | norm 0.5073 | time 465.5662 ms | tok/sec 1126129.9617
for step  185 | loss 6.601233 | norm 0.4868 | time 463.9821 ms | tok/sec 1129974.6134
for step  186 | loss 6.620083 | norm 0.3939 | time 463.8529 ms | tok/sec 1130289.4084
for step  187 | loss 6.678860 | norm 0.3280 | time 463.8307 ms | tok/sec 1130343.4406
for step  188 | loss 6.632674 | norm 0.4045 | time 463.9246 ms | tok/sec 1130114.5651
for step  189 | loss 6.598468 | norm 0.4565 | time 464.6695 ms | tok/sec 1128303.1022
Will loading at 0 from edu_fineweb10B/edufineweb_train_000002.npy
for step  190 | loss 6.643218 | norm 0.5418 | time 1497.0255 ms | tok/sec 350219.8216
for step  191 | loss 6.611314 | norm 0.8443 | time 462.9946 ms | tok/sec 1132384.7573
for step  192 | loss 6.588245 | norm 0.8737 | time 464.2434 ms | tok/sec 1129338.5893
for step  193 | loss 6.597878 | norm 0.5851 | time 464.1693 ms | tok/sec 1129518.9941
for step  194 | loss 6.555305 | norm 0.7129 | time 463.2289 ms | tok/sec 1131811.8400
for step  195 | loss 6.538593 | norm 0.7923 | time 462.6379 ms | tok/sec 1133257.7779
for step  196 | loss 6.530201 | norm 0.4816 | time 464.2305 ms | tok/sec 1129369.9094
for step  197 | loss 6.546318 | norm 0.7072 | time 463.7251 ms | tok/sec 1130600.8911
for step  198 | loss 6.530713 | norm 0.4891 | time 464.6945 ms | tok/sec 1128242.3184
for step  199 | loss 6.532480 | norm 0.5618 | time 464.0887 ms | tok/sec 1129715.1265
for step  200 | loss 6.515645 | norm 0.4774 | time 464.4279 ms | tok/sec 1128889.8574
for step  201 | loss 6.492889 | norm 0.4762 | time 464.4542 ms | tok/sec 1128826.1131
for step  202 | loss 6.492166 | norm 0.4566 | time 464.1931 ms | tok/sec 1129460.9798
for step  203 | loss 6.518741 | norm 0.3968 | time 464.2971 ms | tok/sec 1129208.1072
for step  204 | loss 6.495793 | norm 0.3946 | time 463.6092 ms | tok/sec 1130883.4660
for step  205 | loss 6.543708 | norm 0.5344 | time 465.1096 ms | tok/sec 1127235.4193
for step  206 | loss 6.490124 | norm 0.4349 | time 463.9404 ms | tok/sec 1130076.2347
for step  207 | loss 6.506379 | norm 0.5362 | time 464.2785 ms | tok/sec 1129253.3376
for step  208 | loss 6.396716 | norm 0.4776 | time 464.8049 ms | tok/sec 1127974.3690
for step  209 | loss 6.375568 | norm 0.6449 | time 464.6132 ms | tok/sec 1128439.7449
for step  210 | loss 6.410269 | norm 0.4509 | time 464.8807 ms | tok/sec 1127790.4084
for step  211 | loss 6.452041 | norm 0.5276 | time 463.7392 ms | tok/sec 1130566.5963
for step  212 | loss 6.464120 | norm 0.5380 | time 464.6597 ms | tok/sec 1128326.8386
for step  213 | loss 6.447019 | norm 0.4770 | time 464.1416 ms | tok/sec 1129586.2982
for step  214 | loss 6.435184 | norm 0.4099 | time 463.5789 ms | tok/sec 1130957.3309
for step  215 | loss 6.450715 | norm 0.3691 | time 464.9160 ms | tok/sec 1127704.8119
for step  216 | loss 6.371136 | norm 0.5131 | time 464.2475 ms | tok/sec 1129328.7296
for step  217 | loss 6.389665 | norm 0.6359 | time 464.4554 ms | tok/sec 1128823.2158
for step  218 | loss 6.376711 | norm 0.5066 | time 464.0126 ms | tok/sec 1129900.2962
for step  219 | loss 6.370781 | norm 0.5122 | time 464.9172 ms | tok/sec 1127701.9204
for step  220 | loss 6.299389 | norm 0.5438 | time 464.7553 ms | tok/sec 1128094.7279
for step  221 | loss 6.339257 | norm 0.4391 | time 463.4135 ms | tok/sec 1131361.1409
for step  222 | loss 6.348952 | norm 0.5142 | time 463.9482 ms | tok/sec 1130057.0704
for step  223 | loss 6.276488 | norm 0.4710 | time 464.5324 ms | tok/sec 1128636.0819
for step  224 | loss 6.323413 | norm 0.4522 | time 466.7039 ms | tok/sec 1123384.6755
for step  225 | loss 6.255565 | norm 0.5130 | time 464.5700 ms | tok/sec 1128544.5653
for step  226 | loss 6.268066 | norm 0.4420 | time 464.1187 ms | tok/sec 1129642.0041
for step  227 | loss 6.355777 | norm 0.5283 | time 465.4031 ms | tok/sec 1126524.5602
for step  228 | loss 6.272958 | norm 0.4921 | time 464.6728 ms | tok/sec 1128294.9973
for step  229 | loss 6.294794 | norm 0.4290 | time 465.1451 ms | tok/sec 1127149.3293
for step  230 | loss 6.278800 | norm 0.5236 | time 464.9005 ms | tok/sec 1127742.4034
for step  231 | loss 6.278879 | norm 0.5043 | time 463.8524 ms | tok/sec 1130290.5703
for step  232 | loss 6.436012 | norm 0.4369 | time 463.8777 ms | tok/sec 1130228.9913
for step  233 | loss 6.418217 | norm 0.6166 | time 463.2599 ms | tok/sec 1131736.1161
for step  234 | loss 6.422133 | norm 0.8734 | time 464.3154 ms | tok/sec 1129163.4602
for step  235 | loss 6.431114 | norm 1.1846 | time 464.2045 ms | tok/sec 1129433.1351
for step  236 | loss 6.434992 | norm 0.4575 | time 464.2348 ms | tok/sec 1129359.4692
for step  237 | loss 6.436214 | norm 0.9259 | time 463.5465 ms | tok/sec 1131036.4410
for step  238 | loss 6.369968 | norm 0.7869 | time 465.0919 ms | tok/sec 1127278.1803
for step  239 | loss 6.387688 | norm 0.5813 | time 465.8785 ms | tok/sec 1125374.9959
for step  240 | loss 6.396078 | norm 0.7867 | time 464.4446 ms | tok/sec 1128849.2920
for step  241 | loss 6.401844 | norm 0.6404 | time 463.9966 ms | tok/sec 1129939.1954
for step  242 | loss 6.362330 | norm 0.5858 | time 463.9406 ms | tok/sec 1130075.6539
for step  243 | loss 6.366964 | norm 0.6416 | time 468.0688 ms | tok/sec 1120108.7475
for step  244 | loss 6.286505 | norm 0.5045 | time 464.5078 ms | tok/sec 1128695.7494
for step  245 | loss 6.289527 | norm 0.4934 | time 464.7450 ms | tok/sec 1128119.6130
for step  246 | loss 6.317522 | norm 0.4572 | time 465.2088 ms | tok/sec 1126995.0936
for step  247 | loss 6.367445 | norm 0.4273 | time 464.8175 ms | tok/sec 1127943.7047
for step  248 | loss 6.291306 | norm 0.4617 | time 463.4228 ms | tok/sec 1131338.4408
for step  249 | loss 6.325892 | norm 0.4849 | time 464.3075 ms | tok/sec 1129182.5942
validation loss 6.3279
HellaSwag accuracy: 2501/10042=0.2491
> Hello, I'm a language model, if the brain.
The body could have one will say he is a large, they will make a person, you
> Hello, I'm a language model, we are a small.
This does what you may know, with your good will just an essential to you have a
> Hello, I'm a language model, you go, it is a good to you can like the best, your and you can you to the your your type
> Hello, I'm a language model, we believe that. The is not always say that if and also does this has a own questions is a work?

> Hello, I'm a language model, this is one of a more common that could come to also a. Then we can do is it can see that it
> Hello, I'm a language model, she to their own to be that at the first that and she’s, the second. “And that
> Hello, I'm a language model, I’t see how as your own thing to be about your good.
’s better.”
> > Hello, I'm a language model, I will get a great or a specific people will always be that was an little little more be less people or the time
Hello, I'm a language model, you be I would have a way your way can be you've your body to with your problem,.
C (
> > Hello, I'm a language model, an an important matter the �--in-6--far” to the course in the case.
Hello, I'm a language model, as no not always not you? You are not the point to, the very few years of the word.
You
> Hello, I'm a language model, the word to this. I have it to change the child will learn to be. They to see if you know,> 
Hello, I'm a language model, you could see you want you.
M. If you have about I will have to your problem you do you a
> > Hello, I'm a language model, and I or very little thing, because. In-7 per cent by the use of a day.
-s
Hello, I'm a language model, was an common, are also a other or in a way that may not like to more than in many common information,
>>  Hello, I'm a language model, but it.
You can be a different, if a best is to be the new. I'll do there'sHello, I'm a language model, they could try to know. He is more, you can be about your want your way more. “How the

> Hello, I'm a language model, “”
He is your “We have to be what is, said, but like this is you> > 
Hello, I'm a language model, when not also be only at.
I might be the body) is: a very important for some way.

Hello, I'm a language model, and the
The world and an good children, I was being the name were also the "that to be used of
> > > > Hello, I'm a language model, and a great way that’s great food, this own. A long months and be only. The first and
Hello, I'm a language model, there is when that as the "What
The other types.
the
"
" the
and
-
Hello, I'm a language model, this first”!“1, you’s: ‘t?”,” in
Hello, I'm a language model, he said to be a clear the a very little things for the way to be a bit of the human question has to
> > > > Hello, I'm a language model, says these’s time is sure and then go me, that are not very able to what that and we could
Hello, I'm a language model, I“I do you you need an much you would just? How?
It or just have the book is
Hello, I'm a language model, for an very part of one, there.
|1, you can expect this way, be one, and we
Hello, I'm a language model, and a few time if. So you say on you do?
I is.
This type, you. They
> > > Hello, I'm a language model, where I would just my am be able to be up with a much it.
-d.
- I can
Hello, I'm a language model, the "m. It was a more times an appropriate and there is a high people who cannot feel that a "d
Hello, I'm a language model, he who a own.
After the world of this. The way, the day, in the one means, and
> Hello, I'm a language model, some a few-S
-7 and not a different risk, if a new population, as a problem has is
for step  250 | loss 6.292220 | norm 0.4611 | time 13120.5990 ms | tok/sec 39959.1512
for step  251 | loss 6.286691 | norm 0.4027 | time 460.5751 ms | tok/sec 1138333.3483
for step  252 | loss 6.347652 | norm 0.5122 | time 462.3823 ms | tok/sec 1133884.1932
for step  253 | loss 6.292101 | norm 0.6866 | time 462.4135 ms | tok/sec 1133807.6072
for step  254 | loss 6.277617 | norm 0.6164 | time 462.2819 ms | tok/sec 1134130.3909
for step  255 | loss 6.204474 | norm 0.4324 | time 462.2245 ms | tok/sec 1134271.3740
for step  256 | loss 6.260061 | norm 0.6316 | time 461.4928 ms | tok/sec 1136069.7854
for step  257 | loss 6.236132 | norm 0.5730 | time 462.1584 ms | tok/sec 1134433.4602
for step  258 | loss 6.227417 | norm 0.5011 | time 462.7087 ms | tok/sec 1133084.3505
for step  259 | loss 6.184681 | norm 0.5324 | time 462.5444 ms | tok/sec 1133486.7599
for step  260 | loss 6.206811 | norm 0.5635 | time 462.6398 ms | tok/sec 1133253.1057
for step  261 | loss 6.201638 | norm 0.4739 | time 462.0137 ms | tok/sec 1134788.8074
for step  262 | loss 6.155323 | norm 0.4147 | time 462.3711 ms | tok/sec 1133911.6732
for step  263 | loss 6.162053 | norm 0.4369 | time 462.0709 ms | tok/sec 1134648.2811
for step  264 | loss 6.185578 | norm 0.4258 | time 462.4069 ms | tok/sec 1133823.9758
for step  265 | loss 6.194502 | norm 0.3943 | time 462.4350 ms | tok/sec 1133754.9968
for step  266 | loss 6.216191 | norm 0.4693 | time 462.8761 ms | tok/sec 1132674.6423
for step  267 | loss 6.130742 | norm 0.4734 | time 463.5427 ms | tok/sec 1131045.7488
for step  268 | loss 6.133664 | norm 0.4219 | time 464.2127 ms | tok/sec 1129413.4126
for step  269 | loss 6.187955 | norm 0.5003 | time 462.6338 ms | tok/sec 1133267.7063
for step  270 | loss 6.136573 | norm 0.6571 | time 463.1617 ms | tok/sec 1131976.1374
for step  271 | loss 6.137509 | norm 0.7525 | time 463.8000 ms | tok/sec 1130418.3972
for step  272 | loss 6.138103 | norm 0.9073 | time 464.3307 ms | tok/sec 1129126.3538
for step  273 | loss 6.094844 | norm 0.8365 | time 462.6317 ms | tok/sec 1133272.9626
for step  274 | loss 6.081706 | norm 0.6759 | time 463.4135 ms | tok/sec 1131361.1409
for step  275 | loss 6.200551 | norm 0.6164 | time 462.7752 ms | tok/sec 1132921.4823
for step  276 | loss 6.137905 | norm 0.6690 | time 464.6387 ms | tok/sec 1128377.7884
for step  277 | loss 6.133997 | norm 0.5703 | time 462.7011 ms | tok/sec 1133103.0338
for step  278 | loss 6.239928 | norm 0.6054 | time 463.6104 ms | tok/sec 1130880.5581
for step  279 | loss 6.221736 | norm 0.5574 | time 462.9500 ms | tok/sec 1132493.8113
for step  280 | loss 6.237473 | norm 0.6339 | time 463.9914 ms | tok/sec 1129951.9688
for step  281 | loss 6.215849 | norm 0.7224 | time 463.2556 ms | tok/sec 1131746.6003
for step  282 | loss 6.237266 | norm 0.7493 | time 464.2513 ms | tok/sec 1129319.4500
for step  283 | loss 6.220601 | norm 0.6295 | time 463.1178 ms | tok/sec 1132083.3643
for step  284 | loss 6.257233 | norm 0.5330 | time 463.7427 ms | tok/sec 1130557.8777
for step  285 | loss 6.186627 | norm 0.7216 | time 464.5381 ms | tok/sec 1128622.1797
for step  286 | loss 6.218555 | norm 0.7603 | time 463.7368 ms | tok/sec 1130572.4089
for step  287 | loss 6.196804 | norm 0.5781 | time 463.7237 ms | tok/sec 1130604.3788
for step  288 | loss 6.182058 | norm 0.5679 | time 463.7949 ms | tok/sec 1130430.6003
for step  289 | loss 6.170619 | norm 0.5712 | time 464.5023 ms | tok/sec 1128709.0741
for step  290 | loss 6.103304 | norm 0.5003 | time 465.2045 ms | tok/sec 1127005.4902
for step  291 | loss 6.123446 | norm 0.4762 | time 464.6497 ms | tok/sec 1128351.1549
for step  292 | loss 6.122077 | norm 0.5702 | time 464.2043 ms | tok/sec 1129433.7152
for step  293 | loss 6.114506 | norm 0.4213 | time 465.1744 ms | tok/sec 1127078.2717
for step  294 | loss 6.188867 | norm 0.4754 | time 464.6893 ms | tok/sec 1128255.0535
for step  295 | loss 6.137297 | norm 0.5087 | time 463.5491 ms | tok/sec 1131030.0420
for step  296 | loss 6.121917 | norm 0.5031 | time 463.7246 ms | tok/sec 1130602.0536
for step  297 | loss 6.141124 | norm 0.5036 | time 463.7494 ms | tok/sec 1130541.6032
for step  298 | loss 6.121287 | norm 0.4867 | time 464.5336 ms | tok/sec 1128633.1855
for step  299 | loss 6.111031 | norm 0.4502 | time 464.2444 ms | tok/sec 1129336.2693
for step  300 | loss 6.189828 | norm 0.4685 | time 464.6902 ms | tok/sec 1128252.7380
for step  301 | loss 6.046589 | norm 0.5162 | time 463.9552 ms | tok/sec 1130040.2296
for step  302 | loss 6.081809 | norm 0.5293 | time 464.3736 ms | tok/sec 1129022.0049
for step  303 | loss 6.067887 | norm 0.6199 | time 463.6638 ms | tok/sec 1130750.3010
for step  304 | loss 6.047647 | norm 0.6285 | time 464.0195 ms | tok/sec 1129883.4601
for step  305 | loss 6.024451 | norm 0.4543 | time 463.7158 ms | tok/sec 1130623.5616
for step  306 | loss 6.033318 | norm 0.5396 | time 463.9876 ms | tok/sec 1129961.2588
for step  307 | loss 6.027430 | norm 0.5658 | time 463.8457 ms | tok/sec 1130306.8376
for step  308 | loss 6.018701 | norm 0.4847 | time 464.5424 ms | tok/sec 1128611.7532
for step  309 | loss 6.008496 | norm 0.5152 | time 463.5818 ms | tok/sec 1130950.3511
for step  310 | loss 6.018878 | norm 0.6036 | time 463.8650 ms | tok/sec 1130259.7800
for step  311 | loss 6.002705 | norm 0.8415 | time 464.5607 ms | tok/sec 1128567.1534
for step  312 | loss 6.014290 | norm 0.9146 | time 463.8212 ms | tok/sec 1130366.6819
for step  313 | loss 5.977637 | norm 0.7547 | time 463.7914 ms | tok/sec 1130439.3171
for step  314 | loss 5.984613 | norm 0.6661 | time 464.3681 ms | tok/sec 1129035.3373
for step  315 | loss 5.977041 | norm 0.5370 | time 463.6364 ms | tok/sec 1130817.1703
for step  316 | loss 5.990161 | norm 0.5684 | time 464.8120 ms | tok/sec 1127957.0117
for step  317 | loss 5.946469 | norm 0.6042 | time 464.1678 ms | tok/sec 1129522.4752
for step  318 | loss 5.992980 | norm 0.5519 | time 464.5891 ms | tok/sec 1128498.2334
for step  319 | loss 5.961477 | norm 0.5183 | time 463.9711 ms | tok/sec 1130001.3235
for step  320 | loss 5.911337 | norm 0.5651 | time 464.8743 ms | tok/sec 1127806.0253
for step  321 | loss 5.959829 | norm 0.5620 | time 463.9177 ms | tok/sec 1130131.4081
for step  322 | loss 5.897139 | norm 0.6353 | time 465.4384 ms | tok/sec 1126439.1558
for step  323 | loss 5.941205 | norm 0.6373 | time 465.3132 ms | tok/sec 1126742.1691
for step  324 | loss 6.066360 | norm 0.6630 | time 465.2755 ms | tok/sec 1126833.3937
for step  325 | loss 6.114592 | norm 0.7216 | time 464.4415 ms | tok/sec 1128856.8254
for step  326 | loss 6.086110 | norm 0.6423 | time 465.8587 ms | tok/sec 1125422.7996
for step  327 | loss 6.084848 | norm 0.6094 | time 464.6859 ms | tok/sec 1128263.1578
for step  328 | loss 6.081922 | norm 0.6587 | time 463.8739 ms | tok/sec 1130238.2858
for step  329 | loss 6.096536 | norm 0.6730 | time 464.0577 ms | tok/sec 1129790.5802
for step  330 | loss 6.093918 | norm 0.6698 | time 464.6773 ms | tok/sec 1128283.9980
for step  331 | loss 6.044846 | norm 0.6504 | time 464.7217 ms | tok/sec 1128176.3320
for step  332 | loss 6.080281 | norm 0.6921 | time 464.7655 ms | tok/sec 1128069.8439
for step  333 | loss 6.040106 | norm 0.7184 | time 464.9498 ms | tok/sec 1127622.6978
for step  334 | loss 6.085616 | norm 0.6926 | time 463.7327 ms | tok/sec 1130582.2903
for step  335 | loss 6.061755 | norm 0.5050 | time 465.2371 ms | tok/sec 1126926.3655
for step  336 | loss 5.974963 | norm 0.5221 | time 464.4163 ms | tok/sec 1128918.2549
for step  337 | loss 6.033368 | norm 0.5557 | time 464.1116 ms | tok/sec 1129659.4134
for step  338 | loss 6.035413 | norm 0.4710 | time 463.9597 ms | tok/sec 1130029.1962
for step  339 | loss 5.977720 | norm 0.6182 | time 464.0987 ms | tok/sec 1129690.7514
for step  340 | loss 6.009215 | norm 0.6094 | time 464.0548 ms | tok/sec 1129797.5457
for step  341 | loss 6.030448 | norm 0.5282 | time 464.3204 ms | tok/sec 1129151.2844
for step  342 | loss 5.988671 | norm 0.4743 | time 463.5808 ms | tok/sec 1130952.6777
for step  343 | loss 5.927010 | norm 0.5744 | time 464.2348 ms | tok/sec 1129359.4692
for step  344 | loss 5.967910 | norm 0.6996 | time 463.9468 ms | tok/sec 1130060.5548
for step  345 | loss 5.957476 | norm 0.7843 | time 464.9220 ms | tok/sec 1127690.3544
for step  346 | loss 5.970905 | norm 0.8395 | time 464.3657 ms | tok/sec 1129041.1341
for step  347 | loss 5.937514 | norm 0.8782 | time 464.9999 ms | tok/sec 1127501.2834
for step  348 | loss 5.910486 | norm 1.0174 | time 465.3623 ms | tok/sec 1126623.2530
for step  349 | loss 5.928348 | norm 0.6950 | time 463.9103 ms | tok/sec 1130149.4133
for step  350 | loss 5.889718 | norm 0.7015 | time 464.7982 ms | tok/sec 1127990.5697
for step  351 | loss 5.927857 | norm 0.5942 | time 464.8879 ms | tok/sec 1127773.0567
for step  352 | loss 5.875683 | norm 0.6488 | time 464.7396 ms | tok/sec 1128132.9241
for step  353 | loss 5.906960 | norm 0.6794 | time 465.2283 ms | tok/sec 1126947.7338
for step  354 | loss 5.904477 | norm 0.6028 | time 465.3742 ms | tok/sec 1126594.3937
for step  355 | loss 5.889513 | norm 0.6269 | time 464.9386 ms | tok/sec 1127649.8751
for step  356 | loss 5.840195 | norm 0.6168 | time 465.4803 ms | tok/sec 1126337.6106
for step  357 | loss 5.848612 | norm 0.5928 | time 464.4511 ms | tok/sec 1128833.6462
for step  358 | loss 5.863503 | norm 0.6321 | time 464.7236 ms | tok/sec 1128171.7017
for step  359 | loss 5.808393 | norm 0.4329 | time 464.1891 ms | tok/sec 1129470.8419
for step  360 | loss 5.810799 | norm 0.5782 | time 464.6349 ms | tok/sec 1128387.0524
for step  361 | loss 5.848734 | norm 0.5849 | time 464.0303 ms | tok/sec 1129857.3361
for step  362 | loss 5.865206 | norm 0.7249 | time 464.1733 ms | tok/sec 1129509.1313
for step  363 | loss 5.817215 | norm 0.8931 | time 464.4742 ms | tok/sec 1128777.4405
for step  364 | loss 5.821966 | norm 0.8751 | time 465.0340 ms | tok/sec 1127418.6209
for step  365 | loss 5.780174 | norm 0.6884 | time 464.0663 ms | tok/sec 1129769.6843
for step  366 | loss 5.786468 | norm 0.9643 | time 464.1929 ms | tok/sec 1129461.5600
for step  367 | loss 5.795536 | norm 1.1064 | time 464.9785 ms | tok/sec 1127553.3149
for step  368 | loss 5.841320 | norm 0.7933 | time 464.9124 ms | tok/sec 1127713.4867
for step  369 | loss 5.774359 | norm 0.8185 | time 464.5164 ms | tok/sec 1128674.8940
for step  370 | loss 5.834625 | norm 0.8394 | time 464.6428 ms | tok/sec 1128367.9454
for step  371 | loss 5.893037 | norm 0.6199 | time 464.6893 ms | tok/sec 1128255.0535
for step  372 | loss 5.972772 | norm 0.6551 | time 464.6473 ms | tok/sec 1128356.9447
for step  373 | loss 5.893309 | norm 0.6238 | time 463.3567 ms | tok/sec 1131499.6898
for step  374 | loss 5.913662 | norm 0.6216 | time 464.4902 ms | tok/sec 1128738.6213
for step  375 | loss 5.904494 | norm 0.6819 | time 465.3490 ms | tok/sec 1126655.5773
for step  376 | loss 5.936634 | norm 0.8092 | time 463.2778 ms | tok/sec 1131692.4338
for step  377 | loss 5.939857 | norm 0.8965 | time 464.3269 ms | tok/sec 1129135.6302
for step  378 | loss 5.927316 | norm 1.1926 | time 464.0031 ms | tok/sec 1129923.5193
for step  379 | loss 5.880435 | norm 0.8396 | time 463.9745 ms | tok/sec 1129993.1942
Will loading at 0 from edu_fineweb10B/edufineweb_train_000003.npy
for step  380 | loss 5.885306 | norm 0.7295 | time 1462.8184 ms | tok/sec 358409.4961
for step  381 | loss 5.893557 | norm 0.6977 | time 463.8267 ms | tok/sec 1130353.3181
for step  382 | loss 5.812427 | norm 0.6055 | time 462.8956 ms | tok/sec 1132626.8040
for step  383 | loss 5.839873 | norm 0.6743 | time 462.4777 ms | tok/sec 1133650.3752
for step  384 | loss 5.821883 | norm 0.6054 | time 462.8978 ms | tok/sec 1132621.5537
for step  385 | loss 5.873180 | norm 0.6874 | time 463.5115 ms | tok/sec 1131121.9621
for step  386 | loss 5.963955 | norm 0.6963 | time 464.5998 ms | tok/sec 1128472.1734
for step  387 | loss 5.803597 | norm 0.9828 | time 463.7806 ms | tok/sec 1130465.4680
for step  388 | loss 5.846338 | norm 1.0939 | time 463.6526 ms | tok/sec 1130777.6292
for step  389 | loss 5.824097 | norm 0.8008 | time 465.0559 ms | tok/sec 1127365.4458
for step  390 | loss 5.846420 | norm 0.9010 | time 464.1290 ms | tok/sec 1129617.0518
for step  391 | loss 5.794696 | norm 0.8954 | time 463.8243 ms | tok/sec 1130359.1284
for step  392 | loss 5.818968 | norm 0.7981 | time 463.9349 ms | tok/sec 1130089.5920
for step  393 | loss 5.790665 | norm 0.7691 | time 465.5838 ms | tok/sec 1126087.2878
for step  394 | loss 5.735588 | norm 0.9134 | time 465.8537 ms | tok/sec 1125434.8952
for step  395 | loss 5.705197 | norm 0.6794 | time 464.4148 ms | tok/sec 1128921.7323
for step  396 | loss 5.749609 | norm 0.5639 | time 463.9480 ms | tok/sec 1130057.6511
for step  397 | loss 5.735491 | norm 0.6654 | time 464.8941 ms | tok/sec 1127758.0190
for step  398 | loss 5.723364 | norm 0.7164 | time 464.3726 ms | tok/sec 1129024.3236
for step  399 | loss 5.762628 | norm 0.8944 | time 464.6957 ms | tok/sec 1128239.4241
for step  400 | loss 5.720734 | norm 0.9815 | time 464.0155 ms | tok/sec 1129893.3295
for step  401 | loss 5.745438 | norm 0.9200 | time 465.4989 ms | tok/sec 1126292.6135
for step  402 | loss 5.692460 | norm 0.8545 | time 465.2290 ms | tok/sec 1126946.0012
for step  403 | loss 5.742793 | norm 0.7915 | time 464.5412 ms | tok/sec 1128614.6494
for step  404 | loss 5.688481 | norm 0.7895 | time 464.4077 ms | tok/sec 1128939.1193
for step  405 | loss 5.666307 | norm 0.8395 | time 464.4067 ms | tok/sec 1128941.4376
for step  406 | loss 5.623750 | norm 0.9170 | time 464.0634 ms | tok/sec 1129776.6496
for step  407 | loss 5.643523 | norm 0.8131 | time 464.3159 ms | tok/sec 1129162.3006
for step  408 | loss 5.629282 | norm 0.5754 | time 464.4029 ms | tok/sec 1128950.7110
for step  409 | loss 5.640672 | norm 0.6460 | time 464.8573 ms | tok/sec 1127847.0942
for step  410 | loss 5.655551 | norm 0.6969 | time 463.8660 ms | tok/sec 1130257.4562
for step  411 | loss 5.669515 | norm 0.8490 | time 464.8256 ms | tok/sec 1127924.0342
for step  412 | loss 5.659912 | norm 0.9739 | time 465.5049 ms | tok/sec 1126278.1922
for step  413 | loss 5.627554 | norm 0.9560 | time 464.6425 ms | tok/sec 1128368.5244
for step  414 | loss 5.657913 | norm 0.7964 | time 464.6420 ms | tok/sec 1128369.6824
for step  415 | loss 5.628488 | norm 0.6790 | time 464.0205 ms | tok/sec 1129881.1379
for step  416 | loss 5.768380 | norm 0.7931 | time 464.1652 ms | tok/sec 1129528.8572
for step  417 | loss 5.808861 | norm 0.9741 | time 463.7656 ms | tok/sec 1130502.0813
for step  418 | loss 5.814274 | norm 0.9480 | time 465.0106 ms | tok/sec 1127475.2694
for step  419 | loss 5.782797 | norm 0.8789 | time 468.7350 ms | tok/sec 1118516.9078
for step  420 | loss 5.754945 | norm 0.8508 | time 464.9189 ms | tok/sec 1127697.8722
for step  421 | loss 5.808190 | norm 1.0143 | time 464.3323 ms | tok/sec 1129122.2954
for step  422 | loss 5.750206 | norm 0.8468 | time 464.9832 ms | tok/sec 1127541.7520
for step  423 | loss 5.831691 | norm 0.8083 | time 464.3421 ms | tok/sec 1129098.5255
for step  424 | loss 5.754277 | norm 0.8994 | time 465.2555 ms | tok/sec 1126881.8988
for step  425 | loss 5.751130 | norm 0.7609 | time 463.7561 ms | tok/sec 1130525.3291
for step  426 | loss 5.696746 | norm 0.6859 | time 464.8600 ms | tok/sec 1127840.7313
for step  427 | loss 5.698154 | norm 0.6943 | time 464.5698 ms | tok/sec 1128545.1444
for step  428 | loss 5.741891 | norm 0.8103 | time 464.9332 ms | tok/sec 1127663.1751
for step  429 | loss 5.738919 | norm 0.7465 | time 464.0398 ms | tok/sec 1129834.1158
for step  430 | loss 5.721849 | norm 0.8229 | time 464.8857 ms | tok/sec 1127778.2621
for step  431 | loss 5.738862 | norm 1.0088 | time 465.2495 ms | tok/sec 1126896.3356
for step  432 | loss 5.718828 | norm 0.9627 | time 464.9343 ms | tok/sec 1127660.2838
for step  433 | loss 5.742792 | norm 0.7494 | time 464.1693 ms | tok/sec 1129518.9941
for step  434 | loss 5.645250 | norm 0.7279 | time 464.3726 ms | tok/sec 1129024.3236
for step  435 | loss 5.672477 | norm 0.7834 | time 464.3559 ms | tok/sec 1129064.9016
for step  436 | loss 5.645200 | norm 0.7896 | time 464.0932 ms | tok/sec 1129704.0995
for step  437 | loss 5.797078 | norm 0.7905 | time 464.7355 ms | tok/sec 1128142.7630
for step  438 | loss 5.717689 | norm 1.1533 | time 464.6885 ms | tok/sec 1128256.7901
for step  439 | loss 5.665888 | norm 1.2387 | time 464.7615 ms | tok/sec 1128079.6817
for step  440 | loss 5.569685 | norm 0.9741 | time 465.0953 ms | tok/sec 1127270.0901
for step  441 | loss 5.617661 | norm 0.8840 | time 464.1438 ms | tok/sec 1129581.0760
for step  442 | loss 5.561273 | norm 0.7694 | time 465.3106 ms | tok/sec 1126748.5197
for step  443 | loss 5.588163 | norm 0.9161 | time 465.8377 ms | tok/sec 1125473.4874
for step  444 | loss 5.640440 | norm 0.9292 | time 465.7676 ms | tok/sec 1125642.8640
for step  445 | loss 5.595847 | norm 1.0825 | time 464.5870 ms | tok/sec 1128503.4455
for step  446 | loss 5.619042 | norm 1.1046 | time 464.8831 ms | tok/sec 1127784.6244
for step  447 | loss 5.637418 | norm 0.6900 | time 464.3052 ms | tok/sec 1129188.3925
for step  448 | loss 5.618959 | norm 0.7944 | time 464.8097 ms | tok/sec 1127962.7974
for step  449 | loss 5.573992 | norm 0.7947 | time 464.4549 ms | tok/sec 1128824.3747
for step  450 | loss 5.603776 | norm 0.8773 | time 464.6149 ms | tok/sec 1128435.6914
for step  451 | loss 5.573126 | norm 0.9862 | time 465.4851 ms | tok/sec 1126326.0726
for step  452 | loss 5.515367 | norm 1.0510 | time 464.9920 ms | tok/sec 1127520.3611
for step  453 | loss 5.534743 | norm 1.0608 | time 466.0201 ms | tok/sec 1125033.0014
for step  454 | loss 5.548867 | norm 0.9971 | time 465.1325 ms | tok/sec 1127179.9504
for step  455 | loss 5.487943 | norm 0.9438 | time 465.4226 ms | tok/sec 1126477.2399
for step  456 | loss 5.566837 | norm 1.0199 | time 464.4470 ms | tok/sec 1128843.4972
for step  457 | loss 5.513705 | norm 0.9579 | time 464.9041 ms | tok/sec 1127733.7282
for step  458 | loss 5.530168 | norm 0.8314 | time 464.7739 ms | tok/sec 1128049.5903
for step  459 | loss 5.504953 | norm 0.8458 | time 465.7941 ms | tok/sec 1125578.9097
for step  460 | loss 5.481582 | norm 0.7761 | time 465.0292 ms | tok/sec 1127430.1814
for step  461 | loss 5.469487 | norm 0.8256 | time 464.4306 ms | tok/sec 1128883.4827
for step  462 | loss 5.429395 | norm 0.7633 | time 464.2987 ms | tok/sec 1129204.0482
for step  463 | loss 5.571588 | norm 0.9605 | time 465.7443 ms | tok/sec 1125699.3342
for step  464 | loss 5.620037 | norm 1.0276 | time 464.8178 ms | tok/sec 1127943.1262
for step  465 | loss 5.597668 | norm 0.9839 | time 465.7848 ms | tok/sec 1125601.3793
for step  466 | loss 5.716361 | norm 1.1972 | time 464.5689 ms | tok/sec 1128547.4611
for step  467 | loss 5.632208 | norm 0.8951 | time 464.5960 ms | tok/sec 1128481.4390
for step  468 | loss 5.610770 | norm 1.0191 | time 465.6889 ms | tok/sec 1125833.0414
for step  469 | loss 5.621239 | norm 1.1339 | time 466.0673 ms | tok/sec 1124919.0494
for step  470 | loss 5.570395 | norm 0.7168 | time 465.0846 ms | tok/sec 1127296.0946
for step  471 | loss 5.580966 | norm 0.7221 | time 464.7176 ms | tok/sec 1128186.1716
for step  472 | loss 5.625314 | norm 0.7248 | time 464.9191 ms | tok/sec 1127697.2939
for step  473 | loss 5.616351 | norm 0.8044 | time 464.7999 ms | tok/sec 1127986.5195
for step  474 | loss 5.607786 | norm 0.8725 | time 464.6890 ms | tok/sec 1128255.6324
for step  475 | loss 5.562972 | norm 0.9329 | time 465.1101 ms | tok/sec 1127234.2636
for step  476 | loss 5.566261 | norm 1.1204 | time 465.0397 ms | tok/sec 1127404.7487
for step  477 | loss 5.570189 | norm 0.8693 | time 464.6928 ms | tok/sec 1128246.3705
for step  478 | loss 5.598345 | norm 0.9760 | time 465.2095 ms | tok/sec 1126993.3609
for step  479 | loss 5.569468 | norm 1.0294 | time 464.8128 ms | tok/sec 1127955.2760
for step  480 | loss 5.589324 | norm 1.0482 | time 465.4284 ms | tok/sec 1126463.3909
for step  481 | loss 5.579403 | norm 1.0475 | time 464.9603 ms | tok/sec 1127597.2564
for step  482 | loss 5.584507 | norm 1.0591 | time 464.9224 ms | tok/sec 1127689.1978
for step  483 | loss 5.554859 | norm 1.0143 | time 465.8945 ms | tok/sec 1125336.4104
for step  484 | loss 5.616838 | norm 1.1165 | time 464.4856 ms | tok/sec 1128749.6294
for step  485 | loss 5.519562 | norm 1.2153 | time 464.8139 ms | tok/sec 1127952.3831
for step  486 | loss 5.532810 | norm 0.9329 | time 465.1911 ms | tok/sec 1127037.8364
for step  487 | loss 5.535984 | norm 0.8539 | time 464.4716 ms | tok/sec 1128783.8140
for step  488 | loss 5.454280 | norm 0.8126 | time 464.4148 ms | tok/sec 1128921.7323
for step  489 | loss 5.471028 | norm 0.8554 | time 465.3134 ms | tok/sec 1126741.5918
for step  490 | loss 5.497395 | norm 0.9873 | time 465.4288 ms | tok/sec 1126462.2368
for step  491 | loss 5.480864 | norm 1.1297 | time 464.7346 ms | tok/sec 1128145.0780
for step  492 | loss 5.486576 | norm 0.9122 | time 465.7288 ms | tok/sec 1125736.7921
for step  493 | loss 5.462616 | norm 0.7692 | time 465.0838 ms | tok/sec 1127297.8283
for step  494 | loss 5.477936 | norm 0.8299 | time 464.3483 ms | tok/sec 1129083.4525
for step  495 | loss 5.431277 | norm 1.1553 | time 464.1373 ms | tok/sec 1129596.7426
for step  496 | loss 5.481432 | norm 1.0149 | time 464.3464 ms | tok/sec 1129088.0903
for step  497 | loss 5.443282 | norm 0.9575 | time 465.2002 ms | tok/sec 1127015.8870
for step  498 | loss 5.406737 | norm 0.9065 | time 464.9456 ms | tok/sec 1127633.1060
for step  499 | loss 5.383206 | norm 0.9659 | time 464.7453 ms | tok/sec 1128119.0343
validation loss 5.5380
HellaSwag accuracy: 2358/10042=0.2348
> Hello, I'm a language model, to help them to build the development! The student has shown above the lesson will be used. In the school, the
> Hello, I'm a language model, a student has come to be an interactive work.
One of reading is used in how the computer, the digital design
> Hello, I'm a language model, or have the data that are not for the data. This allows for this:
The study of these methods and methods
> Hello, I'm a language model, and this is very easy from a little “mealing in me that looks like things where they are even though they
> Hello, I'm a language model, I can get a story for a different time and just a great enough story.
If our ability to understand what I
> Hello, I'm a language model, from an individual model and it makes. We could discuss science, our data and science has a few other new process to
> Hello, I'm a language model, is from the story to bring the content and development of the work of the teacher
We were not know that, the
> > > Hello, I'm a language model, that I know that was there have some people who didn't.” It was a strong deal with a lot of
Hello, I'm a language model, and a simple group to find a great tool.<|endoftext|>|An example of the idea, they become the world, like
Hello, I'm a language model, and is now about what we can imagine. This works through. They like the student, what, we can look like
> > > > Hello, I'm a language model, and the data is the system to help how that they need to achieve many people to get enough and understand that are to>Hello, I'm a language model, says I have not to hear the reader. I’ll have, and to remember, I� at the text
Hello, I'm a language model, or how a language or the subject to and other things.
You need to help your students about how a variety of
Hello, I'm a language model, what uses in the text in your child (S) on the first time, when they will be asked, but what

 Hello, I'm a language model, and the language is in our academic experiences. The process is just like well done, which may be sure that you get> > > > > 
Hello, I'm a language model, with a major scale of a unique problem," says; you have any sort of the use of a different way to use
Hello, I'm a language model, but a complex of the number is one and that is the number of the most of the time. You want some people
Hello, I'm a language model,
In a third area, he used as a great interest in the history of the most of the United Kingdom has to
Hello, I'm a language model, but you have a not to change in its understanding with a specific world is to be a new part of science. He
Hello, I'm a language model, are written in the students. I felt that are written that I will still want to have it to go.
It
> > > > > > Hello, I'm a language model, and how the language is only as it is used to explain the context of the language.
- In the author

Hello, I'm a language model, I have been an educational, as writing at the content of information on learning skills and reading and language activities.
What
Hello, I'm a language model, and as an argument of a group of a particular language.
However, I was not very a test and therefore for
Hello, I'm a language model, how it's not, not so-pold, what in a matter of it was no argument of the text at
Hello, I'm a language model, meaning can be the ability to the brain can perform the brain can detect a time you may have a few factors that are
Hello, I'm a language model, I have, writing or even reading to write up and writing writing a reading.
As I have a great essay can
> > > > Hello, I'm a language model, and a single point "word"" the three English", and "A" is, all" and you've
Hello, I'm a language model, is, what is not true.
But how can get on the next!
It wants if it to get you
Hello, I'm a language model, students will take the vocabulary. Students who are looking for the topic or what we will help you can use the students you
Hello, I'm a language model, or using a number of the data, the design of the text will help you through their needs. These are not be
> > Hello, I'm a language model, and "g."
But if the words is to make the other "the text to say why?" "in"
Hello, I'm a language model, he was not available to a team of a game in his community, and an online school, her home, she did
for step  500 | loss 5.434151 | norm 1.2071 | time 12931.7997 ms | tok/sec 40542.5396
for step  501 | loss 5.401653 | norm 0.9640 | time 462.4960 ms | tok/sec 1133605.3763
for step  502 | loss 5.379602 | norm 0.8591 | time 463.0408 ms | tok/sec 1132271.6432
for step  503 | loss 5.374514 | norm 1.0779 | time 463.1176 ms | tok/sec 1132083.9471
for step  504 | loss 5.403565 | norm 0.9044 | time 463.8770 ms | tok/sec 1130230.7340
for step  505 | loss 5.376233 | norm 0.8401 | time 463.5248 ms | tok/sec 1131089.3811
for step  506 | loss 5.492988 | norm 1.0283 | time 463.1577 ms | tok/sec 1131986.0434
for step  507 | loss 5.296862 | norm 1.1796 | time 463.7511 ms | tok/sec 1130537.5346
for step  508 | loss 5.344374 | norm 1.0240 | time 463.4395 ms | tok/sec 1131297.6992
for step  509 | loss 5.428097 | norm 0.9198 | time 463.3348 ms | tok/sec 1131553.2555
for step  510 | loss 5.535982 | norm 0.9936 | time 464.3672 ms | tok/sec 1129037.6560
for step  511 | loss 5.546434 | norm 1.5365 | time 463.1271 ms | tok/sec 1132060.6352
for step  512 | loss 5.530803 | norm 0.9951 | time 463.3160 ms | tok/sec 1131599.2563
for step  513 | loss 5.552298 | norm 1.0374 | time 463.9766 ms | tok/sec 1129987.9683
for step  514 | loss 5.511048 | norm 0.9951 | time 464.8387 ms | tok/sec 1127892.2157
for step  515 | loss 5.613492 | norm 1.2083 | time 463.6014 ms | tok/sec 1130902.6583
for step  516 | loss 5.570668 | norm 0.8408 | time 464.5836 ms | tok/sec 1128511.5534
for step  517 | loss 5.568630 | norm 0.9767 | time 465.1015 ms | tok/sec 1127255.0658
for step  518 | loss 5.488560 | norm 0.8494 | time 464.8261 ms | tok/sec 1127922.8771
for step  519 | loss 5.575275 | norm 0.9023 | time 464.4067 ms | tok/sec 1128941.4376
for step  520 | loss 5.507636 | norm 0.8420 | time 464.8204 ms | tok/sec 1127936.7621
for step  521 | loss 5.491961 | norm 1.1350 | time 465.3084 ms | tok/sec 1126753.7157
for step  522 | loss 5.492845 | norm 1.1362 | time 465.0772 ms | tok/sec 1127314.0095
for step  523 | loss 5.451293 | norm 0.8635 | time 465.0965 ms | tok/sec 1127267.2008
for step  524 | loss 5.421225 | norm 0.8846 | time 463.6810 ms | tok/sec 1130708.4390
for step  525 | loss 5.437120 | norm 0.7707 | time 465.7338 ms | tok/sec 1125724.6900
for step  526 | loss 5.475010 | norm 0.6791 | time 465.7135 ms | tok/sec 1125773.6761
for step  527 | loss 5.389856 | norm 0.6678 | time 465.4877 ms | tok/sec 1126319.7268
for step  528 | loss 5.446770 | norm 0.6689 | time 465.2324 ms | tok/sec 1126937.9158
for step  529 | loss 5.445029 | norm 0.7667 | time 464.6783 ms | tok/sec 1128281.6824
for step  530 | loss 5.417428 | norm 0.8663 | time 464.5815 ms | tok/sec 1128516.7657
for step  531 | loss 5.412098 | norm 1.2171 | time 465.6701 ms | tok/sec 1125878.5782
for step  532 | loss 5.450680 | norm 1.1566 | time 464.5171 ms | tok/sec 1128673.1561
for step  533 | loss 5.389348 | norm 1.0615 | time 465.1415 ms | tok/sec 1127157.9955
for step  534 | loss 5.412385 | norm 1.0815 | time 465.3511 ms | tok/sec 1126650.3822
for step  535 | loss 5.380272 | norm 0.8153 | time 465.7521 ms | tok/sec 1125680.3181
for step  536 | loss 5.428216 | norm 0.9430 | time 465.8945 ms | tok/sec 1125336.4104
for step  537 | loss 5.344193 | norm 0.9309 | time 465.6940 ms | tok/sec 1125820.9373
for step  538 | loss 5.382809 | norm 1.0857 | time 464.5541 ms | tok/sec 1128583.3711
for step  539 | loss 5.418059 | norm 1.1732 | time 465.0273 ms | tok/sec 1127434.8056
for step  540 | loss 5.315502 | norm 1.0531 | time 464.3848 ms | tok/sec 1128994.7615
for step  541 | loss 5.369132 | norm 1.1649 | time 463.8810 ms | tok/sec 1130220.8587
for step  542 | loss 5.377892 | norm 1.0099 | time 465.4288 ms | tok/sec 1126462.2368
for step  543 | loss 5.359859 | norm 0.9948 | time 464.8612 ms | tok/sec 1127837.8390
for step  544 | loss 5.348772 | norm 0.9250 | time 465.6460 ms | tok/sec 1125936.8015
for step  545 | loss 5.313184 | norm 1.0729 | time 464.6244 ms | tok/sec 1128412.5295
for step  546 | loss 5.283897 | norm 1.2515 | time 464.3250 ms | tok/sec 1129140.2684
for step  547 | loss 5.271226 | norm 1.0270 | time 464.2994 ms | tok/sec 1129202.3087
for step  548 | loss 5.295664 | norm 1.0934 | time 465.5869 ms | tok/sec 1126079.7913
for step  549 | loss 5.296807 | norm 1.0669 | time 465.5499 ms | tok/sec 1126169.1784
for step  550 | loss 5.336062 | norm 1.3440 | time 465.5859 ms | tok/sec 1126082.0979
for step  551 | loss 5.266260 | norm 0.8610 | time 465.5504 ms | tok/sec 1126168.0249
for step  552 | loss 5.284213 | norm 0.8782 | time 465.2264 ms | tok/sec 1126952.3541
for step  553 | loss 5.308373 | norm 0.8918 | time 465.2355 ms | tok/sec 1126930.4081
for step  554 | loss 5.247744 | norm 0.9913 | time 464.2270 ms | tok/sec 1129378.6098
for step  555 | loss 5.287808 | norm 1.0167 | time 464.7832 ms | tok/sec 1128027.0229
for step  556 | loss 5.284224 | norm 0.7697 | time 465.3234 ms | tok/sec 1126717.3448
for step  557 | loss 5.406377 | norm 0.7299 | time 465.3223 ms | tok/sec 1126720.2313
for step  558 | loss 5.438044 | norm 0.9109 | time 465.1017 ms | tok/sec 1127254.4880
for step  559 | loss 5.401793 | norm 1.0537 | time 465.4992 ms | tok/sec 1126292.0367
for step  560 | loss 5.418637 | norm 0.9227 | time 464.4094 ms | tok/sec 1128935.0623
for step  561 | loss 5.367811 | norm 1.0110 | time 466.0034 ms | tok/sec 1125073.2930
for step  562 | loss 5.387534 | norm 1.3756 | time 464.5314 ms | tok/sec 1128638.3989
for step  563 | loss 5.420102 | norm 1.0335 | time 464.6392 ms | tok/sec 1128376.6304
for step  564 | loss 5.406110 | norm 1.2983 | time 465.0152 ms | tok/sec 1127464.2861
for step  565 | loss 5.420253 | norm 0.9694 | time 465.0619 ms | tok/sec 1127350.9970
for step  566 | loss 5.408360 | norm 1.0953 | time 465.7574 ms | tok/sec 1125667.6411
for step  567 | loss 5.411816 | norm 1.0885 | time 465.0028 ms | tok/sec 1127494.3462
for step  568 | loss 5.374761 | norm 0.9727 | time 465.4636 ms | tok/sec 1126377.9957
for step  569 | loss 5.339840 | norm 0.8383 | time 465.7326 ms | tok/sec 1125727.5715
for step  570 | loss 5.327690 | norm 0.8501 | time 464.3278 ms | tok/sec 1129133.3111
Will loading at 0 from edu_fineweb10B/edufineweb_train_000004.npy
for step  571 | loss 5.308784 | norm 0.9868 | time 1428.5762 ms | tok/sec 367000.3663
for step  572 | loss 5.385638 | norm 1.1153 | time 464.0472 ms | tok/sec 1129816.1207
for step  573 | loss 5.312810 | norm 0.9057 | time 464.1147 ms | tok/sec 1129651.8693
for step  574 | loss 5.317148 | norm 0.9817 | time 465.3637 ms | tok/sec 1126619.7898
for step  575 | loss 5.339468 | norm 0.9227 | time 463.8071 ms | tok/sec 1130400.9645
for step  576 | loss 5.314344 | norm 0.9315 | time 464.0269 ms | tok/sec 1129865.4635
for step  577 | loss 5.361734 | norm 1.0176 | time 464.3168 ms | tok/sec 1129159.9814
for step  578 | loss 5.297629 | norm 1.0619 | time 463.8910 ms | tok/sec 1130196.4617
for step  579 | loss 5.291570 | norm 0.9587 | time 464.6540 ms | tok/sec 1128340.7335
for step  580 | loss 5.281692 | norm 1.0281 | time 464.4723 ms | tok/sec 1128782.0758
for step  581 | loss 5.183166 | norm 0.9753 | time 464.5188 ms | tok/sec 1128669.1010
for step  582 | loss 5.202337 | norm 0.7837 | time 465.0645 ms | tok/sec 1127344.6396
for step  583 | loss 5.268685 | norm 0.9224 | time 464.7164 ms | tok/sec 1128189.0656
for step  584 | loss 5.239946 | norm 0.9615 | time 464.5140 ms | tok/sec 1128680.6871
for step  585 | loss 5.273841 | norm 1.0640 | time 464.9062 ms | tok/sec 1127728.5232
for step  586 | loss 5.240112 | norm 1.0765 | time 465.1518 ms | tok/sec 1127133.1528
for step  587 | loss 5.306155 | norm 1.0804 | time 464.7694 ms | tok/sec 1128060.5851
for step  588 | loss 5.297315 | norm 0.9484 | time 464.7200 ms | tok/sec 1128180.3836
for step  589 | loss 5.265199 | norm 1.0506 | time 465.7474 ms | tok/sec 1125691.8430
for step  590 | loss 5.320746 | norm 1.2189 | time 464.0877 ms | tok/sec 1129717.4480
for step  591 | loss 5.206732 | norm 0.9056 | time 464.1109 ms | tok/sec 1129661.1543
for step  592 | loss 5.119562 | norm 1.1485 | time 465.5588 ms | tok/sec 1126147.8396
for step  593 | loss 5.178814 | norm 0.9235 | time 464.5650 ms | tok/sec 1128556.7280
for step  594 | loss 5.214870 | norm 0.9444 | time 464.4012 ms | tok/sec 1128954.7681
for step  595 | loss 5.151079 | norm 0.9738 | time 464.2138 ms | tok/sec 1129410.5123
for step  596 | loss 5.154068 | norm 0.9237 | time 465.3490 ms | tok/sec 1126655.5773
for step  597 | loss 5.232106 | norm 1.0204 | time 465.8191 ms | tok/sec 1125518.4191
for step  598 | loss 5.156039 | norm 0.9738 | time 466.2178 ms | tok/sec 1124556.0534
for step  599 | loss 5.158587 | norm 0.8670 | time 465.7900 ms | tok/sec 1125588.7041
for step  600 | loss 5.115198 | norm 0.9160 | time 465.3480 ms | tok/sec 1126657.8862
for step  601 | loss 5.123560 | norm 1.1863 | time 464.8886 ms | tok/sec 1127771.3216
for step  602 | loss 5.177827 | norm 1.1151 | time 464.6385 ms | tok/sec 1128378.3674
for step  603 | loss 5.282497 | norm 1.2119 | time 465.3144 ms | tok/sec 1126739.2825
for step  604 | loss 5.333352 | norm 0.8727 | time 464.6604 ms | tok/sec 1128325.1017
for step  605 | loss 5.281305 | norm 1.0406 | time 465.4641 ms | tok/sec 1126376.8418
for step  606 | loss 5.312658 | norm 1.0226 | time 464.6387 ms | tok/sec 1128377.7884
for step  607 | loss 5.278399 | norm 1.1103 | time 465.4820 ms | tok/sec 1126333.5723
for step  608 | loss 5.287980 | norm 0.9104 | time 465.7874 ms | tok/sec 1125595.0417
for step  609 | loss 5.276444 | norm 0.9210 | time 464.8867 ms | tok/sec 1127775.9486
for step  610 | loss 5.281206 | norm 0.8855 | time 465.6663 ms | tok/sec 1125887.8013
for step  611 | loss 5.269633 | norm 0.8044 | time 465.4038 ms | tok/sec 1126522.8289
for step  612 | loss 5.269107 | norm 0.9307 | time 465.6150 ms | tok/sec 1126011.7513
for step  613 | loss 5.275826 | norm 0.9787 | time 465.1556 ms | tok/sec 1127123.9093
for step  614 | loss 5.290558 | norm 1.0590 | time 464.7255 ms | tok/sec 1128167.0714
for step  615 | loss 5.238272 | norm 1.0857 | time 464.8192 ms | tok/sec 1127939.6549
for step  616 | loss 5.245005 | norm 0.9513 | time 464.7593 ms | tok/sec 1128084.8900
for step  617 | loss 5.209085 | norm 1.0740 | time 464.7224 ms | tok/sec 1128174.5956
for step  618 | loss 5.240770 | norm 0.8376 | time 464.5240 ms | tok/sec 1128656.3565
for step  619 | loss 5.242898 | norm 0.7982 | time 464.6461 ms | tok/sec 1128359.8396
for step  620 | loss 5.203089 | norm 0.7595 | time 464.4361 ms | tok/sec 1128870.1539
for step  621 | loss 5.227832 | norm 0.7454 | time 465.1196 ms | tok/sec 1127211.1510
for step  622 | loss 5.239853 | norm 0.7771 | time 464.5402 ms | tok/sec 1128616.9664
for step  623 | loss 5.205077 | norm 0.9552 | time 464.7903 ms | tok/sec 1128009.6639
for step  624 | loss 5.219977 | norm 1.0204 | time 465.4059 ms | tok/sec 1126517.6350
for step  625 | loss 5.242837 | norm 0.8339 | time 464.5703 ms | tok/sec 1128543.9861
for step  626 | loss 5.180379 | norm 0.8037 | time 464.7844 ms | tok/sec 1128024.1297
for step  627 | loss 5.176395 | norm 0.7458 | time 464.4210 ms | tok/sec 1128906.6639
for step  628 | loss 5.101206 | norm 0.7546 | time 465.4524 ms | tok/sec 1126405.1131
for step  629 | loss 5.179858 | norm 0.8288 | time 464.3536 ms | tok/sec 1129070.6987
for step  630 | loss 5.136239 | norm 0.8182 | time 464.7398 ms | tok/sec 1128132.3454
for step  631 | loss 5.139631 | norm 0.7429 | time 464.6580 ms | tok/sec 1128330.8912
for step  632 | loss 5.174950 | norm 0.9881 | time 465.3091 ms | tok/sec 1126751.9837
for step  633 | loss 5.158055 | norm 1.1224 | time 464.6754 ms | tok/sec 1128288.6293
for step  634 | loss 5.204885 | norm 1.0859 | time 464.9725 ms | tok/sec 1127567.7690
for step  635 | loss 5.197749 | norm 1.1672 | time 464.8538 ms | tok/sec 1127855.7712
for step  636 | loss 5.270847 | norm 1.0170 | time 465.5201 ms | tok/sec 1126241.2751
for step  637 | loss 5.387268 | norm 0.9449 | time 464.8092 ms | tok/sec 1127963.9545
for step  638 | loss 5.114412 | norm 1.1194 | time 464.1545 ms | tok/sec 1129554.9660
for step  639 | loss 5.055880 | norm 1.3566 | time 465.9567 ms | tok/sec 1125186.1248
for step  640 | loss 5.027435 | norm 0.9790 | time 464.5429 ms | tok/sec 1128610.5947
for step  641 | loss 5.119984 | norm 1.0060 | time 464.7465 ms | tok/sec 1128116.1406
for step  642 | loss 5.106931 | norm 0.9318 | time 464.3946 ms | tok/sec 1128970.9970
for step  643 | loss 5.068551 | norm 0.8460 | time 465.3499 ms | tok/sec 1126653.2683
for step  644 | loss 5.057246 | norm 0.8752 | time 465.0044 ms | tok/sec 1127490.2996
for step  645 | loss 5.040565 | norm 0.9409 | time 464.8504 ms | tok/sec 1127863.8697
for step  646 | loss 5.060211 | norm 1.0539 | time 464.6621 ms | tok/sec 1128321.0491
for step  647 | loss 5.106411 | norm 1.1664 | time 464.6235 ms | tok/sec 1128414.8456
for step  648 | loss 5.094531 | norm 1.2599 | time 465.3780 ms | tok/sec 1126585.1590
for step  649 | loss 5.058097 | norm 0.9181 | time 464.6375 ms | tok/sec 1128380.6834
for step  650 | loss 5.226881 | norm 0.9718 | time 464.2091 ms | tok/sec 1129422.1136
for step  651 | loss 5.283247 | norm 0.9936 | time 465.3425 ms | tok/sec 1126671.1628
for step  652 | loss 5.215664 | norm 1.1002 | time 464.8602 ms | tok/sec 1127840.1528
for step  653 | loss 5.239363 | norm 1.3138 | time 465.1513 ms | tok/sec 1127134.3083
for step  654 | loss 5.222864 | norm 0.8808 | time 465.4717 ms | tok/sec 1126358.3798
for step  655 | loss 5.234445 | norm 0.8337 | time 464.7684 ms | tok/sec 1128062.8998
for step  656 | loss 5.177444 | norm 0.7832 | time 464.3984 ms | tok/sec 1128961.7233
for step  657 | loss 5.179369 | norm 0.8271 | time 465.2429 ms | tok/sec 1126912.5053
for step  658 | loss 5.178108 | norm 0.8947 | time 465.3525 ms | tok/sec 1126646.9188
for step  659 | loss 5.230695 | norm 0.9047 | time 464.5238 ms | tok/sec 1128656.9358
for step  660 | loss 5.178374 | norm 0.9668 | time 464.9656 ms | tok/sec 1127584.5362
for step  661 | loss 5.128455 | norm 1.0857 | time 464.4384 ms | tok/sec 1128864.3588
for step  662 | loss 5.121195 | norm 0.8398 | time 464.5219 ms | tok/sec 1128661.5701
for step  663 | loss 5.122857 | norm 0.6947 | time 464.5815 ms | tok/sec 1128516.7657
for step  664 | loss 5.101838 | norm 0.8026 | time 464.8771 ms | tok/sec 1127799.0844
for step  665 | loss 5.128857 | norm 0.8567 | time 464.7229 ms | tok/sec 1128173.4381
for step  666 | loss 5.103440 | norm 0.7929 | time 465.6186 ms | tok/sec 1126003.1028
for step  667 | loss 5.091312 | norm 0.9189 | time 464.7632 ms | tok/sec 1128075.6308
for step  668 | loss 5.122273 | norm 0.9324 | time 464.4191 ms | tok/sec 1128911.3003
for step  669 | loss 5.056722 | norm 0.8670 | time 464.8190 ms | tok/sec 1127940.2334
for step  670 | loss 5.083809 | norm 0.8871 | time 464.9882 ms | tok/sec 1127529.6111
for step  671 | loss 5.115319 | norm 0.9901 | time 465.3921 ms | tok/sec 1126551.1075
for step  672 | loss 5.104724 | norm 1.0837 | time 464.4079 ms | tok/sec 1128938.5398
for step  673 | loss 5.052029 | norm 0.8824 | time 464.7610 ms | tok/sec 1128080.8391
for step  674 | loss 5.059587 | norm 0.8730 | time 465.6539 ms | tok/sec 1125917.7774
for step  675 | loss 5.084633 | norm 0.8247 | time 464.3595 ms | tok/sec 1129056.2060
for step  676 | loss 5.085944 | norm 0.9753 | time 464.9286 ms | tok/sec 1127674.1623
for step  677 | loss 5.124679 | norm 1.0274 | time 464.6621 ms | tok/sec 1128321.0491
for step  678 | loss 5.033868 | norm 1.1603 | time 464.8328 ms | tok/sec 1127906.6784
for step  679 | loss 5.083565 | norm 1.0534 | time 464.9746 ms | tok/sec 1127562.5655
for step  680 | loss 5.059349 | norm 1.2186 | time 464.9661 ms | tok/sec 1127583.3798
for step  681 | loss 5.098546 | norm 0.7891 | time 465.0493 ms | tok/sec 1127381.6290
for step  682 | loss 5.045155 | norm 0.8083 | time 465.7452 ms | tok/sec 1125697.0292
for step  683 | loss 5.074467 | norm 0.7136 | time 464.8194 ms | tok/sec 1127939.0763
for step  684 | loss 4.960471 | norm 0.6811 | time 464.5135 ms | tok/sec 1128681.8457
for step  685 | loss 4.964720 | norm 0.7941 | time 465.3058 ms | tok/sec 1126760.0665
for step  686 | loss 4.990069 | norm 0.8335 | time 464.5498 ms | tok/sec 1128593.7970
for step  687 | loss 4.989651 | norm 0.9211 | time 465.4608 ms | tok/sec 1126384.9192
for step  688 | loss 4.975378 | norm 1.0358 | time 465.2324 ms | tok/sec 1126937.9158
for step  689 | loss 4.968098 | norm 0.9547 | time 465.9882 ms | tok/sec 1125110.1335
for step  690 | loss 4.994365 | norm 0.7483 | time 464.6332 ms | tok/sec 1128391.1055
for step  691 | loss 4.971974 | norm 0.8403 | time 465.3025 ms | tok/sec 1126768.1493
for step  692 | loss 4.957764 | norm 0.8415 | time 465.1837 ms | tok/sec 1127055.7431
for step  693 | loss 5.004324 | norm 0.8846 | time 464.5326 ms | tok/sec 1128635.5026
for step  694 | loss 4.901548 | norm 0.9276 | time 465.0505 ms | tok/sec 1127378.7391
for step  695 | loss 4.998352 | norm 1.0450 | time 464.9835 ms | tok/sec 1127541.1738
for step  696 | loss 5.144253 | norm 1.0928 | time 465.6184 ms | tok/sec 1126003.6793
for step  697 | loss 5.246087 | norm 0.8454 | time 465.6968 ms | tok/sec 1125814.0207
for step  698 | loss 5.104729 | norm 1.0845 | time 466.5086 ms | tok/sec 1123854.8865
for step  699 | loss 5.185927 | norm 1.2914 | time 465.7459 ms | tok/sec 1125695.3005
for step  700 | loss 5.063110 | norm 0.8333 | time 466.0695 ms | tok/sec 1124913.8703
for step  701 | loss 5.086438 | norm 0.8136 | time 466.1353 ms | tok/sec 1124755.0681
for step  702 | loss 5.120879 | norm 0.8418 | time 464.4001 ms | tok/sec 1128957.6661
for step  703 | loss 5.125451 | norm 0.9576 | time 465.5914 ms | tok/sec 1126068.8352
for step  704 | loss 5.075089 | norm 0.9137 | time 466.2745 ms | tok/sec 1124419.1997
for step  705 | loss 5.077099 | norm 0.8971 | time 464.7174 ms | tok/sec 1128186.7504
for step  706 | loss 5.123211 | norm 0.7284 | time 465.4615 ms | tok/sec 1126383.1883
for step  707 | loss 5.106106 | norm 0.8731 | time 464.8504 ms | tok/sec 1127863.8697
for step  708 | loss 5.128109 | norm 1.2248 | time 465.5142 ms | tok/sec 1126255.6955
for step  709 | loss 5.072382 | norm 0.8377 | time 465.4200 ms | tok/sec 1126483.5876
for step  710 | loss 5.050979 | norm 0.7142 | time 464.9427 ms | tok/sec 1127640.0449
for step  711 | loss 5.038483 | norm 0.7156 | time 466.1634 ms | tok/sec 1124687.1881
for step  712 | loss 5.028502 | norm 0.7391 | time 465.3833 ms | tok/sec 1126572.4616
for step  713 | loss 4.996570 | norm 0.6841 | time 464.8354 ms | tok/sec 1127900.3147
for step  714 | loss 5.020905 | norm 0.6408 | time 464.8221 ms | tok/sec 1127932.7123
for step  715 | loss 5.014803 | norm 0.6683 | time 464.6602 ms | tok/sec 1128325.6807
for step  716 | loss 5.038298 | norm 0.7711 | time 466.0733 ms | tok/sec 1124904.6632
for step  717 | loss 5.062820 | norm 0.9077 | time 465.1828 ms | tok/sec 1127058.0537
for step  718 | loss 5.093943 | norm 0.9717 | time 465.5974 ms | tok/sec 1126054.4195
for step  719 | loss 5.013466 | norm 0.8409 | time 464.9699 ms | tok/sec 1127574.1289
for step  720 | loss 4.958286 | norm 0.8400 | time 465.3108 ms | tok/sec 1126747.9424
for step  721 | loss 4.989509 | norm 0.9562 | time 465.1368 ms | tok/sec 1127169.5506
for step  722 | loss 4.935423 | norm 0.8827 | time 466.0070 ms | tok/sec 1125064.6588
for step  723 | loss 5.014297 | norm 0.7891 | time 465.0571 ms | tok/sec 1127362.5560
for step  724 | loss 4.976345 | norm 0.8253 | time 466.0492 ms | tok/sec 1124962.7858
for step  725 | loss 5.036160 | norm 0.8222 | time 465.8189 ms | tok/sec 1125518.9952
for step  726 | loss 4.976656 | norm 0.6988 | time 465.0755 ms | tok/sec 1127318.0549
for step  727 | loss 4.962523 | norm 0.8344 | time 464.9212 ms | tok/sec 1127692.0892
for step  728 | loss 4.951458 | norm 0.8603 | time 465.2960 ms | tok/sec 1126783.7380
for step  729 | loss 4.981849 | norm 0.8834 | time 464.7117 ms | tok/sec 1128200.6419
for step  730 | loss 4.991398 | norm 0.8453 | time 465.3277 ms | tok/sec 1126706.9535
for step  731 | loss 4.888481 | norm 0.8291 | time 465.4040 ms | tok/sec 1126522.2518
for step  732 | loss 4.913401 | norm 0.9489 | time 465.1051 ms | tok/sec 1127246.3982
for step  733 | loss 4.833429 | norm 0.8408 | time 465.1384 ms | tok/sec 1127165.5063
for step  734 | loss 4.844700 | norm 0.7191 | time 464.8764 ms | tok/sec 1127800.8196
for step  735 | loss 4.882405 | norm 0.7418 | time 464.6232 ms | tok/sec 1128415.4247
for step  736 | loss 4.861946 | norm 0.8150 | time 464.9539 ms | tok/sec 1127612.8681
for step  737 | loss 4.881670 | norm 0.7186 | time 465.5819 ms | tok/sec 1126091.9010
for step  738 | loss 4.836905 | norm 0.8660 | time 464.9732 ms | tok/sec 1127566.0345
for step  739 | loss 4.862370 | norm 1.2761 | time 465.6675 ms | tok/sec 1125884.9191
for step  740 | loss 4.874434 | norm 0.8714 | time 465.0946 ms | tok/sec 1127271.8237
for step  741 | loss 4.831211 | norm 0.8262 | time 466.1093 ms | tok/sec 1124817.7781
for step  742 | loss 4.819979 | norm 0.8573 | time 465.4133 ms | tok/sec 1126499.7454
for step  743 | loss 5.000387 | norm 0.9071 | time 466.4674 ms | tok/sec 1123954.2611
for step  744 | loss 4.987190 | norm 0.7310 | time 465.5027 ms | tok/sec 1126283.3838
for step  745 | loss 5.000640 | norm 0.8847 | time 465.5035 ms | tok/sec 1126281.6533
for step  746 | loss 5.059975 | norm 0.8957 | time 465.0979 ms | tok/sec 1127263.7336
for step  747 | loss 5.024595 | norm 0.8211 | time 465.7207 ms | tok/sec 1125756.3864
for step  748 | loss 4.971536 | norm 0.9287 | time 464.8025 ms | tok/sec 1127980.1549
for step  749 | loss 5.013759 | norm 0.7819 | time 466.1021 ms | tok/sec 1124835.0390
validation loss 4.9812
HellaSwag accuracy: 2421/10042=0.2411
> Hello, I'm a language model, to think that the teacher was using A.R.L., and I don't have, the teacher would be a
> Hello, I'm a language model, which means is quite useful.
It is a kind up to, but this in our learning with a language.

> Hello, I'm a language model, I write up.
I think I think I are the following: "I" is not, that we will take
> Hello, I'm a language model, and students are a way as a model but it is simple for their first years. “There is a difference between
> Hello, I'm a language model, but I'll say that.
"As we might be used to have done through some people can learn to think that
> Hello, I'm a language model, my student, 'teachers.' They can find these facts about the meaning and are words of the subject, just like
> Hello, I'm a language model, a structure of thinking that can be traced, then, was to, and then made them back on from the table,
> Hello, I'm a language model, and a person on a word of speech (iiseo,o).
I think "a" will not
> > Hello, I'm a language model, where they are being studied through a group, and how the team may have an integrated approach, and the program. As
Hello, I'm a language model, and I, I'm thinking to read something
This game, my lesson. I've already learned how I think it
> > > Hello, I'm a language model, says: "You can't have this, you can't know, because it's so that's not possible to think
Hello, I'm a language model, that a teacher is very easy to understand about the same way.
We are talking about a child, I make sure
Hello, I'm a language model, if this is a good language that requires a more significant student, that should be the first way that was not a certain
> > > > Hello, I'm a language model, and I'm going, so, how's a difference between those and our subjects?”
Mapping
We
Hello, I'm a language model, it is written on the same term
Brief question I think.I think it is that it works, the same
Hello, I'm a language model, I'll learn to help them develop them into an image of words as the reader.
I am all have no longer
> Hello, I'm a language model, but I see,’s going to get the way we have done.
“I’ve told
> > > > Hello, I'm a language model, and I don't have, and I mean, and I find the world just way in the coming to a new nation
> Hello, I'm a language model, I am a kind of a game that does a change.
We’ll be a model that will depend on
Hello, I'm a language model, and is not a very basic element, and that's important, is a matter of the number of the graph on its
Hello, I'm a language model, but I have no way to see here!
- I'd be familiar for myself in my first and I love,
Hello, I'm a language model, i like, and my students could use in my own words.
I chose a teacher named as though a number of
Hello, I'm a language model, that works with a language and the class, like the language, are often a new concept and it could be viewed here
> > > > Hello, I'm a language model, who didn't know how we do they understand the subject theory that can be used (it be a class of the case
Hello, I'm a language model, how would like. We could also explain that our own language and I think, if I would find what in a language
Hello, I'm a language model, and the concept of something, has been called I will be explained. I’m talking between a sentence (the
Hello, I'm a language model, I like you can see how to say why what a word comes from the letter or “we are, I'm
> > > Hello, I'm a language model, one of a program that is made over 4,000 in a world-based system, and I can use, to
Hello, I'm a language model, an interactive learning, making it in math, so that you can create something different, to learn more about the world in
Hello, I'm a language model, so did not, not I’d think of my students in my own case that’s a program will
> > Hello, I'm a language model, at a moment of how do you've created to help you really know!
A kind of change and how to happen
Hello, I'm a language model,
The theory, that there are a lot of people in the language. For all these things when we know that these
for step  750 | loss 5.067269 | norm 0.7340 | time 12821.0354 ms | tok/sec 40892.7972
for step  751 | loss 4.994474 | norm 0.6935 | time 461.6652 ms | tok/sec 1135645.5995
for step  752 | loss 5.047621 | norm 0.6709 | time 461.7007 ms | tok/sec 1135558.2201
for step  753 | loss 4.963504 | norm 0.7386 | time 463.1674 ms | tok/sec 1131962.1528
for step  754 | loss 4.997921 | norm 0.8232 | time 463.0537 ms | tok/sec 1132240.1619
for step  755 | loss 4.987967 | norm 0.7336 | time 462.7972 ms | tok/sec 1132867.7869
for step  756 | loss 4.964298 | norm 0.7833 | time 462.5874 ms | tok/sec 1133381.6036
for step  757 | loss 4.940604 | norm 0.8640 | time 462.7569 ms | tok/sec 1132966.4269
for step  758 | loss 4.979601 | norm 0.8655 | time 463.4728 ms | tok/sec 1131216.2249
for step  759 | loss 4.969603 | norm 0.8246 | time 463.0263 ms | tok/sec 1132307.2076
for step  760 | loss 4.970364 | norm 0.8965 | time 463.3844 ms | tok/sec 1131432.1575
Will loading at 0 from edu_fineweb10B/edufineweb_train_000005.npy
for step  761 | loss 5.020727 | norm 1.0584 | time 1419.1060 ms | tok/sec 369449.4968
for step  762 | loss 4.942607 | norm 0.7594 | time 462.8830 ms | tok/sec 1132657.7234
for step  763 | loss 4.961405 | norm 0.7753 | time 462.1284 ms | tok/sec 1134507.2043
for step  764 | loss 4.937250 | norm 0.8650 | time 463.0456 ms | tok/sec 1132259.9833
for step  765 | loss 4.944949 | norm 0.8268 | time 463.0802 ms | tok/sec 1132175.4558
for step  766 | loss 4.953738 | norm 0.8499 | time 463.6676 ms | tok/sec 1130740.9980
for step  767 | loss 4.910536 | norm 0.9392 | time 464.1502 ms | tok/sec 1129565.4099
for step  768 | loss 4.870074 | norm 0.8691 | time 464.6308 ms | tok/sec 1128396.8957
for step  769 | loss 4.936821 | norm 0.8416 | time 464.2437 ms | tok/sec 1129338.0093
for step  770 | loss 4.881192 | norm 0.7426 | time 463.6185 ms | tok/sec 1130860.7850
for step  771 | loss 4.900674 | norm 0.7104 | time 464.4907 ms | tok/sec 1128737.4625
for step  772 | loss 4.894957 | norm 0.7054 | time 464.4489 ms | tok/sec 1128838.8614
for step  773 | loss 4.931059 | norm 0.7033 | time 464.2746 ms | tok/sec 1129262.6160
for step  774 | loss 4.860174 | norm 0.6419 | time 464.8433 ms | tok/sec 1127881.2242
for step  775 | loss 4.853160 | norm 0.6732 | time 464.2220 ms | tok/sec 1129390.7905
for step  776 | loss 4.842696 | norm 0.6929 | time 464.8776 ms | tok/sec 1127797.9276
for step  777 | loss 4.787581 | norm 0.6399 | time 464.0958 ms | tok/sec 1129697.7156
for step  778 | loss 4.815489 | norm 0.6883 | time 465.1861 ms | tok/sec 1127049.9667
for step  779 | loss 4.797538 | norm 0.6773 | time 465.2565 ms | tok/sec 1126879.5889
for step  780 | loss 4.792382 | norm 0.6966 | time 465.5859 ms | tok/sec 1126082.0979
for step  781 | loss 4.718086 | norm 0.7637 | time 465.5983 ms | tok/sec 1126052.1131
for step  782 | loss 4.720000 | norm 0.8865 | time 464.7629 ms | tok/sec 1128076.2095
for step  783 | loss 4.766237 | norm 0.9314 | time 465.6596 ms | tok/sec 1125903.9421
for step  784 | loss 4.855312 | norm 0.9208 | time 465.6489 ms | tok/sec 1125929.8836
for step  785 | loss 4.752985 | norm 0.8273 | time 464.6451 ms | tok/sec 1128362.1556
for step  786 | loss 4.792275 | norm 0.9277 | time 464.9904 ms | tok/sec 1127524.4079
for step  787 | loss 4.786463 | norm 1.0196 | time 465.1577 ms | tok/sec 1127118.7099
for step  788 | loss 4.762827 | norm 1.0645 | time 465.0686 ms | tok/sec 1127334.8147
for step  789 | loss 4.920253 | norm 0.9788 | time 464.5433 ms | tok/sec 1128609.4363
for step  790 | loss 4.929600 | norm 0.9975 | time 465.3213 ms | tok/sec 1126722.5405
for step  791 | loss 4.936262 | norm 0.9069 | time 465.3194 ms | tok/sec 1126727.1589
for step  792 | loss 4.931167 | norm 0.7503 | time 465.0638 ms | tok/sec 1127346.3734
for step  793 | loss 4.911490 | norm 0.7433 | time 465.1833 ms | tok/sec 1127056.8984
for step  794 | loss 4.935152 | norm 0.9777 | time 465.4193 ms | tok/sec 1126485.3187
for step  795 | loss 4.959411 | norm 0.7294 | time 465.2209 ms | tok/sec 1126965.6377
for step  796 | loss 4.865069 | norm 0.7052 | time 465.4956 ms | tok/sec 1126300.6897
for step  797 | loss 4.882611 | norm 0.6938 | time 465.5416 ms | tok/sec 1126189.3645
for step  798 | loss 4.875295 | norm 0.6628 | time 464.8252 ms | tok/sec 1127925.1913
for step  799 | loss 4.929289 | norm 1.0177 | time 464.4551 ms | tok/sec 1128823.7953
for step  800 | loss 4.888350 | norm 1.2075 | time 464.6780 ms | tok/sec 1128282.2613
for step  801 | loss 4.907576 | norm 0.7220 | time 465.0347 ms | tok/sec 1127416.8869
for step  802 | loss 4.955690 | norm 0.8377 | time 464.3896 ms | tok/sec 1128983.1689
for step  803 | loss 4.887645 | norm 0.6843 | time 465.4856 ms | tok/sec 1126324.9188
for step  804 | loss 4.869053 | norm 0.6632 | time 463.8281 ms | tok/sec 1130349.8319
for step  805 | loss 4.875959 | norm 0.7215 | time 465.8136 ms | tok/sec 1125531.6689
for step  806 | loss 4.874961 | norm 0.8825 | time 464.0362 ms | tok/sec 1129842.8233
for step  807 | loss 4.897157 | norm 0.9152 | time 465.4043 ms | tok/sec 1126521.6747
for step  808 | loss 4.868633 | norm 0.8434 | time 464.6683 ms | tok/sec 1128305.9968
for step  809 | loss 4.884447 | norm 0.7634 | time 464.9594 ms | tok/sec 1127599.5692
for step  810 | loss 4.806630 | norm 0.7465 | time 464.3893 ms | tok/sec 1128983.7485
for step  811 | loss 4.831680 | norm 0.8359 | time 465.4262 ms | tok/sec 1126468.5842
for step  812 | loss 4.819817 | norm 0.7743 | time 464.6268 ms | tok/sec 1128406.7391
for step  813 | loss 4.805658 | norm 0.7417 | time 464.9777 ms | tok/sec 1127555.0494
for step  814 | loss 4.804703 | norm 0.8459 | time 464.0956 ms | tok/sec 1129698.2959
for step  815 | loss 4.774517 | norm 0.7324 | time 464.2520 ms | tok/sec 1129317.7101
for step  816 | loss 4.756579 | norm 0.7437 | time 465.5888 ms | tok/sec 1126075.1782
for step  817 | loss 4.788715 | norm 0.6967 | time 465.1515 ms | tok/sec 1127133.7305
for step  818 | loss 4.795762 | norm 0.6922 | time 465.3919 ms | tok/sec 1126551.6846
for step  819 | loss 4.750667 | norm 0.7421 | time 463.8581 ms | tok/sec 1130276.6273
for step  820 | loss 4.775848 | norm 0.7069 | time 464.9482 ms | tok/sec 1127626.7454
for step  821 | loss 4.827584 | norm 0.7058 | time 464.0253 ms | tok/sec 1129869.5272
for step  822 | loss 4.811051 | norm 0.7665 | time 464.6823 ms | tok/sec 1128271.8411
for step  823 | loss 4.657183 | norm 0.8722 | time 465.0037 ms | tok/sec 1127492.0338
for step  824 | loss 4.653848 | norm 0.9445 | time 465.3261 ms | tok/sec 1126710.9945
for step  825 | loss 4.713181 | norm 0.9628 | time 465.1513 ms | tok/sec 1127134.3083
for step  826 | loss 4.704853 | norm 0.8008 | time 464.9596 ms | tok/sec 1127598.9910
for step  827 | loss 4.706100 | norm 0.7799 | time 465.7140 ms | tok/sec 1125772.5234
for step  828 | loss 4.656658 | norm 0.6582 | time 465.6928 ms | tok/sec 1125823.8192
for step  829 | loss 4.649960 | norm 0.7640 | time 465.1403 ms | tok/sec 1127160.8843
for step  830 | loss 4.670703 | norm 0.8136 | time 464.3302 ms | tok/sec 1129127.5133
for step  831 | loss 4.591661 | norm 0.6206 | time 465.2178 ms | tok/sec 1126973.1459
for step  832 | loss 4.597493 | norm 0.7146 | time 464.4170 ms | tok/sec 1128916.5163
for step  833 | loss 4.634580 | norm 0.7128 | time 464.3114 ms | tok/sec 1129173.3171
for step  834 | loss 4.735685 | norm 0.7070 | time 466.1901 ms | tok/sec 1124622.7672
for step  835 | loss 4.813997 | norm 0.9057 | time 465.0576 ms | tok/sec 1127361.4001
for step  836 | loss 4.807452 | norm 1.0255 | time 464.7925 ms | tok/sec 1128004.4563
for step  837 | loss 4.844646 | norm 0.9465 | time 465.1890 ms | tok/sec 1127043.0350
for step  838 | loss 4.828721 | norm 0.8872 | time 465.4820 ms | tok/sec 1126333.5723
for step  839 | loss 4.809263 | norm 0.8618 | time 465.0853 ms | tok/sec 1127294.3609
for step  840 | loss 4.837827 | norm 0.7907 | time 465.7421 ms | tok/sec 1125704.5206
for step  841 | loss 4.828621 | norm 0.7769 | time 465.4288 ms | tok/sec 1126462.2368
for step  842 | loss 4.786841 | norm 0.7360 | time 465.7736 ms | tok/sec 1125628.4593
for step  843 | loss 4.860131 | norm 0.6524 | time 465.1406 ms | tok/sec 1127160.3065
for step  844 | loss 4.814902 | norm 0.6935 | time 465.3714 ms | tok/sec 1126601.3198
for step  845 | loss 4.821965 | norm 0.9059 | time 464.7923 ms | tok/sec 1128005.0350
for step  846 | loss 4.812073 | norm 1.1578 | time 464.8142 ms | tok/sec 1127951.8046
for step  847 | loss 4.812682 | norm 0.8529 | time 465.6119 ms | tok/sec 1126019.2469
for step  848 | loss 4.799375 | norm 0.7063 | time 465.1430 ms | tok/sec 1127154.5290
for step  849 | loss 4.785797 | norm 0.7050 | time 464.6766 ms | tok/sec 1128285.7347
for step  850 | loss 4.747334 | norm 0.6707 | time 465.7841 ms | tok/sec 1125603.1078
for step  851 | loss 4.794087 | norm 0.6950 | time 464.7017 ms | tok/sec 1128224.9528
for step  852 | loss 4.785215 | norm 0.6518 | time 465.0292 ms | tok/sec 1127430.1814
for step  853 | loss 4.736367 | norm 0.6255 | time 465.1961 ms | tok/sec 1127025.7063
for step  854 | loss 4.753039 | norm 0.6533 | time 465.9390 ms | tok/sec 1125228.7304
for step  855 | loss 4.765125 | norm 0.6942 | time 465.0657 ms | tok/sec 1127341.7499
for step  856 | loss 4.770710 | norm 0.6564 | time 464.9405 ms | tok/sec 1127645.2491
for step  857 | loss 4.694579 | norm 0.6805 | time 465.2569 ms | tok/sec 1126878.4340
for step  858 | loss 4.708968 | norm 0.7465 | time 465.6138 ms | tok/sec 1126014.6342
for step  859 | loss 4.686345 | norm 0.8625 | time 464.8764 ms | tok/sec 1127800.8196
for step  860 | loss 4.722354 | norm 1.1018 | time 465.1139 ms | tok/sec 1127225.0185
for step  861 | loss 4.708425 | norm 0.9420 | time 464.8960 ms | tok/sec 1127753.3921
for step  862 | loss 4.726035 | norm 0.9890 | time 464.8561 ms | tok/sec 1127849.9865
for step  863 | loss 4.667154 | norm 0.9042 | time 464.6137 ms | tok/sec 1128438.5867
for step  864 | loss 4.715120 | norm 0.8716 | time 464.7460 ms | tok/sec 1128117.2981
for step  865 | loss 4.665915 | norm 0.7886 | time 464.5815 ms | tok/sec 1128516.7657
for step  866 | loss 4.695903 | norm 0.7582 | time 464.1869 ms | tok/sec 1129476.0630
for step  867 | loss 4.634638 | norm 0.6856 | time 467.5448 ms | tok/sec 1121364.2129
for step  868 | loss 4.562981 | norm 0.5747 | time 465.5414 ms | tok/sec 1126189.9413
for step  869 | loss 4.598787 | norm 0.6011 | time 464.7558 ms | tok/sec 1128093.5705
for step  870 | loss 4.563445 | norm 0.5655 | time 465.1175 ms | tok/sec 1127216.3513
for step  871 | loss 4.560216 | norm 0.6319 | time 464.2081 ms | tok/sec 1129424.4339
for step  872 | loss 4.562840 | norm 0.5758 | time 465.1690 ms | tok/sec 1127091.5582
for step  873 | loss 4.557392 | norm 0.5796 | time 464.4601 ms | tok/sec 1128811.6268
for step  874 | loss 4.511171 | norm 0.6757 | time 464.9787 ms | tok/sec 1127552.7368
for step  875 | loss 4.497274 | norm 0.7973 | time 465.1029 ms | tok/sec 1127251.5987
for step  876 | loss 4.567904 | norm 0.7827 | time 464.6766 ms | tok/sec 1128285.7347
for step  877 | loss 4.580682 | norm 0.9103 | time 464.4620 ms | tok/sec 1128806.9912
for step  878 | loss 4.600519 | norm 1.0290 | time 464.7725 ms | tok/sec 1128053.0623
for step  879 | loss 4.544780 | norm 0.8734 | time 464.4234 ms | tok/sec 1128900.8685
for step  880 | loss 4.648204 | norm 0.8573 | time 465.5983 ms | tok/sec 1126052.1131
for step  881 | loss 4.776551 | norm 1.0180 | time 465.2717 ms | tok/sec 1126842.6324
for step  882 | loss 4.809627 | norm 0.9950 | time 465.1313 ms | tok/sec 1127182.8393
for step  883 | loss 4.750644 | norm 0.9182 | time 464.5646 ms | tok/sec 1128557.8864
for step  884 | loss 4.772451 | norm 0.8413 | time 465.4031 ms | tok/sec 1126524.5602
for step  885 | loss 4.696471 | norm 0.7199 | time 465.1163 ms | tok/sec 1127219.2403
for step  886 | loss 4.728056 | norm 0.7460 | time 465.3988 ms | tok/sec 1126534.9481
for step  887 | loss 4.734347 | norm 0.6711 | time 465.3587 ms | tok/sec 1126631.9111
for step  888 | loss 4.717591 | norm 0.7118 | time 464.9851 ms | tok/sec 1127537.1268
for step  889 | loss 4.702229 | norm 0.6037 | time 464.8342 ms | tok/sec 1127903.2073
for step  890 | loss 4.681347 | norm 0.5800 | time 465.5156 ms | tok/sec 1126252.2346
for step  891 | loss 4.722875 | norm 0.5987 | time 465.4930 ms | tok/sec 1126307.0353
for step  892 | loss 4.650052 | norm 0.6039 | time 465.2495 ms | tok/sec 1126896.3356
for step  893 | loss 4.673598 | norm 0.6860 | time 464.0884 ms | tok/sec 1129715.7069
for step  894 | loss 4.681246 | norm 0.8264 | time 465.2781 ms | tok/sec 1126827.0421
for step  895 | loss 4.684214 | norm 1.0255 | time 464.5040 ms | tok/sec 1128705.0187
for step  896 | loss 4.658440 | norm 0.9182 | time 465.8160 ms | tok/sec 1125525.9081
for step  897 | loss 4.666542 | norm 0.9088 | time 465.1351 ms | tok/sec 1127173.5950
for step  898 | loss 4.665930 | norm 0.7820 | time 464.7784 ms | tok/sec 1128038.5958
for step  899 | loss 4.645207 | norm 0.6988 | time 464.7610 ms | tok/sec 1128080.8391
for step  900 | loss 4.635839 | norm 0.5249 | time 465.4827 ms | tok/sec 1126331.8416
for step  901 | loss 4.666770 | norm 0.6072 | time 464.8495 ms | tok/sec 1127866.1836
for step  902 | loss 4.639773 | norm 0.6108 | time 466.1453 ms | tok/sec 1124730.9065
for step  903 | loss 4.556249 | norm 0.6338 | time 464.8354 ms | tok/sec 1127900.3147
for step  904 | loss 4.614267 | norm 0.6247 | time 464.7529 ms | tok/sec 1128100.5151
for step  905 | loss 4.584723 | norm 0.6507 | time 466.0456 ms | tok/sec 1124971.4184
for step  906 | loss 4.591604 | norm 0.6989 | time 464.8132 ms | tok/sec 1127954.1188
for step  907 | loss 4.577176 | norm 0.7036 | time 465.4727 ms | tok/sec 1126356.0720
for step  908 | loss 4.595381 | norm 0.8188 | time 465.2898 ms | tok/sec 1126798.7497
for step  909 | loss 4.638642 | norm 1.0656 | time 465.3552 ms | tok/sec 1126640.5694
for step  910 | loss 4.621723 | norm 1.0625 | time 465.5454 ms | tok/sec 1126180.1365
for step  911 | loss 4.649937 | norm 0.8120 | time 465.4169 ms | tok/sec 1126491.0894
for step  912 | loss 4.620495 | norm 0.8867 | time 464.3626 ms | tok/sec 1129048.6700
for step  913 | loss 4.633566 | norm 0.8794 | time 464.6547 ms | tok/sec 1128338.9966
for step  914 | loss 4.468343 | norm 0.8855 | time 465.2123 ms | tok/sec 1126986.4300
for step  915 | loss 4.518574 | norm 0.8789 | time 464.5138 ms | tok/sec 1128681.2664
for step  916 | loss 4.524986 | norm 0.8523 | time 464.8368 ms | tok/sec 1127896.8437
for step  917 | loss 4.500448 | norm 0.8729 | time 464.4387 ms | tok/sec 1128863.7793
for step  918 | loss 4.523320 | norm 0.9185 | time 465.1380 ms | tok/sec 1127166.6618
for step  919 | loss 4.441174 | norm 0.8154 | time 465.7094 ms | tok/sec 1125783.4738
for step  920 | loss 4.477315 | norm 0.7696 | time 464.8507 ms | tok/sec 1127863.2913
for step  921 | loss 4.495554 | norm 0.6553 | time 464.5574 ms | tok/sec 1128575.2622
for step  922 | loss 4.505131 | norm 0.6804 | time 465.0443 ms | tok/sec 1127393.7667
for step  923 | loss 4.486227 | norm 0.6315 | time 464.0548 ms | tok/sec 1129797.5457
for step  924 | loss 4.478304 | norm 0.7106 | time 465.4942 ms | tok/sec 1126304.1509
for step  925 | loss 4.484565 | norm 0.7336 | time 465.3759 ms | tok/sec 1126590.3535
for step  926 | loss 4.660111 | norm 0.6875 | time 464.5467 ms | tok/sec 1128601.3270
for step  927 | loss 4.686717 | norm 0.6241 | time 464.5507 ms | tok/sec 1128591.4801
for step  928 | loss 4.624793 | norm 0.6775 | time 465.1220 ms | tok/sec 1127205.3730
for step  929 | loss 4.574887 | norm 0.6417 | time 464.5619 ms | tok/sec 1128564.2575
for step  930 | loss 4.667629 | norm 0.6159 | time 464.5331 ms | tok/sec 1128634.3441
for step  931 | loss 4.621663 | norm 0.6703 | time 465.0452 ms | tok/sec 1127391.4548
for step  932 | loss 4.650936 | norm 0.7007 | time 465.9529 ms | tok/sec 1125195.3365
for step  933 | loss 4.637582 | norm 0.7827 | time 465.4036 ms | tok/sec 1126523.4060
for step  934 | loss 4.659886 | norm 0.7089 | time 464.9277 ms | tok/sec 1127676.4754
for step  935 | loss 4.677478 | norm 0.6352 | time 464.7875 ms | tok/sec 1128016.6074
for step  936 | loss 4.609256 | norm 0.6129 | time 465.2119 ms | tok/sec 1126987.5851
for step  937 | loss 4.584844 | norm 0.6497 | time 464.8712 ms | tok/sec 1127813.5448
for step  938 | loss 4.630152 | norm 0.6920 | time 464.5550 ms | tok/sec 1128581.0543
for step  939 | loss 4.618987 | norm 0.7632 | time 464.7791 ms | tok/sec 1128036.8599
for step  940 | loss 4.598654 | norm 0.7897 | time 465.2297 ms | tok/sec 1126944.2686
for step  941 | loss 4.646424 | norm 0.7761 | time 465.5297 ms | tok/sec 1126218.2031
for step  942 | loss 4.606922 | norm 0.7528 | time 465.2781 ms | tok/sec 1126827.0421
for step  943 | loss 4.597745 | norm 0.7481 | time 464.8895 ms | tok/sec 1127769.0080
for step  944 | loss 4.606918 | norm 0.8364 | time 464.7923 ms | tok/sec 1128005.0350
for step  945 | loss 4.579544 | norm 0.8020 | time 465.0569 ms | tok/sec 1127363.1340
for step  946 | loss 4.600683 | norm 0.6739 | time 465.8709 ms | tok/sec 1125393.4258
for step  947 | loss 4.593512 | norm 0.6752 | time 464.7627 ms | tok/sec 1128076.7882
for step  948 | loss 4.604490 | norm 0.6988 | time 465.0638 ms | tok/sec 1127346.3734
for step  949 | loss 4.564081 | norm 0.6949 | time 465.1060 ms | tok/sec 1127244.0868
for step  950 | loss 4.548945 | norm 0.5924 | time 466.1672 ms | tok/sec 1124677.9847
for step  951 | loss 4.469914 | norm 0.6163 | time 464.9980 ms | tok/sec 1127505.9082
Will loading at 0 from edu_fineweb10B/edufineweb_train_000006.npy
for step  952 | loss 4.489756 | norm 0.6261 | time 1398.2596 ms | tok/sec 374957.5437
for step  953 | loss 4.534465 | norm 0.6182 | time 464.7768 ms | tok/sec 1128042.6464
for step  954 | loss 4.521000 | norm 0.7032 | time 464.4608 ms | tok/sec 1128809.8884
for step  955 | loss 4.500078 | norm 0.6847 | time 465.8566 ms | tok/sec 1125427.9834
for step  956 | loss 4.492339 | norm 0.6750 | time 464.0043 ms | tok/sec 1129920.6164
for step  957 | loss 4.457654 | norm 0.7173 | time 464.9553 ms | tok/sec 1127609.3988
for step  958 | loss 4.517343 | norm 0.7555 | time 464.7658 ms | tok/sec 1128069.2653
for step  959 | loss 4.502610 | norm 0.6806 | time 465.0269 ms | tok/sec 1127435.9617
for step  960 | loss 4.451222 | norm 0.9022 | time 465.4751 ms | tok/sec 1126350.3028
for step  961 | loss 4.444617 | norm 1.0211 | time 465.0383 ms | tok/sec 1127408.2167
for step  962 | loss 4.421715 | norm 0.9219 | time 465.0123 ms | tok/sec 1127471.2229
for step  963 | loss 4.376683 | norm 0.8605 | time 465.6012 ms | tok/sec 1126045.1937
for step  964 | loss 4.469697 | norm 0.8111 | time 465.1408 ms | tok/sec 1127159.7288
for step  965 | loss 4.393399 | norm 0.7458 | time 464.2026 ms | tok/sec 1129437.7758
for step  966 | loss 4.395252 | norm 0.6928 | time 465.2889 ms | tok/sec 1126801.0592
for step  967 | loss 4.393418 | norm 0.6071 | time 464.3946 ms | tok/sec 1128970.9970
for step  968 | loss 4.429229 | norm 0.6379 | time 465.5833 ms | tok/sec 1126088.4411
for step  969 | loss 4.397297 | norm 0.6435 | time 465.0006 ms | tok/sec 1127499.5491
for step  970 | loss 4.420083 | norm 0.5820 | time 465.4346 ms | tok/sec 1126448.3881
for step  971 | loss 4.396342 | norm 0.5614 | time 464.5178 ms | tok/sec 1128671.4182
for step  972 | loss 4.531273 | norm 0.6070 | time 465.1515 ms | tok/sec 1127133.7305
for step  973 | loss 4.544633 | norm 0.6037 | time 465.6191 ms | tok/sec 1126001.9496
for step  974 | loss 4.515025 | norm 0.6303 | time 465.4772 ms | tok/sec 1126345.1105
for step  975 | loss 4.589486 | norm 0.6442 | time 466.7015 ms | tok/sec 1123390.4144
for step  976 | loss 4.547063 | norm 0.7247 | time 466.2478 ms | tok/sec 1124483.5973
for step  977 | loss 4.602518 | norm 0.6817 | time 465.6818 ms | tok/sec 1125850.3334
for step  978 | loss 4.585403 | norm 0.7585 | time 465.5147 ms | tok/sec 1126254.5419
for step  979 | loss 4.564665 | norm 0.7758 | time 465.0488 ms | tok/sec 1127382.7850
for step  980 | loss 4.554879 | norm 0.7308 | time 465.9765 ms | tok/sec 1125138.3412
for step  981 | loss 4.517170 | norm 0.7501 | time 464.7682 ms | tok/sec 1128063.4784
for step  982 | loss 4.561180 | norm 0.7327 | time 465.6332 ms | tok/sec 1125967.9333
for step  983 | loss 4.536194 | norm 0.7265 | time 465.2834 ms | tok/sec 1126814.3392
for step  984 | loss 4.495149 | norm 0.6987 | time 464.5550 ms | tok/sec 1128581.0543
for step  985 | loss 4.516481 | norm 0.5876 | time 465.1754 ms | tok/sec 1127075.9610
for step  986 | loss 4.519691 | norm 0.6377 | time 465.6301 ms | tok/sec 1125975.4283
for step  987 | loss 4.531587 | norm 0.6153 | time 464.5391 ms | tok/sec 1128619.8627
for step  988 | loss 4.510911 | norm 0.5451 | time 465.7118 ms | tok/sec 1125777.7104
for step  989 | loss 4.485589 | norm 0.5906 | time 465.5676 ms | tok/sec 1126126.5015
for step  990 | loss 4.544983 | norm 0.6114 | time 465.6971 ms | tok/sec 1125813.4444
for step  991 | loss 4.497151 | norm 0.5875 | time 465.5356 ms | tok/sec 1126203.7837
for step  992 | loss 4.487978 | norm 0.5233 | time 464.8910 ms | tok/sec 1127765.5378
for step  993 | loss 4.428766 | norm 0.5460 | time 464.8998 ms | tok/sec 1127744.1384
for step  994 | loss 4.496393 | norm 0.5036 | time 465.7793 ms | tok/sec 1125614.6311
for step  995 | loss 4.408245 | norm 0.5110 | time 465.3924 ms | tok/sec 1126550.5303
for step  996 | loss 4.453732 | norm 0.5727 | time 464.6661 ms | tok/sec 1128311.2072
for step  997 | loss 4.461398 | norm 0.6291 | time 465.0910 ms | tok/sec 1127280.4918
for step  998 | loss 4.437863 | norm 0.7062 | time 464.9293 ms | tok/sec 1127672.4275
for step  999 | loss 4.419228 | norm 0.8343 | time 465.3916 ms | tok/sec 1126552.2617
validation loss 4.5128
HellaSwag accuracy: 2515/10042=0.2504
> Hello, I'm a language model,
the “How to learn programming?” you write a “How to teach math?” I
> Hello, I'm a language model, a group that gives you a chance to find out and start with I think of or something else so I can't know
> Hello, I'm a language model, with those two types of programming languages, I'm using my code and the learning language. The I have a word processor
> Hello, I'm a language model, but she's very different at the moment
I would still give another, her name her language is a type of language
> Hello, I'm a language model, and I didn't have to do it on my side. I'm probably interested in making and think about that. I
> > > Hello, I'm a language model, making me think one of the ways I want to find these things you like, or, you have to be certain of
>>> Hello, I'm a language model, though they are working out how they know, they need to get those people, to be educated, and that they all
Hello, I'm a language model, when a person will be physically engaged in the language, and then the language is that they are able to talk about and
 Hello, I'm a language model, but I’ve already got you through these classes in the United States to get up with them and to get out Hello, I'm a language model, which’s called a ‘normal’, meaning it’s ‘, there’sHello, I'm a language model, I can use a wide range of language that is language, language, language, language, language, language, language,
> > 

> > Hello, I'm a language model, with your vocabulary, you'll find a combination of math, and it would be an interesting way to get it again.
> Hello, I'm a language model, not a “My students learn more words, yet I’m not that I think that, when I would
> > Hello, I'm a language model, but I'm not so. I feel so.
This looks like people! It looks like us so that the first
Hello, I'm a language model, in its simplest form of music: it’s probably just there, as they can’t use the same
> Hello, I'm a language model, but it may be difficult for me to test all of the test. He uses in our language or mathematics to be learned
> Hello, I'm a language model, my dad asked me.
- There was an alternative pattern that you liked it. It was the way you should get
Hello, I'm a language model, but you all agree about how you could help the company: And then not let me know how my business is doing it
> > Hello, I'm a language model, I don't have to say. So the second thing I've used the most of the things I got that we have
> Hello, I'm a language model, and I think I think it was my, and I think I have found myself that I did I just have discovered a
> Hello, I'm a language model, but not it is a nonnative bird. But in a few words, I'm not sure. I'm the best
Hello, I'm a language model, ‘, I suppose, but so I know that I want to teach math skills when I first need to do,
> Hello, I'm a language model, how with your language and how your ability to write vocabulary while writing it.
To begin this, my teacher is working
Hello, I'm a language model, and I just have never made it all. That is, a great, and most important part of the language is always
> > > Hello, I'm a language model, one of the ways in the school. I have a lot of writing skills to be creative even if they need knowledge that
> > Hello, I'm a language model, I would create a single-language model at the world and, I would go with, what should I? I need
Hello, I'm a language model, but it doesn't hurt me by the big I got, you're not the kind of people getting a big chunk of
Hello, I'm a language model, however, so the model for a word is not that you can not be creative if they can give a look for an
Hello, I'm a language model, for example. She started her new teacher learning her time on her home because she was very interested in her school with his
Hello, I'm a language model, with me, that I’m a favorite language, this has never seen (it in my heart), that a
> > Hello, I'm a language model, but think I don't have some data, because you’ll be able to understand something with learning, you can
Hello, I'm a language model, of which are the same kind of cell or another cell. I have always been in the language language in the language so
for step 1000 | loss 4.536346 | norm 0.9051 | time 12861.6529 ms | tok/sec 40763.6566
for step 1001 | loss 4.454690 | norm 1.0595 | time 461.9346 ms | tok/sec 1134983.2596
for step 1002 | loss 4.546053 | norm 0.9463 | time 464.4728 ms | tok/sec 1128780.9170
for step 1003 | loss 4.505862 | norm 0.7456 | time 462.7254 ms | tok/sec 1133043.4831
for step 1004 | loss 4.464476 | norm 0.7561 | time 463.6614 ms | tok/sec 1130756.1154
for step 1005 | loss 4.434515 | norm 0.7632 | time 463.4569 ms | tok/sec 1131255.2147
for step 1006 | loss 4.381557 | norm 0.7872 | time 463.1393 ms | tok/sec 1132030.9138
for step 1007 | loss 4.367210 | norm 0.7949 | time 463.9597 ms | tok/sec 1130029.1962
for step 1008 | loss 4.307296 | norm 0.6017 | time 463.7914 ms | tok/sec 1130439.3171
for step 1009 | loss 4.362360 | norm 0.5877 | time 463.6252 ms | tok/sec 1130844.5018
for step 1010 | loss 4.378595 | norm 0.6040 | time 464.1092 ms | tok/sec 1129665.2166
for step 1011 | loss 4.330617 | norm 0.6543 | time 466.9807 ms | tok/sec 1122718.7865
for step 1012 | loss 4.366567 | norm 0.6940 | time 463.1743 ms | tok/sec 1131945.2552
for step 1013 | loss 4.326027 | norm 0.6000 | time 464.7338 ms | tok/sec 1128146.8143
for step 1014 | loss 4.313543 | norm 0.5566 | time 464.7298 ms | tok/sec 1128156.6534
for step 1015 | loss 4.318483 | norm 0.5641 | time 464.3488 ms | tok/sec 1129082.2930
for step 1016 | loss 4.351992 | norm 0.5622 | time 464.4990 ms | tok/sec 1128717.1849
for step 1017 | loss 4.359002 | norm 0.6047 | time 464.7236 ms | tok/sec 1128171.7017
for step 1018 | loss 4.528207 | norm 0.6264 | time 465.3490 ms | tok/sec 1126655.5773
for step 1019 | loss 4.543532 | norm 0.6462 | time 464.6969 ms | tok/sec 1128236.5298
for step 1020 | loss 4.469558 | norm 0.6848 | time 465.3375 ms | tok/sec 1126683.2852
for step 1021 | loss 4.510766 | norm 0.6688 | time 464.6418 ms | tok/sec 1128370.2614
for step 1022 | loss 4.521847 | norm 0.6166 | time 464.4561 ms | tok/sec 1128821.4774
for step 1023 | loss 4.485839 | norm 0.5679 | time 465.4849 ms | tok/sec 1126326.6495
for step 1024 | loss 4.524048 | norm 0.6999 | time 465.4119 ms | tok/sec 1126503.2079
for step 1025 | loss 4.497201 | norm 0.7143 | time 464.6804 ms | tok/sec 1128276.4723
for step 1026 | loss 4.529036 | norm 0.7199 | time 465.4198 ms | tok/sec 1126484.1646
for step 1027 | loss 4.491915 | norm 0.6620 | time 466.0039 ms | tok/sec 1125072.1418
for step 1028 | loss 4.510862 | norm 0.6578 | time 464.6449 ms | tok/sec 1128362.7345
for step 1029 | loss 4.489874 | norm 0.5930 | time 464.5283 ms | tok/sec 1128645.9295
for step 1030 | loss 4.442180 | norm 0.6083 | time 464.6473 ms | tok/sec 1128356.9447
for step 1031 | loss 4.481739 | norm 0.6492 | time 464.8049 ms | tok/sec 1127974.3690
for step 1032 | loss 4.509211 | norm 0.7300 | time 465.0056 ms | tok/sec 1127487.4091
for step 1033 | loss 4.550333 | norm 0.6530 | time 465.3378 ms | tok/sec 1126682.7080
for step 1034 | loss 4.470256 | norm 0.5479 | time 465.3223 ms | tok/sec 1126720.2313
for step 1035 | loss 4.504076 | norm 0.6586 | time 465.1105 ms | tok/sec 1127233.1080
for step 1036 | loss 4.464674 | norm 0.6167 | time 466.2397 ms | tok/sec 1124503.1480
for step 1037 | loss 4.441846 | norm 0.5076 | time 465.1549 ms | tok/sec 1127125.6425
for step 1038 | loss 4.452534 | norm 0.5451 | time 464.2541 ms | tok/sec 1129312.4904
for step 1039 | loss 4.398406 | norm 0.5538 | time 465.7714 ms | tok/sec 1125633.6450
for step 1040 | loss 4.456595 | norm 0.5972 | time 465.1093 ms | tok/sec 1127235.9971
for step 1041 | loss 4.420630 | norm 0.7395 | time 465.4052 ms | tok/sec 1126519.3663
for step 1042 | loss 4.379909 | norm 0.7612 | time 465.4098 ms | tok/sec 1126508.4016
for step 1043 | loss 4.373889 | norm 0.6695 | time 465.9231 ms | tok/sec 1125267.3086
for step 1044 | loss 4.373957 | norm 0.6669 | time 464.7381 ms | tok/sec 1128136.3966
for step 1045 | loss 4.413796 | norm 0.5732 | time 465.4081 ms | tok/sec 1126512.4412
for step 1046 | loss 4.348024 | norm 0.5983 | time 465.4329 ms | tok/sec 1126452.4273
for step 1047 | loss 4.351745 | norm 0.6473 | time 466.1021 ms | tok/sec 1124835.0390
for step 1048 | loss 4.400021 | norm 0.6239 | time 464.9518 ms | tok/sec 1127618.0720
for step 1049 | loss 4.362098 | norm 0.6716 | time 465.4922 ms | tok/sec 1126308.7659
for step 1050 | loss 4.382699 | norm 0.6733 | time 465.4870 ms | tok/sec 1126321.4574
for step 1051 | loss 4.401217 | norm 0.6614 | time 466.2697 ms | tok/sec 1124430.6987
for step 1052 | loss 4.281314 | norm 0.6361 | time 465.3249 ms | tok/sec 1126713.8810
for step 1053 | loss 4.288300 | norm 0.6872 | time 464.8957 ms | tok/sec 1127753.9705
for step 1054 | loss 4.326064 | norm 0.8362 | time 465.3296 ms | tok/sec 1126702.3352
for step 1055 | loss 4.338300 | norm 0.8008 | time 465.5027 ms | tok/sec 1126283.3838
for step 1056 | loss 4.368027 | norm 0.7316 | time 464.9436 ms | tok/sec 1127637.7319
for step 1057 | loss 4.309213 | norm 0.6081 | time 465.0691 ms | tok/sec 1127333.6588
for step 1058 | loss 4.341952 | norm 0.5990 | time 466.0606 ms | tok/sec 1124935.1624
for step 1059 | loss 4.309825 | norm 0.6276 | time 465.5097 ms | tok/sec 1126266.6553
for step 1060 | loss 4.344354 | norm 0.5576 | time 465.4520 ms | tok/sec 1126406.2670
for step 1061 | loss 4.285871 | norm 0.5778 | time 465.5478 ms | tok/sec 1126174.3691
for step 1062 | loss 4.248219 | norm 0.6586 | time 465.2255 ms | tok/sec 1126954.6643
for step 1063 | loss 4.319048 | norm 0.6858 | time 465.1935 ms | tok/sec 1127032.0601
for step 1064 | loss 4.538379 | norm 0.6653 | time 465.3766 ms | tok/sec 1126588.6220
for step 1065 | loss 4.525379 | norm 0.7656 | time 464.6235 ms | tok/sec 1128414.8456
for step 1066 | loss 4.519534 | norm 0.7248 | time 464.8292 ms | tok/sec 1127915.3562
for step 1067 | loss 4.462073 | norm 0.6583 | time 465.1024 ms | tok/sec 1127252.7544
for step 1068 | loss 4.440210 | norm 0.6072 | time 465.5585 ms | tok/sec 1126148.4163
for step 1069 | loss 4.482480 | norm 0.6085 | time 465.1320 ms | tok/sec 1127181.1060
for step 1070 | loss 4.439946 | norm 0.6247 | time 465.7092 ms | tok/sec 1125784.0502
for step 1071 | loss 4.456793 | norm 0.5414 | time 464.5612 ms | tok/sec 1128565.9950
for step 1072 | loss 4.470574 | norm 0.5839 | time 465.6980 ms | tok/sec 1125811.1389
for step 1073 | loss 4.475724 | norm 0.7050 | time 465.3223 ms | tok/sec 1126720.2313
for step 1074 | loss 4.424838 | norm 0.8309 | time 464.6964 ms | tok/sec 1128237.6875
for step 1075 | loss 4.423255 | norm 0.7150 | time 465.6203 ms | tok/sec 1125999.0668
for step 1076 | loss 4.420451 | norm 0.5742 | time 464.8583 ms | tok/sec 1127844.7804
for step 1077 | loss 4.442647 | norm 0.6017 | time 465.1620 ms | tok/sec 1127108.3112
for step 1078 | loss 4.452760 | norm 0.5433 | time 465.9538 ms | tok/sec 1125193.0336
for step 1079 | loss 4.420804 | norm 0.5385 | time 465.6479 ms | tok/sec 1125932.1896
for step 1080 | loss 4.458178 | norm 0.5617 | time 464.7410 ms | tok/sec 1128129.4516
for step 1081 | loss 4.391576 | norm 0.5681 | time 465.3521 ms | tok/sec 1126648.0733
for step 1082 | loss 4.407978 | norm 0.5254 | time 465.2064 ms | tok/sec 1127000.8695
for step 1083 | loss 4.378792 | norm 0.5470 | time 466.0139 ms | tok/sec 1125047.9665
for step 1084 | loss 4.449923 | norm 0.5041 | time 464.0951 ms | tok/sec 1129699.4567
for step 1085 | loss 4.402387 | norm 0.4741 | time 464.3705 ms | tok/sec 1129029.5406
for step 1086 | loss 4.458766 | norm 0.5827 | time 464.7222 ms | tok/sec 1128175.1744
for step 1087 | loss 4.360253 | norm 0.6223 | time 465.1544 ms | tok/sec 1127126.7979
for step 1088 | loss 4.387713 | norm 0.7187 | time 465.3628 ms | tok/sec 1126622.0986
for step 1089 | loss 4.443882 | norm 0.7587 | time 465.6320 ms | tok/sec 1125970.8160
for step 1090 | loss 4.368018 | norm 0.7141 | time 465.3068 ms | tok/sec 1126757.7571
for step 1091 | loss 4.330167 | norm 0.6412 | time 465.7643 ms | tok/sec 1125650.9309
for step 1092 | loss 4.396491 | norm 0.6188 | time 466.6173 ms | tok/sec 1123593.0355
for step 1093 | loss 4.343856 | norm 0.5792 | time 465.1601 ms | tok/sec 1127112.9328
for step 1094 | loss 4.376735 | norm 0.5693 | time 465.4403 ms | tok/sec 1126434.5397
for step 1095 | loss 4.374822 | norm 0.5650 | time 465.4443 ms | tok/sec 1126424.7307
for step 1096 | loss 4.326690 | norm 0.5173 | time 464.5829 ms | tok/sec 1128513.2908
for step 1097 | loss 4.327013 | norm 0.4832 | time 464.9591 ms | tok/sec 1127600.1474
for step 1098 | loss 4.192657 | norm 0.4931 | time 463.8300 ms | tok/sec 1130345.1837
for step 1099 | loss 4.234297 | norm 0.4758 | time 463.8395 ms | tok/sec 1130321.9433
for step 1100 | loss 4.239276 | norm 0.5385 | time 465.4803 ms | tok/sec 1126337.6106
for step 1101 | loss 4.204153 | norm 0.5753 | time 465.0247 ms | tok/sec 1127441.1640
for step 1102 | loss 4.208690 | norm 0.6206 | time 464.6277 ms | tok/sec 1128404.4230
for step 1103 | loss 4.177365 | norm 0.6200 | time 465.6715 ms | tok/sec 1125875.1196
for step 1104 | loss 4.218856 | norm 0.6811 | time 465.6863 ms | tok/sec 1125839.3817
for step 1105 | loss 4.305716 | norm 0.7082 | time 464.6919 ms | tok/sec 1128248.6859
for step 1106 | loss 4.369137 | norm 0.7328 | time 465.0981 ms | tok/sec 1127263.1558
for step 1107 | loss 4.196250 | norm 0.6932 | time 465.2352 ms | tok/sec 1126930.9856
for step 1108 | loss 4.240971 | norm 0.6987 | time 465.1234 ms | tok/sec 1127201.9062
for step 1109 | loss 4.274545 | norm 0.6861 | time 464.7286 ms | tok/sec 1128159.5472
for step 1110 | loss 4.362998 | norm 0.7454 | time 464.6018 ms | tok/sec 1128467.5406
for step 1111 | loss 4.415044 | norm 0.7130 | time 465.0486 ms | tok/sec 1127383.3630
for step 1112 | loss 4.376873 | norm 0.6441 | time 465.1539 ms | tok/sec 1127127.9533
for step 1113 | loss 4.368383 | norm 0.5987 | time 464.1500 ms | tok/sec 1129565.9901
for step 1114 | loss 4.394855 | norm 0.6032 | time 464.3078 ms | tok/sec 1129182.0144
for step 1115 | loss 4.350313 | norm 0.5388 | time 464.9432 ms | tok/sec 1127638.8884
for step 1116 | loss 4.348109 | norm 0.5805 | time 465.2185 ms | tok/sec 1126971.4132
for step 1117 | loss 4.330413 | norm 0.6363 | time 465.6227 ms | tok/sec 1125993.3012
for step 1118 | loss 4.332654 | norm 0.6730 | time 465.5750 ms | tok/sec 1126108.6243
for step 1119 | loss 4.393899 | norm 0.6050 | time 465.6098 ms | tok/sec 1126024.4361
for step 1120 | loss 4.370859 | norm 0.5234 | time 465.8120 ms | tok/sec 1125535.7015
for step 1121 | loss 4.420308 | norm 0.5554 | time 464.9656 ms | tok/sec 1127584.5362
for step 1122 | loss 4.382502 | norm 0.5409 | time 466.2714 ms | tok/sec 1124426.6740
for step 1123 | loss 4.381902 | norm 0.4727 | time 465.5027 ms | tok/sec 1126283.3838
for step 1124 | loss 4.341571 | norm 0.4816 | time 464.3619 ms | tok/sec 1129050.4091
for step 1125 | loss 4.352296 | norm 0.4927 | time 465.4996 ms | tok/sec 1126290.8830
for step 1126 | loss 4.352007 | norm 0.4746 | time 464.8027 ms | tok/sec 1127979.5763
for step 1127 | loss 4.336657 | norm 0.4840 | time 464.9234 ms | tok/sec 1127686.8846
for step 1128 | loss 4.371964 | norm 0.5092 | time 464.8252 ms | tok/sec 1127925.1913
for step 1129 | loss 4.409669 | norm 0.5241 | time 465.0655 ms | tok/sec 1127342.3278
for step 1130 | loss 4.352778 | norm 0.6258 | time 465.1649 ms | tok/sec 1127101.3789
for step 1131 | loss 4.364751 | norm 0.5995 | time 465.6067 ms | tok/sec 1126031.9318
for step 1132 | loss 4.410155 | norm 0.6273 | time 465.7068 ms | tok/sec 1125789.8136
for step 1133 | loss 4.335977 | norm 0.5368 | time 465.5156 ms | tok/sec 1126252.2346
for step 1134 | loss 4.297339 | norm 0.5682 | time 464.9220 ms | tok/sec 1127690.3544
for step 1135 | loss 4.308315 | norm 0.5929 | time 465.8048 ms | tok/sec 1125552.9844
for step 1136 | loss 4.326853 | norm 0.5598 | time 465.1613 ms | tok/sec 1127110.0443
for step 1137 | loss 4.366215 | norm 0.5383 | time 464.8006 ms | tok/sec 1127984.7837
for step 1138 | loss 4.296388 | norm 0.6101 | time 465.7440 ms | tok/sec 1125699.9105
for step 1139 | loss 4.389819 | norm 0.5734 | time 466.0776 ms | tok/sec 1124894.3053
for step 1140 | loss 4.322985 | norm 0.6113 | time 465.1306 ms | tok/sec 1127184.5726
for step 1141 | loss 4.330359 | norm 0.5751 | time 465.4825 ms | tok/sec 1126332.4185
Will loading at 0 from edu_fineweb10B/edufineweb_train_000007.npy
for step 1142 | loss 4.309466 | norm 0.6379 | time 1409.8210 ms | tok/sec 371882.6628
for step 1143 | loss 4.308729 | norm 0.5750 | time 463.4333 ms | tok/sec 1131312.8315
for step 1144 | loss 4.284949 | norm 0.5531 | time 468.0316 ms | tok/sec 1120197.7597
for step 1145 | loss 4.185815 | norm 0.5017 | time 464.7429 ms | tok/sec 1128124.8217
for step 1146 | loss 4.184701 | norm 0.5145 | time 464.1087 ms | tok/sec 1129666.3772
for step 1147 | loss 4.226607 | norm 0.5389 | time 465.1508 ms | tok/sec 1127135.4637
for step 1148 | loss 4.172962 | norm 0.5322 | time 464.6885 ms | tok/sec 1128256.7901
for step 1149 | loss 4.169917 | norm 0.5871 | time 464.5722 ms | tok/sec 1128539.3527
for step 1150 | loss 4.220657 | norm 0.6423 | time 464.6790 ms | tok/sec 1128279.9457
for step 1151 | loss 4.158663 | norm 0.6243 | time 465.3735 ms | tok/sec 1126596.1252
for step 1152 | loss 4.185174 | norm 0.5623 | time 465.3697 ms | tok/sec 1126605.3601
for step 1153 | loss 4.181702 | norm 0.6262 | time 465.8170 ms | tok/sec 1125523.6038
for step 1154 | loss 4.155778 | norm 0.6211 | time 465.9383 ms | tok/sec 1125230.4577
for step 1155 | loss 4.179775 | norm 0.5642 | time 465.4121 ms | tok/sec 1126502.6308
for step 1156 | loss 4.282764 | norm 0.5056 | time 465.1585 ms | tok/sec 1127116.9768
for step 1157 | loss 4.288916 | norm 0.5436 | time 466.0306 ms | tok/sec 1125007.6768
for step 1158 | loss 4.323246 | norm 0.5745 | time 466.8975 ms | tok/sec 1122918.8717
for step 1159 | loss 4.360641 | norm 0.6284 | time 466.3301 ms | tok/sec 1124285.2533
for step 1160 | loss 4.355980 | norm 0.6106 | time 465.9817 ms | tok/sec 1125125.6764
for step 1161 | loss 4.327189 | norm 0.5360 | time 465.4422 ms | tok/sec 1126429.9237
for step 1162 | loss 4.319905 | norm 0.5187 | time 465.0414 ms | tok/sec 1127400.7027
for step 1163 | loss 4.322711 | norm 0.5872 | time 465.5597 ms | tok/sec 1126145.5327
for step 1164 | loss 4.325352 | norm 0.6108 | time 465.6255 ms | tok/sec 1125986.3826
for step 1165 | loss 4.324873 | norm 0.5736 | time 465.7433 ms | tok/sec 1125701.6393
for step 1166 | loss 4.337294 | norm 0.5251 | time 465.4617 ms | tok/sec 1126382.6114
for step 1167 | loss 4.319949 | norm 0.5226 | time 465.3897 ms | tok/sec 1126556.8788
for step 1168 | loss 4.412127 | norm 0.5042 | time 464.7353 ms | tok/sec 1128143.3417
for step 1169 | loss 4.291532 | norm 0.4559 | time 464.7789 ms | tok/sec 1128037.4385
for step 1170 | loss 4.287442 | norm 0.5300 | time 464.7515 ms | tok/sec 1128103.9874
for step 1171 | loss 4.297498 | norm 0.5369 | time 465.9119 ms | tok/sec 1125294.3724
for step 1172 | loss 4.298794 | norm 0.5483 | time 465.0490 ms | tok/sec 1127382.2070
for step 1173 | loss 4.282756 | norm 0.5977 | time 465.2746 ms | tok/sec 1126835.7033
for step 1174 | loss 4.317163 | norm 0.6391 | time 465.6262 ms | tok/sec 1125984.6530
for step 1175 | loss 4.345931 | norm 0.6685 | time 464.4482 ms | tok/sec 1128840.5998
for step 1176 | loss 4.308864 | norm 0.6766 | time 464.7450 ms | tok/sec 1128119.6130
for step 1177 | loss 4.295717 | norm 0.5822 | time 465.4102 ms | tok/sec 1126507.2475
for step 1178 | loss 4.292758 | norm 0.5437 | time 465.4131 ms | tok/sec 1126500.3225
for step 1179 | loss 4.268074 | norm 0.5078 | time 465.3802 ms | tok/sec 1126579.9646
for step 1180 | loss 4.298212 | norm 0.4741 | time 464.7326 ms | tok/sec 1128149.7081
for step 1181 | loss 4.277840 | norm 0.5156 | time 465.2667 ms | tok/sec 1126854.7585
for step 1182 | loss 4.207597 | norm 0.5188 | time 464.8082 ms | tok/sec 1127966.2689
for step 1183 | loss 4.280372 | norm 0.4766 | time 465.7495 ms | tok/sec 1125686.6568
for step 1184 | loss 4.302372 | norm 0.4742 | time 465.3301 ms | tok/sec 1126701.1807
for step 1185 | loss 4.264327 | norm 0.5012 | time 465.1809 ms | tok/sec 1127062.6749
for step 1186 | loss 4.228642 | norm 0.5561 | time 465.1527 ms | tok/sec 1127130.8419
for step 1187 | loss 4.204876 | norm 0.5451 | time 465.8520 ms | tok/sec 1125438.9271
for step 1188 | loss 4.230347 | norm 0.5675 | time 465.5797 ms | tok/sec 1126097.0909
for step 1189 | loss 4.283937 | norm 0.5875 | time 465.4710 ms | tok/sec 1126360.1106
for step 1190 | loss 4.205951 | norm 0.5574 | time 465.5874 ms | tok/sec 1126078.6380
for step 1191 | loss 4.182292 | norm 0.5421 | time 465.6348 ms | tok/sec 1125963.8976
for step 1192 | loss 4.120453 | norm 0.5655 | time 464.9405 ms | tok/sec 1127645.2491
for step 1193 | loss 4.129642 | norm 0.5315 | time 465.7950 ms | tok/sec 1125576.6052
for step 1194 | loss 4.139691 | norm 0.4986 | time 465.4958 ms | tok/sec 1126300.1128
for step 1195 | loss 4.183305 | norm 0.5290 | time 465.4222 ms | tok/sec 1126478.3941
for step 1196 | loss 4.152089 | norm 0.5545 | time 467.4401 ms | tok/sec 1121615.3005
for step 1197 | loss 4.152580 | norm 0.6074 | time 465.3556 ms | tok/sec 1126639.4149
for step 1198 | loss 4.149111 | norm 0.6318 | time 465.7834 ms | tok/sec 1125604.8363
for step 1199 | loss 4.140160 | norm 0.5926 | time 465.2650 ms | tok/sec 1126858.8006
for step 1200 | loss 4.159468 | norm 0.5713 | time 465.5066 ms | tok/sec 1126274.1542
for step 1201 | loss 4.175119 | norm 0.5858 | time 465.2689 ms | tok/sec 1126849.5616
for step 1202 | loss 4.118150 | norm 0.5413 | time 466.1655 ms | tok/sec 1124682.0112
for step 1203 | loss 4.187626 | norm 0.5768 | time 465.2717 ms | tok/sec 1126842.6324
for step 1204 | loss 4.261978 | norm 0.5567 | time 465.8167 ms | tok/sec 1125524.1799
for step 1205 | loss 4.303835 | norm 0.5556 | time 466.6250 ms | tok/sec 1123574.6646
for step 1206 | loss 4.313625 | norm 0.6337 | time 466.8622 ms | tok/sec 1123003.7431
for step 1207 | loss 4.346665 | norm 0.7438 | time 466.0876 ms | tok/sec 1124870.1377
for step 1208 | loss 4.320477 | norm 0.6938 | time 465.8670 ms | tok/sec 1125402.6409
for step 1209 | loss 4.287566 | norm 0.5947 | time 466.0106 ms | tok/sec 1125056.0248
for step 1210 | loss 4.303555 | norm 0.5880 | time 465.1165 ms | tok/sec 1127218.6625
for step 1211 | loss 4.363730 | norm 0.5736 | time 466.2647 ms | tok/sec 1124442.7729
for step 1212 | loss 4.269467 | norm 0.5574 | time 465.4989 ms | tok/sec 1126292.6135
for step 1213 | loss 4.274169 | norm 0.5526 | time 466.1045 ms | tok/sec 1124829.2853
for step 1214 | loss 4.306905 | norm 0.4975 | time 466.4359 ms | tok/sec 1124030.0963
for step 1215 | loss 4.263905 | norm 0.4950 | time 466.2092 ms | tok/sec 1124576.7568
for step 1216 | loss 4.254365 | norm 0.4387 | time 466.2862 ms | tok/sec 1124391.0281
for step 1217 | loss 4.269434 | norm 0.4423 | time 465.6453 ms | tok/sec 1125938.5310
for step 1218 | loss 4.258562 | norm 0.4294 | time 465.8628 ms | tok/sec 1125413.0082
for step 1219 | loss 4.281155 | norm 0.4418 | time 466.1744 ms | tok/sec 1124660.7287
for step 1220 | loss 4.287497 | norm 0.4234 | time 466.1143 ms | tok/sec 1124805.6959
for step 1221 | loss 4.246121 | norm 0.3818 | time 465.4491 ms | tok/sec 1126413.1908
for step 1222 | loss 4.287508 | norm 0.4050 | time 464.8073 ms | tok/sec 1127968.5832
for step 1223 | loss 4.276769 | norm 0.4518 | time 465.2672 ms | tok/sec 1126853.6036
for step 1224 | loss 4.151134 | norm 0.4969 | time 464.9024 ms | tok/sec 1127737.7766
for step 1225 | loss 4.285334 | norm 0.4700 | time 466.1953 ms | tok/sec 1124610.1140
for step 1226 | loss 4.249724 | norm 0.5649 | time 465.4877 ms | tok/sec 1126319.7268
for step 1227 | loss 4.197875 | norm 0.5066 | time 465.2512 ms | tok/sec 1126892.2933
for step 1228 | loss 4.185203 | norm 0.5294 | time 465.8265 ms | tok/sec 1125500.5612
for step 1229 | loss 4.173195 | norm 0.5497 | time 464.9887 ms | tok/sec 1127528.4548
for step 1230 | loss 4.216063 | norm 0.5431 | time 466.2843 ms | tok/sec 1124395.6274
for step 1231 | loss 4.197586 | norm 0.6552 | time 465.9808 ms | tok/sec 1125127.9790
for step 1232 | loss 4.220220 | norm 0.6885 | time 465.6928 ms | tok/sec 1125823.8192
for step 1233 | loss 4.214329 | norm 0.7525 | time 465.8289 ms | tok/sec 1125494.8007
for step 1234 | loss 4.226493 | norm 0.6784 | time 466.0258 ms | tok/sec 1125019.1878
for step 1235 | loss 4.162210 | norm 0.5933 | time 466.1984 ms | tok/sec 1124602.6372
for step 1236 | loss 4.142097 | norm 0.5850 | time 464.4935 ms | tok/sec 1128730.5101
for step 1237 | loss 4.203752 | norm 0.5292 | time 464.6757 ms | tok/sec 1128288.0504
for step 1238 | loss 4.210658 | norm 0.5748 | time 466.1984 ms | tok/sec 1124602.6372
for step 1239 | loss 4.170351 | norm 0.5213 | time 465.2638 ms | tok/sec 1126861.6878
for step 1240 | loss 4.106186 | norm 0.5503 | time 466.4714 ms | tok/sec 1123944.4952
for step 1241 | loss 4.135902 | norm 0.5440 | time 465.8377 ms | tok/sec 1125473.4874
for step 1242 | loss 4.121036 | norm 0.4749 | time 465.5674 ms | tok/sec 1126127.0782
for step 1243 | loss 4.107048 | norm 0.4107 | time 465.3187 ms | tok/sec 1126728.8909
for step 1244 | loss 4.104864 | norm 0.4356 | time 465.5814 ms | tok/sec 1126093.0543
for step 1245 | loss 4.128308 | norm 0.3963 | time 465.9359 ms | tok/sec 1125236.2155
for step 1246 | loss 4.128533 | norm 0.4649 | time 465.6291 ms | tok/sec 1125977.7344
for step 1247 | loss 4.107285 | norm 0.4866 | time 465.9712 ms | tok/sec 1125151.0063
for step 1248 | loss 4.083161 | norm 0.5814 | time 464.6485 ms | tok/sec 1128354.0498
for step 1249 | loss 4.146478 | norm 0.6012 | time 465.4372 ms | tok/sec 1126442.0409
validation loss 4.2678
HellaSwag accuracy: 2538/10042=0.2527
> Hello, I'm a language model, like I'm a professor, with teachers. We can write to the students using the latest findings for the first time.
> Hello, I'm a language model, a term of which I'm talking about (and so what I might say?) on either. On a topic, I
> Hello, I'm a language model, which offers you with a particular language that is used to create their identity, and I'm a linguistic, that's
> Hello, I'm a language model, and I'm pretty much curious about these very interesting in-depth thinking with people who read out their mind in front of
> > Hello, I'm a language model, such as the U.S. Board, I'm an American language by which my favorite is to be, I'll
> Hello, I'm a language model, which will make you with a history of genetics. For instance, this could be an area of a protein that could make
> Hello, I'm a language model, and I want to know which ones you read can read are a fun conversation that is what you're looking on. The
> > Hello, I'm a language model, but I know it is my first language language, so I have to read it: a system of speech or a group
> Hello, I'm a language model, not a part of the curriculum. They must be represented by their teacher at the end of the school year, and it
Hello, I'm a language model, we want to use my favorite song and to do it for us to read this blog post, I want to ask him
Hello, I'm a language model, but also used to help you learn when you're up to your needs. So if I like a topic in your area
> > > > Hello, I'm a language model, including my own voice, is something I know, right?
As this means, ‘I know, as well
Hello, I'm a language model, but I think you have a new topic for you, and I have never found your experience. What was the big thing>> Hello, I'm a language model, but it also makes English a huge and powerful role.
For those who have already mentioned the language, I am so
Hello, I'm a language model, but I did that over 200 years ago, or all of this.”
The question is that the language-
> 
>  Hello, I'm a language model, and I'm not just an official language of a people I used in society over my, it's something I want forHello, I'm a language model, and I'm a language study.
With the first word, I really mean playing out that first word, and then
> > Hello, I'm a language model, in that I will work alongside his peers.
It’s important to watch the videos, but it’
> Hello, I'm a language model, I wrote, though you could not understand what the term ‘one’ (i.e. the meaning �

> Hello, I'm a language model, to show how I can connect with the following one? It's my own math or the math game, but it's
Hello, I'm a language model, which you can use to get the right information before you can use from, you should know:
(1) User
> Hello, I'm a language model, which would bring you through a new language which would assist you on your own. So I thought there was something to look
> Hello, I'm a language model, rather than simply a language. But to write a second piece of text, I still say.
To make it to
> Hello, I'm a language model, as well.
What are the benefits it is and what is the role of a whole language?
I thought we
Hello, I'm a language model, and it came through this book. To be clear at this age you had a special role. We have to look for
> Hello, I'm a language model, and you're interested in sharing and sharing with a great opportunity to share with your students.
What are the Best Practices
> Hello, I'm a language model, for example. He added, with two small groups of people who worked. He also found that people who were willing to
> Hello, I'm a language model, which includes a wide range of tools such as an open-access journal. My work and work is primarily on Google's
> Hello, I'm a language model, and the use of high-bandwidth model as an alternative to the standard deviation of the input algorithm, which the two
Hello, I'm a language model, just a new language. That's basically a combination of language. It's a way to start and expand the language of
> > Hello, I'm a language model, and so, most of my best, you'll be able to give you some more time down. At one point,
Hello, I'm a language model, and has a more functional level for that, so you need a good score. But there are certain styles that I would
for step 1250 | loss 4.130760 | norm 0.5449 | time 12873.7817 ms | tok/sec 40725.2518
for step 1251 | loss 4.144163 | norm 0.5042 | time 462.4164 ms | tok/sec 1133800.5922
for step 1252 | loss 4.249656 | norm 0.5005 | time 462.4846 ms | tok/sec 1133633.4271
for step 1253 | loss 4.250899 | norm 0.5399 | time 466.6951 ms | tok/sec 1123405.9098
for step 1254 | loss 4.299998 | norm 0.5084 | time 463.7139 ms | tok/sec 1130628.2121
for step 1255 | loss 4.238559 | norm 0.5019 | time 463.2242 ms | tok/sec 1131823.4907
for step 1256 | loss 4.237946 | norm 0.5006 | time 463.3064 ms | tok/sec 1131622.5492
for step 1257 | loss 4.305874 | norm 0.4419 | time 463.0167 ms | tok/sec 1132330.5297
for step 1258 | loss 4.256001 | norm 0.4355 | time 463.3164 ms | tok/sec 1131598.0917
for step 1259 | loss 4.262905 | norm 0.4347 | time 463.2502 ms | tok/sec 1131759.9971
for step 1260 | loss 4.220695 | norm 0.4677 | time 463.9010 ms | tok/sec 1130172.0657
for step 1261 | loss 4.275927 | norm 0.4567 | time 463.3975 ms | tok/sec 1131400.1407
for step 1262 | loss 4.242371 | norm 0.4633 | time 463.5339 ms | tok/sec 1131067.2736
for step 1263 | loss 4.243862 | norm 0.4488 | time 463.0580 ms | tok/sec 1132229.6685
for step 1264 | loss 4.205659 | norm 0.4781 | time 462.8487 ms | tok/sec 1132741.7394
for step 1265 | loss 4.250899 | norm 0.5525 | time 463.8364 ms | tok/sec 1130329.4963
for step 1266 | loss 4.225033 | norm 0.5133 | time 464.1342 ms | tok/sec 1129604.2860
for step 1267 | loss 4.196748 | norm 0.5972 | time 467.3045 ms | tok/sec 1121940.9095
for step 1268 | loss 4.255188 | norm 0.6243 | time 464.2508 ms | tok/sec 1129320.6099
for step 1269 | loss 4.203797 | norm 0.5815 | time 464.5669 ms | tok/sec 1128552.0945
for step 1270 | loss 4.263111 | norm 0.5619 | time 464.3254 ms | tok/sec 1129139.1089
for step 1271 | loss 4.222934 | norm 0.5520 | time 465.4193 ms | tok/sec 1126485.3187
for step 1272 | loss 4.224024 | norm 0.5342 | time 463.5139 ms | tok/sec 1131116.1440
for step 1273 | loss 4.183953 | norm 0.5397 | time 464.3054 ms | tok/sec 1129187.8127
for step 1274 | loss 4.228791 | norm 0.5155 | time 465.1673 ms | tok/sec 1127095.6020
for step 1275 | loss 4.171189 | norm 0.5366 | time 464.9615 ms | tok/sec 1127594.3654
for step 1276 | loss 4.131512 | norm 0.5047 | time 464.6580 ms | tok/sec 1128330.8912
for step 1277 | loss 4.132178 | norm 0.4591 | time 465.1473 ms | tok/sec 1127144.1297
for step 1278 | loss 4.145398 | norm 0.4570 | time 465.2340 ms | tok/sec 1126933.8732
for step 1279 | loss 4.175531 | norm 0.4466 | time 465.2162 ms | tok/sec 1126977.1888
for step 1280 | loss 4.174418 | norm 0.4489 | time 465.2379 ms | tok/sec 1126924.6329
for step 1281 | loss 4.136301 | norm 0.4435 | time 464.5538 ms | tok/sec 1128583.9503
for step 1282 | loss 4.156878 | norm 0.4835 | time 465.5218 ms | tok/sec 1126237.2374
for step 1283 | loss 4.211165 | norm 0.5472 | time 464.7644 ms | tok/sec 1128072.7374
for step 1284 | loss 4.175990 | norm 0.6121 | time 465.2853 ms | tok/sec 1126809.7201
for step 1285 | loss 4.134580 | norm 0.6233 | time 465.5802 ms | tok/sec 1126095.9376
for step 1286 | loss 4.139306 | norm 0.6371 | time 465.0567 ms | tok/sec 1127363.7120
for step 1287 | loss 4.192851 | norm 0.6710 | time 466.6798 ms | tok/sec 1123442.6412
for step 1288 | loss 4.097201 | norm 0.5475 | time 465.1642 ms | tok/sec 1127103.1120
for step 1289 | loss 4.093261 | norm 0.5144 | time 464.9768 ms | tok/sec 1127557.3620
for step 1290 | loss 4.054934 | norm 0.5291 | time 465.0505 ms | tok/sec 1127378.7391
for step 1291 | loss 4.084486 | norm 0.4492 | time 465.0681 ms | tok/sec 1127335.9705
for step 1292 | loss 4.075781 | norm 0.4156 | time 465.3935 ms | tok/sec 1126547.6447
for step 1293 | loss 4.031766 | norm 0.4297 | time 465.0218 ms | tok/sec 1127448.1005
for step 1294 | loss 4.066872 | norm 0.3955 | time 465.0338 ms | tok/sec 1127419.1989
for step 1295 | loss 4.005923 | norm 0.4135 | time 465.5821 ms | tok/sec 1126091.3244
for step 1296 | loss 4.080006 | norm 0.4411 | time 466.4681 ms | tok/sec 1123952.5377
for step 1297 | loss 4.070475 | norm 0.5606 | time 465.7757 ms | tok/sec 1125623.2737
for step 1298 | loss 4.079187 | norm 0.6041 | time 465.5643 ms | tok/sec 1126134.5753
for step 1299 | loss 4.198031 | norm 0.6140 | time 464.4043 ms | tok/sec 1128947.2335
for step 1300 | loss 4.215991 | norm 0.5447 | time 464.7901 ms | tok/sec 1128010.2425
for step 1301 | loss 4.251001 | norm 0.5424 | time 464.4086 ms | tok/sec 1128936.8010
for step 1302 | loss 4.232044 | norm 0.6146 | time 465.5633 ms | tok/sec 1126136.8821
for step 1303 | loss 4.271509 | norm 0.5677 | time 466.0206 ms | tok/sec 1125031.8503
for step 1304 | loss 4.226320 | norm 0.5778 | time 464.6819 ms | tok/sec 1128272.9989
for step 1305 | loss 4.193624 | norm 0.5710 | time 465.4343 ms | tok/sec 1126448.9651
for step 1306 | loss 4.267683 | norm 0.5297 | time 464.9582 ms | tok/sec 1127602.4603
for step 1307 | loss 4.226185 | norm 0.5273 | time 465.4133 ms | tok/sec 1126499.7454
for step 1308 | loss 4.284180 | norm 0.5008 | time 465.4908 ms | tok/sec 1126312.2272
for step 1309 | loss 4.248794 | norm 0.5339 | time 465.6627 ms | tok/sec 1125896.4481
for step 1310 | loss 4.167363 | norm 0.4773 | time 464.9816 ms | tok/sec 1127545.7990
for step 1311 | loss 4.178592 | norm 0.4559 | time 465.1678 ms | tok/sec 1127094.4466
for step 1312 | loss 4.201439 | norm 0.4350 | time 465.5538 ms | tok/sec 1126159.9507
for step 1313 | loss 4.190461 | norm 0.4304 | time 465.0269 ms | tok/sec 1127435.9617
for step 1314 | loss 4.232506 | norm 0.4448 | time 464.7589 ms | tok/sec 1128086.0474
for step 1315 | loss 4.203949 | norm 0.4433 | time 465.4331 ms | tok/sec 1126451.8502
for step 1316 | loss 4.219117 | norm 0.4290 | time 465.6506 ms | tok/sec 1125925.8482
for step 1317 | loss 4.179380 | norm 0.4287 | time 465.0180 ms | tok/sec 1127457.3494
for step 1318 | loss 4.203142 | norm 0.4483 | time 465.4028 ms | tok/sec 1126525.1373
for step 1319 | loss 4.196229 | norm 0.4879 | time 464.3469 ms | tok/sec 1129086.9308
for step 1320 | loss 4.117095 | norm 0.5524 | time 464.8900 ms | tok/sec 1127767.8513
for step 1321 | loss 4.169129 | norm 0.5592 | time 464.0245 ms | tok/sec 1129871.2688
for step 1322 | loss 4.165476 | norm 0.4863 | time 465.1191 ms | tok/sec 1127212.3066
for step 1323 | loss 4.123587 | norm 0.4336 | time 465.2672 ms | tok/sec 1126853.6036
for step 1324 | loss 4.152741 | norm 0.4508 | time 465.4849 ms | tok/sec 1126326.6495
for step 1325 | loss 4.138142 | norm 0.4347 | time 465.0726 ms | tok/sec 1127324.9899
for step 1326 | loss 4.132835 | norm 0.4410 | time 465.1639 ms | tok/sec 1127103.6897
for step 1327 | loss 4.144188 | norm 0.4654 | time 464.9589 ms | tok/sec 1127600.7257
for step 1328 | loss 4.159909 | norm 0.4953 | time 465.3854 ms | tok/sec 1126567.2673
for step 1329 | loss 4.113194 | norm 0.5044 | time 465.4577 ms | tok/sec 1126392.4197
for step 1330 | loss 4.098919 | norm 0.4464 | time 465.0507 ms | tok/sec 1127378.1612
for step 1331 | loss 4.139942 | norm 0.4676 | time 466.0790 ms | tok/sec 1124890.8528
for step 1332 | loss 4.089692 | norm 0.4570 | time 464.7362 ms | tok/sec 1128141.0267
Will loading at 0 from edu_fineweb10B/edufineweb_train_000008.npy
for step 1333 | loss 4.123655 | norm 0.4810 | time 1417.3601 ms | tok/sec 369904.5938
for step 1334 | loss 4.043265 | norm 0.4549 | time 464.2663 ms | tok/sec 1129282.9132
for step 1335 | loss 4.098534 | norm 0.4663 | time 464.2048 ms | tok/sec 1129432.5550
for step 1336 | loss 4.030045 | norm 0.5143 | time 464.5398 ms | tok/sec 1128618.1249
for step 1337 | loss 4.031580 | norm 0.6069 | time 464.8445 ms | tok/sec 1127878.3318
for step 1338 | loss 4.112032 | norm 0.6247 | time 464.6802 ms | tok/sec 1128277.0512
for step 1339 | loss 4.067611 | norm 0.5974 | time 465.3654 ms | tok/sec 1126615.7495
for step 1340 | loss 4.041274 | norm 0.5441 | time 464.6575 ms | tok/sec 1128332.0491
for step 1341 | loss 4.082127 | norm 0.4980 | time 465.0438 ms | tok/sec 1127394.9227
for step 1342 | loss 4.038023 | norm 0.4681 | time 465.4503 ms | tok/sec 1126410.3059
for step 1343 | loss 4.003627 | norm 0.4655 | time 464.7486 ms | tok/sec 1128110.9321
for step 1344 | loss 4.048111 | norm 0.5079 | time 466.2395 ms | tok/sec 1124503.7230
for step 1345 | loss 4.175517 | norm 0.5305 | time 464.8399 ms | tok/sec 1127889.3231
for step 1346 | loss 4.127178 | norm 0.5777 | time 466.1503 ms | tok/sec 1124718.8261
for step 1347 | loss 4.264575 | norm 0.5791 | time 464.8371 ms | tok/sec 1127896.2652
for step 1348 | loss 4.225676 | norm 0.5328 | time 465.0264 ms | tok/sec 1127437.1177
for step 1349 | loss 4.200556 | norm 0.5971 | time 466.1815 ms | tok/sec 1124643.4731
for step 1350 | loss 4.210855 | norm 0.6393 | time 465.7412 ms | tok/sec 1125706.8256
for step 1351 | loss 4.181927 | norm 0.5595 | time 467.4764 ms | tok/sec 1121528.3508
for step 1352 | loss 4.156294 | norm 0.5166 | time 466.1417 ms | tok/sec 1124739.5356
for step 1353 | loss 4.186883 | norm 0.5507 | time 466.0268 ms | tok/sec 1125016.8856
for step 1354 | loss 4.232535 | norm 0.5135 | time 466.1310 ms | tok/sec 1124765.4234
for step 1355 | loss 4.149957 | norm 0.4851 | time 465.4930 ms | tok/sec 1126307.0353
for step 1356 | loss 4.198697 | norm 0.4927 | time 465.7779 ms | tok/sec 1125618.0881
for step 1357 | loss 4.164516 | norm 0.5424 | time 465.3223 ms | tok/sec 1126720.2313
for step 1358 | loss 4.190076 | norm 0.4888 | time 465.0495 ms | tok/sec 1127381.0510
for step 1359 | loss 4.221816 | norm 0.4609 | time 464.4673 ms | tok/sec 1128794.2436
for step 1360 | loss 4.187596 | norm 0.4370 | time 465.9536 ms | tok/sec 1125193.6093
for step 1361 | loss 4.176745 | norm 0.4944 | time 465.6742 ms | tok/sec 1125868.7788
for step 1362 | loss 4.210148 | norm 0.4946 | time 466.5413 ms | tok/sec 1123776.2036
for step 1363 | loss 4.210077 | norm 0.4555 | time 465.4891 ms | tok/sec 1126316.2654
for step 1364 | loss 4.181953 | norm 0.4207 | time 465.5168 ms | tok/sec 1126249.3505
for step 1365 | loss 4.164113 | norm 0.4211 | time 466.0079 ms | tok/sec 1125062.3564
for step 1366 | loss 4.195414 | norm 0.4162 | time 465.7936 ms | tok/sec 1125580.0620
for step 1367 | loss 4.127466 | norm 0.4207 | time 465.1732 ms | tok/sec 1127081.1600
for step 1368 | loss 4.155025 | norm 0.4221 | time 465.5712 ms | tok/sec 1126117.8512
for step 1369 | loss 4.114373 | norm 0.4177 | time 465.6532 ms | tok/sec 1125919.5068
for step 1370 | loss 4.092584 | norm 0.4251 | time 465.9212 ms | tok/sec 1125271.9151
for step 1371 | loss 4.142792 | norm 0.4505 | time 464.3841 ms | tok/sec 1128996.5004
for step 1372 | loss 4.088101 | norm 0.4751 | time 464.5143 ms | tok/sec 1128680.1078
for step 1373 | loss 4.126410 | norm 0.4778 | time 464.4110 ms | tok/sec 1128931.0053
for step 1374 | loss 4.092822 | norm 0.4657 | time 465.7018 ms | tok/sec 1125801.9170
for step 1375 | loss 4.123478 | norm 0.4667 | time 464.6277 ms | tok/sec 1128404.4230
for step 1376 | loss 4.100901 | norm 0.4777 | time 464.4063 ms | tok/sec 1128942.5968
for step 1377 | loss 4.130730 | norm 0.5109 | time 465.0593 ms | tok/sec 1127357.3544
for step 1378 | loss 4.113086 | norm 0.5054 | time 464.4568 ms | tok/sec 1128819.7391
for step 1379 | loss 4.109407 | norm 0.5115 | time 464.4144 ms | tok/sec 1128922.8914
for step 1380 | loss 4.108753 | norm 0.5029 | time 465.5056 ms | tok/sec 1126276.4616
for step 1381 | loss 4.051481 | norm 0.5888 | time 465.3368 ms | tok/sec 1126685.0170
for step 1382 | loss 4.083680 | norm 0.5484 | time 464.8941 ms | tok/sec 1127758.0190
for step 1383 | loss 4.035331 | norm 0.4577 | time 465.2135 ms | tok/sec 1126983.5421
for step 1384 | loss 4.041517 | norm 0.4483 | time 464.8738 ms | tok/sec 1127807.1822
for step 1385 | loss 4.060169 | norm 0.4152 | time 465.7996 ms | tok/sec 1125565.6589
for step 1386 | loss 4.018706 | norm 0.4334 | time 465.0669 ms | tok/sec 1127338.8602
for step 1387 | loss 4.013964 | norm 0.5014 | time 464.0615 ms | tok/sec 1129781.2931
for step 1388 | loss 4.003890 | norm 0.5343 | time 464.1953 ms | tok/sec 1129455.7589
for step 1389 | loss 4.025233 | norm 0.4824 | time 464.7558 ms | tok/sec 1128093.5705
for step 1390 | loss 4.027516 | norm 0.4480 | time 465.5898 ms | tok/sec 1126072.8716
for step 1391 | loss 4.028037 | norm 0.4229 | time 465.5602 ms | tok/sec 1126144.3793
for step 1392 | loss 3.976986 | norm 0.4417 | time 465.3304 ms | tok/sec 1126700.6034
for step 1393 | loss 4.196419 | norm 0.4307 | time 464.5469 ms | tok/sec 1128600.7477
for step 1394 | loss 4.168126 | norm 0.5354 | time 465.0562 ms | tok/sec 1127364.8679
for step 1395 | loss 4.149758 | norm 0.6061 | time 464.8294 ms | tok/sec 1127914.7777
for step 1396 | loss 4.159853 | norm 0.6299 | time 464.7062 ms | tok/sec 1128213.9549
for step 1397 | loss 4.229830 | norm 0.5473 | time 465.4431 ms | tok/sec 1126427.6157
for step 1398 | loss 4.209869 | norm 0.5112 | time 465.8825 ms | tok/sec 1125365.2053
for step 1399 | loss 4.243652 | norm 0.4985 | time 466.0001 ms | tok/sec 1125081.3517
for step 1400 | loss 4.245255 | norm 0.5108 | time 466.1412 ms | tok/sec 1124740.6861
for step 1401 | loss 4.137843 | norm 0.5464 | time 465.6670 ms | tok/sec 1125886.0720
for step 1402 | loss 4.084723 | norm 0.4795 | time 466.0201 ms | tok/sec 1125033.0014
for step 1403 | loss 4.128190 | norm 0.4770 | time 466.2323 ms | tok/sec 1124520.9742
for step 1404 | loss 4.144763 | norm 0.4761 | time 465.0290 ms | tok/sec 1127430.7594
for step 1405 | loss 4.166013 | norm 0.3976 | time 465.1406 ms | tok/sec 1127160.3065
for step 1406 | loss 4.125648 | norm 0.3883 | time 465.0304 ms | tok/sec 1127427.2912
for step 1407 | loss 4.169086 | norm 0.4047 | time 465.1880 ms | tok/sec 1127045.3456
for step 1408 | loss 4.119823 | norm 0.4137 | time 465.4467 ms | tok/sec 1126418.9607
for step 1409 | loss 4.137251 | norm 0.4282 | time 465.6963 ms | tok/sec 1125815.1735
for step 1410 | loss 4.124711 | norm 0.4707 | time 465.3413 ms | tok/sec 1126674.0491
for step 1411 | loss 4.112339 | norm 0.5204 | time 464.5436 ms | tok/sec 1128608.8570
for step 1412 | loss 4.138951 | norm 0.4928 | time 465.5538 ms | tok/sec 1126159.9507
for step 1413 | loss 4.154891 | norm 0.4499 | time 465.7156 ms | tok/sec 1125768.4891
for step 1414 | loss 4.147983 | norm 0.4515 | time 465.8411 ms | tok/sec 1125465.4232
for step 1415 | loss 4.109291 | norm 0.4834 | time 465.8086 ms | tok/sec 1125543.7668
for step 1416 | loss 4.122352 | norm 0.4700 | time 465.5712 ms | tok/sec 1126117.8512
for step 1417 | loss 4.091446 | norm 0.4671 | time 464.6864 ms | tok/sec 1128262.0001
for step 1418 | loss 4.038382 | norm 0.4738 | time 465.1403 ms | tok/sec 1127160.8843
for step 1419 | loss 4.080669 | norm 0.4870 | time 465.6610 ms | tok/sec 1125900.4833
for step 1420 | loss 4.037012 | norm 0.4513 | time 464.1979 ms | tok/sec 1129449.3777
for step 1421 | loss 4.032755 | norm 0.4301 | time 465.1818 ms | tok/sec 1127060.3643
for step 1422 | loss 4.052904 | norm 0.4063 | time 465.6546 ms | tok/sec 1125916.0480
for step 1423 | loss 4.073847 | norm 0.4470 | time 465.1852 ms | tok/sec 1127052.2772
for step 1424 | loss 4.101328 | norm 0.4994 | time 465.1697 ms | tok/sec 1127089.8252
for step 1425 | loss 4.043045 | norm 0.5285 | time 465.6494 ms | tok/sec 1125928.7306
for step 1426 | loss 4.100243 | norm 0.4997 | time 465.5900 ms | tok/sec 1126072.2950
for step 1427 | loss 4.077523 | norm 0.4455 | time 465.4524 ms | tok/sec 1126405.1131
for step 1428 | loss 3.938891 | norm 0.4648 | time 465.1437 ms | tok/sec 1127152.7958
for step 1429 | loss 4.113703 | norm 0.4259 | time 464.8147 ms | tok/sec 1127950.6475
for step 1430 | loss 3.977184 | norm 0.4689 | time 464.3939 ms | tok/sec 1128972.7358
for step 1431 | loss 4.002927 | norm 0.5040 | time 465.6303 ms | tok/sec 1125974.8517
for step 1432 | loss 4.008876 | norm 0.5062 | time 465.2643 ms | tok/sec 1126860.5329
for step 1433 | loss 3.967019 | norm 0.5253 | time 464.3383 ms | tok/sec 1129107.8015
for step 1434 | loss 3.985679 | norm 0.4884 | time 463.9530 ms | tok/sec 1130045.4560
for step 1435 | loss 3.982078 | norm 0.5081 | time 465.4396 ms | tok/sec 1126436.2707
for step 1436 | loss 4.027160 | norm 0.4657 | time 465.7447 ms | tok/sec 1125698.1817
for step 1437 | loss 3.988166 | norm 0.4485 | time 465.2476 ms | tok/sec 1126900.9555
for step 1438 | loss 3.922302 | norm 0.4504 | time 465.1318 ms | tok/sec 1127181.6837
for step 1439 | loss 4.007473 | norm 0.4490 | time 464.9858 ms | tok/sec 1127535.3924
for step 1440 | loss 4.203354 | norm 0.4956 | time 465.2905 ms | tok/sec 1126797.0176
for step 1441 | loss 4.176352 | norm 0.5470 | time 464.2978 ms | tok/sec 1129206.3676
for step 1442 | loss 4.156886 | norm 0.6210 | time 465.2390 ms | tok/sec 1126921.7454
for step 1443 | loss 4.160489 | norm 0.6039 | time 465.6658 ms | tok/sec 1125888.9542
for step 1444 | loss 4.146008 | norm 0.5533 | time 465.5612 ms | tok/sec 1126142.0724
for step 1445 | loss 4.127331 | norm 0.4773 | time 464.5526 ms | tok/sec 1128586.8464
for step 1446 | loss 4.145171 | norm 0.4610 | time 465.9152 ms | tok/sec 1125286.3107
for step 1447 | loss 4.135217 | norm 0.4104 | time 466.1577 ms | tok/sec 1124700.9936
for step 1448 | loss 4.115677 | norm 0.4152 | time 464.7062 ms | tok/sec 1128213.9549
for step 1449 | loss 4.173326 | norm 0.3666 | time 466.4016 ms | tok/sec 1124112.8372
for step 1450 | loss 4.131454 | norm 0.4113 | time 464.5395 ms | tok/sec 1128618.7042
for step 1451 | loss 4.129225 | norm 0.4028 | time 465.6968 ms | tok/sec 1125814.0207
for step 1452 | loss 4.107905 | norm 0.3635 | time 465.8959 ms | tok/sec 1125332.9551
for step 1453 | loss 4.133288 | norm 0.3872 | time 465.0486 ms | tok/sec 1127383.3630
for step 1454 | loss 4.125388 | norm 0.3826 | time 465.7204 ms | tok/sec 1125756.9627
for step 1455 | loss 4.109477 | norm 0.3835 | time 465.3909 ms | tok/sec 1126553.9931
for step 1456 | loss 4.136237 | norm 0.4107 | time 465.2829 ms | tok/sec 1126815.4940
for step 1457 | loss 4.112702 | norm 0.4354 | time 465.8382 ms | tok/sec 1125472.3354
for step 1458 | loss 4.109474 | norm 0.4323 | time 465.0803 ms | tok/sec 1127306.4968
for step 1459 | loss 4.086016 | norm 0.4948 | time 465.9178 ms | tok/sec 1125279.9766
for step 1460 | loss 4.087574 | norm 0.4721 | time 464.7703 ms | tok/sec 1128058.2704
for step 1461 | loss 4.154125 | norm 0.4361 | time 465.8360 ms | tok/sec 1125477.5196
for step 1462 | loss 4.182317 | norm 0.4328 | time 464.8831 ms | tok/sec 1127784.6244
for step 1463 | loss 4.062804 | norm 0.4679 | time 465.3864 ms | tok/sec 1126564.9587
for step 1464 | loss 4.067682 | norm 0.5483 | time 465.8437 ms | tok/sec 1125459.0870
for step 1465 | loss 4.071209 | norm 0.5501 | time 466.6133 ms | tok/sec 1123602.7953
for step 1466 | loss 4.052093 | norm 0.6233 | time 465.3177 ms | tok/sec 1126731.2001
for step 1467 | loss 4.080498 | norm 0.6020 | time 465.3575 ms | tok/sec 1126634.7972
for step 1468 | loss 4.082726 | norm 0.5687 | time 464.9749 ms | tok/sec 1127561.9873
for step 1469 | loss 4.084698 | norm 0.5118 | time 465.1523 ms | tok/sec 1127131.9974
for step 1470 | loss 4.103155 | norm 0.5310 | time 465.8744 ms | tok/sec 1125384.7867
for step 1471 | loss 4.085759 | norm 0.4985 | time 465.4856 ms | tok/sec 1126324.9188
for step 1472 | loss 4.050592 | norm 0.4745 | time 464.8981 ms | tok/sec 1127748.1869
for step 1473 | loss 4.019826 | norm 0.4039 | time 465.9557 ms | tok/sec 1125188.4277
for step 1474 | loss 4.000294 | norm 0.4202 | time 464.5040 ms | tok/sec 1128705.0187
for step 1475 | loss 4.036873 | norm 0.4368 | time 464.9041 ms | tok/sec 1127733.7282
for step 1476 | loss 4.012532 | norm 0.4418 | time 464.4825 ms | tok/sec 1128757.1615
for step 1477 | loss 3.933106 | norm 0.4174 | time 466.4652 ms | tok/sec 1123959.4313
for step 1478 | loss 3.992002 | norm 0.4177 | time 465.2357 ms | tok/sec 1126929.8306
for step 1479 | loss 4.027746 | norm 0.4231 | time 465.3609 ms | tok/sec 1126626.7163
for step 1480 | loss 3.969617 | norm 0.3910 | time 465.2119 ms | tok/sec 1126987.5851
for step 1481 | loss 3.963541 | norm 0.3871 | time 465.5075 ms | tok/sec 1126271.8469
for step 1482 | loss 3.952660 | norm 0.4218 | time 465.0705 ms | tok/sec 1127330.1912
for step 1483 | loss 3.897965 | norm 0.3977 | time 464.6666 ms | tok/sec 1128310.0493
for step 1484 | loss 3.957862 | norm 0.3811 | time 465.2450 ms | tok/sec 1126907.3079
for step 1485 | loss 3.915569 | norm 0.3616 | time 465.5950 ms | tok/sec 1126060.1857
for step 1486 | loss 4.109737 | norm 0.4110 | time 465.1496 ms | tok/sec 1127138.3524
for step 1487 | loss 4.137829 | norm 0.4548 | time 464.2971 ms | tok/sec 1129208.1072
for step 1488 | loss 4.119922 | norm 0.5204 | time 465.3974 ms | tok/sec 1126538.4108
for step 1489 | loss 4.172225 | norm 0.4869 | time 465.1990 ms | tok/sec 1127018.7750
for step 1490 | loss 4.078865 | norm 0.4502 | time 465.9371 ms | tok/sec 1125233.3366
for step 1491 | loss 4.080835 | norm 0.4751 | time 464.8943 ms | tok/sec 1127757.4406
for step 1492 | loss 4.192926 | norm 0.4674 | time 465.2853 ms | tok/sec 1126809.7201
for step 1493 | loss 4.132433 | norm 0.4824 | time 466.0344 ms | tok/sec 1124998.4681
for step 1494 | loss 4.090969 | norm 0.4781 | time 466.0583 ms | tok/sec 1124940.9172
for step 1495 | loss 4.095575 | norm 0.4250 | time 466.1374 ms | tok/sec 1124749.8906
for step 1496 | loss 4.084272 | norm 0.4441 | time 465.3647 ms | tok/sec 1126617.4811
for step 1497 | loss 4.134063 | norm 0.4427 | time 466.2127 ms | tok/sec 1124568.1303
for step 1498 | loss 4.179116 | norm 0.5167 | time 466.0039 ms | tok/sec 1125072.1418
for step 1499 | loss 4.113739 | norm 0.5071 | time 465.2724 ms | tok/sec 1126840.9001
validation loss 4.1115
HellaSwag accuracy: 2578/10042=0.2567
> Hello, I'm a language model, though, very few students will be struggling in math because these new language skills are not meant to do so.
I
> Hello, I'm a language model, so that we see the difference between us and our learning as a whole. But we want to consider a more complex and
> Hello, I'm a language model, which covers an abstract, and is based on the nature of human speech and emotion.
- 1.1 of 3
> Hello, I'm a language model, but I'm pretty sure why I'm sure that's getting used to it again. Maybe as much as I don't
> Hello, I'm a language model, and I can write some simple language for students with specific language skills. Students also learn more words than students can write.
> Hello, I'm a language model, something you're already talking about:
This tutorial explains just how to define it; then it's easy to sort out
> > Hello, I'm a language model, I say, when my brother is coming. I'm a computer science teacher, when I'm talking: "There are
Hello, I'm a language model, from Wikipedia. (I'm not suggesting a language model)<|endoftext|>The name "dummy" in the brain is also
> Hello, I'm a language model, but you find yourself, as well as others with multiple categories depending upon which the type of learners may have.
I
> Hello, I'm a language model, for example if we do not have anything better. Why do I'm talking about this?
The language is quite important
> Hello, I'm a language model, and I'll try to explain why many people in the world use this data in an article, especially from the sources used
> > Hello, I'm a language model, but I have a few ways, it turns out that any number may produce an effect.
This means that you'll
Hello, I'm a language model, which gives kids a great deal of challenges, with each of your child's favorite songs, then ask for an amazing new
> Hello, I'm a language model, and can be used as a tool.
The software developed in the fall is called "dynamics".
A
> Hello, I'm a language model, and I believe the basic skills in our language as well as on my part.
The first thing I learned was to
> Hello, I'm a language model, but I'm a good source of language learners. I'm a good site for I was so simple for me. I> 
Hello, I'm a language model, and can be used to define a list, that is to assign them to each section of the page; and the address
> Hello, I'm a language model, too, to be my favorite part you would have already achieved.
I'm only beginning to play an end in learning
> Hello, I'm a language model, which you'll learn with.
The world is just one of us who is not necessarily a good resource for any given> 
Hello, I'm a language model, it's not uncommon to find anywhere. The syntax behind the language in these languages is the same as the same name and
> > > Hello, I'm a language model, where people are in control networks. I think it may be that which I can share with in the public, but rather
Hello, I'm a language model, if you use your browser using Windows XP or SQL Server to find new, open up, or access any user (or
Hello, I'm a language model, however. This is what does it show and relates to the languages that come from being language. When you use the keyword
> > > Hello, I'm a language model, or other languages. We use our language vocabulary to understand the meaning of languages.
We use the word, including the
Hello, I'm a language model, and it makes me popular. What was the difference between the two people and one's personal needs?
I'm going
Hello, I'm a language model, but I'm not able to play an action. It's never too surprising which to say this is only because we use
> > > Hello, I'm a language model, which makes sense. If the group changes, we can use it when it's not being used.
A number of
Hello, I'm a language model, meaning it makes the web-based programming languages for students and students, but sometimes for those who go on to get more
Hello, I'm a language model, but the word is a type language. It is a grammar, but it is a language that is used to solve an
> Hello, I'm a language model, but my use is more complicated: I do not use the same and I don't want things to occur and a similar> 
Hello, I'm a language model, but it doesn't include the number of sounds used for me, but I'd need to find for a few simple questions
> Hello, I'm a language model, but did you use it to predict how to do that and what I learned.
It takes a minute, but it
for step 1500 | loss 4.140277 | norm 0.5146 | time 12861.5587 ms | tok/sec 40763.9551
for step 1501 | loss 4.126816 | norm 0.5539 | time 463.0089 ms | tok/sec 1132349.7711
for step 1502 | loss 4.106949 | norm 0.5865 | time 462.9653 ms | tok/sec 1132456.4856
for step 1503 | loss 4.068324 | norm 0.5596 | time 463.2020 ms | tok/sec 1131877.6697
for step 1504 | loss 4.075848 | norm 0.4650 | time 463.7816 ms | tok/sec 1130463.1434
for step 1505 | loss 4.079159 | norm 0.4483 | time 464.0131 ms | tok/sec 1129899.1351
for step 1506 | loss 4.136395 | norm 0.4522 | time 463.6571 ms | tok/sec 1130766.5815
for step 1507 | loss 4.082787 | norm 0.5046 | time 463.5122 ms | tok/sec 1131120.2167
for step 1508 | loss 4.090234 | norm 0.5003 | time 465.7288 ms | tok/sec 1125736.7921
for step 1509 | loss 4.036708 | norm 0.4514 | time 464.6041 ms | tok/sec 1128461.7497
for step 1510 | loss 4.083874 | norm 0.4292 | time 464.0615 ms | tok/sec 1129781.2931
for step 1511 | loss 4.013519 | norm 0.4468 | time 464.3810 ms | tok/sec 1129004.0357
for step 1512 | loss 4.090434 | norm 0.4583 | time 463.9418 ms | tok/sec 1130072.7502
for step 1513 | loss 4.015172 | norm 0.3892 | time 463.7449 ms | tok/sec 1130552.6465
for step 1514 | loss 4.044609 | norm 0.4309 | time 465.0936 ms | tok/sec 1127274.1352
for step 1515 | loss 4.024859 | norm 0.4230 | time 464.0360 ms | tok/sec 1129843.4038
for step 1516 | loss 4.016422 | norm 0.3906 | time 464.4325 ms | tok/sec 1128878.8465
for step 1517 | loss 4.021614 | norm 0.3743 | time 464.9711 ms | tok/sec 1127571.2380
for step 1518 | loss 4.002073 | norm 0.3656 | time 465.1694 ms | tok/sec 1127090.4028
for step 1519 | loss 3.999271 | norm 0.3909 | time 465.6203 ms | tok/sec 1125999.0668
for step 1520 | loss 4.027995 | norm 0.3876 | time 464.2358 ms | tok/sec 1129357.1492
for step 1521 | loss 4.029418 | norm 0.3968 | time 465.5898 ms | tok/sec 1126072.8716
for step 1522 | loss 3.937986 | norm 0.4394 | time 465.1151 ms | tok/sec 1127222.1294
Will loading at 0 from edu_fineweb10B/edufineweb_train_000009.npy
for step 1523 | loss 3.987512 | norm 0.4674 | time 1450.7372 ms | tok/sec 361394.1838
for step 1524 | loss 3.927306 | norm 0.3910 | time 462.6119 ms | tok/sec 1133321.4396
for step 1525 | loss 3.962826 | norm 0.3736 | time 465.3132 ms | tok/sec 1126742.1691
for step 1526 | loss 3.929635 | norm 0.3846 | time 465.0707 ms | tok/sec 1127329.6133
for step 1527 | loss 3.928985 | norm 0.4294 | time 464.8581 ms | tok/sec 1127845.3589
for step 1528 | loss 3.904564 | norm 0.4284 | time 465.2505 ms | tok/sec 1126894.0257
for step 1529 | loss 3.956276 | norm 0.4377 | time 464.9386 ms | tok/sec 1127649.8751
for step 1530 | loss 3.926015 | norm 0.4847 | time 464.8011 ms | tok/sec 1127983.6265
for step 1531 | loss 3.907920 | norm 0.5126 | time 466.6617 ms | tok/sec 1123486.2629
for step 1532 | loss 3.999390 | norm 0.4933 | time 464.8714 ms | tok/sec 1127812.9664
for step 1533 | loss 4.145327 | norm 0.5730 | time 464.8690 ms | tok/sec 1127818.7506
for step 1534 | loss 4.114559 | norm 0.6069 | time 465.1628 ms | tok/sec 1127106.5781
for step 1535 | loss 4.119240 | norm 0.5495 | time 465.4100 ms | tok/sec 1126507.8245
for step 1536 | loss 4.121055 | norm 0.4592 | time 465.2832 ms | tok/sec 1126814.9166
for step 1537 | loss 4.065486 | norm 0.4700 | time 465.2467 ms | tok/sec 1126903.2654
for step 1538 | loss 4.034592 | norm 0.4074 | time 466.3966 ms | tok/sec 1124124.9046
for step 1539 | loss 4.154366 | norm 0.4555 | time 465.3299 ms | tok/sec 1126701.7579
for step 1540 | loss 4.080947 | norm 0.4672 | time 466.1787 ms | tok/sec 1124650.3753
for step 1541 | loss 4.081205 | norm 0.4510 | time 466.0459 ms | tok/sec 1124970.8429
for step 1542 | loss 4.059958 | norm 0.4434 | time 465.1985 ms | tok/sec 1127019.9302
for step 1543 | loss 4.147401 | norm 0.4157 | time 465.7507 ms | tok/sec 1125683.7756
for step 1544 | loss 4.136769 | norm 0.4355 | time 467.3126 ms | tok/sec 1121921.4477
for step 1545 | loss 4.115649 | norm 0.4093 | time 466.9595 ms | tok/sec 1122769.8043
for step 1546 | loss 4.128412 | norm 0.4266 | time 466.6789 ms | tok/sec 1123444.9370
for step 1547 | loss 4.066247 | norm 0.4233 | time 466.9371 ms | tok/sec 1122823.6933
for step 1548 | loss 4.066484 | norm 0.4046 | time 466.8458 ms | tok/sec 1123043.3159
for step 1549 | loss 4.080762 | norm 0.3865 | time 468.5206 ms | tok/sec 1119028.6057
for step 1550 | loss 4.062609 | norm 0.3843 | time 467.5059 ms | tok/sec 1121457.4282
for step 1551 | loss 4.058261 | norm 0.3775 | time 466.4044 ms | tok/sec 1124105.9417
for step 1552 | loss 4.024982 | norm 0.4314 | time 467.1457 ms | tok/sec 1122322.2667
for step 1553 | loss 4.114290 | norm 0.4377 | time 467.6528 ms | tok/sec 1121105.2357
for step 1554 | loss 4.061300 | norm 0.4164 | time 466.3832 ms | tok/sec 1124157.0856
for step 1555 | loss 4.019084 | norm 0.4200 | time 466.8386 ms | tok/sec 1123060.5223
for step 1556 | loss 4.029100 | norm 0.4067 | time 466.5003 ms | tok/sec 1123874.9898
for step 1557 | loss 4.045500 | norm 0.4243 | time 467.2172 ms | tok/sec 1122150.4521
for step 1558 | loss 4.029000 | norm 0.4958 | time 465.9996 ms | tok/sec 1125082.5029
for step 1559 | loss 4.033751 | norm 0.5184 | time 467.1164 ms | tok/sec 1122392.7259
for step 1560 | loss 3.992223 | norm 0.5153 | time 465.8527 ms | tok/sec 1125437.1991
for step 1561 | loss 4.063403 | norm 0.4916 | time 466.4607 ms | tok/sec 1123970.3465
for step 1562 | loss 4.028964 | norm 0.4779 | time 467.9348 ms | tok/sec 1120429.4861
for step 1563 | loss 4.029294 | norm 0.4551 | time 465.7125 ms | tok/sec 1125775.9814
for step 1564 | loss 3.951880 | norm 0.5068 | time 466.9445 ms | tok/sec 1122805.9208
for step 1565 | loss 3.973572 | norm 0.4768 | time 466.6259 ms | tok/sec 1123572.3683
for step 1566 | loss 3.985858 | norm 0.4268 | time 466.2902 ms | tok/sec 1124381.2546
for step 1567 | loss 3.956985 | norm 0.4392 | time 467.0653 ms | tok/sec 1122515.3345
for step 1568 | loss 3.879373 | norm 0.4241 | time 466.1269 ms | tok/sec 1124775.2036
for step 1569 | loss 3.898957 | norm 0.4223 | time 467.2234 ms | tok/sec 1122135.5639
for step 1570 | loss 3.947529 | norm 0.3926 | time 465.8930 ms | tok/sec 1125339.8657
for step 1571 | loss 3.945769 | norm 0.4091 | time 466.6693 ms | tok/sec 1123467.8954
for step 1572 | loss 3.914107 | norm 0.3808 | time 466.2941 ms | tok/sec 1124372.0561
for step 1573 | loss 3.887935 | norm 0.3803 | time 466.2483 ms | tok/sec 1124482.4472
for step 1574 | loss 3.922184 | norm 0.4148 | time 466.5780 ms | tok/sec 1123687.7702
for step 1575 | loss 3.950289 | norm 0.3899 | time 466.4927 ms | tok/sec 1123893.3706
for step 1576 | loss 3.916352 | norm 0.3867 | time 465.9376 ms | tok/sec 1125232.1851
for step 1577 | loss 3.903728 | norm 0.3946 | time 466.9821 ms | tok/sec 1122715.3472
for step 1578 | loss 3.887100 | norm 0.4199 | time 466.0039 ms | tok/sec 1125072.1418
for step 1579 | loss 3.947274 | norm 0.4178 | time 466.4211 ms | tok/sec 1124065.7193
for step 1580 | loss 4.093617 | norm 0.4457 | time 466.3959 ms | tok/sec 1124126.6286
for step 1581 | loss 4.080604 | norm 0.4671 | time 466.8748 ms | tok/sec 1122973.3485
for step 1582 | loss 4.025044 | norm 0.4688 | time 466.0575 ms | tok/sec 1124942.6436
for step 1583 | loss 4.141393 | norm 0.4965 | time 466.5136 ms | tok/sec 1123842.8249
for step 1584 | loss 4.100987 | norm 0.5340 | time 466.3413 ms | tok/sec 1124258.2380
for step 1585 | loss 4.114518 | norm 0.5433 | time 466.6739 ms | tok/sec 1123456.9901
for step 1586 | loss 4.072633 | norm 0.5892 | time 466.6111 ms | tok/sec 1123607.9623
for step 1587 | loss 4.082307 | norm 0.5058 | time 467.0696 ms | tok/sec 1122505.0206
for step 1588 | loss 4.064336 | norm 0.4693 | time 466.4395 ms | tok/sec 1124021.4781
for step 1589 | loss 4.085997 | norm 0.4604 | time 466.8345 ms | tok/sec 1123070.2729
for step 1590 | loss 4.009338 | norm 0.4272 | time 466.6665 ms | tok/sec 1123474.7832
for step 1591 | loss 4.106725 | norm 0.3682 | time 466.2435 ms | tok/sec 1124493.9476
for step 1592 | loss 4.033516 | norm 0.3807 | time 466.7518 ms | tok/sec 1123269.3359
for step 1593 | loss 4.046884 | norm 0.3782 | time 466.1222 ms | tok/sec 1124786.7099
for step 1594 | loss 4.064811 | norm 0.3806 | time 467.3874 ms | tok/sec 1121741.7448
for step 1595 | loss 4.050791 | norm 0.3941 | time 466.6607 ms | tok/sec 1123488.5589
for step 1596 | loss 4.041468 | norm 0.3825 | time 467.4015 ms | tok/sec 1121707.9854
for step 1597 | loss 4.032464 | norm 0.4195 | time 466.2318 ms | tok/sec 1124522.1243
for step 1598 | loss 4.125925 | norm 0.4415 | time 466.8705 ms | tok/sec 1122983.6710
for step 1599 | loss 4.095081 | norm 0.4242 | time 467.0513 ms | tok/sec 1122549.1426
for step 1600 | loss 4.042016 | norm 0.4552 | time 466.4817 ms | tok/sec 1123919.7939
for step 1601 | loss 4.049690 | norm 0.4395 | time 466.8117 ms | tok/sec 1123125.3380
for step 1602 | loss 4.011762 | norm 0.4039 | time 466.5647 ms | tok/sec 1123719.9261
for step 1603 | loss 4.027204 | norm 0.3786 | time 466.3110 ms | tok/sec 1124331.2399
for step 1604 | loss 4.038302 | norm 0.3650 | time 466.4187 ms | tok/sec 1124071.4652
for step 1605 | loss 3.948241 | norm 0.3630 | time 466.8255 ms | tok/sec 1123092.0689
for step 1606 | loss 4.025405 | norm 0.3995 | time 466.9881 ms | tok/sec 1122701.0173
for step 1607 | loss 3.972062 | norm 0.3940 | time 466.5928 ms | tok/sec 1123652.1710
for step 1608 | loss 3.996277 | norm 0.4088 | time 466.2106 ms | tok/sec 1124573.3062
for step 1609 | loss 4.070822 | norm 0.4244 | time 466.2571 ms | tok/sec 1124461.1723
for step 1610 | loss 3.976388 | norm 0.4516 | time 466.5627 ms | tok/sec 1123724.5200
for step 1611 | loss 3.998428 | norm 0.4757 | time 466.6526 ms | tok/sec 1123508.0750
for step 1612 | loss 3.952119 | norm 0.4411 | time 466.1243 ms | tok/sec 1124781.5321
for step 1613 | loss 4.005811 | norm 0.4047 | time 466.4316 ms | tok/sec 1124040.4382
for step 1614 | loss 3.881535 | norm 0.3876 | time 466.2864 ms | tok/sec 1124390.4531
for step 1615 | loss 3.908461 | norm 0.3930 | time 466.4180 ms | tok/sec 1124073.1890
for step 1616 | loss 3.889761 | norm 0.4041 | time 466.3858 ms | tok/sec 1124150.7642
for step 1617 | loss 3.888847 | norm 0.3806 | time 466.1324 ms | tok/sec 1124761.9717
for step 1618 | loss 3.927333 | norm 0.4232 | time 469.2423 ms | tok/sec 1117307.5437
for step 1619 | loss 3.889951 | norm 0.4115 | time 465.1847 ms | tok/sec 1127053.4325
for step 1620 | loss 3.875592 | norm 0.4302 | time 464.4060 ms | tok/sec 1128943.1764
for step 1621 | loss 3.880066 | norm 0.4372 | time 465.7454 ms | tok/sec 1125696.4530
for step 1622 | loss 3.875217 | norm 0.4121 | time 465.9030 ms | tok/sec 1125315.6790
for step 1623 | loss 3.931260 | norm 0.4530 | time 465.9314 ms | tok/sec 1125247.1555
for step 1624 | loss 3.827300 | norm 0.4990 | time 465.0745 ms | tok/sec 1127320.3666
for step 1625 | loss 3.964635 | norm 0.4883 | time 465.1823 ms | tok/sec 1127059.2090
for step 1626 | loss 4.127094 | norm 0.5148 | time 465.1139 ms | tok/sec 1127225.0185
for step 1627 | loss 4.060040 | norm 0.5564 | time 465.6084 ms | tok/sec 1126027.8957
for step 1628 | loss 4.086836 | norm 0.5271 | time 465.9710 ms | tok/sec 1125151.5820
for step 1629 | loss 4.169443 | norm 0.5037 | time 464.8430 ms | tok/sec 1127881.8027
for step 1630 | loss 4.062519 | norm 0.5162 | time 466.0094 ms | tok/sec 1125058.9028
for step 1631 | loss 4.099877 | norm 0.5005 | time 465.0888 ms | tok/sec 1127285.6927
for step 1632 | loss 4.059916 | norm 0.4549 | time 465.4036 ms | tok/sec 1126523.4060
for step 1633 | loss 4.094440 | norm 0.4658 | time 465.6203 ms | tok/sec 1125999.0668
for step 1634 | loss 4.036912 | norm 0.4154 | time 465.2891 ms | tok/sec 1126800.4818
for step 1635 | loss 4.051073 | norm 0.4256 | time 465.4870 ms | tok/sec 1126321.4574
for step 1636 | loss 4.139062 | norm 0.4252 | time 464.1962 ms | tok/sec 1129453.4384
for step 1637 | loss 4.017099 | norm 0.4085 | time 466.1181 ms | tok/sec 1124796.4905
for step 1638 | loss 3.986224 | norm 0.3785 | time 465.7450 ms | tok/sec 1125697.6055
for step 1639 | loss 3.991672 | norm 0.3787 | time 465.9450 ms | tok/sec 1125214.3363
for step 1640 | loss 4.010973 | norm 0.3728 | time 465.9171 ms | tok/sec 1125281.7041
for step 1641 | loss 4.015172 | norm 0.3627 | time 466.3994 ms | tok/sec 1124118.0089
for step 1642 | loss 4.030085 | norm 0.3726 | time 466.6376 ms | tok/sec 1123544.2390
for step 1643 | loss 4.019744 | norm 0.3801 | time 466.1741 ms | tok/sec 1124661.3039
for step 1644 | loss 3.985689 | norm 0.3597 | time 465.4391 ms | tok/sec 1126437.4248
for step 1645 | loss 4.014093 | norm 0.3649 | time 465.2863 ms | tok/sec 1126807.4105
for step 1646 | loss 3.960042 | norm 0.4002 | time 465.3406 ms | tok/sec 1126675.7809
for step 1647 | loss 4.023063 | norm 0.3847 | time 464.4704 ms | tok/sec 1128786.7111
for step 1648 | loss 4.014832 | norm 0.4049 | time 465.1296 ms | tok/sec 1127186.8837
for step 1649 | loss 4.003353 | norm 0.4239 | time 464.9115 ms | tok/sec 1127715.7999
for step 1650 | loss 3.938100 | norm 0.4555 | time 465.7166 ms | tok/sec 1125766.1838
for step 1651 | loss 4.010827 | norm 0.4550 | time 465.3351 ms | tok/sec 1126689.0579
for step 1652 | loss 3.971769 | norm 0.4693 | time 465.4627 ms | tok/sec 1126380.3035
for step 1653 | loss 3.974934 | norm 0.4689 | time 465.3599 ms | tok/sec 1126629.0251
for step 1654 | loss 3.986301 | norm 0.3969 | time 464.8354 ms | tok/sec 1127900.3147
for step 1655 | loss 3.931527 | norm 0.3964 | time 464.3786 ms | tok/sec 1129009.8322
for step 1656 | loss 3.952922 | norm 0.4453 | time 465.4641 ms | tok/sec 1126376.8418
for step 1657 | loss 3.945850 | norm 0.4087 | time 465.1852 ms | tok/sec 1127052.2772
for step 1658 | loss 4.008557 | norm 0.4370 | time 465.7872 ms | tok/sec 1125595.6178
for step 1659 | loss 3.935961 | norm 0.4407 | time 465.2827 ms | tok/sec 1126816.0714
for step 1660 | loss 3.857385 | norm 0.4838 | time 464.6196 ms | tok/sec 1128424.1103
for step 1661 | loss 3.945455 | norm 0.5189 | time 465.0736 ms | tok/sec 1127322.6782
for step 1662 | loss 3.946701 | norm 0.5128 | time 465.6904 ms | tok/sec 1125829.5830
for step 1663 | loss 3.883576 | norm 0.5036 | time 465.6808 ms | tok/sec 1125852.6390
for step 1664 | loss 3.882732 | norm 0.4833 | time 465.1871 ms | tok/sec 1127047.6561
for step 1665 | loss 3.936161 | norm 0.4942 | time 465.5681 ms | tok/sec 1126125.3481
for step 1666 | loss 3.905662 | norm 0.4941 | time 465.3547 ms | tok/sec 1126641.7238
for step 1667 | loss 3.971761 | norm 0.4803 | time 465.4951 ms | tok/sec 1126301.8434
for step 1668 | loss 3.912129 | norm 0.4881 | time 465.0931 ms | tok/sec 1127275.2909
for step 1669 | loss 3.875650 | norm 0.4649 | time 464.5209 ms | tok/sec 1128663.8873
for step 1670 | loss 3.930324 | norm 0.4288 | time 465.7137 ms | tok/sec 1125773.0998
for step 1671 | loss 3.875161 | norm 0.3802 | time 465.7843 ms | tok/sec 1125602.5317
for step 1672 | loss 3.843986 | norm 0.3660 | time 464.9880 ms | tok/sec 1127530.1892
for step 1673 | loss 3.998297 | norm 0.3963 | time 464.9377 ms | tok/sec 1127652.1881
for step 1674 | loss 4.070687 | norm 0.4062 | time 464.4303 ms | tok/sec 1128884.0622
for step 1675 | loss 4.039708 | norm 0.3926 | time 466.0866 ms | tok/sec 1124872.4394
for step 1676 | loss 4.052693 | norm 0.4105 | time 465.1058 ms | tok/sec 1127244.6646
for step 1677 | loss 4.058780 | norm 0.4152 | time 465.0936 ms | tok/sec 1127274.1352
for step 1678 | loss 4.056372 | norm 0.3990 | time 465.5991 ms | tok/sec 1126050.3832
for step 1679 | loss 4.058210 | norm 0.3477 | time 465.2565 ms | tok/sec 1126879.5889
for step 1680 | loss 4.060048 | norm 0.3936 | time 465.0881 ms | tok/sec 1127287.4263
for step 1681 | loss 4.044958 | norm 0.3995 | time 465.3163 ms | tok/sec 1126734.6640
for step 1682 | loss 4.008105 | norm 0.3797 | time 466.5098 ms | tok/sec 1123852.0147
for step 1683 | loss 3.994502 | norm 0.4050 | time 466.1915 ms | tok/sec 1124619.3163
for step 1684 | loss 4.014132 | norm 0.3660 | time 465.2269 ms | tok/sec 1126951.1990
for step 1685 | loss 4.039067 | norm 0.3429 | time 465.8649 ms | tok/sec 1125407.8245
for step 1686 | loss 4.063377 | norm 0.4363 | time 465.6358 ms | tok/sec 1125961.5915
for step 1687 | loss 4.020948 | norm 0.5673 | time 466.7187 ms | tok/sec 1123349.0956
for step 1688 | loss 4.039720 | norm 0.5827 | time 465.0116 ms | tok/sec 1127472.9571
for step 1689 | loss 4.070440 | norm 0.4558 | time 465.1504 ms | tok/sec 1127136.6192
for step 1690 | loss 4.047361 | norm 0.4961 | time 466.4397 ms | tok/sec 1124020.9036
for step 1691 | loss 4.054160 | norm 0.4569 | time 465.3261 ms | tok/sec 1126710.9945
for step 1692 | loss 4.050847 | norm 0.5092 | time 465.7419 ms | tok/sec 1125705.0968
for step 1693 | loss 4.025126 | norm 0.4738 | time 465.7416 ms | tok/sec 1125705.6731
for step 1694 | loss 4.006417 | norm 0.4041 | time 465.4064 ms | tok/sec 1126516.4809
for step 1695 | loss 4.001122 | norm 0.3860 | time 464.6304 ms | tok/sec 1128398.0538
for step 1696 | loss 4.000628 | norm 0.4070 | time 466.0604 ms | tok/sec 1124935.7379
for step 1697 | loss 3.988747 | norm 0.4156 | time 465.1046 ms | tok/sec 1127247.5538
for step 1698 | loss 3.918649 | norm 0.4059 | time 464.6375 ms | tok/sec 1128380.6834
for step 1699 | loss 3.947353 | norm 0.3638 | time 465.5883 ms | tok/sec 1126076.3315
for step 1700 | loss 3.977221 | norm 0.3622 | time 465.1999 ms | tok/sec 1127016.4646
for step 1701 | loss 3.925125 | norm 0.3862 | time 466.1777 ms | tok/sec 1124652.6760
for step 1702 | loss 3.986120 | norm 0.3396 | time 465.1763 ms | tok/sec 1127073.6504
for step 1703 | loss 3.931430 | norm 0.3460 | time 464.7694 ms | tok/sec 1128060.5851
for step 1704 | loss 3.962722 | norm 0.3333 | time 465.2052 ms | tok/sec 1127003.7574
for step 1705 | loss 3.911687 | norm 0.3426 | time 466.7757 ms | tok/sec 1123211.9618
for step 1706 | loss 3.932083 | norm 0.3301 | time 464.6752 ms | tok/sec 1128289.2082
for step 1707 | loss 3.976901 | norm 0.3366 | time 465.6315 ms | tok/sec 1125971.9690
for step 1708 | loss 3.918042 | norm 0.3762 | time 465.3335 ms | tok/sec 1126693.0988
for step 1709 | loss 3.889678 | norm 0.4453 | time 464.8318 ms | tok/sec 1127908.9925
for step 1710 | loss 3.837660 | norm 0.4802 | time 465.0922 ms | tok/sec 1127277.6024
for step 1711 | loss 3.841372 | norm 0.4582 | time 465.0748 ms | tok/sec 1127319.7887
for step 1712 | loss 3.861481 | norm 0.5081 | time 466.5868 ms | tok/sec 1123666.5252
for step 1713 | loss 3.835086 | norm 0.5381 | time 465.1124 ms | tok/sec 1127228.4854
Will loading at 0 from edu_fineweb10B/edufineweb_train_000010.npy
for step 1714 | loss 3.879539 | norm 0.5103 | time 1429.4729 ms | tok/sec 366770.1511
for step 1715 | loss 3.859077 | norm 0.4876 | time 463.8772 ms | tok/sec 1130230.1531
for step 1716 | loss 3.837734 | norm 0.4857 | time 464.0720 ms | tok/sec 1129755.7542
for step 1717 | loss 3.852240 | norm 0.4313 | time 463.9156 ms | tok/sec 1130136.6354
for step 1718 | loss 3.879987 | norm 0.3838 | time 464.4659 ms | tok/sec 1128797.7202
for step 1719 | loss 3.895747 | norm 0.4055 | time 465.7836 ms | tok/sec 1125604.2601
for step 1720 | loss 4.046984 | norm 0.4115 | time 464.4890 ms | tok/sec 1128741.5181
for step 1721 | loss 4.080225 | norm 0.4041 | time 464.7298 ms | tok/sec 1128156.6534
for step 1722 | loss 4.065578 | norm 0.4117 | time 467.2084 ms | tok/sec 1122171.6397
for step 1723 | loss 4.036829 | norm 0.4855 | time 465.4155 ms | tok/sec 1126494.5518
for step 1724 | loss 4.047594 | norm 0.4084 | time 464.4654 ms | tok/sec 1128798.8791
for step 1725 | loss 4.000839 | norm 0.3638 | time 466.3594 ms | tok/sec 1124214.5563
for step 1726 | loss 4.031430 | norm 0.4201 | time 464.1287 ms | tok/sec 1129617.6321
for step 1727 | loss 4.002167 | norm 0.3964 | time 464.7245 ms | tok/sec 1128169.3865
for step 1728 | loss 4.037561 | norm 0.4031 | time 464.9212 ms | tok/sec 1127692.0892
for step 1729 | loss 4.010500 | norm 0.3778 | time 465.4415 ms | tok/sec 1126431.6547
for step 1730 | loss 4.028616 | norm 0.3727 | time 465.5113 ms | tok/sec 1126262.6175
for step 1731 | loss 3.976466 | norm 0.3394 | time 465.4231 ms | tok/sec 1126476.0858
for step 1732 | loss 3.997081 | norm 0.3663 | time 465.1291 ms | tok/sec 1127188.0393
for step 1733 | loss 3.993477 | norm 0.3718 | time 466.1736 ms | tok/sec 1124662.4542
for step 1734 | loss 3.995273 | norm 0.4116 | time 465.1864 ms | tok/sec 1127049.3890
for step 1735 | loss 3.986728 | norm 0.4734 | time 465.2014 ms | tok/sec 1127012.9990
for step 1736 | loss 4.031150 | norm 0.4645 | time 466.3358 ms | tok/sec 1124271.4581
for step 1737 | loss 3.960392 | norm 0.4271 | time 465.3604 ms | tok/sec 1126627.8707
for step 1738 | loss 3.947476 | norm 0.4154 | time 465.3730 ms | tok/sec 1126597.2796
for step 1739 | loss 4.016500 | norm 0.3860 | time 465.2467 ms | tok/sec 1126903.2654
for step 1740 | loss 3.957352 | norm 0.3990 | time 465.7555 ms | tok/sec 1125672.2509
for step 1741 | loss 3.941033 | norm 0.3633 | time 464.9231 ms | tok/sec 1127687.4629
for step 1742 | loss 4.000732 | norm 0.3606 | time 466.0673 ms | tok/sec 1124919.0494
for step 1743 | loss 3.936820 | norm 0.3708 | time 465.5919 ms | tok/sec 1126067.6819
for step 1744 | loss 3.944867 | norm 0.3666 | time 464.9920 ms | tok/sec 1127520.3611
for step 1745 | loss 3.929168 | norm 0.4016 | time 464.9084 ms | tok/sec 1127723.3182
for step 1746 | loss 3.913949 | norm 0.4182 | time 465.8995 ms | tok/sec 1125324.3170
for step 1747 | loss 3.933231 | norm 0.4125 | time 465.7454 ms | tok/sec 1125696.4530
for step 1748 | loss 3.947260 | norm 0.4277 | time 464.8414 ms | tok/sec 1127885.8522
for step 1749 | loss 3.938379 | norm 0.4571 | time 464.6776 ms | tok/sec 1128283.4191
validation loss 4.0015
HellaSwag accuracy: 2554/10042=0.2543
> Hello, I'm a language model, like I'm a native speaker. Also called as I'm my native speaker, I'll have my native speaker.

> Hello, I'm a language model, a game of science, and I'm still learning it like I'm studying a really long time ... I'm a bit
> Hello, I'm a language model, I hope this blog is a great resource for anyone interested, from my blog.
I'll use my website and then
> Hello, I'm a language model, and I'm very proud we'll take her to see from every school at college. With two other students, she's
> Hello, I'm a language model, is that you could use these skills to help your child get to know where they're going to learn. There are some
> > Hello, I'm a language model, but I do not know whether it is so easy for me to learn something in a programming or at least using the sameHello, I'm a language model, the things we've made with language? One of my favorite languages: Spanish, Spanish, Spanish and Portuguese: Spanish,

> > Hello, I'm a language model, but I think this is a game like these are a game changer's (a game, if that's easy or
Hello, I'm a language model, from an early 1950 to a 1970s. A study had found that some children and parents who were trained at each of
> Hello, I'm a language model, I know we're a bit more advanced to make you work, and I'm very different what a lot I can do> 
Hello, I'm a language model, so use it would not mean that they can be used to communicate a message. In a matter of being able to speak
> > > Hello, I'm a language model, and I was a bit concerned to find someone out there. I started wondering where I had a good time in the library
Hello, I'm a language model, and I'm a lot a bit complicated without the I like to do as well it.
These are just a simple
> > Hello, I'm a language model, and it got an I like the concept and not in the idea. Here is a link. In this article I'll
> > Hello, I'm a language model, I'll have to learn how to make words. My goal is to understand how the word is in a language. But
Hello, I'm a language model, and I'm not just looking at everything as a game and being a business environment. In fact, what's more I
Hello, I'm a language model, but, I'm going to learn programming languages and this in the meantime so we'll learn programming language (which is my
Hello, I'm a language model, yet why did you think you would try making a way faster?
I'm interested in using the above in this class
> > > > > Hello, I'm a language model, how many language is said?
My favorite program is a bit special. It means a language in any medium or medium
Hello, I'm a language model, and a few other languages – including languages. But a little bit more than a language, I'm also a good teacher
Hello, I'm a language model, though not a model but a learning system, I want to use other language in my classroom and I'm going to see
Hello, I'm a language model, but I wanted to say:
“He [the son of] the daughter of [the son of [his
Hello, I'm a language model, let's talk (all of the) myself. She's going to take a quiz.
I'm a programming project
> > > > > Hello, I'm a language model, but it may not come from somewhere between the two languages.
For example, a bit of a language is a grammar
Hello, I'm a language model, and you have to change the two languages here.
So here's the difference is that they differ in the differences:
Hello, I'm a language model, you can have a hard-forward language without having any language.
So, you have a great idea that you can
Hello, I'm a language model, and even a very different language, but not because I'm actually playing chess and still learning languages, though I've played
Hello, I'm a language model, etc. In my own culture, each time I'm going to learn English.
In my native language, I did
> > > Hello, I'm a language model, especially the use of the languages in the language is also an exercise for the family and their children/teachers, in
Hello, I'm a language model, and here's why I see how the word has become the same word in the classroom. (I get to know the
Hello, I'm a language model, I want to add one language and it's an app. It sounds very nice, but it's only when my first
> Hello, I'm a language model, and my teacher and her teacher can help your child learn to read a book and learn the reading skills your kid’
for step 1750 | loss 3.933550 | norm 0.3842 | time 12928.6222 ms | tok/sec 40552.5036
for step 1751 | loss 3.959524 | norm 0.3846 | time 461.9725 ms | tok/sec 1134890.1251
for step 1752 | loss 3.975165 | norm 0.3872 | time 462.5418 ms | tok/sec 1133493.1868
for step 1753 | loss 3.925751 | norm 0.3733 | time 463.8801 ms | tok/sec 1130223.1823
for step 1754 | loss 3.853610 | norm 0.3551 | time 463.6345 ms | tok/sec 1130821.8224
for step 1755 | loss 3.820640 | norm 0.3407 | time 464.6263 ms | tok/sec 1128407.8972
for step 1756 | loss 3.866489 | norm 0.3539 | time 464.0901 ms | tok/sec 1129711.6443
for step 1757 | loss 3.833812 | norm 0.3887 | time 463.6607 ms | tok/sec 1130757.8597
for step 1758 | loss 3.804577 | norm 0.3979 | time 464.5438 ms | tok/sec 1128608.2778
for step 1759 | loss 3.819057 | norm 0.3999 | time 465.4865 ms | tok/sec 1126322.6112
for step 1760 | loss 3.840582 | norm 0.4965 | time 467.4439 ms | tok/sec 1121606.1473
for step 1761 | loss 3.815455 | norm 0.6167 | time 464.5841 ms | tok/sec 1128510.3951
for step 1762 | loss 3.849558 | norm 0.5264 | time 463.8669 ms | tok/sec 1130255.1325
for step 1763 | loss 3.876891 | norm 0.4235 | time 463.7973 ms | tok/sec 1130424.7893
for step 1764 | loss 3.817514 | norm 0.4458 | time 463.9747 ms | tok/sec 1129992.6135
for step 1765 | loss 3.950408 | norm 0.4111 | time 464.6280 ms | tok/sec 1128403.8440
for step 1766 | loss 4.034395 | norm 0.3806 | time 464.5789 ms | tok/sec 1128523.1363
for step 1767 | loss 4.005762 | norm 0.4062 | time 464.8125 ms | tok/sec 1127955.8545
for step 1768 | loss 3.961991 | norm 0.3927 | time 464.1678 ms | tok/sec 1129522.4752
for step 1769 | loss 3.975726 | norm 0.3946 | time 464.6494 ms | tok/sec 1128351.7339
for step 1770 | loss 3.969516 | norm 0.4062 | time 463.9192 ms | tok/sec 1130127.9233
for step 1771 | loss 4.023403 | norm 0.4348 | time 465.3702 ms | tok/sec 1126604.2057
for step 1772 | loss 3.930873 | norm 0.3935 | time 464.3075 ms | tok/sec 1129182.5942
for step 1773 | loss 3.979929 | norm 0.3875 | time 464.1619 ms | tok/sec 1129536.9798
for step 1774 | loss 4.046387 | norm 0.4713 | time 464.7865 ms | tok/sec 1128018.9220
for step 1775 | loss 4.012751 | norm 0.4882 | time 466.1069 ms | tok/sec 1124823.5317
for step 1776 | loss 3.926369 | norm 0.4342 | time 465.8825 ms | tok/sec 1125365.2053
for step 1777 | loss 4.036909 | norm 0.3975 | time 465.1542 ms | tok/sec 1127127.3756
for step 1778 | loss 3.960353 | norm 0.5155 | time 465.7807 ms | tok/sec 1125611.1741
for step 1779 | loss 3.997234 | norm 0.4919 | time 464.8564 ms | tok/sec 1127849.4081
for step 1780 | loss 3.992920 | norm 0.4948 | time 465.1303 ms | tok/sec 1127185.1504
for step 1781 | loss 3.973316 | norm 0.4219 | time 465.0731 ms | tok/sec 1127323.8341
for step 1782 | loss 4.021138 | norm 0.3610 | time 465.4293 ms | tok/sec 1126461.0827
for step 1783 | loss 3.957366 | norm 0.3674 | time 465.2629 ms | tok/sec 1126863.9976
for step 1784 | loss 3.962430 | norm 0.3329 | time 464.7202 ms | tok/sec 1128179.8048
for step 1785 | loss 3.962808 | norm 0.3515 | time 465.4100 ms | tok/sec 1126507.8245
for step 1786 | loss 3.943572 | norm 0.4219 | time 466.1746 ms | tok/sec 1124660.1535
for step 1787 | loss 3.964027 | norm 0.3838 | time 464.8604 ms | tok/sec 1127839.5744
for step 1788 | loss 3.932500 | norm 0.3754 | time 465.0335 ms | tok/sec 1127419.7769
for step 1789 | loss 3.924939 | norm 0.3519 | time 464.4716 ms | tok/sec 1128783.8140
for step 1790 | loss 3.859803 | norm 0.3730 | time 466.0761 ms | tok/sec 1124897.7579
for step 1791 | loss 3.883077 | norm 0.3558 | time 465.8010 ms | tok/sec 1125562.2022
for step 1792 | loss 3.909278 | norm 0.3795 | time 465.1170 ms | tok/sec 1127217.5069
for step 1793 | loss 3.933257 | norm 0.3856 | time 465.8735 ms | tok/sec 1125387.0905
for step 1794 | loss 3.923311 | norm 0.4464 | time 465.0450 ms | tok/sec 1127392.0327
for step 1795 | loss 3.925492 | norm 0.4519 | time 468.2884 ms | tok/sec 1119583.5211
for step 1796 | loss 3.890843 | norm 0.4215 | time 464.8516 ms | tok/sec 1127860.9774
for step 1797 | loss 3.844575 | norm 0.4471 | time 465.4391 ms | tok/sec 1126437.4248
for step 1798 | loss 3.926703 | norm 0.4726 | time 464.9749 ms | tok/sec 1127561.9873
for step 1799 | loss 3.843024 | norm 0.4760 | time 465.7326 ms | tok/sec 1125727.5715
for step 1800 | loss 3.857036 | norm 0.4847 | time 465.1468 ms | tok/sec 1127145.2852
for step 1801 | loss 3.850033 | norm 0.4319 | time 464.7865 ms | tok/sec 1128018.9220
for step 1802 | loss 3.774444 | norm 0.4074 | time 464.6773 ms | tok/sec 1128283.9980
for step 1803 | loss 3.849147 | norm 0.4309 | time 464.7281 ms | tok/sec 1128160.7048
for step 1804 | loss 3.856490 | norm 0.4110 | time 464.3760 ms | tok/sec 1129016.2083
for step 1805 | loss 3.791059 | norm 0.3822 | time 464.5374 ms | tok/sec 1128623.9174
for step 1806 | loss 3.852668 | norm 0.3662 | time 465.1842 ms | tok/sec 1127054.5878
for step 1807 | loss 3.841824 | norm 0.3423 | time 465.2293 ms | tok/sec 1126945.4237
for step 1808 | loss 3.830094 | norm 0.3620 | time 465.7116 ms | tok/sec 1125778.2868
for step 1809 | loss 3.850681 | norm 0.3595 | time 465.7171 ms | tok/sec 1125765.0312
for step 1810 | loss 3.739372 | norm 0.3473 | time 464.4835 ms | tok/sec 1128754.8439
for step 1811 | loss 3.883254 | norm 0.3593 | time 465.1716 ms | tok/sec 1127085.2037
for step 1812 | loss 3.929193 | norm 0.3511 | time 464.8919 ms | tok/sec 1127763.2243
for step 1813 | loss 4.037150 | norm 0.4082 | time 464.7803 ms | tok/sec 1128033.9666
for step 1814 | loss 3.926092 | norm 0.4386 | time 465.4100 ms | tok/sec 1126507.8245
for step 1815 | loss 3.966482 | norm 0.4734 | time 465.1282 ms | tok/sec 1127190.3504
for step 1816 | loss 4.052702 | norm 0.5401 | time 465.1997 ms | tok/sec 1127017.0422
for step 1817 | loss 3.984841 | norm 0.5061 | time 464.5076 ms | tok/sec 1128696.3287
for step 1818 | loss 4.004984 | norm 0.4095 | time 464.7191 ms | tok/sec 1128182.6988
for step 1819 | loss 3.971608 | norm 0.4145 | time 465.9007 ms | tok/sec 1125321.4376
for step 1820 | loss 4.014241 | norm 0.4207 | time 465.1570 ms | tok/sec 1127120.4430
for step 1821 | loss 3.954699 | norm 0.4154 | time 465.0788 ms | tok/sec 1127309.9642
for step 1822 | loss 3.928617 | norm 0.4047 | time 465.1401 ms | tok/sec 1127161.4620
for step 1823 | loss 3.973207 | norm 0.4331 | time 464.3288 ms | tok/sec 1129130.9920
for step 1824 | loss 4.011444 | norm 0.4213 | time 465.9455 ms | tok/sec 1125213.1847
for step 1825 | loss 3.939332 | norm 0.3790 | time 465.1051 ms | tok/sec 1127246.3982
for step 1826 | loss 3.976243 | norm 0.3418 | time 464.8013 ms | tok/sec 1127983.0479
for step 1827 | loss 3.991245 | norm 0.3713 | time 466.6264 ms | tok/sec 1123571.2201
for step 1828 | loss 3.932077 | norm 0.3658 | time 465.3928 ms | tok/sec 1126549.3761
for step 1829 | loss 3.990776 | norm 0.3585 | time 465.4260 ms | tok/sec 1126469.1613
for step 1830 | loss 3.947808 | norm 0.3643 | time 465.7755 ms | tok/sec 1125623.8498
for step 1831 | loss 3.978223 | norm 0.3529 | time 465.5073 ms | tok/sec 1126272.4237
for step 1832 | loss 4.002358 | norm 0.4073 | time 465.9629 ms | tok/sec 1125171.1560
for step 1833 | loss 4.037597 | norm 0.4622 | time 465.0712 ms | tok/sec 1127328.4575
for step 1834 | loss 3.990556 | norm 0.5171 | time 466.0521 ms | tok/sec 1124955.8799
for step 1835 | loss 3.913668 | norm 0.4669 | time 465.5595 ms | tok/sec 1126146.1094
for step 1836 | loss 3.911158 | norm 0.3920 | time 465.3349 ms | tok/sec 1126689.6352
for step 1837 | loss 3.871356 | norm 0.3763 | time 465.2011 ms | tok/sec 1127013.5766
for step 1838 | loss 3.898207 | norm 0.3687 | time 465.3778 ms | tok/sec 1126585.7362
for step 1839 | loss 3.912170 | norm 0.3412 | time 464.6471 ms | tok/sec 1128357.5237
for step 1840 | loss 3.915783 | norm 0.3629 | time 465.0760 ms | tok/sec 1127316.8991
for step 1841 | loss 3.888917 | norm 0.3292 | time 465.0295 ms | tok/sec 1127429.6033
for step 1842 | loss 3.960505 | norm 0.3414 | time 465.3347 ms | tok/sec 1126690.2124
for step 1843 | loss 3.935688 | norm 0.3639 | time 465.5035 ms | tok/sec 1126281.6533
for step 1844 | loss 3.894853 | norm 0.3852 | time 464.9172 ms | tok/sec 1127701.9204
for step 1845 | loss 3.902884 | norm 0.3722 | time 466.7046 ms | tok/sec 1123382.9539
for step 1846 | loss 3.796746 | norm 0.3242 | time 465.9212 ms | tok/sec 1125271.9151
for step 1847 | loss 3.777743 | norm 0.3160 | time 465.2574 ms | tok/sec 1126877.2791
for step 1848 | loss 3.802199 | norm 0.3565 | time 465.1790 ms | tok/sec 1127067.2961
for step 1849 | loss 3.802441 | norm 0.3594 | time 465.1020 ms | tok/sec 1127253.9101
for step 1850 | loss 3.761780 | norm 0.4223 | time 465.2038 ms | tok/sec 1127007.2230
for step 1851 | loss 3.769051 | norm 0.4119 | time 465.1630 ms | tok/sec 1127106.0004
for step 1852 | loss 3.823652 | norm 0.4067 | time 464.4899 ms | tok/sec 1128739.2006
for step 1853 | loss 3.879237 | norm 0.4962 | time 465.5533 ms | tok/sec 1126161.1041
for step 1854 | loss 3.767810 | norm 0.5302 | time 465.2829 ms | tok/sec 1126815.4940
for step 1855 | loss 3.773363 | norm 0.4943 | time 465.6870 ms | tok/sec 1125837.6525
for step 1856 | loss 3.809703 | norm 0.4228 | time 465.9929 ms | tok/sec 1125098.6206
for step 1857 | loss 3.901333 | norm 0.4124 | time 464.5975 ms | tok/sec 1128477.9644
for step 1858 | loss 3.926086 | norm 0.3876 | time 465.8098 ms | tok/sec 1125540.8863
for step 1859 | loss 3.946692 | norm 0.3959 | time 466.3119 ms | tok/sec 1124328.9405
for step 1860 | loss 3.958500 | norm 0.4006 | time 464.6928 ms | tok/sec 1128246.3705
for step 1861 | loss 3.960268 | norm 0.3976 | time 465.8060 ms | tok/sec 1125550.1039
for step 1862 | loss 3.969014 | norm 0.4077 | time 465.9696 ms | tok/sec 1125155.0362
for step 1863 | loss 3.954350 | norm 0.3932 | time 466.7461 ms | tok/sec 1123283.1065
for step 1864 | loss 3.996157 | norm 0.4270 | time 465.5054 ms | tok/sec 1126277.0385
for step 1865 | loss 3.933700 | norm 0.4428 | time 465.1630 ms | tok/sec 1127106.0004
for step 1866 | loss 4.018501 | norm 0.3746 | time 465.1890 ms | tok/sec 1127043.0350
for step 1867 | loss 3.972192 | norm 0.4095 | time 465.3587 ms | tok/sec 1126631.9111
for step 1868 | loss 3.989768 | norm 0.4370 | time 466.5885 ms | tok/sec 1123662.5060
for step 1869 | loss 3.989552 | norm 0.3833 | time 465.6718 ms | tok/sec 1125874.5431
for step 1870 | loss 3.963042 | norm 0.3955 | time 465.5812 ms | tok/sec 1126093.6310
for step 1871 | loss 3.984694 | norm 0.3688 | time 465.2719 ms | tok/sec 1126842.0550
for step 1872 | loss 3.924159 | norm 0.3879 | time 465.1346 ms | tok/sec 1127174.7505
for step 1873 | loss 3.968972 | norm 0.3780 | time 465.6723 ms | tok/sec 1125873.3903
for step 1874 | loss 4.004646 | norm 0.4104 | time 464.1891 ms | tok/sec 1129470.8419
for step 1875 | loss 3.927692 | norm 0.3965 | time 465.2371 ms | tok/sec 1126926.3655
for step 1876 | loss 3.998778 | norm 0.3933 | time 464.7532 ms | tok/sec 1128099.9364
for step 1877 | loss 3.950691 | norm 0.3771 | time 464.3362 ms | tok/sec 1129113.0192
for step 1878 | loss 3.916895 | norm 0.3895 | time 464.7834 ms | tok/sec 1128026.4442
for step 1879 | loss 3.946331 | norm 0.3953 | time 466.3932 ms | tok/sec 1124132.9497
for step 1880 | loss 3.991551 | norm 0.4264 | time 465.4148 ms | tok/sec 1126496.2830
for step 1881 | loss 3.881996 | norm 0.4187 | time 465.2872 ms | tok/sec 1126805.1009
for step 1882 | loss 3.909972 | norm 0.5033 | time 465.1752 ms | tok/sec 1127076.5387
for step 1883 | loss 3.970842 | norm 0.5985 | time 465.3192 ms | tok/sec 1126727.7362
for step 1884 | loss 3.925095 | norm 0.5916 | time 465.9297 ms | tok/sec 1125251.1860
for step 1885 | loss 3.933085 | norm 0.4925 | time 466.4202 ms | tok/sec 1124068.0177
for step 1886 | loss 3.861355 | norm 0.4238 | time 465.5333 ms | tok/sec 1126209.5514
for step 1887 | loss 3.852952 | norm 0.3921 | time 465.3594 ms | tok/sec 1126630.1795
for step 1888 | loss 3.888029 | norm 0.3785 | time 465.6761 ms | tok/sec 1125864.1674
for step 1889 | loss 3.834858 | norm 0.3918 | time 465.1363 ms | tok/sec 1127170.7061
for step 1890 | loss 4.033359 | norm 0.3888 | time 464.8929 ms | tok/sec 1127760.9108
for step 1891 | loss 3.877641 | norm 0.3929 | time 465.4789 ms | tok/sec 1126341.0721
for step 1892 | loss 3.888604 | norm 0.3546 | time 465.5037 ms | tok/sec 1126281.0764
for step 1893 | loss 3.838575 | norm 0.3454 | time 464.3314 ms | tok/sec 1129124.6145
for step 1894 | loss 3.817675 | norm 0.3246 | time 466.2981 ms | tok/sec 1124362.2830
for step 1895 | loss 3.951955 | norm 0.3470 | time 465.0629 ms | tok/sec 1127348.6852
for step 1896 | loss 3.819637 | norm 0.4272 | time 464.7710 ms | tok/sec 1128056.5343
for step 1897 | loss 3.752610 | norm 0.4642 | time 464.2725 ms | tok/sec 1129267.8352
for step 1898 | loss 3.801462 | norm 0.4067 | time 465.6284 ms | tok/sec 1125979.4640
for step 1899 | loss 3.807702 | norm 0.3580 | time 464.6566 ms | tok/sec 1128334.3650
for step 1900 | loss 3.788776 | norm 0.3305 | time 466.5413 ms | tok/sec 1123776.2036
for step 1901 | loss 3.786206 | norm 0.3602 | time 465.1563 ms | tok/sec 1127122.1762
for step 1902 | loss 3.794023 | norm 0.3590 | time 464.9949 ms | tok/sec 1127513.4237
for step 1903 | loss 3.757966 | norm 0.3705 | time 464.7307 ms | tok/sec 1128154.3383
Will loading at 0 from edu_fineweb10B/edufineweb_train_000011.npy
for step 1904 | loss 3.873982 | norm 0.3739 | time 1446.1432 ms | tok/sec 362542.2558
for step 1905 | loss 3.964693 | norm 0.3792 | time 465.1122 ms | tok/sec 1127229.0632
for step 1906 | loss 3.949773 | norm 0.4054 | time 464.3414 ms | tok/sec 1129100.2648
for step 1907 | loss 3.922603 | norm 0.4175 | time 464.9260 ms | tok/sec 1127680.5234
for step 1908 | loss 3.976115 | norm 0.4142 | time 463.5050 ms | tok/sec 1131137.6715
for step 1909 | loss 3.967514 | norm 0.3724 | time 464.3841 ms | tok/sec 1128996.5004
for step 1910 | loss 3.977489 | norm 0.3614 | time 464.4792 ms | tok/sec 1128765.2730
for step 1911 | loss 3.994758 | norm 0.3878 | time 465.0486 ms | tok/sec 1127383.3630
for step 1912 | loss 3.935976 | norm 0.3763 | time 465.3172 ms | tok/sec 1126732.3547
for step 1913 | loss 3.925625 | norm 0.4045 | time 464.9711 ms | tok/sec 1127571.2380
for step 1914 | loss 3.982899 | norm 0.4679 | time 464.9184 ms | tok/sec 1127699.0289
for step 1915 | loss 3.912966 | norm 0.4579 | time 465.1539 ms | tok/sec 1127127.9533
for step 1916 | loss 3.885617 | norm 0.4306 | time 465.5724 ms | tok/sec 1126114.9678
for step 1917 | loss 3.922229 | norm 0.4296 | time 464.6583 ms | tok/sec 1128330.3123
for step 1918 | loss 3.970636 | norm 0.4035 | time 465.0221 ms | tok/sec 1127447.5225
for step 1919 | loss 3.931444 | norm 0.4129 | time 464.7212 ms | tok/sec 1128177.4896
for step 1920 | loss 3.958045 | norm 0.4356 | time 464.9894 ms | tok/sec 1127526.7204
for step 1921 | loss 3.936207 | norm 0.3765 | time 465.2746 ms | tok/sec 1126835.7033
for step 1922 | loss 3.961123 | norm 0.3840 | time 465.7691 ms | tok/sec 1125639.4069
for step 1923 | loss 3.920915 | norm 0.3928 | time 465.6012 ms | tok/sec 1126045.1937
for step 1924 | loss 3.901345 | norm 0.3781 | time 465.1368 ms | tok/sec 1127169.5506
for step 1925 | loss 3.935072 | norm 0.3607 | time 465.2667 ms | tok/sec 1126854.7585
for step 1926 | loss 3.922374 | norm 0.3569 | time 464.6752 ms | tok/sec 1128289.2082
for step 1927 | loss 3.888180 | norm 0.3942 | time 464.3998 ms | tok/sec 1128958.2457
for step 1928 | loss 3.900665 | norm 0.3590 | time 465.9050 ms | tok/sec 1125311.0721
for step 1929 | loss 3.862479 | norm 0.3727 | time 465.3151 ms | tok/sec 1126737.5506
for step 1930 | loss 3.893765 | norm 0.3394 | time 465.2996 ms | tok/sec 1126775.0776
for step 1931 | loss 3.902390 | norm 0.3295 | time 465.3685 ms | tok/sec 1126608.2460
for step 1932 | loss 3.877379 | norm 0.3092 | time 465.7404 ms | tok/sec 1125708.5544
for step 1933 | loss 3.873338 | norm 0.3223 | time 465.1868 ms | tok/sec 1127048.2337
for step 1934 | loss 3.901799 | norm 0.3134 | time 464.8800 ms | tok/sec 1127792.1436
for step 1935 | loss 3.879778 | norm 0.3122 | time 464.4384 ms | tok/sec 1128864.3588
for step 1936 | loss 3.924738 | norm 0.3490 | time 468.2982 ms | tok/sec 1119560.1512
for step 1937 | loss 3.868223 | norm 0.3953 | time 464.4182 ms | tok/sec 1128913.6185
for step 1938 | loss 3.849481 | norm 0.3897 | time 465.0097 ms | tok/sec 1127477.5817
for step 1939 | loss 3.866655 | norm 0.4093 | time 465.8179 ms | tok/sec 1125521.2995
for step 1940 | loss 3.773523 | norm 0.4631 | time 464.7088 ms | tok/sec 1128207.5878
for step 1941 | loss 3.804248 | norm 0.4326 | time 464.7243 ms | tok/sec 1128169.9653
for step 1942 | loss 3.789375 | norm 0.4450 | time 465.1062 ms | tok/sec 1127243.5090
for step 1943 | loss 3.797199 | norm 0.4530 | time 465.4016 ms | tok/sec 1126528.0228
for step 1944 | loss 3.764195 | norm 0.3890 | time 465.1163 ms | tok/sec 1127219.2403
for step 1945 | loss 3.804296 | norm 0.4057 | time 465.8020 ms | tok/sec 1125559.8977
for step 1946 | loss 3.769019 | norm 0.3567 | time 465.4253 ms | tok/sec 1126470.8924
for step 1947 | loss 3.775184 | norm 0.3440 | time 464.4263 ms | tok/sec 1128893.9141
for step 1948 | loss 3.782615 | norm 0.3574 | time 464.5224 ms | tok/sec 1128660.4116
for step 1949 | loss 3.712898 | norm 0.3495 | time 466.1505 ms | tok/sec 1124718.2509
for step 1950 | loss 3.746219 | norm 0.3497 | time 465.4043 ms | tok/sec 1126521.6747
for step 1951 | loss 3.804294 | norm 0.3225 | time 464.7367 ms | tok/sec 1128139.8692
for step 1952 | loss 3.952828 | norm 0.3333 | time 464.0408 ms | tok/sec 1129831.7938
for step 1953 | loss 3.980924 | norm 0.3731 | time 465.6210 ms | tok/sec 1125997.3371
for step 1954 | loss 3.913515 | norm 0.3491 | time 464.6144 ms | tok/sec 1128436.8495
for step 1955 | loss 3.910954 | norm 0.3126 | time 465.7922 ms | tok/sec 1125583.5188
for step 1956 | loss 3.986462 | norm 0.3162 | time 464.3826 ms | tok/sec 1128999.9782
for step 1957 | loss 3.936474 | norm 0.3731 | time 464.7262 ms | tok/sec 1128165.3350
for step 1958 | loss 3.901294 | norm 0.4484 | time 465.2441 ms | tok/sec 1126909.6179
for step 1959 | loss 3.918313 | norm 0.4691 | time 464.6621 ms | tok/sec 1128321.0491
for step 1960 | loss 3.957063 | norm 0.4038 | time 464.3028 ms | tok/sec 1129194.1909
for step 1961 | loss 4.000736 | norm 0.4276 | time 464.4978 ms | tok/sec 1128720.0817
for step 1962 | loss 3.960306 | norm 0.4500 | time 465.1892 ms | tok/sec 1127042.4574
for step 1963 | loss 3.949202 | norm 0.4389 | time 464.9994 ms | tok/sec 1127502.4396
for step 1964 | loss 3.917574 | norm 0.4523 | time 464.7672 ms | tok/sec 1128065.7932
for step 1965 | loss 3.905221 | norm 0.4398 | time 465.1735 ms | tok/sec 1127080.5824
for step 1966 | loss 3.934134 | norm 0.4290 | time 465.0621 ms | tok/sec 1127350.4190
for step 1967 | loss 3.988977 | norm 0.4326 | time 464.6339 ms | tok/sec 1128389.3685
for step 1968 | loss 3.979970 | norm 0.4168 | time 464.4942 ms | tok/sec 1128728.7720
for step 1969 | loss 3.911959 | norm 0.3729 | time 465.0857 ms | tok/sec 1127293.2052
for step 1970 | loss 3.935230 | norm 0.3487 | time 465.7297 ms | tok/sec 1125734.4869
for step 1971 | loss 3.930033 | norm 0.3497 | time 465.3227 ms | tok/sec 1126719.0767
for step 1972 | loss 3.891081 | norm 0.3332 | time 465.2493 ms | tok/sec 1126896.9131
for step 1973 | loss 3.906379 | norm 0.3439 | time 465.3964 ms | tok/sec 1126540.7193
for step 1974 | loss 3.954150 | norm 0.3496 | time 465.2050 ms | tok/sec 1127004.3350
for step 1975 | loss 3.847089 | norm 0.4056 | time 464.6802 ms | tok/sec 1128277.0512
for step 1976 | loss 3.940830 | norm 0.3791 | time 465.2083 ms | tok/sec 1126996.2488
for step 1977 | loss 3.901212 | norm 0.3624 | time 465.8546 ms | tok/sec 1125432.5912
for step 1978 | loss 3.843011 | norm 0.3858 | time 464.7455 ms | tok/sec 1128118.4556
for step 1979 | loss 3.828983 | norm 0.3833 | time 465.8186 ms | tok/sec 1125519.5713
for step 1980 | loss 3.800416 | norm 0.3723 | time 465.1632 ms | tok/sec 1127105.4227
for step 1981 | loss 3.886459 | norm 0.3397 | time 464.9818 ms | tok/sec 1127545.2208
for step 1982 | loss 3.862609 | norm 0.3684 | time 464.9723 ms | tok/sec 1127568.3472
for step 1983 | loss 3.842556 | norm 0.3391 | time 465.3230 ms | tok/sec 1126718.4994
for step 1984 | loss 3.883930 | norm 0.3380 | time 466.3727 ms | tok/sec 1124182.3720
for step 1985 | loss 3.840295 | norm 0.3446 | time 466.6424 ms | tok/sec 1123532.7581
for step 1986 | loss 3.749199 | norm 0.3372 | time 465.6265 ms | tok/sec 1125984.0764
for step 1987 | loss 3.743818 | norm 0.3383 | time 466.3441 ms | tok/sec 1124251.3407
for step 1988 | loss 3.894723 | norm 0.3601 | time 465.5945 ms | tok/sec 1126061.3390
for step 1989 | loss 3.741822 | norm 0.3955 | time 465.4698 ms | tok/sec 1126362.9952
for step 1990 | loss 3.754607 | norm 0.4271 | time 466.0881 ms | tok/sec 1124868.9869
for step 1991 | loss 3.753699 | norm 0.3853 | time 465.6684 ms | tok/sec 1125882.6133
for step 1992 | loss 3.797956 | norm 0.4058 | time 465.4350 ms | tok/sec 1126447.2340
for step 1993 | loss 3.739239 | norm 0.3944 | time 465.7371 ms | tok/sec 1125716.6222
for step 1994 | loss 3.805132 | norm 0.4216 | time 466.3739 ms | tok/sec 1124179.4985
for step 1995 | loss 3.758746 | norm 0.4219 | time 465.7130 ms | tok/sec 1125774.8288
for step 1996 | loss 3.763715 | norm 0.3530 | time 465.6727 ms | tok/sec 1125872.2374
for step 1997 | loss 3.898556 | norm 0.3706 | time 465.3778 ms | tok/sec 1126585.7362
for step 1998 | loss 3.928975 | norm 0.4246 | time 466.2466 ms | tok/sec 1124486.4723
for step 1999 | loss 3.936639 | norm 0.4345 | time 465.4684 ms | tok/sec 1126366.4568
validation loss 3.9249
HellaSwag accuracy: 2524/10042=0.2513
> Hello, I'm a language model, but I'm a fan of the best way to teach, like the language my teacher will love about. My teacher is
> Hello, I'm a language model, which I am really interested in. The original language is more than just a dictionary, is generally only one language.

> Hello, I'm a language model, so go to the same language model. I'm thinking that "language is one language model. But that is just one
>>  Hello, I'm a language model, really good. What I like to mean, is that it means that I can write a language model, as well asHello, I'm a language model, but I'm trying to put the right mix of that definition of dialect as either a subset of language or a subset of

> Hello, I'm a language model, you can define and learn language. That also means looking at a single level of language. So, we're going to
> Hello, I'm a language model, and I think it's not as it is in the language as the only one in the world." -- Jeth's
> Hello, I'm a language model, but I'll want you to have my job hard now. I’ll have the one who will get you the> 
Hello, I'm a language model, so what have you say about the English language? There is another language model but what do you see in a different language
>>  Hello, I'm a language model, and there's a great deal of some of those kinds of language (particularly language, I'm not aware of) itHello, I'm a language model, and I don't have a lot of years, so I'm going to try to understand languages? I'll have a

> > Hello, I'm a language model, used in the modern age to the first half of my 11th grade school I used my English language arts classes a little
Hello, I'm a language model, and I always know for that game that I do. I always think about it, and I'm a language model,
> > > Hello, I'm a language model, that you can't understand just what a word means. But, to say this, I still have a lot of languagesHello, I'm a language model, I find it from other languages. The most important thing is how they are used throughout the world. Some languages include:
> Hello, I'm a language model, but one of the most important languages is a written language model. If you have questions about all the languages, then consider

> Hello, I'm a language model, but I'm a good communicator. Its programming is good for the native language code. You only need a language code
> Hello, I'm a language model, but the entire set of language is, and there's a more general purpose of speaking. I do think that way is
Hello, I'm a language model, because I've been an expert in languages programming. It's a pretty new language, so it's really very interactive.
> Hello, I'm a language model, we like a language model, that the other language model had been introduced to the New Testament in the 1960s, in> 
Hello, I'm a language model, so I thought it's really important to write down a language, then I decided to write down a language or one of
> Hello, I'm a language model, but I am only making a game called 'bundel' by itself. I am a linguist and it was> 
Hello, I'm a language model, but I'm not very much an expert in linguistics. I'm a research associate. Here's my experience using Language
> Hello, I'm a language model, of which I'm using at least 15 languages that I'm adding.<|endoftext|>The term "Vagavirus" has
> > Hello, I'm a language model, so please do my research. I'm doing my research, but I do get the ICT from the other languages that
Hello, I'm a language model, but I'm not talking much. How are they in that code? Oh one. Is that sound nice, it has
> Hello, I'm a language model, although in practice the second is often taught as more than just language study, it’s just really. I think> 
Hello, I'm a language model, but what is the difference? It's the same in English, and it's the same in English, and we can
> Hello, I'm a language model, and it means I feel like it all the time. I'm gonna talk to the children about the same language. There
> > Hello, I'm a language model, but it doesn't belong to I'm...
3. So I've had 2 languages: You're going here for
Hello, I'm a language model, just a new programming language with the help of you at your university, but after going through a book for the next level
> Hello, I'm a language model, but has a good background for it. I don't need a background, but I've made a change. I don
for step 2000 | loss 3.980334 | norm 0.4284 | time 17585.5451 ms | tok/sec 29813.5769
for step 2001 | loss 3.964516 | norm 0.4186 | time 461.3380 ms | tok/sec 1136450.8254
for step 2002 | loss 3.956202 | norm 0.3966 | time 463.0468 ms | tok/sec 1132257.0683
for step 2003 | loss 3.944523 | norm 0.4040 | time 462.4615 ms | tok/sec 1133690.1174
for step 2004 | loss 3.988541 | norm 0.4215 | time 463.4833 ms | tok/sec 1131190.6211
for step 2005 | loss 3.883888 | norm 0.4374 | time 463.1240 ms | tok/sec 1132068.2114
for step 2006 | loss 3.927096 | norm 0.4138 | time 464.4470 ms | tok/sec 1128843.4972
for step 2007 | loss 3.888407 | norm 0.3814 | time 463.8054 ms | tok/sec 1130405.0321
for step 2008 | loss 3.833572 | norm 0.3563 | time 462.9567 ms | tok/sec 1132477.4810
for step 2009 | loss 3.927482 | norm 0.3427 | time 463.6948 ms | tok/sec 1130674.7190
for step 2010 | loss 3.867005 | norm 0.3469 | time 464.1054 ms | tok/sec 1129674.5018
for step 2011 | loss 3.922897 | norm 0.3389 | time 464.4980 ms | tok/sec 1128719.5023
for step 2012 | loss 3.934271 | norm 0.3547 | time 464.9708 ms | tok/sec 1127571.8162
for step 2013 | loss 3.871665 | norm 0.4654 | time 464.1306 ms | tok/sec 1129612.9899
for step 2014 | loss 3.901797 | norm 0.3980 | time 463.6281 ms | tok/sec 1130837.5234
for step 2015 | loss 3.947309 | norm 0.3689 | time 464.0501 ms | tok/sec 1129809.1550
for step 2016 | loss 3.927440 | norm 0.3667 | time 464.3800 ms | tok/sec 1129006.3543
for step 2017 | loss 3.885144 | norm 0.3373 | time 463.8073 ms | tok/sec 1130400.3835
for step 2018 | loss 3.881372 | norm 0.3676 | time 465.9817 ms | tok/sec 1125125.6764
for step 2019 | loss 3.941751 | norm 0.3939 | time 465.4338 ms | tok/sec 1126450.1192
for step 2020 | loss 3.885683 | norm 0.3875 | time 465.0230 ms | tok/sec 1127445.2103
for step 2021 | loss 3.872026 | norm 0.3625 | time 465.8492 ms | tok/sec 1125445.8390
for step 2022 | loss 3.893791 | norm 0.4112 | time 465.1742 ms | tok/sec 1127078.8494
for step 2023 | loss 3.858694 | norm 0.3694 | time 465.3232 ms | tok/sec 1126717.9221
for step 2024 | loss 3.837265 | norm 0.3567 | time 464.6587 ms | tok/sec 1128329.1544
for step 2025 | loss 3.883172 | norm 0.3847 | time 466.2166 ms | tok/sec 1124558.9288
for step 2026 | loss 3.861445 | norm 0.3781 | time 465.3704 ms | tok/sec 1126603.6285
for step 2027 | loss 3.856905 | norm 0.3633 | time 465.3409 ms | tok/sec 1126675.2036
for step 2028 | loss 3.875891 | norm 0.3355 | time 465.7073 ms | tok/sec 1125788.6609
for step 2029 | loss 3.858717 | norm 0.3725 | time 465.7624 ms | tok/sec 1125655.5405
for step 2030 | loss 3.885104 | norm 0.3766 | time 465.1833 ms | tok/sec 1127056.8984
for step 2031 | loss 3.836367 | norm 0.3688 | time 464.9806 ms | tok/sec 1127548.1116
for step 2032 | loss 3.780650 | norm 0.3595 | time 464.8662 ms | tok/sec 1127825.6918
for step 2033 | loss 3.803822 | norm 0.3567 | time 464.5982 ms | tok/sec 1128476.2271
for step 2034 | loss 3.805412 | norm 0.3922 | time 464.2787 ms | tok/sec 1129252.7577
for step 2035 | loss 3.769482 | norm 0.4245 | time 464.6509 ms | tok/sec 1128348.2601
for step 2036 | loss 3.771865 | norm 0.4285 | time 465.0738 ms | tok/sec 1127322.1003
for step 2037 | loss 3.734662 | norm 0.3662 | time 464.8244 ms | tok/sec 1127926.9269
for step 2038 | loss 3.726190 | norm 0.3640 | time 465.3668 ms | tok/sec 1126612.2863
for step 2039 | loss 3.741457 | norm 0.3405 | time 464.9832 ms | tok/sec 1127541.7520
for step 2040 | loss 3.764462 | norm 0.4073 | time 464.6275 ms | tok/sec 1128405.0021
for step 2041 | loss 3.716866 | norm 0.3694 | time 464.5543 ms | tok/sec 1128582.7919
for step 2042 | loss 3.740995 | norm 0.3289 | time 464.5479 ms | tok/sec 1128598.4308
for step 2043 | loss 3.708327 | norm 0.3391 | time 464.9856 ms | tok/sec 1127535.9706
for step 2044 | loss 3.952920 | norm 0.3850 | time 463.9361 ms | tok/sec 1130086.6882
for step 2045 | loss 3.957789 | norm 0.4507 | time 465.4450 ms | tok/sec 1126422.9997
for step 2046 | loss 3.960396 | norm 0.4627 | time 464.7856 ms | tok/sec 1128021.2365
for step 2047 | loss 3.873508 | norm 0.4432 | time 466.1663 ms | tok/sec 1124680.2855
for step 2048 | loss 3.895039 | norm 0.4050 | time 464.3271 ms | tok/sec 1129135.0504
for step 2049 | loss 3.922596 | norm 0.4080 | time 464.6599 ms | tok/sec 1128326.2596
for step 2050 | loss 3.922546 | norm 0.3870 | time 464.5233 ms | tok/sec 1128658.0944
for step 2051 | loss 3.885282 | norm 0.3899 | time 465.7772 ms | tok/sec 1125619.8166
for step 2052 | loss 3.911595 | norm 0.4117 | time 464.8821 ms | tok/sec 1127786.9380
for step 2053 | loss 3.985182 | norm 0.3813 | time 464.9823 ms | tok/sec 1127544.0645
for step 2054 | loss 3.931953 | norm 0.3945 | time 464.3240 ms | tok/sec 1129142.5876
for step 2055 | loss 3.917072 | norm 0.4063 | time 465.7276 ms | tok/sec 1125739.6736
for step 2056 | loss 3.874372 | norm 0.3949 | time 465.1959 ms | tok/sec 1127026.2840
for step 2057 | loss 3.919404 | norm 0.3289 | time 464.4544 ms | tok/sec 1128825.5336
for step 2058 | loss 3.885663 | norm 0.3344 | time 464.6523 ms | tok/sec 1128344.7863
for step 2059 | loss 3.895522 | norm 0.3376 | time 466.2063 ms | tok/sec 1124583.6581
for step 2060 | loss 3.892732 | norm 0.3405 | time 465.3499 ms | tok/sec 1126653.2683
for step 2061 | loss 3.958787 | norm 0.3865 | time 465.3723 ms | tok/sec 1126599.0111
for step 2062 | loss 3.878286 | norm 0.4240 | time 464.8373 ms | tok/sec 1127895.6867
for step 2063 | loss 3.977531 | norm 0.4492 | time 465.0321 ms | tok/sec 1127423.2451
for step 2064 | loss 3.887619 | norm 0.3867 | time 465.9350 ms | tok/sec 1125238.5186
for step 2065 | loss 3.918873 | norm 0.3823 | time 464.4725 ms | tok/sec 1128781.4964
for step 2066 | loss 3.892642 | norm 0.3572 | time 465.5910 ms | tok/sec 1126069.9885
for step 2067 | loss 3.820291 | norm 0.3178 | time 465.8298 ms | tok/sec 1125492.4966
for step 2068 | loss 3.849863 | norm 0.3081 | time 464.7939 ms | tok/sec 1128000.9846
for step 2069 | loss 3.922652 | norm 0.3329 | time 464.8569 ms | tok/sec 1127848.2512
for step 2070 | loss 3.846803 | norm 0.3521 | time 465.6992 ms | tok/sec 1125808.2570
for step 2071 | loss 3.871860 | norm 0.3481 | time 465.5888 ms | tok/sec 1126075.1782
for step 2072 | loss 3.834963 | norm 0.3678 | time 465.1699 ms | tok/sec 1127089.2475
for step 2073 | loss 3.848752 | norm 0.3354 | time 464.5789 ms | tok/sec 1128523.1363
for step 2074 | loss 3.826050 | norm 0.3557 | time 465.1847 ms | tok/sec 1127053.4325
for step 2075 | loss 3.827168 | norm 0.3520 | time 465.6031 ms | tok/sec 1126040.5808
for step 2076 | loss 3.846889 | norm 0.3587 | time 466.2144 ms | tok/sec 1124564.1046
for step 2077 | loss 3.845843 | norm 0.3649 | time 465.1427 ms | tok/sec 1127155.1068
for step 2078 | loss 3.940100 | norm 0.3918 | time 465.4179 ms | tok/sec 1126488.7811
for step 2079 | loss 3.803007 | norm 0.4300 | time 466.3982 ms | tok/sec 1124120.8821
for step 2080 | loss 3.793231 | norm 0.4203 | time 464.3199 ms | tok/sec 1129152.4440
for step 2081 | loss 3.699653 | norm 0.3855 | time 465.3974 ms | tok/sec 1126538.4108
for step 2082 | loss 3.790868 | norm 0.3835 | time 465.2948 ms | tok/sec 1126786.6248
for step 2083 | loss 3.728822 | norm 0.4219 | time 466.2328 ms | tok/sec 1124519.8241
for step 2084 | loss 3.770001 | norm 0.3957 | time 465.2121 ms | tok/sec 1126987.0075
for step 2085 | loss 3.700731 | norm 0.3702 | time 465.8279 ms | tok/sec 1125497.1049
for step 2086 | loss 3.722068 | norm 0.3219 | time 465.1341 ms | tok/sec 1127175.9060
for step 2087 | loss 3.722269 | norm 0.3346 | time 465.4951 ms | tok/sec 1126301.8434
for step 2088 | loss 3.753728 | norm 0.3581 | time 465.9111 ms | tok/sec 1125296.1000
for step 2089 | loss 3.767961 | norm 0.4070 | time 464.9765 ms | tok/sec 1127557.9402
for step 2090 | loss 3.888011 | norm 0.3661 | time 465.8184 ms | tok/sec 1125520.1473
for step 2091 | loss 3.986573 | norm 0.3950 | time 464.8767 ms | tok/sec 1127800.2412
for step 2092 | loss 3.972648 | norm 0.4787 | time 464.4814 ms | tok/sec 1128760.0584
for step 2093 | loss 3.935803 | norm 0.5126 | time 465.9634 ms | tok/sec 1125170.0046
for step 2094 | loss 3.942561 | norm 0.5656 | time 466.6469 ms | tok/sec 1123521.8515
Will loading at 0 from edu_fineweb10B/edufineweb_train_000012.npy
for step 2095 | loss 3.916421 | norm 0.5533 | time 1486.9790 ms | tok/sec 352586.0132
for step 2096 | loss 3.909836 | norm 0.4752 | time 464.8063 ms | tok/sec 1127970.8975
for step 2097 | loss 3.986642 | norm 0.4202 | time 464.2546 ms | tok/sec 1129311.3305
for step 2098 | loss 3.941827 | norm 0.4071 | time 464.6904 ms | tok/sec 1128252.1592
for step 2099 | loss 3.916853 | norm 0.3827 | time 464.2334 ms | tok/sec 1129362.9493
for step 2100 | loss 3.871589 | norm 0.3262 | time 465.1999 ms | tok/sec 1127016.4646
for step 2101 | loss 3.886494 | norm 0.3304 | time 464.8509 ms | tok/sec 1127862.7128
for step 2102 | loss 3.966771 | norm 0.3361 | time 465.8673 ms | tok/sec 1125402.0650
for step 2103 | loss 3.875517 | norm 0.3386 | time 465.5526 ms | tok/sec 1126162.8343
for step 2104 | loss 3.870925 | norm 0.3481 | time 465.0984 ms | tok/sec 1127262.5779
for step 2105 | loss 3.876969 | norm 0.3743 | time 465.3516 ms | tok/sec 1126649.2277
for step 2106 | loss 3.794061 | norm 0.3902 | time 465.4002 ms | tok/sec 1126531.4855
for step 2107 | loss 3.882006 | norm 0.4403 | time 465.7962 ms | tok/sec 1125573.7246
for step 2108 | loss 3.892570 | norm 0.4005 | time 465.4908 ms | tok/sec 1126312.2272
for step 2109 | loss 3.891551 | norm 0.4025 | time 464.8595 ms | tok/sec 1127841.8882
for step 2110 | loss 3.901991 | norm 0.3306 | time 464.9506 ms | tok/sec 1127620.9631
for step 2111 | loss 3.881971 | norm 0.3188 | time 465.1635 ms | tok/sec 1127104.8450
for step 2112 | loss 3.845349 | norm 0.3035 | time 465.0798 ms | tok/sec 1127307.6526
for step 2113 | loss 3.848737 | norm 0.3195 | time 466.3463 ms | tok/sec 1124246.1677
for step 2114 | loss 3.827004 | norm 0.3219 | time 465.7853 ms | tok/sec 1125600.2270
for step 2115 | loss 3.761911 | norm 0.3225 | time 465.3335 ms | tok/sec 1126693.0988
for step 2116 | loss 3.847597 | norm 0.3021 | time 465.2002 ms | tok/sec 1127015.8870
for step 2117 | loss 3.817797 | norm 0.3556 | time 465.8697 ms | tok/sec 1125396.3055
for step 2118 | loss 3.878310 | norm 0.3861 | time 465.9991 ms | tok/sec 1125083.6542
for step 2119 | loss 3.789150 | norm 0.3903 | time 465.6179 ms | tok/sec 1126004.8325
for step 2120 | loss 3.840511 | norm 0.3789 | time 465.8940 ms | tok/sec 1125337.5622
for step 2121 | loss 3.811693 | norm 0.3603 | time 465.8399 ms | tok/sec 1125468.3033
for step 2122 | loss 3.933786 | norm 0.3898 | time 464.9291 ms | tok/sec 1127673.0057
for step 2123 | loss 3.801188 | norm 0.4148 | time 465.5218 ms | tok/sec 1126237.2374
for step 2124 | loss 3.852129 | norm 0.4091 | time 465.7774 ms | tok/sec 1125619.2404
for step 2125 | loss 3.716277 | norm 0.4120 | time 464.8418 ms | tok/sec 1127884.6952
for step 2126 | loss 3.694076 | norm 0.3707 | time 465.7173 ms | tok/sec 1125764.4549
for step 2127 | loss 3.771185 | norm 0.3502 | time 465.1847 ms | tok/sec 1127053.4325
for step 2128 | loss 3.684308 | norm 0.3544 | time 465.5216 ms | tok/sec 1126237.8142
for step 2129 | loss 3.695050 | norm 0.3773 | time 465.6529 ms | tok/sec 1125920.0833
for step 2130 | loss 3.746266 | norm 0.3397 | time 464.9603 ms | tok/sec 1127597.2564
for step 2131 | loss 3.666933 | norm 0.3144 | time 465.0645 ms | tok/sec 1127344.6396
for step 2132 | loss 3.673260 | norm 0.3368 | time 464.6287 ms | tok/sec 1128402.1069
for step 2133 | loss 3.610693 | norm 0.3499 | time 464.7722 ms | tok/sec 1128053.6410
for step 2134 | loss 3.644843 | norm 0.3734 | time 465.2307 ms | tok/sec 1126941.9585
for step 2135 | loss 3.686196 | norm 0.3365 | time 465.6379 ms | tok/sec 1125956.4028
for step 2136 | loss 3.813055 | norm 0.3874 | time 464.9503 ms | tok/sec 1127621.5414
for step 2137 | loss 3.904833 | norm 0.4667 | time 465.5192 ms | tok/sec 1126243.5823
for step 2138 | loss 3.937015 | norm 0.5019 | time 465.3933 ms | tok/sec 1126548.2218
for step 2139 | loss 3.950803 | norm 0.5665 | time 464.3245 ms | tok/sec 1129141.4280
for step 2140 | loss 4.061674 | norm 0.6834 | time 464.5357 ms | tok/sec 1128627.9722
for step 2141 | loss 3.951012 | norm 0.6062 | time 465.3161 ms | tok/sec 1126735.2413
for step 2142 | loss 3.939650 | norm 0.5048 | time 465.2300 ms | tok/sec 1126943.6911
for step 2143 | loss 3.881049 | norm 0.4480 | time 465.7657 ms | tok/sec 1125647.4736
for step 2144 | loss 3.935309 | norm 0.4259 | time 465.7376 ms | tok/sec 1125715.4696
for step 2145 | loss 3.840299 | norm 0.3885 | time 465.8148 ms | tok/sec 1125528.7885
for step 2146 | loss 3.959041 | norm 0.4084 | time 465.2381 ms | tok/sec 1126924.0554
for step 2147 | loss 3.897546 | norm 0.3449 | time 465.4882 ms | tok/sec 1126318.5730
for step 2148 | loss 3.889756 | norm 0.3349 | time 465.8058 ms | tok/sec 1125550.6800
for step 2149 | loss 3.878316 | norm 0.3526 | time 466.1238 ms | tok/sec 1124782.6827
for step 2150 | loss 3.846694 | norm 0.3413 | time 465.9948 ms | tok/sec 1125094.0155
for step 2151 | loss 3.863995 | norm 0.3035 | time 465.3444 ms | tok/sec 1126666.5448
for step 2152 | loss 3.887120 | norm 0.3098 | time 465.7178 ms | tok/sec 1125763.3022
for step 2153 | loss 3.900231 | norm 0.3176 | time 466.0594 ms | tok/sec 1124938.0398
for step 2154 | loss 3.953516 | norm 0.3336 | time 466.4297 ms | tok/sec 1124045.0347
for step 2155 | loss 3.805589 | norm 0.3254 | time 464.6835 ms | tok/sec 1128268.9467
for step 2156 | loss 3.881235 | norm 0.3362 | time 465.8675 ms | tok/sec 1125401.4890
for step 2157 | loss 3.969640 | norm 0.3383 | time 465.6367 ms | tok/sec 1125959.2854
for step 2158 | loss 3.869576 | norm 0.3487 | time 465.2250 ms | tok/sec 1126955.8194
for step 2159 | loss 3.873830 | norm 0.3723 | time 466.5806 ms | tok/sec 1123681.4540
for step 2160 | loss 3.875919 | norm 0.3894 | time 465.3463 ms | tok/sec 1126661.9269
for step 2161 | loss 3.802957 | norm 0.3496 | time 465.6811 ms | tok/sec 1125852.0626
for step 2162 | loss 3.765814 | norm 0.3413 | time 465.8425 ms | tok/sec 1125461.9671
for step 2163 | loss 3.797857 | norm 0.3449 | time 466.5034 ms | tok/sec 1123867.5228
for step 2164 | loss 3.835419 | norm 0.3805 | time 465.0996 ms | tok/sec 1127259.6886
for step 2165 | loss 3.845506 | norm 0.4126 | time 465.9262 ms | tok/sec 1125259.8230
for step 2166 | loss 3.784133 | norm 0.4168 | time 465.9657 ms | tok/sec 1125164.2475
for step 2167 | loss 3.802123 | norm 0.3233 | time 465.1871 ms | tok/sec 1127047.6561
for step 2168 | loss 3.794986 | norm 0.3137 | time 465.5516 ms | tok/sec 1126165.1413
for step 2169 | loss 3.828968 | norm 0.3746 | time 465.6570 ms | tok/sec 1125910.2832
for step 2170 | loss 3.819142 | norm 0.3199 | time 466.3320 ms | tok/sec 1124280.6549
for step 2171 | loss 3.732051 | norm 0.3130 | time 465.1265 ms | tok/sec 1127194.3949
for step 2172 | loss 3.724617 | norm 0.3569 | time 466.1813 ms | tok/sec 1124644.0483
for step 2173 | loss 3.631546 | norm 0.3463 | time 465.8406 ms | tok/sec 1125466.5752
for step 2174 | loss 3.639710 | norm 0.3811 | time 466.4237 ms | tok/sec 1124059.3990
for step 2175 | loss 3.657545 | norm 0.3820 | time 465.8334 ms | tok/sec 1125483.8560
for step 2176 | loss 3.631475 | norm 0.3445 | time 468.6575 ms | tok/sec 1118701.8391
for step 2177 | loss 3.591545 | norm 0.3397 | time 466.0313 ms | tok/sec 1125005.9501
for step 2178 | loss 3.650505 | norm 0.3194 | time 465.4822 ms | tok/sec 1126332.9954
for step 2179 | loss 3.674152 | norm 0.3582 | time 466.4991 ms | tok/sec 1123877.8618
for step 2180 | loss 3.719194 | norm 0.3828 | time 465.2770 ms | tok/sec 1126829.9292
for step 2181 | loss 3.644031 | norm 0.3957 | time 465.2879 ms | tok/sec 1126803.3688
for step 2182 | loss 3.628296 | norm 0.4647 | time 466.0914 ms | tok/sec 1124860.9313
for step 2183 | loss 3.757353 | norm 0.3908 | time 465.4412 ms | tok/sec 1126432.2317
for step 2184 | loss 3.911802 | norm 0.3734 | time 466.4354 ms | tok/sec 1124031.2454
for step 2185 | loss 3.873742 | norm 0.3836 | time 466.3754 ms | tok/sec 1124176.0503
for step 2186 | loss 4.010971 | norm 0.5004 | time 465.3485 ms | tok/sec 1126656.7317
for step 2187 | loss 4.014308 | norm 0.6122 | time 466.8584 ms | tok/sec 1123012.9191
for step 2188 | loss 3.955285 | norm 0.6323 | time 465.7273 ms | tok/sec 1125740.2499
for step 2189 | loss 3.935673 | norm 0.4872 | time 466.5203 ms | tok/sec 1123826.7432
for step 2190 | loss 3.834279 | norm 0.4591 | time 466.0206 ms | tok/sec 1125031.8503
for step 2191 | loss 3.905655 | norm 0.4190 | time 466.2483 ms | tok/sec 1124482.4472
for step 2192 | loss 3.924579 | norm 0.4019 | time 465.8468 ms | tok/sec 1125451.5990
for step 2193 | loss 3.892961 | norm 0.3882 | time 465.6384 ms | tok/sec 1125955.2498
for step 2194 | loss 3.860152 | norm 0.3537 | time 466.5172 ms | tok/sec 1123834.2097
for step 2195 | loss 3.875153 | norm 0.3495 | time 465.9388 ms | tok/sec 1125229.3062
for step 2196 | loss 3.879733 | norm 0.3211 | time 466.1291 ms | tok/sec 1124770.0259
for step 2197 | loss 3.905968 | norm 0.3448 | time 465.5299 ms | tok/sec 1126217.6263
for step 2198 | loss 3.835732 | norm 0.3286 | time 465.1866 ms | tok/sec 1127048.8114
for step 2199 | loss 3.830998 | norm 0.3105 | time 465.3430 ms | tok/sec 1126670.0083
for step 2200 | loss 3.887110 | norm 0.3048 | time 465.4717 ms | tok/sec 1126358.3798
for step 2201 | loss 3.877268 | norm 0.3167 | time 466.0416 ms | tok/sec 1124981.2022
for step 2202 | loss 3.844027 | norm 0.3285 | time 465.3857 ms | tok/sec 1126566.6901
for step 2203 | loss 3.836439 | norm 0.3114 | time 465.5874 ms | tok/sec 1126078.6380
for step 2204 | loss 3.865186 | norm 0.3215 | time 465.7326 ms | tok/sec 1125727.5715
for step 2205 | loss 3.852269 | norm 0.3037 | time 464.8323 ms | tok/sec 1127907.8354
for step 2206 | loss 3.854080 | norm 0.3128 | time 465.9297 ms | tok/sec 1125251.1860
for step 2207 | loss 3.798347 | norm 0.3184 | time 465.5082 ms | tok/sec 1126270.1163
for step 2208 | loss 3.859354 | norm 0.3559 | time 466.4292 ms | tok/sec 1124046.1838
for step 2209 | loss 3.793043 | norm 0.3427 | time 466.0776 ms | tok/sec 1124894.3053
for step 2210 | loss 3.818364 | norm 0.4314 | time 465.3246 ms | tok/sec 1126714.4583
for step 2211 | loss 3.816829 | norm 0.4050 | time 466.3436 ms | tok/sec 1124252.4902
for step 2212 | loss 3.831579 | norm 0.3770 | time 465.9510 ms | tok/sec 1125199.9425
for step 2213 | loss 3.795170 | norm 0.3378 | time 465.8716 ms | tok/sec 1125391.6980
for step 2214 | loss 3.780590 | norm 0.3100 | time 466.3975 ms | tok/sec 1124122.6061
for step 2215 | loss 3.771724 | norm 0.3159 | time 466.7804 ms | tok/sec 1123200.4877
for step 2216 | loss 3.832321 | norm 0.3641 | time 465.4291 ms | tok/sec 1126461.6598
for step 2217 | loss 3.812264 | norm 0.3906 | time 465.2040 ms | tok/sec 1127006.6454
for step 2218 | loss 3.654332 | norm 0.3614 | time 466.7780 ms | tok/sec 1123206.2247
for step 2219 | loss 3.634384 | norm 0.4044 | time 465.7338 ms | tok/sec 1125724.6900
for step 2220 | loss 3.706391 | norm 0.4096 | time 466.0368 ms | tok/sec 1124992.7127
for step 2221 | loss 3.664365 | norm 0.3782 | time 466.1140 ms | tok/sec 1124806.2712
for step 2222 | loss 3.599362 | norm 0.3650 | time 465.6773 ms | tok/sec 1125861.2853
for step 2223 | loss 3.619689 | norm 0.3806 | time 465.2088 ms | tok/sec 1126995.0936
for step 2224 | loss 3.690917 | norm 0.3481 | time 465.3039 ms | tok/sec 1126764.6852
for step 2225 | loss 3.653953 | norm 0.3394 | time 465.6978 ms | tok/sec 1125811.7153
for step 2226 | loss 3.673128 | norm 0.3272 | time 465.0578 ms | tok/sec 1127360.8222
for step 2227 | loss 3.635661 | norm 0.3525 | time 466.1684 ms | tok/sec 1124675.1087
for step 2228 | loss 3.664823 | norm 0.3402 | time 466.1398 ms | tok/sec 1124744.1378
for step 2229 | loss 3.658859 | norm 0.3362 | time 464.7238 ms | tok/sec 1128171.1229
for step 2230 | loss 3.874352 | norm 0.3224 | time 465.7536 ms | tok/sec 1125676.8607
for step 2231 | loss 3.940606 | norm 0.3808 | time 466.7840 ms | tok/sec 1123191.8822
for step 2232 | loss 3.863887 | norm 0.4069 | time 465.8151 ms | tok/sec 1125528.2124
for step 2233 | loss 3.841567 | norm 0.4545 | time 464.4907 ms | tok/sec 1128737.4625
for step 2234 | loss 3.915060 | norm 0.4201 | time 465.2605 ms | tok/sec 1126869.7721
for step 2235 | loss 3.876879 | norm 0.4065 | time 466.3293 ms | tok/sec 1124286.9778
for step 2236 | loss 3.893667 | norm 0.4017 | time 465.7462 ms | tok/sec 1125694.7242
for step 2237 | loss 3.893509 | norm 0.3925 | time 465.2846 ms | tok/sec 1126811.4522
for step 2238 | loss 3.950068 | norm 0.3704 | time 465.0776 ms | tok/sec 1127312.8537
for step 2239 | loss 3.869024 | norm 0.3744 | time 465.6968 ms | tok/sec 1125814.0207
for step 2240 | loss 3.852947 | norm 0.3485 | time 466.3672 ms | tok/sec 1124195.5904
for step 2241 | loss 3.905272 | norm 0.3399 | time 465.1515 ms | tok/sec 1127133.7305
for step 2242 | loss 3.839978 | norm 0.3447 | time 464.8740 ms | tok/sec 1127806.6038
for step 2243 | loss 3.844064 | norm 0.3236 | time 466.0046 ms | tok/sec 1125070.4149
for step 2244 | loss 3.875808 | norm 0.3049 | time 466.4319 ms | tok/sec 1124039.8637
for step 2245 | loss 3.874349 | norm 0.3407 | time 465.4741 ms | tok/sec 1126352.6105
for step 2246 | loss 3.862991 | norm 0.3876 | time 465.6296 ms | tok/sec 1125976.5813
for step 2247 | loss 3.869167 | norm 0.4335 | time 464.6893 ms | tok/sec 1128255.0535
for step 2248 | loss 3.853456 | norm 0.4171 | time 465.4059 ms | tok/sec 1126517.6350
for step 2249 | loss 3.870912 | norm 0.3718 | time 465.5919 ms | tok/sec 1126067.6819
validation loss 3.8553
HellaSwag accuracy: 2564/10042=0.2553
> Hello, I'm a language model, and that is a nice place to relax, or do any harm.
My favorite way to learn is to become a
> Hello, I'm a language model, a model of using the language, that takes a lot further and that doesn't become reality. Many languages are able to
> Hello, I'm a language model, so at this point, I'm not going to waste time as much, but I'm talking a little about that,
> Hello, I'm a language model, and I'm trying to give it a stop to start: for people on something, someone may say something, you're
> Hello, I'm a language model, says one day ago.
The best thing to do in this case is that in the country it is so easy to
> > > Hello, I'm a language model, and I can't think there would be, there would be, to it all, that language was, but I would
Hello, I'm a language model, but I know that in fact, the term is simply a "true "person" which includes. So the definition is
Hello, I'm a language model, where I just had a very tough grasp
I mean, "If one of the people you speak me," says L
> > > > Hello, I'm a language model, but in the end we're doing:
The system includes the whole kernel.
In the kernel-based system,
Hello, I'm a language model, and I'm a good candidate for all levels of knowledge, and I got a second in both content-rich and a
Hello, I'm a language model, i have a simple syntax that uses a variable like “I”. So let’s get simple:
Hello, I'm a language model, but I don't understand the word but I can't understand why the rules do affect the language at all.
It
> > > > Hello, I'm a language model, but I love the stuff at the beginning of language arts. So it's a pretty much like a simple language model,
Hello, I'm a language model, let me understand the concept. But to keep it simple now, I'm not using a computer. There are some ways
Hello, I'm a language model, I learned very extensively, yet I am going to be a linguistic, and how I did a long way for my
Hello, I'm a language model, so why's it important to use it?"
How can one use a language model? Use The Reading Test, How
> > > > Hello, I'm a language model, and in my own language.
What's That?
1. "A strong link between a spoken language, so
Hello, I'm a language model, and I think so, I see one language model on the other. What is a language model? Well, how can
Hello, I'm a language model, and you, me, you're still using my words and have all my own letters. I just want to share the
Hello, I'm a language model, an online language model of writing and speaking, and that I have seen the term "language model" in a lot of
> Hello, I'm a language model, however. We have an important part so that each of us can be successful in this process. One of the most influential> 
Hello, I'm a language model, and I'll be sharing with me. For instance, I'll be discussing a lesson in the past, as well as
> Hello, I'm a language model, so the two languages are actually a model. It's a good choice, especially if you're a very different culture,
> Hello, I'm a language model, and we can't just say a lot.<|endoftext|>The first thing we're thinking is how does the immune-associated antibodies
> > Hello, I'm a language model, so my daughter is interested in her, especially if she has a high vocabulary and a high vocabulary book. She does so
Hello, I'm a language model, and I'm not so far away the point of this problem. I're learning more through our children."
It wasn
> Hello, I'm a language model, then there are two groups (i.e. the other is important in them; I think they might prefer the same> 
Hello, I'm a language model, and a great place to start and bring together all of our languages together.
I'm a programming language, as you
> Hello, I'm a language model, and it means you always have the best chance to have a good job. That is especially true for me, and because
> > Hello, I'm a language model, and it has a series of models. A game version is available for download here.
There are a number of programs
Hello, I'm a language model, though it's probably not something I'm talking about and is often heard by me or her, depending on how you look
> Hello, I'm a language model, and most of those are
which is the basic programming and the use of programming. There is a "listen"
for step 2250 | loss 3.794840 | norm 0.3667 | time 17728.2109 ms | tok/sec 29573.6554
for step 2251 | loss 3.875698 | norm 0.3459 | time 461.3922 ms | tok/sec 1136317.5204
for step 2252 | loss 3.753331 | norm 0.3680 | time 461.4367 ms | tok/sec 1136207.7288
for step 2253 | loss 3.785954 | norm 0.3968 | time 462.6975 ms | tok/sec 1133111.7917
for step 2254 | loss 3.834622 | norm 0.3989 | time 462.5862 ms | tok/sec 1133384.5244
for step 2255 | loss 3.816583 | norm 0.4287 | time 463.2792 ms | tok/sec 1131688.9394
for step 2256 | loss 3.782416 | norm 0.3795 | time 463.5236 ms | tok/sec 1131092.2901
for step 2257 | loss 3.774037 | norm 0.5102 | time 462.4724 ms | tok/sec 1133663.2327
for step 2258 | loss 3.884703 | norm 0.5219 | time 463.5243 ms | tok/sec 1131090.5447
for step 2259 | loss 3.797556 | norm 0.4985 | time 463.0947 ms | tok/sec 1132139.8997
for step 2260 | loss 3.752394 | norm 0.4306 | time 463.7070 ms | tok/sec 1130645.0704
for step 2261 | loss 3.841531 | norm 0.3783 | time 463.9251 ms | tok/sec 1130113.4036
for step 2262 | loss 3.800119 | norm 0.3424 | time 464.9422 ms | tok/sec 1127641.2014
for step 2263 | loss 3.795587 | norm 0.3333 | time 463.7485 ms | tok/sec 1130543.9281
for step 2264 | loss 3.803167 | norm 0.3167 | time 464.9825 ms | tok/sec 1127543.4864
for step 2265 | loss 3.681137 | norm 0.3272 | time 464.6382 ms | tok/sec 1128378.9464
for step 2266 | loss 3.654263 | norm 0.3142 | time 464.5231 ms | tok/sec 1128658.6737
for step 2267 | loss 3.599511 | norm 0.3332 | time 465.0223 ms | tok/sec 1127446.9444
for step 2268 | loss 3.681284 | norm 0.3591 | time 465.7607 ms | tok/sec 1125659.5740
for step 2269 | loss 3.879569 | norm 0.4190 | time 464.6380 ms | tok/sec 1128379.5254
for step 2270 | loss 3.720567 | norm 0.4760 | time 464.1635 ms | tok/sec 1129532.9184
for step 2271 | loss 3.617795 | norm 0.4682 | time 466.0728 ms | tok/sec 1124905.8141
for step 2272 | loss 3.698750 | norm 0.4447 | time 465.7996 ms | tok/sec 1125565.6589
for step 2273 | loss 3.679629 | norm 0.3715 | time 470.7437 ms | tok/sec 1113744.1645
for step 2274 | loss 3.737235 | norm 0.3551 | time 465.4241 ms | tok/sec 1126473.7776
for step 2275 | loss 3.639738 | norm 0.3396 | time 465.5683 ms | tok/sec 1126124.7714
for step 2276 | loss 3.668640 | norm 0.3636 | time 465.9038 ms | tok/sec 1125313.9514
for step 2277 | loss 3.827511 | norm 0.3269 | time 465.2901 ms | tok/sec 1126798.1723
for step 2278 | loss 3.865707 | norm 0.3319 | time 465.2412 ms | tok/sec 1126916.5478
for step 2279 | loss 3.959315 | norm 0.3279 | time 465.3361 ms | tok/sec 1126686.7488
for step 2280 | loss 3.888562 | norm 0.3348 | time 465.8587 ms | tok/sec 1125422.7996
for step 2281 | loss 3.846690 | norm 0.3342 | time 465.6324 ms | tok/sec 1125969.6629
for step 2282 | loss 3.849113 | norm 0.3494 | time 464.5309 ms | tok/sec 1128639.5575
for step 2283 | loss 3.897455 | norm 0.5482 | time 465.5795 ms | tok/sec 1126097.6676
for step 2284 | loss 3.874001 | norm 0.3354 | time 465.9388 ms | tok/sec 1125229.3062
Will loading at 0 from edu_fineweb10B/edufineweb_train_000013.npy
for step 2285 | loss 3.912090 | norm 0.3614 | time 1627.9960 ms | tok/sec 322045.0237
for step 2286 | loss 3.855090 | norm 0.3330 | time 463.9893 ms | tok/sec 1129957.1944
for step 2287 | loss 3.937470 | norm 0.3829 | time 463.8011 ms | tok/sec 1130415.4917
for step 2288 | loss 3.883342 | norm 0.3376 | time 464.9012 ms | tok/sec 1127740.6683
for step 2289 | loss 3.868718 | norm 0.3264 | time 465.2948 ms | tok/sec 1126786.6248
for step 2290 | loss 3.850981 | norm 0.3461 | time 465.6076 ms | tok/sec 1126029.6255
for step 2291 | loss 3.832687 | norm 0.3229 | time 465.3308 ms | tok/sec 1126699.4488
for step 2292 | loss 3.907221 | norm 0.3419 | time 464.9255 ms | tok/sec 1127681.6800
for step 2293 | loss 3.847455 | norm 0.3328 | time 463.8057 ms | tok/sec 1130404.4510
for step 2294 | loss 3.890451 | norm 0.3962 | time 465.2376 ms | tok/sec 1126925.2104
for step 2295 | loss 3.811218 | norm 0.3557 | time 466.5923 ms | tok/sec 1123653.3193
for step 2296 | loss 3.855484 | norm 0.3587 | time 464.7923 ms | tok/sec 1128005.0350
for step 2297 | loss 3.821900 | norm 0.3629 | time 465.7049 ms | tok/sec 1125794.4244
for step 2298 | loss 3.843325 | norm 0.3610 | time 464.8252 ms | tok/sec 1127925.1913
for step 2299 | loss 3.842016 | norm 0.3613 | time 465.1377 ms | tok/sec 1127167.2396
for step 2300 | loss 3.806833 | norm 0.3294 | time 465.1797 ms | tok/sec 1127065.5631
for step 2301 | loss 3.772167 | norm 0.3043 | time 464.9665 ms | tok/sec 1127582.2234
for step 2302 | loss 3.783427 | norm 0.3137 | time 466.4862 ms | tok/sec 1123908.8798
for step 2303 | loss 3.774453 | norm 0.3234 | time 465.1854 ms | tok/sec 1127051.6996
for step 2304 | loss 3.767222 | norm 0.3155 | time 464.9994 ms | tok/sec 1127502.4396
for step 2305 | loss 3.814584 | norm 0.3389 | time 465.5385 ms | tok/sec 1126196.8624
for step 2306 | loss 3.764204 | norm 0.3443 | time 464.6466 ms | tok/sec 1128358.6817
for step 2307 | loss 3.883784 | norm 0.3740 | time 465.3194 ms | tok/sec 1126727.1589
for step 2308 | loss 3.812853 | norm 0.4456 | time 465.5845 ms | tok/sec 1126085.5578
for step 2309 | loss 3.807790 | norm 0.4435 | time 465.0707 ms | tok/sec 1127329.6133
for step 2310 | loss 3.825221 | norm 0.4432 | time 464.6361 ms | tok/sec 1128384.1574
for step 2311 | loss 3.622749 | norm 0.4143 | time 465.6274 ms | tok/sec 1125981.7702
for step 2312 | loss 3.648969 | norm 0.4437 | time 465.6446 ms | tok/sec 1125940.2605
for step 2313 | loss 3.677480 | norm 0.3613 | time 465.4312 ms | tok/sec 1126456.4665
for step 2314 | loss 3.654978 | norm 0.3721 | time 464.8602 ms | tok/sec 1127840.1528
for step 2315 | loss 3.646673 | norm 0.3252 | time 464.9441 ms | tok/sec 1127636.5754
for step 2316 | loss 3.622333 | norm 0.3203 | time 465.9727 ms | tok/sec 1125147.5522
for step 2317 | loss 3.635137 | norm 0.3553 | time 465.2429 ms | tok/sec 1126912.5053
for step 2318 | loss 3.631242 | norm 0.3922 | time 464.7231 ms | tok/sec 1128172.8593
for step 2319 | loss 3.696305 | norm 0.3964 | time 464.6373 ms | tok/sec 1128381.2624
for step 2320 | loss 3.643176 | norm 0.3709 | time 465.6303 ms | tok/sec 1125974.8517
for step 2321 | loss 3.630594 | norm 0.3695 | time 465.7342 ms | tok/sec 1125723.5375
for step 2322 | loss 3.697687 | norm 0.3685 | time 464.8225 ms | tok/sec 1127931.5552
for step 2323 | loss 3.849256 | norm 0.3748 | time 465.7605 ms | tok/sec 1125660.1502
for step 2324 | loss 3.856147 | norm 0.3198 | time 464.8733 ms | tok/sec 1127808.3390
for step 2325 | loss 3.902613 | norm 0.3408 | time 464.7372 ms | tok/sec 1128138.7117
for step 2326 | loss 3.881557 | norm 0.3782 | time 465.3640 ms | tok/sec 1126619.2127
for step 2327 | loss 3.845881 | norm 0.3794 | time 464.9475 ms | tok/sec 1127628.4801
for step 2328 | loss 3.878699 | norm 0.3344 | time 465.1966 ms | tok/sec 1127024.5511
for step 2329 | loss 3.851496 | norm 0.3356 | time 466.0745 ms | tok/sec 1124901.7860
for step 2330 | loss 3.919313 | norm 0.3851 | time 465.6372 ms | tok/sec 1125958.1324
for step 2331 | loss 4.034393 | norm 0.4054 | time 465.5845 ms | tok/sec 1126085.5578
for step 2332 | loss 3.878143 | norm 0.3644 | time 465.3459 ms | tok/sec 1126663.0814
for step 2333 | loss 3.893257 | norm 0.3748 | time 465.7612 ms | tok/sec 1125658.4216
for step 2334 | loss 3.844548 | norm 0.3598 | time 465.9967 ms | tok/sec 1125089.4104
for step 2335 | loss 3.788794 | norm 0.3500 | time 465.1101 ms | tok/sec 1127234.2636
for step 2336 | loss 3.829539 | norm 0.3233 | time 465.6086 ms | tok/sec 1126027.3191
for step 2337 | loss 3.914718 | norm 0.3285 | time 465.6596 ms | tok/sec 1125903.9421
for step 2338 | loss 3.856845 | norm 0.3329 | time 466.2960 ms | tok/sec 1124367.4570
for step 2339 | loss 3.828003 | norm 0.3575 | time 466.6345 ms | tok/sec 1123551.7018
for step 2340 | loss 3.828512 | norm 0.3341 | time 465.2412 ms | tok/sec 1126916.5478
for step 2341 | loss 3.821309 | norm 0.3611 | time 466.1553 ms | tok/sec 1124706.7459
for step 2342 | loss 3.797364 | norm 0.3717 | time 465.6582 ms | tok/sec 1125907.4009
for step 2343 | loss 3.858999 | norm 0.3870 | time 465.2889 ms | tok/sec 1126801.0592
for step 2344 | loss 3.822200 | norm 0.3498 | time 465.1079 ms | tok/sec 1127239.4641
for step 2345 | loss 3.753183 | norm 0.3589 | time 465.5056 ms | tok/sec 1126276.4616
for step 2346 | loss 3.785500 | norm 0.3689 | time 465.1525 ms | tok/sec 1127131.4196
for step 2347 | loss 3.819075 | norm 0.3483 | time 465.7784 ms | tok/sec 1125616.9357
for step 2348 | loss 3.803041 | norm 0.3357 | time 465.5049 ms | tok/sec 1126278.1922
for step 2349 | loss 3.806484 | norm 0.3690 | time 465.8732 ms | tok/sec 1125387.6664
for step 2350 | loss 3.759233 | norm 0.3412 | time 466.0199 ms | tok/sec 1125033.5770
for step 2351 | loss 3.759498 | norm 0.3723 | time 465.5287 ms | tok/sec 1126220.5103
for step 2352 | loss 3.740992 | norm 0.3504 | time 466.6955 ms | tok/sec 1123404.7620
for step 2353 | loss 3.776714 | norm 0.3328 | time 465.4672 ms | tok/sec 1126369.3415
for step 2354 | loss 3.765006 | norm 0.3227 | time 465.4346 ms | tok/sec 1126448.3881
for step 2355 | loss 3.815189 | norm 0.3469 | time 465.5876 ms | tok/sec 1126078.0614
for step 2356 | loss 3.677493 | norm 0.3480 | time 466.0938 ms | tok/sec 1124855.1773
for step 2357 | loss 3.614988 | norm 0.3131 | time 465.3707 ms | tok/sec 1126603.0514
for step 2358 | loss 3.613596 | norm 0.3257 | time 465.7655 ms | tok/sec 1125648.0498
for step 2359 | loss 3.553663 | norm 0.3627 | time 465.7907 ms | tok/sec 1125586.9757
for step 2360 | loss 3.592756 | norm 0.3632 | time 464.5917 ms | tok/sec 1128491.8631
for step 2361 | loss 3.627776 | norm 0.3441 | time 465.5380 ms | tok/sec 1126198.0160
for step 2362 | loss 3.633237 | norm 0.3567 | time 466.6054 ms | tok/sec 1123621.7413
for step 2363 | loss 3.617115 | norm 0.3575 | time 466.5151 ms | tok/sec 1123839.3788
for step 2364 | loss 3.598141 | norm 0.3719 | time 466.0587 ms | tok/sec 1124939.7663
for step 2365 | loss 3.615927 | norm 0.3896 | time 466.0256 ms | tok/sec 1125019.7634
for step 2366 | loss 3.638378 | norm 0.3429 | time 466.2719 ms | tok/sec 1124425.5241
for step 2367 | loss 3.677806 | norm 0.3447 | time 466.1276 ms | tok/sec 1124773.4777
for step 2368 | loss 3.807752 | norm 0.3426 | time 466.4009 ms | tok/sec 1124114.5611
for step 2369 | loss 3.917887 | norm 0.3359 | time 465.9998 ms | tok/sec 1125081.9273
for step 2370 | loss 3.877320 | norm 0.3260 | time 465.9305 ms | tok/sec 1125249.4586
for step 2371 | loss 3.903604 | norm 0.3310 | time 466.5947 ms | tok/sec 1123647.5777
for step 2372 | loss 3.857216 | norm 0.3714 | time 465.4870 ms | tok/sec 1126321.4574
for step 2373 | loss 3.829162 | norm 0.3736 | time 465.8942 ms | tok/sec 1125336.9863
for step 2374 | loss 3.870007 | norm 0.3851 | time 464.9298 ms | tok/sec 1127671.2709
for step 2375 | loss 3.837736 | norm 0.4167 | time 466.0366 ms | tok/sec 1124993.2883
for step 2376 | loss 3.826420 | norm 0.3870 | time 465.1108 ms | tok/sec 1127232.5302
for step 2377 | loss 3.972172 | norm 0.4039 | time 466.0766 ms | tok/sec 1124896.6071
for step 2378 | loss 3.869875 | norm 0.4574 | time 464.9866 ms | tok/sec 1127533.6580
for step 2379 | loss 3.818822 | norm 0.4360 | time 465.1568 ms | tok/sec 1127121.0207
for step 2380 | loss 3.768477 | norm 0.3608 | time 466.5380 ms | tok/sec 1123784.2437
for step 2381 | loss 3.813663 | norm 0.3470 | time 464.8454 ms | tok/sec 1127876.0178
for step 2382 | loss 3.869692 | norm 0.3921 | time 465.0173 ms | tok/sec 1127459.0835
for step 2383 | loss 3.822416 | norm 0.3660 | time 465.3041 ms | tok/sec 1126764.1079
for step 2384 | loss 3.902387 | norm 0.3713 | time 465.1594 ms | tok/sec 1127114.6659
for step 2385 | loss 3.837343 | norm 0.3492 | time 464.2375 ms | tok/sec 1129353.0891
for step 2386 | loss 3.775132 | norm 0.3327 | time 466.0306 ms | tok/sec 1125007.6768
for step 2387 | loss 3.817125 | norm 0.3199 | time 465.5817 ms | tok/sec 1126092.4777
for step 2388 | loss 3.834337 | norm 0.3185 | time 465.6339 ms | tok/sec 1125966.2037
for step 2389 | loss 3.828964 | norm 0.3079 | time 464.1867 ms | tok/sec 1129476.6431
for step 2390 | loss 3.756001 | norm 0.3166 | time 465.6885 ms | tok/sec 1125834.1942
for step 2391 | loss 3.820172 | norm 0.3041 | time 465.3649 ms | tok/sec 1126616.9039
for step 2392 | loss 3.797407 | norm 0.3326 | time 466.1622 ms | tok/sec 1124690.0642
for step 2393 | loss 3.815726 | norm 0.3391 | time 465.8010 ms | tok/sec 1125562.2022
for step 2394 | loss 3.726249 | norm 0.3362 | time 465.5056 ms | tok/sec 1126276.4616
for step 2395 | loss 3.752985 | norm 0.3317 | time 466.1520 ms | tok/sec 1124714.7994
for step 2396 | loss 3.782221 | norm 0.3132 | time 465.9383 ms | tok/sec 1125230.4577
for step 2397 | loss 3.699400 | norm 0.3005 | time 465.2417 ms | tok/sec 1126915.3928
for step 2398 | loss 3.712529 | norm 0.3170 | time 466.6557 ms | tok/sec 1123500.6129
for step 2399 | loss 3.756845 | norm 0.3338 | time 465.3664 ms | tok/sec 1126613.4407
for step 2400 | loss 3.804845 | norm 0.3629 | time 465.2615 ms | tok/sec 1126867.4623
for step 2401 | loss 3.737859 | norm 0.3835 | time 465.9860 ms | tok/sec 1125115.3144
for step 2402 | loss 3.570829 | norm 0.3972 | time 464.7498 ms | tok/sec 1128108.0384
for step 2403 | loss 3.557694 | norm 0.4332 | time 465.8284 ms | tok/sec 1125495.9528
for step 2404 | loss 3.672689 | norm 0.4861 | time 465.9300 ms | tok/sec 1125250.6102
for step 2405 | loss 3.593890 | norm 0.4056 | time 465.7419 ms | tok/sec 1125705.0968
for step 2406 | loss 3.519285 | norm 0.3727 | time 465.2989 ms | tok/sec 1126776.8096
for step 2407 | loss 3.576428 | norm 0.3458 | time 465.5790 ms | tok/sec 1126098.8209
for step 2408 | loss 3.570146 | norm 0.3194 | time 465.0364 ms | tok/sec 1127412.8408
for step 2409 | loss 3.618092 | norm 0.3163 | time 464.8705 ms | tok/sec 1127815.2800
for step 2410 | loss 3.599045 | norm 0.3356 | time 465.2314 ms | tok/sec 1126940.2259
for step 2411 | loss 3.608972 | norm 0.3219 | time 466.4745 ms | tok/sec 1123937.0273
for step 2412 | loss 3.648528 | norm 0.3233 | time 465.1630 ms | tok/sec 1127106.0004
for step 2413 | loss 3.688136 | norm 0.3041 | time 465.1983 ms | tok/sec 1127020.5078
for step 2414 | loss 3.811624 | norm 0.3266 | time 465.4939 ms | tok/sec 1126304.7278
for step 2415 | loss 3.860778 | norm 0.3244 | time 465.3883 ms | tok/sec 1126560.3416
for step 2416 | loss 3.781682 | norm 0.3104 | time 465.1804 ms | tok/sec 1127063.8302
for step 2417 | loss 3.821259 | norm 0.3656 | time 465.7965 ms | tok/sec 1125573.1485
for step 2418 | loss 3.806571 | norm 0.4856 | time 466.4185 ms | tok/sec 1124072.0398
for step 2419 | loss 3.802607 | norm 0.4932 | time 465.6179 ms | tok/sec 1126004.8325
for step 2420 | loss 3.775283 | norm 0.4330 | time 465.3189 ms | tok/sec 1126728.3136
for step 2421 | loss 3.850983 | norm 0.4519 | time 465.2112 ms | tok/sec 1126989.3178
for step 2422 | loss 3.807857 | norm 0.4557 | time 465.9853 ms | tok/sec 1125117.0414
for step 2423 | loss 3.848169 | norm 0.4272 | time 465.6191 ms | tok/sec 1126001.9496
for step 2424 | loss 3.818465 | norm 0.3580 | time 465.7822 ms | tok/sec 1125607.7171
for step 2425 | loss 3.826231 | norm 0.3897 | time 466.2240 ms | tok/sec 1124541.1013
for step 2426 | loss 3.873171 | norm 0.3453 | time 465.6837 ms | tok/sec 1125845.7221
for step 2427 | loss 3.810062 | norm 0.3271 | time 466.4619 ms | tok/sec 1123967.4741
for step 2428 | loss 3.819489 | norm 0.3575 | time 466.2569 ms | tok/sec 1124461.7473
for step 2429 | loss 3.819367 | norm 0.3271 | time 467.1545 ms | tok/sec 1122301.0734
for step 2430 | loss 3.841286 | norm 0.3156 | time 466.3615 ms | tok/sec 1124209.3837
for step 2431 | loss 3.825665 | norm 0.3308 | time 465.1542 ms | tok/sec 1127127.3756
for step 2432 | loss 3.811835 | norm 0.3596 | time 465.3857 ms | tok/sec 1126566.6901
for step 2433 | loss 3.843718 | norm 0.4620 | time 465.4734 ms | tok/sec 1126354.3413
for step 2434 | loss 3.899961 | norm 0.5231 | time 465.9758 ms | tok/sec 1125140.0683
for step 2435 | loss 3.860653 | norm 0.5705 | time 466.4237 ms | tok/sec 1124059.3990
for step 2436 | loss 3.837753 | norm 0.4116 | time 466.3796 ms | tok/sec 1124165.7059
for step 2437 | loss 3.807804 | norm 0.4117 | time 465.3311 ms | tok/sec 1126698.8715
for step 2438 | loss 3.808964 | norm 0.3588 | time 466.1849 ms | tok/sec 1124635.4208
for step 2439 | loss 3.774906 | norm 0.3307 | time 465.7321 ms | tok/sec 1125728.7240
for step 2440 | loss 3.748491 | norm 0.3390 | time 465.8985 ms | tok/sec 1125326.6204
for step 2441 | loss 3.766254 | norm 0.3290 | time 466.9507 ms | tok/sec 1122791.0153
for step 2442 | loss 3.742440 | norm 0.3026 | time 466.0645 ms | tok/sec 1124925.9550
for step 2443 | loss 3.757041 | norm 0.2947 | time 466.3763 ms | tok/sec 1124173.7515
for step 2444 | loss 3.799393 | norm 0.2980 | time 465.4629 ms | tok/sec 1126379.7266
for step 2445 | loss 3.799824 | norm 0.2982 | time 466.9387 ms | tok/sec 1122819.6801
for step 2446 | loss 3.767897 | norm 0.3241 | time 465.0950 ms | tok/sec 1127270.6680
for step 2447 | loss 3.816133 | norm 0.2999 | time 465.6701 ms | tok/sec 1125878.5782
for step 2448 | loss 3.702314 | norm 0.2749 | time 466.3584 ms | tok/sec 1124216.8553
for step 2449 | loss 3.690263 | norm 0.3214 | time 465.6215 ms | tok/sec 1125996.1840
for step 2450 | loss 3.630884 | norm 0.3856 | time 465.9157 ms | tok/sec 1125285.1591
for step 2451 | loss 3.585408 | norm 0.3927 | time 465.0183 ms | tok/sec 1127456.7713
for step 2452 | loss 3.605442 | norm 0.3422 | time 466.0051 ms | tok/sec 1125069.2637
for step 2453 | loss 3.632237 | norm 0.3472 | time 465.5170 ms | tok/sec 1126248.7737
for step 2454 | loss 3.671762 | norm 0.3255 | time 466.3203 ms | tok/sec 1124308.8210
for step 2455 | loss 3.552198 | norm 0.3552 | time 466.3842 ms | tok/sec 1124154.7869
for step 2456 | loss 3.622081 | norm 0.4197 | time 465.2555 ms | tok/sec 1126881.8988
for step 2457 | loss 3.564487 | norm 0.4444 | time 466.2011 ms | tok/sec 1124596.3108
for step 2458 | loss 3.592756 | norm 0.4000 | time 466.3935 ms | tok/sec 1124132.3751
for step 2459 | loss 3.648852 | norm 0.3430 | time 465.6770 ms | tok/sec 1125861.8617
for step 2460 | loss 3.566022 | norm 0.3195 | time 465.5461 ms | tok/sec 1126178.4063
for step 2461 | loss 3.704692 | norm 0.3414 | time 466.7733 ms | tok/sec 1123217.6989
for step 2462 | loss 3.821815 | norm 0.3500 | time 465.7652 ms | tok/sec 1125648.6260
for step 2463 | loss 3.796195 | norm 0.3288 | time 465.7857 ms | tok/sec 1125599.0747
for step 2464 | loss 3.793035 | norm 0.3515 | time 465.9977 ms | tok/sec 1125087.1079
for step 2465 | loss 3.873582 | norm 0.3837 | time 465.7454 ms | tok/sec 1125696.4530
for step 2466 | loss 3.774731 | norm 0.4008 | time 465.5104 ms | tok/sec 1126264.9248
for step 2467 | loss 3.816268 | norm 0.3807 | time 466.3889 ms | tok/sec 1124143.2936
for step 2468 | loss 3.827760 | norm 0.3861 | time 465.7147 ms | tok/sec 1125770.7944
for step 2469 | loss 3.801813 | norm 0.3891 | time 466.3734 ms | tok/sec 1124180.6479
for step 2470 | loss 3.770902 | norm 0.3463 | time 466.2006 ms | tok/sec 1124597.4610
for step 2471 | loss 3.805949 | norm 0.3477 | time 466.2395 ms | tok/sec 1124503.7230
for step 2472 | loss 3.811186 | norm 0.3276 | time 465.8709 ms | tok/sec 1125393.4258
for step 2473 | loss 3.796835 | norm 0.3090 | time 465.3816 ms | tok/sec 1126576.5017
for step 2474 | loss 3.797539 | norm 0.3172 | time 466.2344 ms | tok/sec 1124515.7988
for step 2475 | loss 3.822301 | norm 0.3491 | time 465.1256 ms | tok/sec 1127196.7061
Will loading at 0 from edu_fineweb10B/edufineweb_train_000014.npy
for step 2476 | loss 3.834615 | norm 0.3743 | time 1603.0941 ms | tok/sec 327047.5512
for step 2477 | loss 3.776360 | norm 0.3740 | time 465.1837 ms | tok/sec 1127055.7431
for step 2478 | loss 3.801419 | norm 0.3673 | time 464.5922 ms | tok/sec 1128490.7048
for step 2479 | loss 3.810664 | norm 0.3817 | time 463.6681 ms | tok/sec 1130739.8352
for step 2480 | loss 3.816582 | norm 0.3434 | time 464.2313 ms | tok/sec 1129368.1694
for step 2481 | loss 3.784680 | norm 0.3575 | time 465.3232 ms | tok/sec 1126717.9221
for step 2482 | loss 3.825290 | norm 0.3823 | time 464.9260 ms | tok/sec 1127680.5234
for step 2483 | loss 3.802798 | norm 0.3531 | time 464.7577 ms | tok/sec 1128088.9409
for step 2484 | loss 3.773124 | norm 0.3994 | time 465.4117 ms | tok/sec 1126503.7850
for step 2485 | loss 3.737011 | norm 0.3367 | time 465.7373 ms | tok/sec 1125716.0459
for step 2486 | loss 3.744288 | norm 0.3591 | time 465.8127 ms | tok/sec 1125533.9732
for step 2487 | loss 3.772597 | norm 0.3399 | time 465.1575 ms | tok/sec 1127119.2876
for step 2488 | loss 3.774152 | norm 0.3797 | time 464.8232 ms | tok/sec 1127929.8196
for step 2489 | loss 3.724014 | norm 0.3539 | time 465.7164 ms | tok/sec 1125766.7602
for step 2490 | loss 3.799356 | norm 0.3502 | time 465.6169 ms | tok/sec 1126007.1388
for step 2491 | loss 3.675043 | norm 0.3663 | time 465.7729 ms | tok/sec 1125630.1878
for step 2492 | loss 3.790707 | norm 0.3626 | time 467.0181 ms | tok/sec 1122628.8000
for step 2493 | loss 3.801161 | norm 0.4030 | time 466.0571 ms | tok/sec 1124943.7946
for step 2494 | loss 3.763629 | norm 0.3784 | time 466.3613 ms | tok/sec 1124209.9585
for step 2495 | loss 3.759147 | norm 0.3806 | time 466.9738 ms | tok/sec 1122735.4097
for step 2496 | loss 3.657599 | norm 0.3726 | time 466.0153 ms | tok/sec 1125044.5130
for step 2497 | loss 3.541867 | norm 0.3625 | time 465.8756 ms | tok/sec 1125381.9071
for step 2498 | loss 3.601618 | norm 0.3270 | time 466.4156 ms | tok/sec 1124078.9349
for step 2499 | loss 3.615220 | norm 0.3384 | time 467.2244 ms | tok/sec 1122133.2735
validation loss 3.8082
HellaSwag accuracy: 2575/10042=0.2564
> Hello, I'm a language model, and you'll be more than happy one, if I'm looking for a place to talk, if I'm a language
> Hello, I'm a language model, a game of course, and I'm writing a blog!
3.3) To answer these, I'm a
> Hello, I'm a language model, I believe I don't understand the difference between the two types of languages and the difference between two languages, but if I
> Hello, I'm a language model, and I'm pretty much happy. My kid is like reading this all around them. Just for my kid, I'm
> Hello, I'm a language model, but I was a computer-based model where you can use this to teach and communicate information easily.
If you are
> > Hello, I'm a language model, although there's still a lot more to it.
So what is a culture change?
There's not a whole
Hello, I'm a language model, she has a name with a simple suffixes. So then I'm asking my kids "why are these people (or
> Hello, I'm a language model, the code as it is used to identify many different technologies. It will include a set of input components and an input output> 
Hello, I'm a language model, so I'm writing it all the time and I'm the best of all the fun I'm writing (I just want
> Hello, I'm a language model, but I want to make a difference that was easy. I need to say the first part, as part of my conversation> 
Hello, I'm a language model, and you all learned it in a year or another when I said, "We used to say and write a number.
> Hello, I'm a language model, I have four major categories of words. Some of them are in the same environment (in language format, context and context
> Hello, I'm a language model, and I was a little surprised if I want to make it a language because there are a lot of other words that have
> > Hello, I'm a language model, an object, or one would be like to be useful, so there are two people who live in the world. What
Hello, I'm a language model, but don't know if I can code it in plain French, like just my native languages. I would have to learn
> > > Hello, I'm a language model, and I'm a linguist. I find it all on my list but I're pretty confused the way it is,
Hello, I'm a language model, and I can't see anything, but is there. I'm a math-like subject.
- A type of
Hello, I'm a language model, but I's a culture designer.
I wanted to be very different and I wanted to be a software developer, so
> > > Hello, I'm a language model, let's start with a concept called G/S programmatic approach.
I love the term G/S program to
Hello, I'm a language model, I don't have any idea. This is an open-ended set of tutorials that would be useful for other developers,
Hello, I'm a language model, and in the case of the first day of finishing the book, I can see which languages, and they are often forgotten
> > Hello, I'm a language model, and I am like everyone else - they can't live anymore, or there's a lot of fun. I know how
Hello, I'm a language model, and I'm not as good as you're talking about it. I'm writing about a variety of subjects for myself right
> Hello, I'm a language model, maybe a way to teach students how to be 'muthyd, so I want to give them the best choice'.
> > Hello, I'm a language model, I was able to be a translator and speak with some other languages like I would. I'm going to have to speak
Hello, I'm a language model, and I'm not really learning the things it is all about. I couldn't learn this for 10 more languages, although
> Hello, I'm a language model, in English. But a simple word meaning a long, long.
- Is It? A new thing with the same> 
Hello, I'm a language model, and we want to be great to show how a student can be a winner.
I'm going to be teaching you
> > Hello, I'm a language model, but it has been long-range as a lot of fun. My favourite name, My name, I'm a big
Hello, I'm a language model, and it’s a combination of programming languages that can help you learn and solve problems quickly. I am trying to
> > Hello, I'm a language model, though. For my students, I'm not interested enough for the definition of 'molecules' and 'sole
Hello, I'm a language model, and yes, that's fine when I am going to get a job in the backseat, and will call you "
for step 2500 | loss 3.642519 | norm 0.3313 | time 17748.4326 ms | tok/sec 29539.9606
for step 2501 | loss 3.612972 | norm 0.3031 | time 461.5335 ms | tok/sec 1135969.4307
for step 2502 | loss 3.602911 | norm 0.2983 | time 461.7817 ms | tok/sec 1135358.8814
for step 2503 | loss 3.655390 | norm 0.3100 | time 462.9757 ms | tok/sec 1132430.8257
for step 2504 | loss 3.632021 | norm 0.3128 | time 462.6203 ms | tok/sec 1133300.9970
for step 2505 | loss 3.554267 | norm 0.3485 | time 463.7289 ms | tok/sec 1130591.5906
for step 2506 | loss 3.619867 | norm 0.3733 | time 462.1558 ms | tok/sec 1134439.8978
for step 2507 | loss 3.589700 | norm 0.3367 | time 462.3132 ms | tok/sec 1134053.7716
for step 2508 | loss 3.756180 | norm 0.3287 | time 462.1661 ms | tok/sec 1134414.7331
for step 2509 | loss 3.800303 | norm 0.3357 | time 463.1114 ms | tok/sec 1132099.1004
for step 2510 | loss 3.800749 | norm 0.3194 | time 463.3112 ms | tok/sec 1131610.9026
for step 2511 | loss 3.832803 | norm 0.3668 | time 463.6204 ms | tok/sec 1130856.1326
for step 2512 | loss 3.794101 | norm 0.3927 | time 464.1659 ms | tok/sec 1129527.1166
for step 2513 | loss 3.799967 | norm 0.4282 | time 464.4639 ms | tok/sec 1128802.3557
for step 2514 | loss 3.791229 | norm 0.4300 | time 464.0291 ms | tok/sec 1129860.2387
for step 2515 | loss 3.770634 | norm 0.4463 | time 464.2377 ms | tok/sec 1129352.5091
for step 2516 | loss 3.807633 | norm 0.3893 | time 463.8612 ms | tok/sec 1130269.0750
for step 2517 | loss 3.801214 | norm 0.4129 | time 464.4313 ms | tok/sec 1128881.7441
for step 2518 | loss 3.797050 | norm 0.4090 | time 464.7377 ms | tok/sec 1128137.5541
for step 2519 | loss 3.803239 | norm 0.3566 | time 465.1802 ms | tok/sec 1127064.4078
for step 2520 | loss 3.788629 | norm 0.3817 | time 464.4690 ms | tok/sec 1128790.1877
for step 2521 | loss 3.777595 | norm 0.3553 | time 463.7845 ms | tok/sec 1130456.1697
for step 2522 | loss 3.849689 | norm 0.3390 | time 466.5003 ms | tok/sec 1123874.9898
for step 2523 | loss 3.751533 | norm 0.3615 | time 464.5758 ms | tok/sec 1128530.6653
for step 2524 | loss 3.791425 | norm 0.3409 | time 464.0079 ms | tok/sec 1129911.9077
for step 2525 | loss 3.821734 | norm 0.3493 | time 464.9310 ms | tok/sec 1127668.3795
for step 2526 | loss 3.900767 | norm 0.3736 | time 464.8974 ms | tok/sec 1127749.9219
for step 2527 | loss 3.793147 | norm 0.3511 | time 465.7304 ms | tok/sec 1125732.7580
for step 2528 | loss 3.780837 | norm 0.3285 | time 464.8046 ms | tok/sec 1127974.9476
for step 2529 | loss 3.805722 | norm 0.3294 | time 465.0769 ms | tok/sec 1127314.5874
for step 2530 | loss 3.793318 | norm 0.3497 | time 465.3549 ms | tok/sec 1126641.1466
for step 2531 | loss 3.778217 | norm 0.3228 | time 464.8800 ms | tok/sec 1127792.1436
for step 2532 | loss 3.738183 | norm 0.3120 | time 465.9870 ms | tok/sec 1125113.0118
for step 2533 | loss 3.719772 | norm 0.3262 | time 466.0194 ms | tok/sec 1125034.7281
for step 2534 | loss 3.742238 | norm 0.3328 | time 465.2603 ms | tok/sec 1126870.3496
for step 2535 | loss 3.670647 | norm 0.3485 | time 465.1821 ms | tok/sec 1127059.7866
for step 2536 | loss 3.708025 | norm 0.3551 | time 465.2903 ms | tok/sec 1126797.5949
for step 2537 | loss 3.724996 | norm 0.3262 | time 466.0647 ms | tok/sec 1124925.3795
for step 2538 | loss 3.777486 | norm 0.3353 | time 466.3754 ms | tok/sec 1124176.0503
for step 2539 | loss 3.757284 | norm 0.3261 | time 465.3764 ms | tok/sec 1126589.1992
for step 2540 | loss 3.686090 | norm 0.3274 | time 465.5757 ms | tok/sec 1126106.8943
for step 2541 | loss 3.725709 | norm 0.3209 | time 465.8480 ms | tok/sec 1125448.7190
for step 2542 | loss 3.651447 | norm 0.3593 | time 465.6408 ms | tok/sec 1125949.4847
for step 2543 | loss 3.525702 | norm 0.3613 | time 466.1365 ms | tok/sec 1124752.1917
for step 2544 | loss 3.562095 | norm 0.3763 | time 467.2952 ms | tok/sec 1121963.2341
for step 2545 | loss 3.580514 | norm 0.3993 | time 465.7376 ms | tok/sec 1125715.4696
for step 2546 | loss 3.575302 | norm 0.3891 | time 465.0943 ms | tok/sec 1127272.4016
for step 2547 | loss 3.593076 | norm 0.3798 | time 465.8058 ms | tok/sec 1125550.6800
for step 2548 | loss 3.592574 | norm 0.3399 | time 465.8120 ms | tok/sec 1125535.7015
for step 2549 | loss 3.561810 | norm 0.3530 | time 465.6358 ms | tok/sec 1125961.5915
for step 2550 | loss 3.610049 | norm 0.3190 | time 465.0695 ms | tok/sec 1127332.5030
for step 2551 | loss 3.626500 | norm 0.3372 | time 466.4028 ms | tok/sec 1124109.9641
for step 2552 | loss 3.560692 | norm 0.3665 | time 465.8139 ms | tok/sec 1125531.0928
for step 2553 | loss 3.542989 | norm 0.3644 | time 465.4024 ms | tok/sec 1126526.2915
for step 2554 | loss 3.720206 | norm 0.3573 | time 466.4965 ms | tok/sec 1123884.1801
for step 2555 | loss 3.851411 | norm 0.3642 | time 465.7176 ms | tok/sec 1125763.8785
for step 2556 | loss 3.811809 | norm 0.3327 | time 465.1668 ms | tok/sec 1127096.7574
for step 2557 | loss 3.823943 | norm 0.3299 | time 466.5575 ms | tok/sec 1123737.1533
for step 2558 | loss 3.852093 | norm 0.3722 | time 465.3120 ms | tok/sec 1126745.0558
for step 2559 | loss 3.863455 | norm 0.5191 | time 466.4376 ms | tok/sec 1124026.0744
for step 2560 | loss 3.842014 | norm 0.5405 | time 465.6041 ms | tok/sec 1126038.2744
for step 2561 | loss 3.773320 | norm 0.3749 | time 465.5640 ms | tok/sec 1126135.1520
for step 2562 | loss 3.777729 | norm 0.4363 | time 465.9429 ms | tok/sec 1125219.5181
for step 2563 | loss 3.820627 | norm 0.3871 | time 466.0983 ms | tok/sec 1124844.2450
for step 2564 | loss 3.821607 | norm 0.3534 | time 465.4911 ms | tok/sec 1126311.6503
for step 2565 | loss 3.860923 | norm 0.3737 | time 465.8127 ms | tok/sec 1125533.9732
for step 2566 | loss 3.768581 | norm 0.3763 | time 465.8940 ms | tok/sec 1125337.5622
for step 2567 | loss 3.798187 | norm 0.3569 | time 466.2147 ms | tok/sec 1124563.5295
for step 2568 | loss 3.801235 | norm 0.3139 | time 465.3938 ms | tok/sec 1126547.0676
for step 2569 | loss 3.779927 | norm 0.3452 | time 466.1691 ms | tok/sec 1124673.3830
for step 2570 | loss 3.767586 | norm 0.3626 | time 464.5829 ms | tok/sec 1128513.2908
for step 2571 | loss 3.830106 | norm 0.3329 | time 466.0509 ms | tok/sec 1124958.7573
for step 2572 | loss 3.767906 | norm 0.3233 | time 465.9085 ms | tok/sec 1125302.4343
for step 2573 | loss 3.816128 | norm 0.3014 | time 465.2517 ms | tok/sec 1126891.1383
for step 2574 | loss 3.825376 | norm 0.3094 | time 464.5498 ms | tok/sec 1128593.7970
for step 2575 | loss 3.771692 | norm 0.2800 | time 465.4346 ms | tok/sec 1126448.3881
for step 2576 | loss 3.715716 | norm 0.2925 | time 465.5461 ms | tok/sec 1126178.4063
for step 2577 | loss 3.782268 | norm 0.3164 | time 466.6007 ms | tok/sec 1123633.2240
for step 2578 | loss 3.772210 | norm 0.3216 | time 465.6463 ms | tok/sec 1125936.2250
for step 2579 | loss 3.719473 | norm 0.2858 | time 466.0022 ms | tok/sec 1125076.1711
for step 2580 | loss 3.706048 | norm 0.3114 | time 465.2078 ms | tok/sec 1126997.4040
for step 2581 | loss 3.698359 | norm 0.3028 | time 465.0815 ms | tok/sec 1127303.6073
for step 2582 | loss 3.712633 | norm 0.3121 | time 465.6956 ms | tok/sec 1125816.9026
for step 2583 | loss 3.696530 | norm 0.3190 | time 464.9744 ms | tok/sec 1127563.1437
for step 2584 | loss 3.700100 | norm 0.3612 | time 465.2581 ms | tok/sec 1126875.5467
for step 2585 | loss 3.701297 | norm 0.3661 | time 465.7907 ms | tok/sec 1125586.9757
for step 2586 | loss 3.712276 | norm 0.3596 | time 464.4592 ms | tok/sec 1128813.9446
for step 2587 | loss 3.721620 | norm 0.3264 | time 465.5142 ms | tok/sec 1126255.6955
for step 2588 | loss 3.717654 | norm 0.3291 | time 465.1592 ms | tok/sec 1127115.2437
for step 2589 | loss 3.638556 | norm 0.3792 | time 465.4777 ms | tok/sec 1126343.9567
for step 2590 | loss 3.609044 | norm 0.3851 | time 465.5275 ms | tok/sec 1126223.3942
for step 2591 | loss 3.626559 | norm 0.4280 | time 466.1119 ms | tok/sec 1124811.4493
for step 2592 | loss 3.581857 | norm 0.4341 | time 465.4365 ms | tok/sec 1126443.7719
for step 2593 | loss 3.736393 | norm 0.4180 | time 465.8589 ms | tok/sec 1125422.2236
for step 2594 | loss 3.534984 | norm 0.3801 | time 465.0571 ms | tok/sec 1127362.5560
for step 2595 | loss 3.522095 | norm 0.3605 | time 466.0201 ms | tok/sec 1125033.0014
for step 2596 | loss 3.532873 | norm 0.3912 | time 465.1654 ms | tok/sec 1127100.2235
for step 2597 | loss 3.583216 | norm 0.3579 | time 465.5433 ms | tok/sec 1126185.3273
for step 2598 | loss 3.547167 | norm 0.3154 | time 465.6055 ms | tok/sec 1126034.8148
for step 2599 | loss 3.571855 | norm 0.3479 | time 464.9448 ms | tok/sec 1127634.8407
for step 2600 | loss 3.569485 | norm 0.3134 | time 464.2432 ms | tok/sec 1129339.1692
for step 2601 | loss 3.691692 | norm 0.3229 | time 465.3652 ms | tok/sec 1126616.3267
for step 2602 | loss 3.786942 | norm 0.3380 | time 465.9607 ms | tok/sec 1125176.3374
for step 2603 | loss 3.733742 | norm 0.3008 | time 465.0159 ms | tok/sec 1127462.5519
for step 2604 | loss 3.871286 | norm 0.3083 | time 465.4479 ms | tok/sec 1126416.0757
for step 2605 | loss 3.797886 | norm 0.3260 | time 465.8606 ms | tok/sec 1125418.1918
for step 2606 | loss 3.896798 | norm 0.3138 | time 465.6465 ms | tok/sec 1125935.6485
for step 2607 | loss 3.808057 | norm 0.3246 | time 465.8504 ms | tok/sec 1125442.9590
for step 2608 | loss 3.791261 | norm 0.3192 | time 465.1995 ms | tok/sec 1127017.6198
for step 2609 | loss 3.793163 | norm 0.3204 | time 466.0046 ms | tok/sec 1125070.4149
for step 2610 | loss 3.791001 | norm 0.3413 | time 465.5464 ms | tok/sec 1126177.8295
for step 2611 | loss 3.795518 | norm 0.3497 | time 466.0890 ms | tok/sec 1124866.6853
for step 2612 | loss 3.793457 | norm 0.3709 | time 466.2664 ms | tok/sec 1124438.7482
for step 2613 | loss 3.831360 | norm 0.3702 | time 466.1493 ms | tok/sec 1124721.1271
for step 2614 | loss 3.826354 | norm 0.4143 | time 466.0144 ms | tok/sec 1125046.8153
for step 2615 | loss 3.840610 | norm 0.4123 | time 465.4508 ms | tok/sec 1126409.1519
for step 2616 | loss 3.797334 | norm 0.3651 | time 465.0540 ms | tok/sec 1127370.0696
for step 2617 | loss 3.740978 | norm 0.3527 | time 465.5201 ms | tok/sec 1126241.2751
for step 2618 | loss 3.774671 | norm 0.3733 | time 465.8477 ms | tok/sec 1125449.2950
for step 2619 | loss 3.800183 | norm 0.3815 | time 466.3646 ms | tok/sec 1124201.9123
for step 2620 | loss 3.862910 | norm 0.3997 | time 466.4493 ms | tok/sec 1123997.9225
for step 2621 | loss 3.820041 | norm 0.3959 | time 465.3773 ms | tok/sec 1126586.8905
for step 2622 | loss 3.781746 | norm 0.3566 | time 465.8725 ms | tok/sec 1125389.3942
for step 2623 | loss 3.843530 | norm 0.3248 | time 465.1039 ms | tok/sec 1127249.2874
for step 2624 | loss 3.747958 | norm 0.3557 | time 465.6520 ms | tok/sec 1125922.3893
for step 2625 | loss 3.754204 | norm 0.3394 | time 465.5566 ms | tok/sec 1126153.0300
for step 2626 | loss 3.751818 | norm 0.3006 | time 465.6143 ms | tok/sec 1126013.4811
for step 2627 | loss 3.707865 | norm 0.2853 | time 466.3317 ms | tok/sec 1124281.2297
for step 2628 | loss 3.693236 | norm 0.2962 | time 465.9884 ms | tok/sec 1125109.5579
for step 2629 | loss 3.691208 | norm 0.3061 | time 465.6529 ms | tok/sec 1125920.0833
for step 2630 | loss 3.791708 | norm 0.3156 | time 466.1086 ms | tok/sec 1124819.5042
for step 2631 | loss 3.723407 | norm 0.2954 | time 466.5236 ms | tok/sec 1123818.7025
for step 2632 | loss 3.693911 | norm 0.3327 | time 465.4627 ms | tok/sec 1126380.3035
for step 2633 | loss 3.814131 | norm 0.3240 | time 465.5917 ms | tok/sec 1126068.2586
for step 2634 | loss 3.709934 | norm 0.3802 | time 465.6162 ms | tok/sec 1126008.8685
for step 2635 | loss 3.810234 | norm 0.4448 | time 465.4126 ms | tok/sec 1126501.4767
for step 2636 | loss 3.681312 | norm 0.4323 | time 465.6303 ms | tok/sec 1125974.8517
for step 2637 | loss 3.669122 | norm 0.3499 | time 465.4720 ms | tok/sec 1126357.8028
for step 2638 | loss 3.496227 | norm 0.3949 | time 466.0234 ms | tok/sec 1125024.9434
for step 2639 | loss 3.630232 | norm 0.4297 | time 464.5839 ms | tok/sec 1128510.9743
for step 2640 | loss 3.622818 | norm 0.4175 | time 465.8449 ms | tok/sec 1125456.2070
for step 2641 | loss 3.563557 | norm 0.3994 | time 465.3585 ms | tok/sec 1126632.4884
for step 2642 | loss 3.618474 | norm 0.3569 | time 465.8978 ms | tok/sec 1125328.3481
for step 2643 | loss 3.526350 | norm 0.3331 | time 466.1024 ms | tok/sec 1124834.4636
for step 2644 | loss 3.533551 | norm 0.3688 | time 466.0625 ms | tok/sec 1124930.5587
for step 2645 | loss 3.545589 | norm 0.2982 | time 465.8775 ms | tok/sec 1125377.2996
for step 2646 | loss 3.570123 | norm 0.3154 | time 466.4857 ms | tok/sec 1123910.0286
for step 2647 | loss 3.588641 | norm 0.3050 | time 465.4191 ms | tok/sec 1126485.8958
for step 2648 | loss 3.618472 | norm 0.3129 | time 465.6885 ms | tok/sec 1125834.1942
for step 2649 | loss 3.588072 | norm 0.3035 | time 465.7269 ms | tok/sec 1125741.4025
for step 2650 | loss 3.780550 | norm 0.3378 | time 466.7966 ms | tok/sec 1123161.4774
for step 2651 | loss 3.768835 | norm 0.3515 | time 466.7153 ms | tok/sec 1123357.1296
for step 2652 | loss 3.787731 | norm 0.3224 | time 466.5463 ms | tok/sec 1123764.1436
for step 2653 | loss 3.833908 | norm 0.3172 | time 465.6944 ms | tok/sec 1125819.7845
for step 2654 | loss 3.758767 | norm 0.3488 | time 465.1215 ms | tok/sec 1127206.5286
for step 2655 | loss 3.781055 | norm 0.3536 | time 465.9443 ms | tok/sec 1125216.0635
for step 2656 | loss 3.759038 | norm 0.3668 | time 465.0254 ms | tok/sec 1127439.4299
for step 2657 | loss 3.789199 | norm 0.3490 | time 465.8039 ms | tok/sec 1125555.2888
for step 2658 | loss 3.755175 | norm 0.3787 | time 466.0513 ms | tok/sec 1124957.6064
for step 2659 | loss 3.853122 | norm 0.4069 | time 465.4248 ms | tok/sec 1126472.0465
for step 2660 | loss 3.776819 | norm 0.4166 | time 465.2348 ms | tok/sec 1126932.1406
for step 2661 | loss 3.829397 | norm 0.4158 | time 464.7763 ms | tok/sec 1128043.8037
for step 2662 | loss 3.772093 | norm 0.4074 | time 466.2597 ms | tok/sec 1124454.8474
for step 2663 | loss 3.719390 | norm 0.4209 | time 466.2766 ms | tok/sec 1124414.0252
for step 2664 | loss 3.774072 | norm 0.4202 | time 466.4853 ms | tok/sec 1123911.1775
for step 2665 | loss 3.814433 | norm 0.4028 | time 466.0871 ms | tok/sec 1124871.2885
Will loading at 0 from edu_fineweb10B/edufineweb_train_000015.npy
for step 2666 | loss 3.720801 | norm 0.3820 | time 1617.4672 ms | tok/sec 324141.3559
for step 2667 | loss 3.853042 | norm 0.3481 | time 463.3834 ms | tok/sec 1131434.4861
for step 2668 | loss 3.802325 | norm 0.3854 | time 464.1230 ms | tok/sec 1129631.5588
for step 2669 | loss 3.775409 | norm 0.3454 | time 465.2348 ms | tok/sec 1126932.1406
for step 2670 | loss 3.811146 | norm 0.3147 | time 463.6016 ms | tok/sec 1130902.0767
for step 2671 | loss 3.715337 | norm 0.3523 | time 464.4611 ms | tok/sec 1128809.3090
for step 2672 | loss 3.824313 | norm 0.3207 | time 464.7839 ms | tok/sec 1128025.2870
for step 2673 | loss 3.751631 | norm 0.3673 | time 465.6706 ms | tok/sec 1125877.4253
for step 2674 | loss 3.727849 | norm 0.3969 | time 464.8888 ms | tok/sec 1127770.7432
for step 2675 | loss 3.753121 | norm 0.3172 | time 465.0762 ms | tok/sec 1127316.3212
for step 2676 | loss 3.841261 | norm 0.3348 | time 465.9913 ms | tok/sec 1125102.6501
for step 2677 | loss 3.758868 | norm 0.3294 | time 465.7843 ms | tok/sec 1125602.5317
for step 2678 | loss 3.716160 | norm 0.3208 | time 466.2781 ms | tok/sec 1124410.5756
for step 2679 | loss 3.692399 | norm 0.3255 | time 466.0118 ms | tok/sec 1125053.1468
for step 2680 | loss 3.695187 | norm 0.3332 | time 465.8446 ms | tok/sec 1125456.7830
for step 2681 | loss 3.704455 | norm 0.2931 | time 464.8578 ms | tok/sec 1127845.9373
for step 2682 | loss 3.726809 | norm 0.3122 | time 465.6079 ms | tok/sec 1126029.0489
for step 2683 | loss 3.699793 | norm 0.2970 | time 465.9612 ms | tok/sec 1125175.1860
for step 2684 | loss 3.698715 | norm 0.2892 | time 465.3971 ms | tok/sec 1126538.9879
for step 2685 | loss 3.713645 | norm 0.2980 | time 464.9501 ms | tok/sec 1127622.1196
for step 2686 | loss 3.575303 | norm 0.2893 | time 465.7316 ms | tok/sec 1125729.8766
for step 2687 | loss 3.601967 | norm 0.3012 | time 465.4417 ms | tok/sec 1126431.0777
for step 2688 | loss 3.576817 | norm 0.3241 | time 466.0890 ms | tok/sec 1124866.6853
for step 2689 | loss 3.501037 | norm 0.3144 | time 465.6212 ms | tok/sec 1125996.7606
for step 2690 | loss 3.510854 | norm 0.3234 | time 466.2254 ms | tok/sec 1124537.6509
for step 2691 | loss 3.580789 | norm 0.3226 | time 465.9472 ms | tok/sec 1125209.1544
for step 2692 | loss 3.553315 | norm 0.3266 | time 471.3166 ms | tok/sec 1112390.3268
for step 2693 | loss 3.585235 | norm 0.3286 | time 466.3842 ms | tok/sec 1124154.7869
for step 2694 | loss 3.589066 | norm 0.3248 | time 466.1262 ms | tok/sec 1124776.9296
for step 2695 | loss 3.548566 | norm 0.3216 | time 466.1138 ms | tok/sec 1124806.8465
for step 2696 | loss 3.561318 | norm 0.3669 | time 465.5573 ms | tok/sec 1126151.2998
for step 2697 | loss 3.681207 | norm 0.3706 | time 465.2226 ms | tok/sec 1126961.5948
for step 2698 | loss 3.782521 | norm 0.3722 | time 465.8101 ms | tok/sec 1125540.3102
for step 2699 | loss 3.827021 | norm 0.3356 | time 465.6119 ms | tok/sec 1126019.2469
for step 2700 | loss 3.737659 | norm 0.3281 | time 464.7150 ms | tok/sec 1128192.5385
for step 2701 | loss 3.764396 | norm 0.3510 | time 465.9879 ms | tok/sec 1125110.7092
for step 2702 | loss 3.811933 | norm 0.3665 | time 466.0575 ms | tok/sec 1124942.6436
for step 2703 | loss 3.781116 | norm 0.3886 | time 466.4974 ms | tok/sec 1123881.8825
for step 2704 | loss 3.758832 | norm 0.3711 | time 465.2705 ms | tok/sec 1126845.5195
for step 2705 | loss 3.736089 | norm 0.3506 | time 465.8606 ms | tok/sec 1125418.1918
for step 2706 | loss 3.815705 | norm 0.3378 | time 465.6403 ms | tok/sec 1125950.6377
for step 2707 | loss 3.739844 | norm 0.3831 | time 465.6348 ms | tok/sec 1125963.8976
for step 2708 | loss 3.735799 | norm 0.3900 | time 465.0784 ms | tok/sec 1127311.1200
for step 2709 | loss 3.773806 | norm 0.3855 | time 465.9822 ms | tok/sec 1125124.5250
for step 2710 | loss 3.746902 | norm 0.3744 | time 465.6327 ms | tok/sec 1125969.0864
for step 2711 | loss 3.725018 | norm 0.3191 | time 465.4348 ms | tok/sec 1126447.8111
for step 2712 | loss 3.782377 | norm 0.3405 | time 466.2476 ms | tok/sec 1124484.1723
for step 2713 | loss 3.757056 | norm 0.4220 | time 465.4660 ms | tok/sec 1126372.2263
for step 2714 | loss 3.742926 | norm 0.4646 | time 465.9336 ms | tok/sec 1125241.9734
for step 2715 | loss 3.798985 | norm 0.4128 | time 465.9462 ms | tok/sec 1125211.4575
for step 2716 | loss 3.792560 | norm 0.4732 | time 465.4386 ms | tok/sec 1126438.5788
for step 2717 | loss 3.800592 | norm 0.4519 | time 465.5125 ms | tok/sec 1126259.7333
for step 2718 | loss 3.774743 | norm 0.4404 | time 465.3897 ms | tok/sec 1126556.8788
for step 2719 | loss 3.756223 | norm 0.3937 | time 466.5802 ms | tok/sec 1123682.6024
for step 2720 | loss 3.787933 | norm 0.3606 | time 465.3015 ms | tok/sec 1126770.4587
for step 2721 | loss 3.752933 | norm 0.3367 | time 465.5371 ms | tok/sec 1126200.3230
for step 2722 | loss 3.715429 | norm 0.3081 | time 466.8226 ms | tok/sec 1123098.9520
for step 2723 | loss 3.743386 | norm 0.3193 | time 465.9791 ms | tok/sec 1125132.0087
for step 2724 | loss 3.793535 | norm 0.3083 | time 465.4033 ms | tok/sec 1126523.9831
for step 2725 | loss 3.716332 | norm 0.3336 | time 466.2931 ms | tok/sec 1124374.3557
for step 2726 | loss 3.659989 | norm 0.2965 | time 465.5054 ms | tok/sec 1126277.0385
for step 2727 | loss 3.709595 | norm 0.2967 | time 466.2433 ms | tok/sec 1124494.5226
for step 2728 | loss 3.681781 | norm 0.3222 | time 465.6510 ms | tok/sec 1125924.6952
for step 2729 | loss 3.714530 | norm 0.2966 | time 465.5230 ms | tok/sec 1126234.3534
for step 2730 | loss 3.708677 | norm 0.3575 | time 466.9256 ms | tok/sec 1122851.2131
for step 2731 | loss 3.690976 | norm 0.3686 | time 465.7772 ms | tok/sec 1125619.8166
for step 2732 | loss 3.765832 | norm 0.3673 | time 467.0074 ms | tok/sec 1122654.5908
for step 2733 | loss 3.685090 | norm 0.3391 | time 465.9045 ms | tok/sec 1125312.2238
for step 2734 | loss 3.631988 | norm 0.3475 | time 465.9994 ms | tok/sec 1125083.0785
for step 2735 | loss 3.547483 | norm 0.3504 | time 466.7251 ms | tok/sec 1123333.6018
for step 2736 | loss 3.571034 | norm 0.3608 | time 466.5353 ms | tok/sec 1123790.5610
for step 2737 | loss 3.511836 | norm 0.3289 | time 466.1143 ms | tok/sec 1124805.6959
for step 2738 | loss 3.574825 | norm 0.3321 | time 465.1897 ms | tok/sec 1127041.3021
for step 2739 | loss 3.528982 | norm 0.3181 | time 465.2784 ms | tok/sec 1126826.4647
for step 2740 | loss 3.507858 | norm 0.3102 | time 465.3249 ms | tok/sec 1126713.8810
for step 2741 | loss 3.558950 | norm 0.3401 | time 466.3103 ms | tok/sec 1124332.9645
for step 2742 | loss 3.585250 | norm 0.3160 | time 466.2089 ms | tok/sec 1124577.3319
for step 2743 | loss 3.578998 | norm 0.3325 | time 466.2013 ms | tok/sec 1124595.7357
for step 2744 | loss 3.567616 | norm 0.3533 | time 465.4939 ms | tok/sec 1126304.7278
for step 2745 | loss 3.555247 | norm 0.3750 | time 466.9530 ms | tok/sec 1122785.2825
for step 2746 | loss 3.821514 | norm 0.3639 | time 465.3018 ms | tok/sec 1126769.8814
for step 2747 | loss 3.749650 | norm 0.3387 | time 465.8420 ms | tok/sec 1125463.1191
for step 2748 | loss 3.817503 | norm 0.3532 | time 465.9443 ms | tok/sec 1125216.0635
for step 2749 | loss 3.764568 | norm 0.4026 | time 466.3627 ms | tok/sec 1124206.5101
validation loss 3.7617
HellaSwag accuracy: 2622/10042=0.2611
> Hello, I'm a language model, and you can't make a model sound like we are seeing all the time.
The "M" in the "
> Hello, I'm a language model, and it is based on the rules in it. So to get around the house and explain it (or to the house
> Hello, I'm a language model, so I have been able to use a language model to teach them how to use language. And so the students in my
> Hello, I'm a language model, and I'm also an older language model if you're already familiar with that other language group. It's a lot of
> Hello, I'm a language model, not so much. And here is a picture of it, that’s fine. I'm a computer science language
> Hello, I'm a language model, and I've been taught by some of other languages that have been introduced at the same time, but have no explicit language
> Hello, I'm a language model, so I'll always put a little one on this list. If you want me to put a little one on there,
> > > Hello, I'm a language model, but I know that it's a very small piece of language to write this:
You must now find yourself with the
Hello, I'm a language model, and I've got some of that. I have a lot of an idea.
So just because the program started last
Hello, I'm a language model, and that is it's hard for me. I don't want to read it's weird, and I need to write
> Hello, I'm a language model, to learn the prefixes. Then, it says 'twist," as well as 'no' and 'wait'.
> Hello, I'm a language model, but I could not explain the exact position without the slightest hint, which means that the question is "yes."
But> 
Hello, I'm a language model, I read at conferences in England, my parents love to speak and a really fun piece for some children?
There is
> Hello, I'm a language model, but I'll also try to teach me by asking my students questions and writing questions on the subject you don't think of> 
Hello, I'm a language model, and you learned from your language. At a conference (May), we received the question, 'I am a language model
> Hello, I'm a language model, so there are a number of ways I got to my friend a few times from. If everything will go smoothly and what> 
Hello, I'm a language model, and I'm a programming language. I never use a language model.<|endoftext|>At school, my siblings and friends are more
> Hello, I'm a language model, using a combination of both, in different ways. To simplify this, I have just completed the same tutorial in my tutorial
> >Hello, I'm a language model, and I am writing about the English language of the way I'm written. I'm a language model, but that language
 Hello, I'm a language model, and I was a very, very much made word. I'm using very complicated language. I think that, in this
> Hello, I'm a language model, but it's not that kind of for English teachers.
Sylvia is a large, flat-topped plastic> 
Hello, I'm a language model, but with a little bit of work to do with other techniques, so I have a lot of practice when I need to
> Hello, I'm a language model, but I didn't mean my only ability to say "I want it," but I said it was one of my most
> Hello, I'm a language model, but with a few exceptions. I would like to be a few months before my exam. We'll give you the perfect
> > Hello, I'm a language model, so we have a general idea of the following.
I'm using the sentence, "I've got to try."
Hello, I'm a language model, and I'm going from one of the above languages, and have been getting along, too.
|The whole book
> > Hello, I'm a language model, who would need more translations from different languages and learn more about English. I also know how many words can become phonetically
Hello, I'm a language model, and a very good way to explain what you can do with the words "C" and "C" and a little
> Hello, I'm a language model, but it sounds a bunch of random holes. It's a really silly idea. It shows the steps to make the world> 
Hello, I'm a language model, and it can be pretty straightforward.
1. Now that we have the idea that the idea "I'm thinking of
> Hello, I'm a language model, i want to play an easy-to-play "with a" language." That means that those words are "out> 
Hello, I'm a language model, and having a strong vocabulary, with a good understanding of a word, especially if you haven't always have the language.
for step 2750 | loss 3.740524 | norm 0.4090 | time 17726.2714 ms | tok/sec 29576.8912
for step 2751 | loss 3.773429 | norm 0.3601 | time 461.1742 ms | tok/sec 1136854.4545
for step 2752 | loss 3.942333 | norm 0.3854 | time 461.9176 ms | tok/sec 1135024.8529
for step 2753 | loss 3.780395 | norm 0.3910 | time 462.4417 ms | tok/sec 1133738.6302
for step 2754 | loss 3.864329 | norm 0.4283 | time 463.1147 ms | tok/sec 1132090.9409
for step 2755 | loss 3.802339 | norm 0.4264 | time 463.7144 ms | tok/sec 1130627.0495
for step 2756 | loss 3.756622 | norm 0.3808 | time 463.4941 ms | tok/sec 1131164.4366
for step 2757 | loss 3.795994 | norm 0.3298 | time 463.6292 ms | tok/sec 1130834.6158
for step 2758 | loss 3.725003 | norm 0.3127 | time 464.5808 ms | tok/sec 1128518.5031
for step 2759 | loss 3.761191 | norm 0.3163 | time 464.0527 ms | tok/sec 1129802.7698
for step 2760 | loss 3.765473 | norm 0.3013 | time 463.3496 ms | tok/sec 1131517.1563
for step 2761 | loss 3.785942 | norm 0.3191 | time 463.6345 ms | tok/sec 1130821.8224
for step 2762 | loss 3.742080 | norm 0.3157 | time 462.9180 ms | tok/sec 1132571.9699
for step 2763 | loss 3.696367 | norm 0.3277 | time 463.3574 ms | tok/sec 1131497.9431
for step 2764 | loss 3.776165 | norm 0.3232 | time 464.0608 ms | tok/sec 1129783.0344
for step 2765 | loss 3.732269 | norm 0.3153 | time 464.2634 ms | tok/sec 1129289.8724
for step 2766 | loss 3.722721 | norm 0.2949 | time 463.7442 ms | tok/sec 1130554.3902
for step 2767 | loss 3.731588 | norm 0.3202 | time 463.9089 ms | tok/sec 1130152.8982
for step 2768 | loss 3.771604 | norm 0.2979 | time 463.8088 ms | tok/sec 1130396.8970
for step 2769 | loss 3.760157 | norm 0.3008 | time 464.6757 ms | tok/sec 1128288.0504
for step 2770 | loss 3.751120 | norm 0.2995 | time 464.0377 ms | tok/sec 1129839.3403
for step 2771 | loss 3.672785 | norm 0.3146 | time 464.2031 ms | tok/sec 1129436.6156
for step 2772 | loss 3.684508 | norm 0.3011 | time 464.7179 ms | tok/sec 1128185.5928
for step 2773 | loss 3.712192 | norm 0.3073 | time 464.8881 ms | tok/sec 1127772.4783
for step 2774 | loss 3.650587 | norm 0.2971 | time 463.6519 ms | tok/sec 1130779.3736
for step 2775 | loss 3.699040 | norm 0.2984 | time 463.8884 ms | tok/sec 1130202.8513
for step 2776 | loss 3.730077 | norm 0.3206 | time 465.6310 ms | tok/sec 1125973.1221
for step 2777 | loss 3.723595 | norm 0.3508 | time 465.0092 ms | tok/sec 1127478.7379
for step 2778 | loss 3.731776 | norm 0.3114 | time 464.6406 ms | tok/sec 1128373.1564
for step 2779 | loss 3.691651 | norm 0.3194 | time 464.6051 ms | tok/sec 1128459.4334
for step 2780 | loss 3.716351 | norm 0.3485 | time 464.3574 ms | tok/sec 1129061.4233
for step 2781 | loss 3.627895 | norm 0.3233 | time 464.8144 ms | tok/sec 1127951.2260
for step 2782 | loss 3.576902 | norm 0.3850 | time 465.2135 ms | tok/sec 1126983.5421
for step 2783 | loss 3.585010 | norm 0.4787 | time 464.4039 ms | tok/sec 1128948.3926
for step 2784 | loss 3.584645 | norm 0.5119 | time 465.7967 ms | tok/sec 1125572.5723
for step 2785 | loss 3.548731 | norm 0.4005 | time 465.9824 ms | tok/sec 1125123.9494
for step 2786 | loss 3.535285 | norm 0.3850 | time 465.6794 ms | tok/sec 1125856.0975
for step 2787 | loss 3.523954 | norm 0.3973 | time 465.2293 ms | tok/sec 1126945.4237
for step 2788 | loss 3.577978 | norm 0.3933 | time 465.4458 ms | tok/sec 1126421.2687
for step 2789 | loss 3.652698 | norm 0.3711 | time 464.9370 ms | tok/sec 1127653.9229
for step 2790 | loss 3.571740 | norm 0.3737 | time 466.2042 ms | tok/sec 1124588.8342
for step 2791 | loss 3.559213 | norm 0.3556 | time 465.4286 ms | tok/sec 1126462.8138
for step 2792 | loss 3.503678 | norm 0.2951 | time 465.6332 ms | tok/sec 1125967.9333
for step 2793 | loss 3.643442 | norm 0.3300 | time 465.1110 ms | tok/sec 1127231.9523
for step 2794 | loss 3.764641 | norm 0.3513 | time 465.7059 ms | tok/sec 1125792.1190
for step 2795 | loss 3.697469 | norm 0.3399 | time 466.4140 ms | tok/sec 1124082.9571
for step 2796 | loss 3.799684 | norm 0.3420 | time 465.8000 ms | tok/sec 1125564.5066
for step 2797 | loss 3.783244 | norm 0.3375 | time 464.4518 ms | tok/sec 1128831.9077
for step 2798 | loss 3.808818 | norm 0.3720 | time 465.0538 ms | tok/sec 1127370.6475
for step 2799 | loss 3.745421 | norm 0.3747 | time 465.9917 ms | tok/sec 1125101.4988
for step 2800 | loss 3.760739 | norm 0.3633 | time 466.4972 ms | tok/sec 1123882.4569
for step 2801 | loss 3.742511 | norm 0.3838 | time 465.3668 ms | tok/sec 1126612.2863
for step 2802 | loss 3.815775 | norm 0.3612 | time 464.7155 ms | tok/sec 1128191.3809
for step 2803 | loss 3.853786 | norm 0.3811 | time 465.4691 ms | tok/sec 1126364.7260
for step 2804 | loss 3.768649 | norm 0.3495 | time 464.9360 ms | tok/sec 1127656.2359
for step 2805 | loss 3.757613 | norm 0.3623 | time 465.9414 ms | tok/sec 1125222.9727
for step 2806 | loss 3.738769 | norm 0.3479 | time 465.3895 ms | tok/sec 1126557.4559
for step 2807 | loss 3.741052 | norm 0.3843 | time 465.7469 ms | tok/sec 1125692.9955
for step 2808 | loss 3.739492 | norm 0.3645 | time 465.8346 ms | tok/sec 1125480.9758
for step 2809 | loss 3.703389 | norm 0.3733 | time 465.6467 ms | tok/sec 1125935.0720
for step 2810 | loss 3.693582 | norm 0.4264 | time 465.2069 ms | tok/sec 1126999.7143
for step 2811 | loss 3.796024 | norm 0.3735 | time 465.1115 ms | tok/sec 1127230.7967
for step 2812 | loss 3.718193 | norm 0.3193 | time 464.9360 ms | tok/sec 1127656.2359
for step 2813 | loss 3.773513 | norm 0.3396 | time 465.7230 ms | tok/sec 1125750.6233
for step 2814 | loss 3.718775 | norm 0.3298 | time 466.1531 ms | tok/sec 1124711.9231
for step 2815 | loss 3.742507 | norm 0.3243 | time 465.2910 ms | tok/sec 1126795.8628
for step 2816 | loss 3.694313 | norm 0.3141 | time 465.1809 ms | tok/sec 1127062.6749
for step 2817 | loss 3.759333 | norm 0.3109 | time 465.6579 ms | tok/sec 1125907.9774
for step 2818 | loss 3.688132 | norm 0.3292 | time 465.5609 ms | tok/sec 1126142.6491
for step 2819 | loss 3.646995 | norm 0.2939 | time 465.1611 ms | tok/sec 1127110.6220
for step 2820 | loss 3.725505 | norm 0.2972 | time 466.1343 ms | tok/sec 1124757.3693
for step 2821 | loss 3.710876 | norm 0.2771 | time 465.9698 ms | tok/sec 1125154.4605
for step 2822 | loss 3.724434 | norm 0.2953 | time 465.3046 ms | tok/sec 1126762.9532
for step 2823 | loss 3.724692 | norm 0.2967 | time 465.4326 ms | tok/sec 1126453.0043
for step 2824 | loss 3.688029 | norm 0.3023 | time 465.2534 ms | tok/sec 1126887.0960
for step 2825 | loss 3.682907 | norm 0.2940 | time 465.8005 ms | tok/sec 1125563.3544
for step 2826 | loss 3.670609 | norm 0.3022 | time 465.8401 ms | tok/sec 1125467.7272
for step 2827 | loss 3.661959 | norm 0.3045 | time 465.8403 ms | tok/sec 1125467.1512
for step 2828 | loss 3.683637 | norm 0.3180 | time 465.7826 ms | tok/sec 1125606.5648
for step 2829 | loss 3.620656 | norm 0.3412 | time 465.8458 ms | tok/sec 1125453.9030
for step 2830 | loss 3.530028 | norm 0.3277 | time 466.1188 ms | tok/sec 1124794.7645
for step 2831 | loss 3.492167 | norm 0.3657 | time 465.1375 ms | tok/sec 1127167.8173
for step 2832 | loss 3.556186 | norm 0.4472 | time 466.5635 ms | tok/sec 1123722.7973
for step 2833 | loss 3.497497 | norm 0.4244 | time 465.0245 ms | tok/sec 1127441.7421
for step 2834 | loss 3.549779 | norm 0.3909 | time 466.0966 ms | tok/sec 1124848.2727
for step 2835 | loss 3.580485 | norm 0.3842 | time 465.3139 ms | tok/sec 1126740.4372
for step 2836 | loss 3.519119 | norm 0.3227 | time 466.0101 ms | tok/sec 1125057.1760
for step 2837 | loss 3.537758 | norm 0.3365 | time 465.2517 ms | tok/sec 1126891.1383
for step 2838 | loss 3.517436 | norm 0.2976 | time 465.4489 ms | tok/sec 1126413.7678
for step 2839 | loss 3.490000 | norm 0.3090 | time 466.3677 ms | tok/sec 1124194.4409
for step 2840 | loss 3.496431 | norm 0.3095 | time 466.2011 ms | tok/sec 1124596.3108
for step 2841 | loss 3.554511 | norm 0.3164 | time 465.8899 ms | tok/sec 1125347.3523
for step 2842 | loss 3.776093 | norm 0.3584 | time 464.8786 ms | tok/sec 1127795.6140
for step 2843 | loss 3.746114 | norm 0.3690 | time 467.3324 ms | tok/sec 1121873.9410
for step 2844 | loss 3.789561 | norm 0.3830 | time 465.7021 ms | tok/sec 1125801.3407
for step 2845 | loss 3.738202 | norm 0.3852 | time 466.1086 ms | tok/sec 1124819.5042
for step 2846 | loss 3.700272 | norm 0.3770 | time 466.3405 ms | tok/sec 1124259.9623
for step 2847 | loss 3.799584 | norm 0.3572 | time 466.5067 ms | tok/sec 1123859.4815
for step 2848 | loss 3.743208 | norm 0.3598 | time 466.5174 ms | tok/sec 1123833.6353
for step 2849 | loss 3.719502 | norm 0.3497 | time 466.3148 ms | tok/sec 1124322.0423
for step 2850 | loss 3.748314 | norm 0.3414 | time 466.9290 ms | tok/sec 1122843.1863
for step 2851 | loss 3.756384 | norm 0.3599 | time 466.0738 ms | tok/sec 1124903.5123
for step 2852 | loss 3.697173 | norm 0.3142 | time 465.8167 ms | tok/sec 1125524.1799
for step 2853 | loss 3.749560 | norm 0.3275 | time 465.9872 ms | tok/sec 1125112.4361
for step 2854 | loss 3.743817 | norm 0.3188 | time 465.5015 ms | tok/sec 1126286.2681
for step 2855 | loss 3.752057 | norm 0.3124 | time 465.4915 ms | tok/sec 1126310.4966
for step 2856 | loss 3.764549 | norm 0.2964 | time 465.9603 ms | tok/sec 1125177.4889
Will loading at 0 from edu_fineweb10B/edufineweb_train_000016.npy
for step 2857 | loss 3.721624 | norm 0.3059 | time 1619.5769 ms | tok/sec 323719.1083
for step 2858 | loss 3.727161 | norm 0.3067 | time 464.4594 ms | tok/sec 1128813.3651
for step 2859 | loss 3.760171 | norm 0.3215 | time 465.4338 ms | tok/sec 1126450.1192
for step 2860 | loss 3.729041 | norm 0.4072 | time 465.0056 ms | tok/sec 1127487.4091
for step 2861 | loss 3.746330 | norm 0.3785 | time 465.7216 ms | tok/sec 1125754.0812
for step 2862 | loss 3.747076 | norm 0.3774 | time 466.9814 ms | tok/sec 1122717.0668
for step 2863 | loss 3.753095 | norm 0.3569 | time 465.5967 ms | tok/sec 1126056.1494
for step 2864 | loss 3.635587 | norm 0.3214 | time 465.4162 ms | tok/sec 1126492.8206
for step 2865 | loss 3.666878 | norm 0.3201 | time 465.8537 ms | tok/sec 1125434.8952
for step 2866 | loss 3.657215 | norm 0.3068 | time 465.7805 ms | tok/sec 1125611.7502
for step 2867 | loss 3.727233 | norm 0.3221 | time 464.8588 ms | tok/sec 1127843.6235
for step 2868 | loss 3.684655 | norm 0.3221 | time 466.1136 ms | tok/sec 1124807.4219
for step 2869 | loss 3.655190 | norm 0.3298 | time 465.0939 ms | tok/sec 1127273.5573
for step 2870 | loss 3.688016 | norm 0.3502 | time 466.1961 ms | tok/sec 1124608.3886
for step 2871 | loss 3.710002 | norm 0.3366 | time 465.8358 ms | tok/sec 1125478.0957
for step 2872 | loss 3.668798 | norm 0.2930 | time 466.2993 ms | tok/sec 1124359.4085
for step 2873 | loss 3.668323 | norm 0.2894 | time 464.8044 ms | tok/sec 1127975.5262
for step 2874 | loss 3.701672 | norm 0.3191 | time 466.2225 ms | tok/sec 1124544.5518
for step 2875 | loss 3.652370 | norm 0.3072 | time 465.6992 ms | tok/sec 1125808.2570
for step 2876 | loss 3.656229 | norm 0.3113 | time 465.1551 ms | tok/sec 1127125.0647
for step 2877 | loss 3.539904 | norm 0.3180 | time 465.7862 ms | tok/sec 1125597.9224
for step 2878 | loss 3.478431 | norm 0.3274 | time 466.1808 ms | tok/sec 1124645.1987
for step 2879 | loss 3.498470 | norm 0.3310 | time 465.8315 ms | tok/sec 1125488.4643
for step 2880 | loss 3.498435 | norm 0.3425 | time 465.2154 ms | tok/sec 1126978.9215
for step 2881 | loss 3.522291 | norm 0.3506 | time 465.8105 ms | tok/sec 1125539.1580
for step 2882 | loss 3.497579 | norm 0.3352 | time 466.0611 ms | tok/sec 1124934.0115
for step 2883 | loss 3.578495 | norm 0.3192 | time 465.3463 ms | tok/sec 1126661.9269
for step 2884 | loss 3.522686 | norm 0.3179 | time 465.6639 ms | tok/sec 1125893.5658
for step 2885 | loss 3.537798 | norm 0.3113 | time 466.2063 ms | tok/sec 1124583.6581
for step 2886 | loss 3.590999 | norm 0.3550 | time 466.4485 ms | tok/sec 1123999.6461
for step 2887 | loss 3.556171 | norm 0.3607 | time 468.6069 ms | tok/sec 1118822.5042
for step 2888 | loss 3.599058 | norm 0.3857 | time 464.4921 ms | tok/sec 1128733.9863
for step 2889 | loss 3.731925 | norm 0.4487 | time 464.9210 ms | tok/sec 1127692.6675
for step 2890 | loss 3.770474 | norm 0.4051 | time 465.3137 ms | tok/sec 1126741.0145
for step 2891 | loss 3.781450 | norm 0.3314 | time 465.9238 ms | tok/sec 1125265.5811
for step 2892 | loss 3.753319 | norm 0.3742 | time 465.4529 ms | tok/sec 1126403.9591
for step 2893 | loss 3.722166 | norm 0.4045 | time 466.1489 ms | tok/sec 1124722.2776
for step 2894 | loss 3.764468 | norm 0.3613 | time 465.0838 ms | tok/sec 1127297.8283
for step 2895 | loss 3.777138 | norm 0.3511 | time 464.4771 ms | tok/sec 1128770.4876
for step 2896 | loss 3.722003 | norm 0.3507 | time 464.5722 ms | tok/sec 1128539.3527
for step 2897 | loss 3.749404 | norm 0.3183 | time 465.4388 ms | tok/sec 1126438.0018
for step 2898 | loss 3.774694 | norm 0.3300 | time 464.7400 ms | tok/sec 1128131.7666
for step 2899 | loss 3.768781 | norm 0.3376 | time 464.2789 ms | tok/sec 1129252.1778
for step 2900 | loss 3.760576 | norm 0.3295 | time 464.8399 ms | tok/sec 1127889.3231
for step 2901 | loss 3.723439 | norm 0.3302 | time 464.6075 ms | tok/sec 1128453.6426
for step 2902 | loss 3.730202 | norm 0.3283 | time 464.8573 ms | tok/sec 1127847.0942
for step 2903 | loss 3.742531 | norm 0.3653 | time 465.3020 ms | tok/sec 1126769.3040
for step 2904 | loss 3.774893 | norm 0.3626 | time 465.2069 ms | tok/sec 1126999.7143
for step 2905 | loss 3.760291 | norm 0.3571 | time 464.6702 ms | tok/sec 1128301.3654
for step 2906 | loss 3.757531 | norm 0.3164 | time 465.1132 ms | tok/sec 1127226.7519
for step 2907 | loss 3.763024 | norm 0.3222 | time 464.4868 ms | tok/sec 1128746.7325
for step 2908 | loss 3.763539 | norm 0.3251 | time 465.6692 ms | tok/sec 1125880.8840
for step 2909 | loss 3.689884 | norm 0.2994 | time 466.3067 ms | tok/sec 1124341.5874
for step 2910 | loss 3.676158 | norm 0.2784 | time 465.8439 ms | tok/sec 1125458.5110
for step 2911 | loss 3.722386 | norm 0.3103 | time 465.7891 ms | tok/sec 1125591.0087
for step 2912 | loss 3.660115 | norm 0.3079 | time 465.4093 ms | tok/sec 1126509.5558
for step 2913 | loss 3.634427 | norm 0.3010 | time 465.7633 ms | tok/sec 1125653.2357
for step 2914 | loss 3.693954 | norm 0.3332 | time 465.6606 ms | tok/sec 1125901.6362
for step 2915 | loss 3.644406 | norm 0.3265 | time 465.3041 ms | tok/sec 1126764.1079
for step 2916 | loss 3.671917 | norm 0.3709 | time 465.7421 ms | tok/sec 1125704.5206
for step 2917 | loss 3.726643 | norm 0.3882 | time 465.3378 ms | tok/sec 1126682.7080
for step 2918 | loss 3.722558 | norm 0.3597 | time 464.9150 ms | tok/sec 1127707.1252
for step 2919 | loss 3.686228 | norm 0.3922 | time 465.0064 ms | tok/sec 1127485.6749
for step 2920 | loss 3.684191 | norm 0.3818 | time 465.1220 ms | tok/sec 1127205.3730
for step 2921 | loss 3.660920 | norm 0.3743 | time 465.6901 ms | tok/sec 1125830.1594
for step 2922 | loss 3.590671 | norm 0.3738 | time 465.0419 ms | tok/sec 1127399.5467
for step 2923 | loss 3.696251 | norm 0.3787 | time 465.0466 ms | tok/sec 1127387.9868
for step 2924 | loss 3.605228 | norm 0.3422 | time 465.5612 ms | tok/sec 1126142.0724
for step 2925 | loss 3.585271 | norm 0.3498 | time 466.3310 ms | tok/sec 1124282.9541
for step 2926 | loss 3.526821 | norm 0.3396 | time 465.1992 ms | tok/sec 1127018.1974
for step 2927 | loss 3.518840 | norm 0.3546 | time 464.4930 ms | tok/sec 1128731.6689
for step 2928 | loss 3.470118 | norm 0.3177 | time 465.2131 ms | tok/sec 1126984.6972
for step 2929 | loss 3.514593 | norm 0.3136 | time 464.7696 ms | tok/sec 1128060.0064
for step 2930 | loss 3.537532 | norm 0.3269 | time 465.1539 ms | tok/sec 1127127.9533
for step 2931 | loss 3.530550 | norm 0.3368 | time 465.7969 ms | tok/sec 1125571.9962
for step 2932 | loss 3.537124 | norm 0.3496 | time 465.2357 ms | tok/sec 1126929.8306
for step 2933 | loss 3.480223 | norm 0.3436 | time 464.7975 ms | tok/sec 1127992.3055
for step 2934 | loss 3.521639 | norm 0.3442 | time 465.9808 ms | tok/sec 1125127.9790
for step 2935 | loss 3.572627 | norm 0.3170 | time 465.3003 ms | tok/sec 1126773.3455
for step 2936 | loss 3.683843 | norm 0.3113 | time 465.1837 ms | tok/sec 1127055.7431
for step 2937 | loss 3.774644 | norm 0.3699 | time 464.4370 ms | tok/sec 1128867.8358
for step 2938 | loss 3.775517 | norm 0.3816 | time 464.8364 ms | tok/sec 1127898.0007
for step 2939 | loss 3.713630 | norm 0.3202 | time 465.2555 ms | tok/sec 1126881.8988
for step 2940 | loss 3.715127 | norm 0.3305 | time 464.6444 ms | tok/sec 1128363.8925
for step 2941 | loss 3.784468 | norm 0.3487 | time 464.7272 ms | tok/sec 1128163.0199
for step 2942 | loss 3.768354 | norm 0.3307 | time 465.2021 ms | tok/sec 1127011.2662
for step 2943 | loss 3.761531 | norm 0.3502 | time 465.2324 ms | tok/sec 1126937.9158
for step 2944 | loss 3.704974 | norm 0.4021 | time 465.7183 ms | tok/sec 1125762.1496
for step 2945 | loss 3.748106 | norm 0.4051 | time 466.0895 ms | tok/sec 1124865.5345
for step 2946 | loss 3.761944 | norm 0.3674 | time 465.0996 ms | tok/sec 1127259.6886
for step 2947 | loss 3.759751 | norm 0.3431 | time 465.4431 ms | tok/sec 1126427.6157
for step 2948 | loss 3.690946 | norm 0.3015 | time 464.9453 ms | tok/sec 1127633.6842
for step 2949 | loss 3.709387 | norm 0.3246 | time 465.0090 ms | tok/sec 1127479.3159
for step 2950 | loss 3.741622 | norm 0.3178 | time 464.6223 ms | tok/sec 1128417.7408
for step 2951 | loss 3.691619 | norm 0.3017 | time 465.5209 ms | tok/sec 1126239.5446
for step 2952 | loss 3.743105 | norm 0.3021 | time 465.6065 ms | tok/sec 1126032.5084
for step 2953 | loss 3.686211 | norm 0.3072 | time 465.4899 ms | tok/sec 1126314.5348
for step 2954 | loss 3.699376 | norm 0.3013 | time 464.6869 ms | tok/sec 1128260.8423
for step 2955 | loss 3.720979 | norm 0.3379 | time 465.2481 ms | tok/sec 1126899.8005
for step 2956 | loss 3.703220 | norm 0.3441 | time 465.2483 ms | tok/sec 1126899.2230
for step 2957 | loss 3.792600 | norm 0.3378 | time 463.9356 ms | tok/sec 1130087.8497
for step 2958 | loss 3.749042 | norm 0.3418 | time 464.4582 ms | tok/sec 1128816.2624
for step 2959 | loss 3.744335 | norm 0.3829 | time 464.6790 ms | tok/sec 1128279.9457
for step 2960 | loss 3.615140 | norm 0.3910 | time 466.0800 ms | tok/sec 1124888.5510
for step 2961 | loss 3.638251 | norm 0.4330 | time 465.0249 ms | tok/sec 1127440.5860
for step 2962 | loss 3.672459 | norm 0.4278 | time 466.2156 ms | tok/sec 1124561.2292
for step 2963 | loss 3.642831 | norm 0.3633 | time 465.7786 ms | tok/sec 1125616.3596
for step 2964 | loss 3.634684 | norm 0.3231 | time 464.8974 ms | tok/sec 1127749.9219
for step 2965 | loss 3.702559 | norm 0.3190 | time 465.2371 ms | tok/sec 1126926.3655
for step 2966 | loss 3.617536 | norm 0.3137 | time 465.5135 ms | tok/sec 1126257.4260
for step 2967 | loss 3.692046 | norm 0.3315 | time 465.1017 ms | tok/sec 1127254.4880
for step 2968 | loss 3.641464 | norm 0.3279 | time 465.3780 ms | tok/sec 1126585.1590
for step 2969 | loss 3.658495 | norm 0.3060 | time 465.7950 ms | tok/sec 1125576.6052
for step 2970 | loss 3.671154 | norm 0.3238 | time 465.3006 ms | tok/sec 1126772.7681
for step 2971 | loss 3.610935 | norm 0.3547 | time 465.0397 ms | tok/sec 1127404.7487
for step 2972 | loss 3.509564 | norm 0.3288 | time 465.8422 ms | tok/sec 1125462.5431
for step 2973 | loss 3.509422 | norm 0.3078 | time 465.2674 ms | tok/sec 1126853.0262
for step 2974 | loss 3.514020 | norm 0.3271 | time 464.6797 ms | tok/sec 1128278.2090
for step 2975 | loss 3.505064 | norm 0.3323 | time 465.8952 ms | tok/sec 1125334.6827
for step 2976 | loss 3.554015 | norm 0.3465 | time 465.4787 ms | tok/sec 1126341.6490
for step 2977 | loss 3.577910 | norm 0.3326 | time 464.7000 ms | tok/sec 1128229.0047
for step 2978 | loss 3.533914 | norm 0.3448 | time 464.9408 ms | tok/sec 1127644.6708
for step 2979 | loss 3.547745 | norm 0.3523 | time 464.8094 ms | tok/sec 1127963.3760
for step 2980 | loss 3.472265 | norm 0.3770 | time 465.4436 ms | tok/sec 1126426.4617
for step 2981 | loss 3.485993 | norm 0.3302 | time 465.3072 ms | tok/sec 1126756.6024
for step 2982 | loss 3.519179 | norm 0.3658 | time 465.5602 ms | tok/sec 1126144.3793
for step 2983 | loss 3.793727 | norm 0.3661 | time 466.1944 ms | tok/sec 1124612.4146
for step 2984 | loss 3.786962 | norm 0.3469 | time 466.2173 ms | tok/sec 1124557.2035
for step 2985 | loss 3.685074 | norm 0.3788 | time 465.2193 ms | tok/sec 1126969.6806
for step 2986 | loss 3.697079 | norm 0.3756 | time 465.0319 ms | tok/sec 1127423.8231
for step 2987 | loss 3.735711 | norm 0.3990 | time 466.3100 ms | tok/sec 1124333.5393
for step 2988 | loss 3.753500 | norm 0.4139 | time 465.2386 ms | tok/sec 1126922.9004
for step 2989 | loss 3.705485 | norm 0.4304 | time 465.6372 ms | tok/sec 1125958.1324
for step 2990 | loss 3.769497 | norm 0.3594 | time 465.4307 ms | tok/sec 1126457.6205
for step 2991 | loss 3.734773 | norm 0.3482 | time 464.2127 ms | tok/sec 1129413.4126
for step 2992 | loss 3.687565 | norm 0.3458 | time 465.8926 ms | tok/sec 1125341.0175
for step 2993 | loss 3.731919 | norm 0.3359 | time 465.8983 ms | tok/sec 1125327.1963
for step 2994 | loss 3.713569 | norm 0.3185 | time 464.7291 ms | tok/sec 1128158.3897
for step 2995 | loss 3.669520 | norm 0.2989 | time 465.6751 ms | tok/sec 1125866.4731
for step 2996 | loss 3.700481 | norm 0.3019 | time 465.0302 ms | tok/sec 1127427.8692
for step 2997 | loss 3.751204 | norm 0.2946 | time 465.8849 ms | tok/sec 1125359.4462
for step 2998 | loss 3.730700 | norm 0.2990 | time 465.2050 ms | tok/sec 1127004.3350
for step 2999 | loss 3.699897 | norm 0.2988 | time 466.0392 ms | tok/sec 1124986.9574
validation loss 3.7179
HellaSwag accuracy: 2584/10042=0.2573
> Hello, I'm a language model, and I'm a language model. Okay I don't speak Mandarin, but I'm just learning about the same language.
> Hello, I'm a language model, which I am aware of. I don't know anything by myself. I just accept you as I'm a language model
> Hello, I'm a language model, so I think you can use it to make a good friend at a very large audience. So the best thing, but
> Hello, I'm a language model, and I'm using the words, "All the above letters I heard yesterday by myself were born" (pp. 31
> Hello, I'm a language model, and I don't have a good enough description right now, I would put that into it because there's no way it
> > Hello, I'm a language model, but at the same time I am reading a language called B-T, so I can have the answers I'm missing
Hello, I'm a language model, but I want to teach me how to play it. I am a linguist, and I want to give me the
> > Hello, I'm a language model, but I will take a deeper look to the basics.
It has many different applications. It is a language model that
Hello, I'm a language model, that has a key to it being integrated in a society!
For most languages this is the language model. How many
> Hello, I'm a language model, and all the rules I've created, but which I'm referring to as the two models we've all learned, not> 
Hello, I'm a language model, I mean, there's more than two, and I'm very limited to one way and I'm able to think about
> Hello, I'm a language model, and you guys know it's a single piece of material, maybe a square root. I think of a lot of different
> Hello, I'm a language model, and I'm a very important language model/reading system, and I need to really see myself developing and developing a computer> 
>>Hello, I'm a language model, so this tutorial is not a way to integrate language theory with a new approach. It is a powerful tool that lets me
 Hello, I'm a language model, now a lot to me.
As I mentioned, it was a matter of saying that I was not able to learn Hello, I'm a language model, right? I'm a high school school teacher, working here, and I'm pretty sure there's been no other teachers> 
> 
Hello, I'm a language model, though a lot of these models do fit in and out, so I'll still need some help and guidance: I'm
> > Hello, I'm a language model, and I'm not saying the language at all, I'm going to move right into another language model which is what happens
> > Hello, I'm a language model, I don't know any answers to these. I have no problems on the topic, but I'm not a language model
Hello, I'm a language model, so I give my students a blank assignment but I often end in the lab. I'm very often using a small blank
> Hello, I'm a language model, and I am currently moving to the computer and I also want to know when I am working with a computer. And it
Hello, I'm a language model, but it would not apply to everyone except for my own. I like to talk a thousand words, so I'm pretty
> > Hello, I'm a language model, and a lot of people actually understand how the words are. I think that's why I'm trying to build out an
> > Hello, I'm a language model, and I think I need a different language when I get to the front lines.
- The main difference here is that
Hello, I'm a language model, but I am a language model for both. I am a language model for all language models, from that point-point
> Hello, I'm a language model, but it's not all my native languages. My best bet is that in my opinion, I like to think of it
Hello, I'm a language model, while my first post is not the language model. So what I do is determine exactly which languages this is? I will
> > Hello, I'm a language model, and it doesn't follow the details of English code.
A language model doesn't have to know what it wants.
Hello, I'm a language model, so I wanted to try it on my own. I wanted the concept of using a Python code, while in-game
Hello, I'm a language model, so we must ask a lot of questions...
3. Can I use a "list box" in order for that
> > Hello, I'm a language model, and because I can't control all languages, and it will not work well for me. I'm afraid of a lot
Hello, I'm a language model, I'll just keep working on that, but I'm not going to be sure what the process is – how? Because
for step 3000 | loss 3.704191 | norm 0.3196 | time 13041.4112 ms | tok/sec 40201.7844
for step 3001 | loss 3.743978 | norm 0.3186 | time 462.7881 ms | tok/sec 1132889.9648
for step 3002 | loss 3.724580 | norm 0.2878 | time 462.4648 ms | tok/sec 1133681.9350
for step 3003 | loss 3.736623 | norm 0.2834 | time 462.9257 ms | tok/sec 1132553.3041
for step 3004 | loss 3.722941 | norm 0.3078 | time 463.5921 ms | tok/sec 1130925.3410
for step 3005 | loss 3.849711 | norm 0.4117 | time 462.2800 ms | tok/sec 1134135.0703
for step 3006 | loss 3.722942 | norm 0.4659 | time 463.8898 ms | tok/sec 1130199.3661
for step 3007 | loss 3.650088 | norm 0.4802 | time 463.8059 ms | tok/sec 1130403.8699
for step 3008 | loss 3.685794 | norm 0.3576 | time 462.8415 ms | tok/sec 1132759.2444
for step 3009 | loss 3.730227 | norm 0.3859 | time 463.4223 ms | tok/sec 1131339.6049
for step 3010 | loss 3.685288 | norm 0.3491 | time 463.4726 ms | tok/sec 1131216.8068
for step 3011 | loss 3.687757 | norm 0.3355 | time 463.9940 ms | tok/sec 1129945.5821
for step 3012 | loss 3.598892 | norm 0.3581 | time 463.6064 ms | tok/sec 1130890.4450
for step 3013 | loss 3.626225 | norm 0.3036 | time 464.5047 ms | tok/sec 1128703.2807
for step 3014 | loss 3.686298 | norm 0.3417 | time 465.5592 ms | tok/sec 1126146.6861
for step 3015 | loss 3.667040 | norm 0.3054 | time 464.6847 ms | tok/sec 1128266.0523
for step 3016 | loss 3.623480 | norm 0.3024 | time 465.7953 ms | tok/sec 1125576.0291
for step 3017 | loss 3.613806 | norm 0.3076 | time 465.2822 ms | tok/sec 1126817.2262
for step 3018 | loss 3.608126 | norm 0.3059 | time 464.0856 ms | tok/sec 1129722.6715
for step 3019 | loss 3.541085 | norm 0.2962 | time 464.6549 ms | tok/sec 1128338.4177
for step 3020 | loss 3.463188 | norm 0.3126 | time 464.8023 ms | tok/sec 1127980.7335
for step 3021 | loss 3.531801 | norm 0.3300 | time 465.4179 ms | tok/sec 1126488.7811
for step 3022 | loss 3.453903 | norm 0.3343 | time 465.0695 ms | tok/sec 1127332.5030
for step 3023 | loss 3.531370 | norm 0.3506 | time 465.1308 ms | tok/sec 1127183.9948
for step 3024 | loss 3.521532 | norm 0.3586 | time 465.4682 ms | tok/sec 1126367.0338
for step 3025 | loss 3.458444 | norm 0.3210 | time 465.3087 ms | tok/sec 1126753.1384
for step 3026 | loss 3.462852 | norm 0.3475 | time 464.9560 ms | tok/sec 1127607.6641
for step 3027 | loss 3.497788 | norm 0.3428 | time 465.5266 ms | tok/sec 1126225.7014
for step 3028 | loss 3.491406 | norm 0.3957 | time 466.0497 ms | tok/sec 1124961.6348
for step 3029 | loss 3.472024 | norm 0.4489 | time 465.3490 ms | tok/sec 1126655.5773
for step 3030 | loss 3.618801 | norm 0.4506 | time 464.6883 ms | tok/sec 1128257.3690
for step 3031 | loss 3.757704 | norm 0.3721 | time 465.6193 ms | tok/sec 1126001.3731
for step 3032 | loss 3.709943 | norm 0.3132 | time 466.2111 ms | tok/sec 1124572.1560
for step 3033 | loss 3.695498 | norm 0.3220 | time 465.9472 ms | tok/sec 1125209.1544
for step 3034 | loss 3.739955 | norm 0.3162 | time 465.5645 ms | tok/sec 1126133.9986
for step 3035 | loss 3.741568 | norm 0.3232 | time 465.7531 ms | tok/sec 1125678.0132
for step 3036 | loss 3.722169 | norm 0.3091 | time 465.1732 ms | tok/sec 1127081.1600
for step 3037 | loss 3.724204 | norm 0.3277 | time 465.0712 ms | tok/sec 1127328.4575
for step 3038 | loss 3.720804 | norm 0.3617 | time 465.8313 ms | tok/sec 1125489.0403
for step 3039 | loss 3.751337 | norm 0.3867 | time 465.7240 ms | tok/sec 1125748.3181
for step 3040 | loss 3.759510 | norm 0.3473 | time 465.5826 ms | tok/sec 1126090.1710
for step 3041 | loss 3.770451 | norm 0.2855 | time 465.5623 ms | tok/sec 1126139.1889
for step 3042 | loss 3.703707 | norm 0.3322 | time 465.9944 ms | tok/sec 1125095.1668
for step 3043 | loss 3.726472 | norm 0.3232 | time 464.9653 ms | tok/sec 1127585.1144
for step 3044 | loss 3.681659 | norm 0.3219 | time 465.6777 ms | tok/sec 1125860.1325
for step 3045 | loss 3.659797 | norm 0.3233 | time 465.2317 ms | tok/sec 1126939.6484
for step 3046 | loss 3.738903 | norm 0.3218 | time 465.6153 ms | tok/sec 1126011.1748
Will loading at 0 from edu_fineweb10B/edufineweb_train_000017.npy
for step 3047 | loss 3.747722 | norm 0.3342 | time 1630.3024 ms | tok/sec 321589.4123
for step 3048 | loss 3.742529 | norm 0.3209 | time 463.6810 ms | tok/sec 1130708.4390
for step 3049 | loss 3.667387 | norm 0.3179 | time 464.4430 ms | tok/sec 1128853.3484
for step 3050 | loss 3.691457 | norm 0.3302 | time 464.1323 ms | tok/sec 1129608.9281
for step 3051 | loss 3.668511 | norm 0.3444 | time 463.9726 ms | tok/sec 1129997.8395
for step 3052 | loss 3.703949 | norm 0.3440 | time 465.1895 ms | tok/sec 1127041.8798
for step 3053 | loss 3.649674 | norm 0.3223 | time 465.0357 ms | tok/sec 1127414.5748
for step 3054 | loss 3.676641 | norm 0.3316 | time 463.9816 ms | tok/sec 1129975.7747
for step 3055 | loss 3.607736 | norm 0.3319 | time 464.0205 ms | tok/sec 1129881.1379
for step 3056 | loss 3.596402 | norm 0.3490 | time 464.4225 ms | tok/sec 1128903.1867
for step 3057 | loss 3.666838 | norm 0.2876 | time 465.1074 ms | tok/sec 1127240.6198
for step 3058 | loss 3.671406 | norm 0.3016 | time 464.3342 ms | tok/sec 1129117.6573
for step 3059 | loss 3.627880 | norm 0.2904 | time 464.8261 ms | tok/sec 1127922.8771
for step 3060 | loss 3.646731 | norm 0.3029 | time 465.6148 ms | tok/sec 1126012.3279
for step 3061 | loss 3.647929 | norm 0.2994 | time 465.6031 ms | tok/sec 1126040.5808
for step 3062 | loss 3.610419 | norm 0.3435 | time 464.9255 ms | tok/sec 1127681.6800
for step 3063 | loss 3.666734 | norm 0.3376 | time 465.2832 ms | tok/sec 1126814.9166
for step 3064 | loss 3.517969 | norm 0.3709 | time 465.2967 ms | tok/sec 1126782.0059
for step 3065 | loss 3.480644 | norm 0.3850 | time 464.7310 ms | tok/sec 1128153.7595
for step 3066 | loss 3.497110 | norm 0.4193 | time 464.2258 ms | tok/sec 1129381.5100
for step 3067 | loss 3.530516 | norm 0.3602 | time 465.4424 ms | tok/sec 1126429.3467
for step 3068 | loss 3.516001 | norm 0.3535 | time 465.6138 ms | tok/sec 1126014.6342
for step 3069 | loss 3.444020 | norm 0.3512 | time 465.1258 ms | tok/sec 1127196.1283
for step 3070 | loss 3.583896 | norm 0.4019 | time 465.7700 ms | tok/sec 1125637.1021
for step 3071 | loss 3.503037 | norm 0.4113 | time 465.0054 ms | tok/sec 1127487.9872
for step 3072 | loss 3.520426 | norm 0.4109 | time 464.8898 ms | tok/sec 1127768.4297
for step 3073 | loss 3.468981 | norm 0.3445 | time 465.2991 ms | tok/sec 1126776.2323
for step 3074 | loss 3.421381 | norm 0.3186 | time 465.5821 ms | tok/sec 1126091.3244
for step 3075 | loss 3.439305 | norm 0.3170 | time 465.5139 ms | tok/sec 1126256.2723
for step 3076 | loss 3.668077 | norm 0.3400 | time 466.2404 ms | tok/sec 1124501.4229
for step 3077 | loss 3.754595 | norm 0.3392 | time 465.7886 ms | tok/sec 1125592.1609
for step 3078 | loss 3.662282 | norm 0.3404 | time 464.5162 ms | tok/sec 1128675.4733
for step 3079 | loss 3.740654 | norm 0.3120 | time 465.4360 ms | tok/sec 1126444.9260
for step 3080 | loss 3.664711 | norm 0.3255 | time 465.9252 ms | tok/sec 1125262.1263
for step 3081 | loss 3.774641 | norm 0.3126 | time 465.3757 ms | tok/sec 1126590.9307
for step 3082 | loss 3.695825 | norm 0.3556 | time 465.2174 ms | tok/sec 1126974.3010
for step 3083 | loss 3.697124 | norm 0.3739 | time 466.1055 ms | tok/sec 1124826.9839
for step 3084 | loss 3.701413 | norm 0.3713 | time 465.0559 ms | tok/sec 1127365.4458
for step 3085 | loss 3.726640 | norm 0.3488 | time 465.3845 ms | tok/sec 1126569.5758
for step 3086 | loss 3.672924 | norm 0.3422 | time 464.7663 ms | tok/sec 1128068.1079
for step 3087 | loss 3.716594 | norm 0.3253 | time 465.0302 ms | tok/sec 1127427.8692
for step 3088 | loss 3.751227 | norm 0.3387 | time 464.7217 ms | tok/sec 1128176.3320
for step 3089 | loss 3.670631 | norm 0.3372 | time 465.3609 ms | tok/sec 1126626.7163
for step 3090 | loss 3.734659 | norm 0.3268 | time 465.2638 ms | tok/sec 1126861.6878
for step 3091 | loss 3.690791 | norm 0.3113 | time 465.4856 ms | tok/sec 1126324.9188
for step 3092 | loss 3.653742 | norm 0.3375 | time 465.9541 ms | tok/sec 1125192.4578
for step 3093 | loss 3.705205 | norm 0.4157 | time 465.1275 ms | tok/sec 1127192.0838
for step 3094 | loss 3.680254 | norm 0.3998 | time 465.6620 ms | tok/sec 1125898.1775
for step 3095 | loss 3.706608 | norm 0.3967 | time 465.7357 ms | tok/sec 1125720.0798
for step 3096 | loss 3.722299 | norm 0.3287 | time 465.5354 ms | tok/sec 1126204.3604
for step 3097 | loss 3.754293 | norm 0.3398 | time 465.2143 ms | tok/sec 1126981.8094
for step 3098 | loss 3.754615 | norm 0.4066 | time 464.8263 ms | tok/sec 1127922.2986
for step 3099 | loss 3.597082 | norm 0.3849 | time 465.3029 ms | tok/sec 1126766.9946
for step 3100 | loss 3.609629 | norm 0.3039 | time 466.1195 ms | tok/sec 1124793.0385
for step 3101 | loss 3.657204 | norm 0.3413 | time 464.7405 ms | tok/sec 1128130.6091
for step 3102 | loss 3.572455 | norm 0.3142 | time 465.5328 ms | tok/sec 1126210.7050
for step 3103 | loss 3.740698 | norm 0.2880 | time 465.1303 ms | tok/sec 1127185.1504
for step 3104 | loss 3.637146 | norm 0.2847 | time 464.3037 ms | tok/sec 1129191.8715
for step 3105 | loss 3.645930 | norm 0.2829 | time 465.2548 ms | tok/sec 1126883.6312
for step 3106 | loss 3.735769 | norm 0.3021 | time 464.9637 ms | tok/sec 1127589.1617
for step 3107 | loss 3.656774 | norm 0.3044 | time 465.2162 ms | tok/sec 1126977.1888
for step 3108 | loss 3.656167 | norm 0.3451 | time 464.6630 ms | tok/sec 1128318.7334
for step 3109 | loss 3.641831 | norm 0.3309 | time 465.0531 ms | tok/sec 1127372.3814
for step 3110 | loss 3.646066 | norm 0.3078 | time 465.5054 ms | tok/sec 1126277.0385
for step 3111 | loss 3.616937 | norm 0.3015 | time 464.3993 ms | tok/sec 1128959.4049
for step 3112 | loss 3.509151 | norm 0.2935 | time 464.5355 ms | tok/sec 1128628.5515
for step 3113 | loss 3.439333 | norm 0.2925 | time 465.8668 ms | tok/sec 1125403.2169
for step 3114 | loss 3.491302 | norm 0.2981 | time 465.1296 ms | tok/sec 1127186.8837
for step 3115 | loss 3.482163 | norm 0.3301 | time 464.8838 ms | tok/sec 1127782.8892
for step 3116 | loss 3.532422 | norm 0.3616 | time 464.7453 ms | tok/sec 1128119.0343
for step 3117 | loss 3.478517 | norm 0.3293 | time 465.9922 ms | tok/sec 1125100.3475
for step 3118 | loss 3.467413 | norm 0.3501 | time 464.6301 ms | tok/sec 1128398.6328
for step 3119 | loss 3.524945 | norm 0.3666 | time 465.0567 ms | tok/sec 1127363.7120
for step 3120 | loss 3.523937 | norm 0.4207 | time 465.5199 ms | tok/sec 1126241.8519
for step 3121 | loss 3.476673 | norm 0.4467 | time 463.8963 ms | tok/sec 1130183.6827
for step 3122 | loss 3.474095 | norm 0.3721 | time 465.2410 ms | tok/sec 1126917.1253
for step 3123 | loss 3.552588 | norm 0.3565 | time 464.3931 ms | tok/sec 1128974.4746
for step 3124 | loss 3.643870 | norm 0.3705 | time 465.5929 ms | tok/sec 1126065.3754
for step 3125 | loss 3.782982 | norm 0.3368 | time 465.2398 ms | tok/sec 1126920.0129
for step 3126 | loss 3.633834 | norm 0.3438 | time 465.0795 ms | tok/sec 1127308.2305
for step 3127 | loss 3.682363 | norm 0.3691 | time 465.3223 ms | tok/sec 1126720.2313
for step 3128 | loss 3.641207 | norm 0.3569 | time 465.2255 ms | tok/sec 1126954.6643
for step 3129 | loss 3.700484 | norm 0.3599 | time 464.6528 ms | tok/sec 1128343.6283
for step 3130 | loss 3.685118 | norm 0.3165 | time 464.5922 ms | tok/sec 1128490.7048
for step 3131 | loss 3.668458 | norm 0.3596 | time 464.5350 ms | tok/sec 1128629.7100
for step 3132 | loss 3.651571 | norm 0.3101 | time 464.9131 ms | tok/sec 1127711.7517
for step 3133 | loss 3.630762 | norm 0.3491 | time 464.3974 ms | tok/sec 1128964.0417
for step 3134 | loss 3.680644 | norm 0.3314 | time 464.4542 ms | tok/sec 1128826.1131
for step 3135 | loss 3.624796 | norm 0.3464 | time 465.2760 ms | tok/sec 1126832.2388
for step 3136 | loss 3.751231 | norm 0.3620 | time 465.0028 ms | tok/sec 1127494.3462
for step 3137 | loss 3.726748 | norm 0.3032 | time 465.4086 ms | tok/sec 1126511.2870
for step 3138 | loss 3.765566 | norm 0.3192 | time 464.4854 ms | tok/sec 1128750.2088
for step 3139 | loss 3.730385 | norm 0.3287 | time 464.5157 ms | tok/sec 1128676.6319
for step 3140 | loss 3.764644 | norm 0.3038 | time 464.9999 ms | tok/sec 1127501.2834
for step 3141 | loss 3.739294 | norm 0.3292 | time 465.2288 ms | tok/sec 1126946.5788
for step 3142 | loss 3.735148 | norm 0.3244 | time 466.1975 ms | tok/sec 1124604.9378
for step 3143 | loss 3.752038 | norm 0.3173 | time 464.5944 ms | tok/sec 1128485.4928
for step 3144 | loss 3.724158 | norm 0.3058 | time 465.5762 ms | tok/sec 1126105.7410
for step 3145 | loss 3.695251 | norm 0.3115 | time 466.5270 ms | tok/sec 1123810.6619
for step 3146 | loss 3.682158 | norm 0.2898 | time 464.7584 ms | tok/sec 1128087.2048
for step 3147 | loss 3.657749 | norm 0.3237 | time 465.2493 ms | tok/sec 1126896.9131
for step 3148 | loss 3.674268 | norm 0.3066 | time 465.5881 ms | tok/sec 1126076.9081
for step 3149 | loss 3.629014 | norm 0.2821 | time 466.0447 ms | tok/sec 1124973.7205
for step 3150 | loss 3.607554 | norm 0.3379 | time 465.8577 ms | tok/sec 1125425.1035
for step 3151 | loss 3.664815 | norm 0.3104 | time 465.9684 ms | tok/sec 1125157.9147
for step 3152 | loss 3.663075 | norm 0.3042 | time 464.8297 ms | tok/sec 1127914.1992
for step 3153 | loss 3.630957 | norm 0.3367 | time 465.2417 ms | tok/sec 1126915.3928
for step 3154 | loss 3.631820 | norm 0.3594 | time 464.9620 ms | tok/sec 1127593.2091
for step 3155 | loss 3.575309 | norm 0.3545 | time 464.4134 ms | tok/sec 1128925.2097
for step 3156 | loss 3.648795 | norm 0.3323 | time 465.2638 ms | tok/sec 1126861.6878
for step 3157 | loss 3.652803 | norm 0.3386 | time 464.5483 ms | tok/sec 1128597.2724
for step 3158 | loss 3.624152 | norm 0.3445 | time 465.3482 ms | tok/sec 1126657.3090
for step 3159 | loss 3.444245 | norm 0.3553 | time 464.5922 ms | tok/sec 1128490.7048
for step 3160 | loss 3.493062 | norm 0.3538 | time 465.1911 ms | tok/sec 1127037.8364
for step 3161 | loss 3.567154 | norm 0.3424 | time 465.0578 ms | tok/sec 1127360.8222
for step 3162 | loss 3.477143 | norm 0.3107 | time 465.8539 ms | tok/sec 1125434.3192
for step 3163 | loss 3.491079 | norm 0.3607 | time 464.7431 ms | tok/sec 1128124.2429
for step 3164 | loss 3.483923 | norm 0.3745 | time 465.3404 ms | tok/sec 1126676.3581
for step 3165 | loss 3.421805 | norm 0.3820 | time 466.0587 ms | tok/sec 1124939.7663
for step 3166 | loss 3.479686 | norm 0.3275 | time 465.6289 ms | tok/sec 1125978.3110
for step 3167 | loss 3.485685 | norm 0.3123 | time 465.5752 ms | tok/sec 1126108.0476
for step 3168 | loss 3.454951 | norm 0.3528 | time 465.2388 ms | tok/sec 1126922.3229
for step 3169 | loss 3.465497 | norm 0.3740 | time 465.8816 ms | tok/sec 1125367.5090
for step 3170 | loss 3.572071 | norm 0.3569 | time 464.8709 ms | tok/sec 1127814.1232
for step 3171 | loss 3.669749 | norm 0.3449 | time 464.9122 ms | tok/sec 1127714.0650
for step 3172 | loss 3.671035 | norm 0.3534 | time 464.9224 ms | tok/sec 1127689.1978
for step 3173 | loss 3.666677 | norm 0.3639 | time 464.7751 ms | tok/sec 1128046.6970
for step 3174 | loss 3.687250 | norm 0.3659 | time 465.1713 ms | tok/sec 1127085.7814
for step 3175 | loss 3.656961 | norm 0.3673 | time 464.6351 ms | tok/sec 1128386.4734
for step 3176 | loss 3.664200 | norm 0.3449 | time 464.9894 ms | tok/sec 1127526.7204
for step 3177 | loss 3.690181 | norm 0.3200 | time 464.8674 ms | tok/sec 1127822.7996
for step 3178 | loss 3.628572 | norm 0.3541 | time 466.1460 ms | tok/sec 1124729.1807
for step 3179 | loss 3.688514 | norm 0.3150 | time 465.9829 ms | tok/sec 1125122.7980
for step 3180 | loss 3.724361 | norm 0.3322 | time 465.4882 ms | tok/sec 1126318.5730
for step 3181 | loss 3.656315 | norm 0.3256 | time 465.1647 ms | tok/sec 1127101.9566
for step 3182 | loss 3.655353 | norm 0.3183 | time 465.8878 ms | tok/sec 1125352.5354
for step 3183 | loss 3.685300 | norm 0.3317 | time 465.0166 ms | tok/sec 1127460.8177
for step 3184 | loss 3.706492 | norm 0.3327 | time 465.1041 ms | tok/sec 1127248.7095
for step 3185 | loss 3.666819 | norm 0.3301 | time 465.2565 ms | tok/sec 1126879.5889
for step 3186 | loss 3.614624 | norm 0.3156 | time 465.8256 ms | tok/sec 1125502.8654
for step 3187 | loss 3.733482 | norm 0.4202 | time 465.3852 ms | tok/sec 1126567.8444
for step 3188 | loss 3.701729 | norm 0.5707 | time 465.4033 ms | tok/sec 1126523.9831
for step 3189 | loss 3.671237 | norm 0.5316 | time 465.2064 ms | tok/sec 1127000.8695
for step 3190 | loss 3.661807 | norm 0.4239 | time 466.2430 ms | tok/sec 1124495.0976
for step 3191 | loss 3.677527 | norm 0.4516 | time 465.1368 ms | tok/sec 1127169.5506
for step 3192 | loss 3.692054 | norm 0.3549 | time 465.2820 ms | tok/sec 1126817.8036
for step 3193 | loss 3.665962 | norm 0.3492 | time 465.4176 ms | tok/sec 1126489.3582
for step 3194 | loss 3.604665 | norm 0.3567 | time 466.2020 ms | tok/sec 1124594.0103
for step 3195 | loss 3.620073 | norm 0.3124 | time 466.0623 ms | tok/sec 1124931.1341
for step 3196 | loss 3.678847 | norm 0.3144 | time 464.7374 ms | tok/sec 1128138.1329
for step 3197 | loss 3.626431 | norm 0.3072 | time 465.0612 ms | tok/sec 1127352.7308
for step 3198 | loss 3.634430 | norm 0.3201 | time 464.9503 ms | tok/sec 1127621.5414
for step 3199 | loss 3.616632 | norm 0.3110 | time 466.1222 ms | tok/sec 1124786.7099
for step 3200 | loss 3.620172 | norm 0.3058 | time 464.5929 ms | tok/sec 1128488.9675
for step 3201 | loss 3.625162 | norm 0.2821 | time 465.1947 ms | tok/sec 1127029.1720
for step 3202 | loss 3.572134 | norm 0.3027 | time 465.0459 ms | tok/sec 1127389.7208
for step 3203 | loss 3.715087 | norm 0.3319 | time 466.2271 ms | tok/sec 1124533.6255
for step 3204 | loss 3.656471 | norm 0.3661 | time 466.0387 ms | tok/sec 1124988.1085
for step 3205 | loss 3.558329 | norm 0.3124 | time 466.3327 ms | tok/sec 1124278.9305
for step 3206 | loss 3.508240 | norm 0.3387 | time 464.9849 ms | tok/sec 1127537.7050
for step 3207 | loss 3.547568 | norm 0.3698 | time 464.9525 ms | tok/sec 1127616.3374
for step 3208 | loss 3.462525 | norm 0.3139 | time 465.3771 ms | tok/sec 1126587.4677
for step 3209 | loss 3.452843 | norm 0.3625 | time 465.2040 ms | tok/sec 1127006.6454
for step 3210 | loss 3.404451 | norm 0.3339 | time 465.5764 ms | tok/sec 1126105.1643
for step 3211 | loss 3.479593 | norm 0.3530 | time 464.3304 ms | tok/sec 1129126.9336
for step 3212 | loss 3.469952 | norm 0.3370 | time 465.0109 ms | tok/sec 1127474.6913
for step 3213 | loss 3.481112 | norm 0.3261 | time 466.0656 ms | tok/sec 1124923.0776
for step 3214 | loss 3.533475 | norm 0.3345 | time 465.7085 ms | tok/sec 1125785.7792
for step 3215 | loss 3.462942 | norm 0.3363 | time 465.4245 ms | tok/sec 1126472.6235
for step 3216 | loss 3.439829 | norm 0.3141 | time 465.1177 ms | tok/sec 1127215.7734
for step 3217 | loss 3.657775 | norm 0.3541 | time 465.4202 ms | tok/sec 1126483.0105
for step 3218 | loss 3.714810 | norm 0.3796 | time 464.2816 ms | tok/sec 1129245.7989
for step 3219 | loss 3.719220 | norm 0.3673 | time 465.8029 ms | tok/sec 1125557.5933
for step 3220 | loss 3.704128 | norm 0.3452 | time 464.8421 ms | tok/sec 1127884.1167
for step 3221 | loss 3.665272 | norm 0.3315 | time 465.0464 ms | tok/sec 1127388.5648
for step 3222 | loss 3.695034 | norm 0.3785 | time 465.2333 ms | tok/sec 1126935.6057
for step 3223 | loss 3.677903 | norm 0.3502 | time 465.8182 ms | tok/sec 1125520.7234
for step 3224 | loss 3.708715 | norm 0.3481 | time 464.8705 ms | tok/sec 1127815.2800
for step 3225 | loss 3.693362 | norm 0.3206 | time 465.5843 ms | tok/sec 1126086.1345
for step 3226 | loss 3.622676 | norm 0.3243 | time 465.5154 ms | tok/sec 1126252.8114
for step 3227 | loss 3.747129 | norm 0.3062 | time 465.1263 ms | tok/sec 1127194.9727
for step 3228 | loss 3.678455 | norm 0.3087 | time 464.6273 ms | tok/sec 1128405.5811
for step 3229 | loss 3.666663 | norm 0.2944 | time 465.8656 ms | tok/sec 1125406.0967
for step 3230 | loss 3.657975 | norm 0.2941 | time 465.5504 ms | tok/sec 1126168.0249
for step 3231 | loss 3.803954 | norm 0.3095 | time 464.5193 ms | tok/sec 1128667.9424
for step 3232 | loss 3.698919 | norm 0.2933 | time 465.4033 ms | tok/sec 1126523.9831
for step 3233 | loss 3.647972 | norm 0.2879 | time 464.8356 ms | tok/sec 1127899.7362
for step 3234 | loss 3.695588 | norm 0.2928 | time 464.9692 ms | tok/sec 1127575.8634
for step 3235 | loss 3.703683 | norm 0.2989 | time 464.9763 ms | tok/sec 1127558.5184
for step 3236 | loss 3.735434 | norm 0.2736 | time 465.4772 ms | tok/sec 1126345.1105
for step 3237 | loss 3.671909 | norm 0.3310 | time 464.2437 ms | tok/sec 1129338.0093
Will loading at 0 from edu_fineweb10B/edufineweb_train_000018.npy
for step 3238 | loss 3.900665 | norm 0.5695 | time 1633.1677 ms | tok/sec 321025.1991
for step 3239 | loss 3.700080 | norm 0.6576 | time 465.6944 ms | tok/sec 1125819.7845
for step 3240 | loss 3.669198 | norm 0.4900 | time 464.2260 ms | tok/sec 1129380.9299
for step 3241 | loss 3.624231 | norm 0.4125 | time 463.8076 ms | tok/sec 1130399.8024
for step 3242 | loss 3.631823 | norm 0.3746 | time 464.9229 ms | tok/sec 1127688.0412
for step 3243 | loss 3.575389 | norm 0.3409 | time 464.0927 ms | tok/sec 1129705.2603
for step 3244 | loss 3.651522 | norm 0.3456 | time 464.9010 ms | tok/sec 1127741.2467
for step 3245 | loss 3.595746 | norm 0.3564 | time 464.9642 ms | tok/sec 1127588.0053
for step 3246 | loss 3.646901 | norm 0.3418 | time 464.3593 ms | tok/sec 1129056.7857
for step 3247 | loss 3.597438 | norm 0.3237 | time 464.3757 ms | tok/sec 1129016.7880
for step 3248 | loss 3.606217 | norm 0.2834 | time 464.5524 ms | tok/sec 1128587.4256
for step 3249 | loss 3.648720 | norm 0.2898 | time 465.2395 ms | tok/sec 1126920.5904
validation loss 3.6872
HellaSwag accuracy: 2595/10042=0.2584
> Hello, I'm a language model, and I'm a grammar coach. On this level I work from the first edition of this article in the language arts class
> Hello, I'm a language model, and it is hard to imagine. In an effort to provide a coherent, accurate and nuanced information you need to have a
> Hello, I'm a language model, so I've never been to the language model. I am no better.
I'm not the one to write anything
> Hello, I'm a language model, and I'm an expert blogger, and more.
Sydney Bae is author of 'The Meaning of '
> Hello, I'm a language model, but I was interested in all the different topics and concepts.
I would really like to offer a new perspective on how
> Hello, I'm a language model, while you're the teacher. What did you learn from scratch?
It took 3 seconds for me to see another thing> 
Hello, I'm a language model, and I love the way in which people express interest and need to talk about something.
(You don't have to
> Hello, I'm a language model, I say all kinds of skills are equally important.
When my teacher said, "The more I talk about our students
> Hello, I'm a language model, so when I go into the game at the beginning for another, I find out I have found that.
Let's> 
Hello, I'm a language model, and you take on some of the commonalities to explore the possibilities of grammar, sentence structure and idiomatic expressions. You
> Hello, I'm a language model, but I feel I get better at being able to find out about this language.
As I said, I am constantly
> Hello, I'm a language model, so they're really very fun to use, here's a free code and you get your ideas, in my free library
> Hello, I'm a language model, so the first thing I want to do is look at a "what-is" equation. What I mean is "
> Hello, I'm a language model, trying to teach my mother's languages to me since I'm writing (I know you are going to start the process of
> Hello, I'm a language model, but I have to turn to a friend and get to know him/her with the real world.
I'm an> 
Hello, I'm a language model, and I'm a native speaker. I teach English with my wife and we say Spanish, and we use Spanish to translate
> Hello, I'm a language model, meaning "not", but how I speak.
A. I like to focus on what is defined in a sentence.> > 
Hello, I'm a language model, language engineer. I'm very impressed with the language model where I learned the language model and what I did to get back
> Hello, I'm a language model, learning, and reading with other language experts, and all the tools and tools we use to make this a success. All
> Hello, I'm a language model, and I'm going to try it now. I will have a lot of math. This might be so helpful for learning
> Hello, I'm a language model, and I've taken the time to explore this topic thoroughly. I hope our students will learn more and have more of it
> Hello, I'm a language model, and I think that this will be a new, powerful language model. We are in this new language model because we are
> Hello, I'm a language model, and a good way to see.
- Use the terms "English" and "English" for example. When the
> Hello, I'm a language model, but it's a lot, so now I'd like to read about each language and just move along that.
How
> Hello, I'm a language model, so I thought it was going into that. In the end it made sense!
There are many uses for different language
Hello, I'm a language model, and it’s fun to learn it now.
But I think this is a pretty powerful tool. For most
> Hello, I'm a language model, and I think it's a nice way to help you to understand.
Can I just love the computer game?
> 
Hello, I'm a language model, and yes, there's nothing about language, but there're no grammar.
At the moment, an early language teacher
> Hello, I'm a language model, and we have so many fun, easy to use, I can't remember the name of the person who said. The
> Hello, I'm a language model, so I haven't figured out how these questions are developed. It's important to be able to say and follow the simple
> Hello, I'm a language model, but I have always thought about what an effective language model is. The key is that if we try to get closer,
> Hello, I'm a language model, so what type of learners are they? There are actually some examples of how English is the target language for learning. What
for step 3250 | loss 3.628045 | norm 0.2956 | time 12990.3214 ms | tok/sec 40359.8944
for step 3251 | loss 3.515580 | norm 0.3519 | time 461.9851 ms | tok/sec 1134859.0837
for step 3252 | loss 3.543686 | norm 0.3559 | time 462.4639 ms | tok/sec 1133684.2728
for step 3253 | loss 3.442261 | norm 0.3317 | time 462.3334 ms | tok/sec 1134004.0623
for step 3254 | loss 3.415405 | norm 0.3543 | time 462.1022 ms | tok/sec 1134571.5918
for step 3255 | loss 3.496612 | norm 0.3244 | time 463.1937 ms | tok/sec 1131898.0610
for step 3256 | loss 3.469623 | norm 0.3147 | time 463.3086 ms | tok/sec 1131617.3082
for step 3257 | loss 3.426022 | norm 0.3595 | time 462.7552 ms | tok/sec 1132970.5130
for step 3258 | loss 3.483833 | norm 0.3272 | time 463.1581 ms | tok/sec 1131984.8780
for step 3259 | loss 3.455921 | norm 0.2880 | time 463.8865 ms | tok/sec 1130207.4983
for step 3260 | loss 3.437528 | norm 0.3350 | time 463.5756 ms | tok/sec 1130965.4741
for step 3261 | loss 3.473092 | norm 0.3464 | time 464.6358 ms | tok/sec 1128384.7364
for step 3262 | loss 3.412247 | norm 0.3070 | time 463.5193 ms | tok/sec 1131102.7624
for step 3263 | loss 3.664552 | norm 0.3051 | time 464.3133 ms | tok/sec 1129168.6785
for step 3264 | loss 3.660547 | norm 0.3753 | time 464.4015 ms | tok/sec 1128954.1885
for step 3265 | loss 3.711443 | norm 0.3779 | time 464.2451 ms | tok/sec 1129334.5294
for step 3266 | loss 3.649963 | norm 0.3680 | time 465.0962 ms | tok/sec 1127267.7787
for step 3267 | loss 3.711261 | norm 0.3618 | time 464.2150 ms | tok/sec 1129407.6120
for step 3268 | loss 3.724612 | norm 0.3173 | time 464.0465 ms | tok/sec 1129817.8621
for step 3269 | loss 3.742443 | norm 0.3310 | time 463.9454 ms | tok/sec 1130064.0391
for step 3270 | loss 3.655181 | norm 0.3714 | time 464.9422 ms | tok/sec 1127641.2014
for step 3271 | loss 3.669677 | norm 0.3643 | time 464.9010 ms | tok/sec 1127741.2467
for step 3272 | loss 3.686400 | norm 0.3819 | time 464.1767 ms | tok/sec 1129501.0091
for step 3273 | loss 3.699207 | norm 0.3367 | time 465.2181 ms | tok/sec 1126972.5683
for step 3274 | loss 3.741605 | norm 0.3535 | time 465.4791 ms | tok/sec 1126340.4952
for step 3275 | loss 3.674966 | norm 0.3139 | time 465.0629 ms | tok/sec 1127348.6852
for step 3276 | loss 3.628162 | norm 0.3204 | time 464.8440 ms | tok/sec 1127879.4887
for step 3277 | loss 3.666085 | norm 0.3049 | time 465.5113 ms | tok/sec 1126262.6175
for step 3278 | loss 3.666354 | norm 0.2755 | time 465.2576 ms | tok/sec 1126876.7016
for step 3279 | loss 3.698609 | norm 0.2766 | time 465.0898 ms | tok/sec 1127283.3812
for step 3280 | loss 3.702957 | norm 0.2712 | time 465.0552 ms | tok/sec 1127367.1797
for step 3281 | loss 3.642799 | norm 0.2912 | time 465.6334 ms | tok/sec 1125967.3568
for step 3282 | loss 3.672980 | norm 0.2903 | time 465.1515 ms | tok/sec 1127133.7305
for step 3283 | loss 3.650374 | norm 0.3049 | time 464.1762 ms | tok/sec 1129502.1694
for step 3284 | loss 3.678280 | norm 0.2989 | time 464.9107 ms | tok/sec 1127717.5349
for step 3285 | loss 3.662005 | norm 0.3135 | time 465.5988 ms | tok/sec 1126050.9598
for step 3286 | loss 3.575205 | norm 0.3410 | time 465.1968 ms | tok/sec 1127023.9735
for step 3287 | loss 3.617399 | norm 0.3476 | time 464.4973 ms | tok/sec 1128721.2404
for step 3288 | loss 3.638569 | norm 0.3266 | time 465.3032 ms | tok/sec 1126766.4173
for step 3289 | loss 3.653350 | norm 0.3346 | time 465.0333 ms | tok/sec 1127420.3550
for step 3290 | loss 3.675231 | norm 0.3311 | time 464.6418 ms | tok/sec 1128370.2614
for step 3291 | loss 3.620064 | norm 0.3112 | time 466.0342 ms | tok/sec 1124999.0436
for step 3292 | loss 3.621159 | norm 0.3312 | time 465.1620 ms | tok/sec 1127108.3112
for step 3293 | loss 3.631267 | norm 0.3646 | time 464.9794 ms | tok/sec 1127551.0023
for step 3294 | loss 3.614131 | norm 0.3263 | time 464.9086 ms | tok/sec 1127722.7398
for step 3295 | loss 3.642986 | norm 0.3082 | time 464.6585 ms | tok/sec 1128329.7333
for step 3296 | loss 3.622089 | norm 0.3296 | time 465.4200 ms | tok/sec 1126483.5876
for step 3297 | loss 3.534200 | norm 0.3213 | time 465.1980 ms | tok/sec 1127021.0855
for step 3298 | loss 3.419486 | norm 0.3325 | time 464.3955 ms | tok/sec 1128968.6785
for step 3299 | loss 3.418503 | norm 0.3349 | time 465.5769 ms | tok/sec 1126104.0109
for step 3300 | loss 3.517065 | norm 0.3611 | time 464.0446 ms | tok/sec 1129822.5060
for step 3301 | loss 3.459438 | norm 0.3633 | time 465.5802 ms | tok/sec 1126095.9376
for step 3302 | loss 3.536638 | norm 0.3144 | time 464.5944 ms | tok/sec 1128485.4928
for step 3303 | loss 3.426405 | norm 0.2932 | time 464.3342 ms | tok/sec 1129117.6573
for step 3304 | loss 3.449591 | norm 0.3514 | time 465.1082 ms | tok/sec 1127238.8863
for step 3305 | loss 3.457498 | norm 0.3089 | time 464.9243 ms | tok/sec 1127684.5714
for step 3306 | loss 3.459625 | norm 0.4126 | time 465.5244 ms | tok/sec 1126230.8926
for step 3307 | loss 3.463036 | norm 0.4009 | time 465.2045 ms | tok/sec 1127005.4902
for step 3308 | loss 3.443260 | norm 0.3590 | time 465.1484 ms | tok/sec 1127141.2410
for step 3309 | loss 3.625523 | norm 0.3246 | time 465.4078 ms | tok/sec 1126513.0183
for step 3310 | loss 3.681754 | norm 0.3350 | time 464.6542 ms | tok/sec 1128340.1546
for step 3311 | loss 3.675405 | norm 0.3693 | time 464.5321 ms | tok/sec 1128636.6611
for step 3312 | loss 3.703456 | norm 0.3294 | time 464.9692 ms | tok/sec 1127575.8634
for step 3313 | loss 3.663214 | norm 0.3283 | time 465.2083 ms | tok/sec 1126996.2488
for step 3314 | loss 3.682253 | norm 0.2954 | time 464.6771 ms | tok/sec 1128284.5769
for step 3315 | loss 3.626328 | norm 0.3333 | time 464.7150 ms | tok/sec 1128192.5385
for step 3316 | loss 3.654470 | norm 0.3086 | time 466.0487 ms | tok/sec 1124963.9368
for step 3317 | loss 3.667527 | norm 0.3086 | time 465.1325 ms | tok/sec 1127179.9504
for step 3318 | loss 3.765009 | norm 0.3244 | time 465.4040 ms | tok/sec 1126522.2518
for step 3319 | loss 3.686061 | norm 0.3276 | time 464.3412 ms | tok/sec 1129100.8445
for step 3320 | loss 3.654522 | norm 0.2842 | time 464.9746 ms | tok/sec 1127562.5655
for step 3321 | loss 3.614216 | norm 0.2994 | time 465.0993 ms | tok/sec 1127260.2665
for step 3322 | loss 3.608087 | norm 0.3451 | time 463.9952 ms | tok/sec 1129942.6790
for step 3323 | loss 3.679382 | norm 0.3125 | time 464.7591 ms | tok/sec 1128085.4687
for step 3324 | loss 3.651661 | norm 0.3006 | time 463.7542 ms | tok/sec 1130529.9788
for step 3325 | loss 3.666779 | norm 0.3265 | time 464.7377 ms | tok/sec 1128137.5541
for step 3326 | loss 3.652026 | norm 0.3935 | time 464.9346 ms | tok/sec 1127659.7055
for step 3327 | loss 3.654842 | norm 0.3469 | time 464.5381 ms | tok/sec 1128622.1797
for step 3328 | loss 3.661988 | norm 0.2988 | time 465.5027 ms | tok/sec 1126283.3838
for step 3329 | loss 3.681412 | norm 0.3188 | time 469.0907 ms | tok/sec 1117668.7149
for step 3330 | loss 3.665949 | norm 0.3499 | time 465.5356 ms | tok/sec 1126203.7837
for step 3331 | loss 3.685657 | norm 0.4186 | time 465.0593 ms | tok/sec 1127357.3544
for step 3332 | loss 3.635879 | norm 0.3687 | time 464.3016 ms | tok/sec 1129197.0901
for step 3333 | loss 3.680779 | norm 0.4047 | time 464.6533 ms | tok/sec 1128342.4704
for step 3334 | loss 3.609186 | norm 0.4016 | time 464.8411 ms | tok/sec 1127886.4307
for step 3335 | loss 3.590164 | norm 0.4311 | time 465.4026 ms | tok/sec 1126525.7144
for step 3336 | loss 3.627730 | norm 0.3426 | time 464.9475 ms | tok/sec 1127628.4801
for step 3337 | loss 3.640852 | norm 0.3558 | time 465.1368 ms | tok/sec 1127169.5506
for step 3338 | loss 3.596997 | norm 0.3704 | time 465.3578 ms | tok/sec 1126634.2200
for step 3339 | loss 3.602761 | norm 0.3168 | time 464.3211 ms | tok/sec 1129149.5450
for step 3340 | loss 3.644730 | norm 0.3442 | time 464.2887 ms | tok/sec 1129228.4024
for step 3341 | loss 3.634095 | norm 0.3314 | time 464.9835 ms | tok/sec 1127541.1738
for step 3342 | loss 3.605955 | norm 0.2865 | time 465.3845 ms | tok/sec 1126569.5758
for step 3343 | loss 3.520593 | norm 0.3152 | time 464.2792 ms | tok/sec 1129251.5979
for step 3344 | loss 3.431754 | norm 0.3358 | time 464.5584 ms | tok/sec 1128572.9454
for step 3345 | loss 3.475302 | norm 0.3651 | time 464.5979 ms | tok/sec 1128476.8062
for step 3346 | loss 3.444534 | norm 0.3541 | time 465.1923 ms | tok/sec 1127034.9482
for step 3347 | loss 3.435761 | norm 0.3188 | time 463.6731 ms | tok/sec 1130727.6253
for step 3348 | loss 3.396648 | norm 0.3904 | time 465.1489 ms | tok/sec 1127140.0856
for step 3349 | loss 3.444590 | norm 0.4169 | time 464.3834 ms | tok/sec 1128998.2393
for step 3350 | loss 3.565806 | norm 0.4634 | time 464.0882 ms | tok/sec 1129716.2873
for step 3351 | loss 3.496979 | norm 0.3774 | time 465.4551 ms | tok/sec 1126398.7663
for step 3352 | loss 3.489424 | norm 0.3427 | time 464.6354 ms | tok/sec 1128385.8944
for step 3353 | loss 3.477018 | norm 0.3412 | time 465.3158 ms | tok/sec 1126735.8186
for step 3354 | loss 3.505233 | norm 0.3035 | time 464.4148 ms | tok/sec 1128921.7323
for step 3355 | loss 3.697608 | norm 0.3462 | time 465.4658 ms | tok/sec 1126372.8032
for step 3356 | loss 3.763701 | norm 0.4797 | time 465.5545 ms | tok/sec 1126158.2205
for step 3357 | loss 3.738555 | norm 0.3633 | time 464.2880 ms | tok/sec 1129230.1420
for step 3358 | loss 3.627955 | norm 0.3441 | time 464.8385 ms | tok/sec 1127892.7942
for step 3359 | loss 3.844749 | norm 0.3807 | time 465.9052 ms | tok/sec 1125310.4962
for step 3360 | loss 3.691229 | norm 0.3761 | time 464.6623 ms | tok/sec 1128320.4702
for step 3361 | loss 3.647888 | norm 0.3350 | time 464.6266 ms | tok/sec 1128407.3182
for step 3362 | loss 3.661981 | norm 0.3483 | time 464.3734 ms | tok/sec 1129022.5846
for step 3363 | loss 3.670024 | norm 0.3126 | time 464.9589 ms | tok/sec 1127600.7257
for step 3364 | loss 3.632196 | norm 0.3012 | time 465.3850 ms | tok/sec 1126568.4216
for step 3365 | loss 3.663268 | norm 0.3008 | time 465.1284 ms | tok/sec 1127189.7726
for step 3366 | loss 3.711091 | norm 0.3134 | time 465.8968 ms | tok/sec 1125330.6516
for step 3367 | loss 3.783055 | norm 0.3380 | time 465.4632 ms | tok/sec 1126379.1496
for step 3368 | loss 3.701917 | norm 0.3729 | time 464.8054 ms | tok/sec 1127973.2118
for step 3369 | loss 3.673418 | norm 0.3279 | time 464.4673 ms | tok/sec 1128794.2436
for step 3370 | loss 3.678151 | norm 0.3638 | time 464.3099 ms | tok/sec 1129176.7960
for step 3371 | loss 3.673082 | norm 0.3296 | time 467.1609 ms | tok/sec 1122285.6085
for step 3372 | loss 3.659881 | norm 0.3533 | time 465.2276 ms | tok/sec 1126949.4664
for step 3373 | loss 3.741901 | norm 0.3384 | time 465.5998 ms | tok/sec 1126048.6534
for step 3374 | loss 3.703824 | norm 0.3185 | time 465.3544 ms | tok/sec 1126642.3010
for step 3375 | loss 3.675279 | norm 0.3486 | time 464.8316 ms | tok/sec 1127909.5710
for step 3376 | loss 3.727196 | norm 0.3151 | time 464.4508 ms | tok/sec 1128834.2256
for step 3377 | loss 3.668105 | norm 0.3539 | time 465.2774 ms | tok/sec 1126828.7743
for step 3378 | loss 3.671560 | norm 0.3807 | time 465.0400 ms | tok/sec 1127404.1707
for step 3379 | loss 3.640595 | norm 0.3678 | time 466.2442 ms | tok/sec 1124492.2225
for step 3380 | loss 3.594869 | norm 0.3428 | time 466.5451 ms | tok/sec 1123767.0150
for step 3381 | loss 3.592055 | norm 0.3473 | time 465.8582 ms | tok/sec 1125423.9515
for step 3382 | loss 3.647460 | norm 0.3312 | time 464.7653 ms | tok/sec 1128070.4226
for step 3383 | loss 3.599524 | norm 0.2899 | time 464.4721 ms | tok/sec 1128782.6552
for step 3384 | loss 3.604167 | norm 0.2930 | time 464.7393 ms | tok/sec 1128133.5029
for step 3385 | loss 3.677975 | norm 0.3088 | time 465.0590 ms | tok/sec 1127357.9324
for step 3386 | loss 3.620958 | norm 0.2850 | time 465.4455 ms | tok/sec 1126421.8457
for step 3387 | loss 3.581386 | norm 0.3140 | time 465.3223 ms | tok/sec 1126720.2313
for step 3388 | loss 3.611090 | norm 0.2957 | time 464.5026 ms | tok/sec 1128708.4948
for step 3389 | loss 3.632095 | norm 0.3178 | time 464.6335 ms | tok/sec 1128390.5265
for step 3390 | loss 3.458575 | norm 0.3157 | time 465.6258 ms | tok/sec 1125985.8060
for step 3391 | loss 3.491130 | norm 0.3297 | time 465.3199 ms | tok/sec 1126726.0043
for step 3392 | loss 3.422259 | norm 0.3377 | time 464.7007 ms | tok/sec 1128227.2682
for step 3393 | loss 3.437819 | norm 0.3289 | time 465.1711 ms | tok/sec 1127086.3591
for step 3394 | loss 3.546180 | norm 0.3149 | time 465.0917 ms | tok/sec 1127278.7581
for step 3395 | loss 3.422120 | norm 0.2894 | time 465.0221 ms | tok/sec 1127447.5225
for step 3396 | loss 3.436219 | norm 0.3096 | time 465.7319 ms | tok/sec 1125729.3003
for step 3397 | loss 3.514253 | norm 0.3195 | time 464.7641 ms | tok/sec 1128073.3161
for step 3398 | loss 3.393106 | norm 0.3303 | time 465.4448 ms | tok/sec 1126423.5767
for step 3399 | loss 3.599461 | norm 0.3459 | time 465.6165 ms | tok/sec 1126008.2919
for step 3400 | loss 3.482485 | norm 0.2931 | time 465.7965 ms | tok/sec 1125573.1485
for step 3401 | loss 3.481550 | norm 0.2972 | time 463.7544 ms | tok/sec 1130529.3976
for step 3402 | loss 3.640022 | norm 0.3223 | time 464.2208 ms | tok/sec 1129393.6907
for step 3403 | loss 3.741642 | norm 0.3546 | time 465.2922 ms | tok/sec 1126792.9759
for step 3404 | loss 3.695715 | norm 0.4003 | time 465.0521 ms | tok/sec 1127374.6933
for step 3405 | loss 3.712948 | norm 0.4173 | time 465.1575 ms | tok/sec 1127119.2876
for step 3406 | loss 3.690781 | norm 0.4361 | time 464.9181 ms | tok/sec 1127699.6072
for step 3407 | loss 3.689939 | norm 0.3715 | time 465.4186 ms | tok/sec 1126487.0499
for step 3408 | loss 3.693074 | norm 0.3231 | time 464.9446 ms | tok/sec 1127635.4189
for step 3409 | loss 3.694096 | norm 0.3581 | time 465.0345 ms | tok/sec 1127417.4649
for step 3410 | loss 3.723394 | norm 0.3320 | time 465.1544 ms | tok/sec 1127126.7979
for step 3411 | loss 3.658147 | norm 0.3228 | time 464.4387 ms | tok/sec 1128863.7793
for step 3412 | loss 3.606938 | norm 0.3545 | time 465.8258 ms | tok/sec 1125502.2894
for step 3413 | loss 3.708266 | norm 0.3972 | time 464.0036 ms | tok/sec 1129922.3581
for step 3414 | loss 3.647700 | norm 0.3895 | time 465.4989 ms | tok/sec 1126292.6135
for step 3415 | loss 3.671083 | norm 0.3539 | time 465.0581 ms | tok/sec 1127360.2442
for step 3416 | loss 3.724721 | norm 0.3398 | time 464.4804 ms | tok/sec 1128762.3760
for step 3417 | loss 3.650923 | norm 0.3500 | time 463.6247 ms | tok/sec 1130845.6649
for step 3418 | loss 3.649324 | norm 0.3691 | time 464.8309 ms | tok/sec 1127911.3066
for step 3419 | loss 3.657026 | norm 0.3392 | time 465.2276 ms | tok/sec 1126949.4664
for step 3420 | loss 3.657538 | norm 0.4107 | time 464.7732 ms | tok/sec 1128051.3263
for step 3421 | loss 3.646578 | norm 0.3583 | time 465.2572 ms | tok/sec 1126877.8565
for step 3422 | loss 3.679083 | norm 0.3597 | time 464.7837 ms | tok/sec 1128025.8656
for step 3423 | loss 3.664392 | norm 0.3813 | time 464.0603 ms | tok/sec 1129784.1953
for step 3424 | loss 3.625386 | norm 0.3521 | time 464.5154 ms | tok/sec 1128677.2113
for step 3425 | loss 3.669681 | norm 0.3604 | time 464.1540 ms | tok/sec 1129556.1264
for step 3426 | loss 3.618221 | norm 0.3651 | time 464.3908 ms | tok/sec 1128980.2708
for step 3427 | loss 3.639540 | norm 0.3369 | time 464.5655 ms | tok/sec 1128555.5696
Will loading at 0 from edu_fineweb10B/edufineweb_train_000019.npy
for step 3428 | loss 3.633243 | norm 0.3494 | time 1613.8587 ms | tok/sec 324866.1113
for step 3429 | loss 3.621565 | norm 0.3548 | time 462.4090 ms | tok/sec 1133818.7144
for step 3430 | loss 3.602932 | norm 0.3027 | time 464.9944 ms | tok/sec 1127514.5799
for step 3431 | loss 3.573931 | norm 0.3245 | time 463.9707 ms | tok/sec 1130002.4848
for step 3432 | loss 3.587359 | norm 0.3053 | time 464.8917 ms | tok/sec 1127763.8027
for step 3433 | loss 3.688824 | norm 0.3065 | time 464.6540 ms | tok/sec 1128340.7335
for step 3434 | loss 3.565836 | norm 0.3027 | time 465.0106 ms | tok/sec 1127475.2694
for step 3435 | loss 3.595304 | norm 0.3157 | time 465.1070 ms | tok/sec 1127241.7755
for step 3436 | loss 3.641012 | norm 0.3188 | time 465.6982 ms | tok/sec 1125810.5625
for step 3437 | loss 3.603068 | norm 0.2951 | time 465.3478 ms | tok/sec 1126658.4635
for step 3438 | loss 3.448787 | norm 0.3133 | time 464.7112 ms | tok/sec 1128201.7995
for step 3439 | loss 3.463803 | norm 0.3904 | time 466.1198 ms | tok/sec 1124792.4632
for step 3440 | loss 3.368992 | norm 0.4095 | time 465.3244 ms | tok/sec 1126715.0356
for step 3441 | loss 3.389236 | norm 0.3432 | time 465.4434 ms | tok/sec 1126427.0387
for step 3442 | loss 3.480477 | norm 0.3840 | time 465.3134 ms | tok/sec 1126741.5918
for step 3443 | loss 3.388278 | norm 0.3287 | time 465.9529 ms | tok/sec 1125195.3365
for step 3444 | loss 3.415915 | norm 0.3157 | time 466.3746 ms | tok/sec 1124177.7744
for step 3445 | loss 3.401740 | norm 0.3434 | time 468.8854 ms | tok/sec 1118158.0311
for step 3446 | loss 3.413081 | norm 0.3073 | time 465.5983 ms | tok/sec 1126052.1131
for step 3447 | loss 3.406694 | norm 0.2990 | time 466.2037 ms | tok/sec 1124589.9844
for step 3448 | loss 3.496960 | norm 0.3101 | time 465.2722 ms | tok/sec 1126841.4776
for step 3449 | loss 3.513175 | norm 0.3072 | time 465.8351 ms | tok/sec 1125479.8237
for step 3450 | loss 3.696693 | norm 0.3235 | time 466.5170 ms | tok/sec 1123834.7840
for step 3451 | loss 3.584517 | norm 0.4225 | time 465.1778 ms | tok/sec 1127070.1844
for step 3452 | loss 3.658046 | norm 0.3618 | time 465.6594 ms | tok/sec 1125904.5185
for step 3453 | loss 3.670425 | norm 0.3709 | time 465.0328 ms | tok/sec 1127421.5110
for step 3454 | loss 3.629746 | norm 0.3514 | time 465.2710 ms | tok/sec 1126844.3647
for step 3455 | loss 3.610858 | norm 0.3455 | time 465.6212 ms | tok/sec 1125996.7606
for step 3456 | loss 3.662615 | norm 0.3413 | time 465.5864 ms | tok/sec 1126080.9446
for step 3457 | loss 3.651612 | norm 0.3170 | time 466.0769 ms | tok/sec 1124896.0316
for step 3458 | loss 3.711768 | norm 0.3379 | time 465.4684 ms | tok/sec 1126366.4568
for step 3459 | loss 3.641342 | norm 0.3359 | time 465.3914 ms | tok/sec 1126552.8388
for step 3460 | loss 3.707212 | norm 0.3361 | time 465.7316 ms | tok/sec 1125729.8766
for step 3461 | loss 3.682265 | norm 0.3290 | time 466.0926 ms | tok/sec 1124858.0543
for step 3462 | loss 3.646987 | norm 0.3086 | time 465.6303 ms | tok/sec 1125974.8517
for step 3463 | loss 3.621607 | norm 0.2908 | time 465.7609 ms | tok/sec 1125658.9978
for step 3464 | loss 3.678294 | norm 0.3138 | time 464.9034 ms | tok/sec 1127735.4632
for step 3465 | loss 3.647980 | norm 0.2982 | time 465.3790 ms | tok/sec 1126582.8504
for step 3466 | loss 3.704115 | norm 0.3116 | time 465.4174 ms | tok/sec 1126489.9352
for step 3467 | loss 3.614421 | norm 0.2945 | time 465.2400 ms | tok/sec 1126919.4354
for step 3468 | loss 3.658888 | norm 0.3057 | time 465.3890 ms | tok/sec 1126558.6102
for step 3469 | loss 3.626562 | norm 0.3134 | time 465.1372 ms | tok/sec 1127168.3951
for step 3470 | loss 3.672435 | norm 0.3271 | time 465.4663 ms | tok/sec 1126371.6493
for step 3471 | loss 3.624591 | norm 0.3411 | time 465.4052 ms | tok/sec 1126519.3663
for step 3472 | loss 3.596230 | norm 0.3044 | time 465.3163 ms | tok/sec 1126734.6640
for step 3473 | loss 3.636481 | norm 0.3088 | time 464.9012 ms | tok/sec 1127740.6683
for step 3474 | loss 3.619973 | norm 0.3213 | time 466.2590 ms | tok/sec 1124456.5724
for step 3475 | loss 3.588069 | norm 0.3323 | time 465.2400 ms | tok/sec 1126919.4354
for step 3476 | loss 3.592449 | norm 0.3450 | time 465.4484 ms | tok/sec 1126414.9218
for step 3477 | loss 3.665046 | norm 0.4165 | time 464.9396 ms | tok/sec 1127647.5621
for step 3478 | loss 3.596230 | norm 0.4895 | time 465.8670 ms | tok/sec 1125402.6409
for step 3479 | loss 3.623799 | norm 0.4284 | time 465.6913 ms | tok/sec 1125827.2775
for step 3480 | loss 3.571770 | norm 0.4064 | time 465.1961 ms | tok/sec 1127025.7063
for step 3481 | loss 3.647039 | norm 0.3910 | time 464.7887 ms | tok/sec 1128013.7143
for step 3482 | loss 3.594908 | norm 0.3539 | time 464.9553 ms | tok/sec 1127609.3988
for step 3483 | loss 3.695632 | norm 0.3650 | time 465.4143 ms | tok/sec 1126497.4371
for step 3484 | loss 3.483538 | norm 0.3326 | time 465.8318 ms | tok/sec 1125487.8882
for step 3485 | loss 3.362378 | norm 0.3527 | time 465.1873 ms | tok/sec 1127047.0785
for step 3486 | loss 3.403558 | norm 0.3244 | time 464.6616 ms | tok/sec 1128322.2070
for step 3487 | loss 3.443117 | norm 0.2949 | time 465.5552 ms | tok/sec 1126156.4903
for step 3488 | loss 3.420097 | norm 0.3032 | time 464.6835 ms | tok/sec 1128268.9467
for step 3489 | loss 3.436787 | norm 0.3391 | time 465.3797 ms | tok/sec 1126581.1189
for step 3490 | loss 3.468117 | norm 0.3840 | time 465.2162 ms | tok/sec 1126977.1888
for step 3491 | loss 3.378864 | norm 0.3695 | time 465.6141 ms | tok/sec 1126014.0576
for step 3492 | loss 3.469393 | norm 0.3545 | time 465.8253 ms | tok/sec 1125503.4415
for step 3493 | loss 3.420297 | norm 0.3243 | time 465.3451 ms | tok/sec 1126664.8131
for step 3494 | loss 3.421482 | norm 0.3593 | time 465.4260 ms | tok/sec 1126469.1613
for step 3495 | loss 3.479897 | norm 0.3390 | time 465.3425 ms | tok/sec 1126671.1628
for step 3496 | loss 3.746757 | norm 0.3009 | time 464.8259 ms | tok/sec 1127923.4557
for step 3497 | loss 3.641998 | norm 0.3470 | time 465.5969 ms | tok/sec 1126055.5728
for step 3498 | loss 3.747531 | norm 0.3787 | time 464.6041 ms | tok/sec 1128461.7497
for step 3499 | loss 3.654305 | norm 0.3965 | time 465.3876 ms | tok/sec 1126562.0730
validation loss 3.6584
HellaSwag accuracy: 2649/10042=0.2638
> Hello, I'm a language model, and you have to keep track of two different words, i.e. if you are speaking with a foreign language,
> Hello, I'm a language model, and it is in the language. This sounds like a bit of a big challenge. After the publication, I had to
> Hello, I'm a language model, so can be translated into English.
I'm a member of my community, and I've got my name out there
> Hello, I'm a language model, and I'm also an object-oriented subject. I's used to teaching it to adults over the course of one year
> Hello, I'm a language model, but I was a good linguist, with the title "Mabel. It's good about . . .
-
> Hello, I'm a language model, just for the question I am referring to myself, one as a language translator and, most of all, the topic.
> Hello, I'm a language model, I always do myself with an English. My Spanish is a bilingual, but I always like to learn other languages or to
> > Hello, I'm a language model, and a language professor. I have some ideas about it in a few years, and I have been going with a very
Hello, I'm a language model, and you learned an idea for the other, that all you'll have to learn to use it! I'm one.
> Hello, I'm a language model, my language system and my language interpreter I didn't believe I'd write or write in a language interpreter, but I don
> > >>Hello, I'm a language model, and I'm not sure what the word might mean in any language, to go through an IIS program, but for
Hello, I'm a language model, but I have never used the same terms over the past few years. When I see someone from other languages, I see
 Hello, I'm a language model, and I love to have your students understand better than I have. I feel like teachers that the community will have a better>  Hello, I'm a language model, so the students are usually very specific about who is at home. You want to know how to teach you a language language> > 
Hello, I'm a language model, an open source web project from K.I. and the author. I think they're also a good resource for creating

Hello, I'm a language model, and we've been working on developing a new language model that will help us understand the language.
The result can be
Hello, I'm a language model, and this lesson is for the first time we want to use what we have learned from a specific time period of the year
> > > > Hello, I'm a language model, what is language?
|E, G, R, P, E, Q|
|L, C (
Hello, I'm a language model, and I can't use the words I speak. My language model is so simple, I'm not going to use the
Hello, I'm a language model, that uses the word "cite." It should be used with words like "the man," "hay" and
Hello, I'm a language model, and it doesn't allow me to understand everything we are saying; it allows me to understand that and understand what this means
> > > > Hello, I'm a language model, and I've spoken from a range of sources. With the exception of other languages, I've been very confused because the
Hello, I'm a language model, and my life is different from the real world. I have to have an easy way of thinking about who the other languages
Hello, I'm a language model, and it takes a variety of methods, including:
- Multicolor Method (which is a method of the grammar
Hello, I'm a language model, and just like I use a large unit.
So if I were going to explain that one was one, I'd
> > > Hello, I'm a language model, she is a language model and I used for more than a thousand words each. I am a person who is familiar with
Hello, I'm a language model, and my life is an adventure-like game that is a great resource for anyone interested to figure out.<|endoftext|>Dates
Hello, I'm a language model, meaning, in my language-based language, that can mean something for me (which, in time, means that I
> Hello, I'm a language model, and I don't understand if we use models like these to describe things which is not a natural language, and I know
> Hello, I'm a language model, but in the end it's the kind of person singular: I think all that is the difference. There are only the
> Hello, I'm a language model, but I just haven't figured out it out the way it would.
I'll take a look at the two lists
> Hello, I'm a language model, and is a part of the English language, grammar, and spelling practice. It tells me who I am and I use
for step 3500 | loss 3.648032 | norm 0.3831 | time 13068.5818 ms | tok/sec 40118.2016
for step 3501 | loss 3.681051 | norm 0.3199 | time 463.1240 ms | tok/sec 1132068.2114
for step 3502 | loss 3.694149 | norm 0.3305 | time 462.9502 ms | tok/sec 1132493.2281
for step 3503 | loss 3.642229 | norm 0.3475 | time 463.3980 ms | tok/sec 1131398.9765
for step 3504 | loss 3.663487 | norm 0.3594 | time 463.2335 ms | tok/sec 1131800.7720
for step 3505 | loss 3.651653 | norm 0.3203 | time 462.7457 ms | tok/sec 1132993.8624
for step 3506 | loss 3.737415 | norm 0.3245 | time 462.5757 ms | tok/sec 1133410.2276
for step 3507 | loss 3.677396 | norm 0.3237 | time 463.4473 ms | tok/sec 1131278.4935
for step 3508 | loss 3.615545 | norm 0.3032 | time 464.1161 ms | tok/sec 1129648.3875
for step 3509 | loss 3.595602 | norm 0.3084 | time 463.1605 ms | tok/sec 1131979.0509
for step 3510 | loss 3.620476 | norm 0.3133 | time 463.6137 ms | tok/sec 1130872.4162
for step 3511 | loss 3.664287 | norm 0.2858 | time 465.1158 ms | tok/sec 1127220.3959
for step 3512 | loss 3.664570 | norm 0.3233 | time 463.4371 ms | tok/sec 1131303.5193
for step 3513 | loss 3.653121 | norm 0.3390 | time 464.1397 ms | tok/sec 1129590.9401
for step 3514 | loss 3.628182 | norm 0.3068 | time 463.8584 ms | tok/sec 1130276.0463
for step 3515 | loss 3.572181 | norm 0.2948 | time 464.2348 ms | tok/sec 1129359.4692
for step 3516 | loss 3.579821 | norm 0.2954 | time 464.7839 ms | tok/sec 1128025.2870
for step 3517 | loss 3.575324 | norm 0.2846 | time 464.7963 ms | tok/sec 1127995.1985
for step 3518 | loss 3.679832 | norm 0.3205 | time 465.6236 ms | tok/sec 1125990.9950
for step 3519 | loss 3.596437 | norm 0.3183 | time 464.8023 ms | tok/sec 1127980.7335
for step 3520 | loss 3.639979 | norm 0.3123 | time 465.3730 ms | tok/sec 1126597.2796
for step 3521 | loss 3.606479 | norm 0.3298 | time 465.1985 ms | tok/sec 1127019.9302
for step 3522 | loss 3.549768 | norm 0.2895 | time 465.7288 ms | tok/sec 1125736.7921
for step 3523 | loss 3.561577 | norm 0.3222 | time 464.8991 ms | tok/sec 1127745.8735
for step 3524 | loss 3.567573 | norm 0.3175 | time 464.9646 ms | tok/sec 1127586.8489
for step 3525 | loss 3.526672 | norm 0.3692 | time 465.3895 ms | tok/sec 1126557.4559
for step 3526 | loss 3.563228 | norm 0.3842 | time 464.8383 ms | tok/sec 1127893.3727
for step 3527 | loss 3.564140 | norm 0.3695 | time 465.7063 ms | tok/sec 1125790.9663
for step 3528 | loss 3.643478 | norm 0.3974 | time 465.2562 ms | tok/sec 1126880.1664
for step 3529 | loss 3.604590 | norm 0.3221 | time 464.8466 ms | tok/sec 1127873.1254
for step 3530 | loss 3.612453 | norm 0.3006 | time 465.4942 ms | tok/sec 1126304.1509
for step 3531 | loss 3.509406 | norm 0.2977 | time 466.6553 ms | tok/sec 1123501.7609
for step 3532 | loss 3.484254 | norm 0.3072 | time 466.3532 ms | tok/sec 1124229.4997
for step 3533 | loss 3.464857 | norm 0.2955 | time 464.8726 ms | tok/sec 1127810.0743
for step 3534 | loss 3.410979 | norm 0.3192 | time 466.3925 ms | tok/sec 1124134.6737
for step 3535 | loss 3.405298 | norm 0.3207 | time 465.3196 ms | tok/sec 1126726.5816
for step 3536 | loss 3.459927 | norm 0.2968 | time 464.6165 ms | tok/sec 1128431.6380
for step 3537 | loss 3.443445 | norm 0.3066 | time 465.8818 ms | tok/sec 1125366.9331
for step 3538 | loss 3.396625 | norm 0.3248 | time 464.8404 ms | tok/sec 1127888.1661
for step 3539 | loss 3.441327 | norm 0.3808 | time 464.9744 ms | tok/sec 1127563.1437
for step 3540 | loss 3.403701 | norm 0.4001 | time 465.2009 ms | tok/sec 1127014.1542
for step 3541 | loss 3.464337 | norm 0.4078 | time 465.5588 ms | tok/sec 1126147.8396
for step 3542 | loss 3.469869 | norm 0.3872 | time 465.7612 ms | tok/sec 1125658.4216
for step 3543 | loss 3.568286 | norm 0.3604 | time 464.9551 ms | tok/sec 1127609.9770
for step 3544 | loss 3.628405 | norm 0.3369 | time 464.5057 ms | tok/sec 1128700.9634
for step 3545 | loss 3.609965 | norm 0.3604 | time 464.8633 ms | tok/sec 1127832.6330
for step 3546 | loss 3.611063 | norm 0.3844 | time 466.1906 ms | tok/sec 1124621.6169
for step 3547 | loss 3.703551 | norm 0.3730 | time 465.1892 ms | tok/sec 1127042.4574
for step 3548 | loss 3.757210 | norm 0.3715 | time 465.4689 ms | tok/sec 1126365.3030
for step 3549 | loss 3.771780 | norm 0.4120 | time 465.4400 ms | tok/sec 1126435.1167
for step 3550 | loss 3.650458 | norm 0.3799 | time 465.2071 ms | tok/sec 1126999.1367
for step 3551 | loss 3.671899 | norm 0.3696 | time 465.5819 ms | tok/sec 1126091.9010
for step 3552 | loss 3.603849 | norm 0.3781 | time 465.2975 ms | tok/sec 1126780.2738
for step 3553 | loss 3.640886 | norm 0.3290 | time 465.5676 ms | tok/sec 1126126.5015
for step 3554 | loss 3.710973 | norm 0.3508 | time 464.9634 ms | tok/sec 1127589.7399
for step 3555 | loss 3.681581 | norm 0.3582 | time 465.0860 ms | tok/sec 1127292.6273
for step 3556 | loss 3.611187 | norm 0.3345 | time 465.8222 ms | tok/sec 1125510.9303
for step 3557 | loss 3.740633 | norm 0.3332 | time 465.4045 ms | tok/sec 1126521.0976
for step 3558 | loss 3.638211 | norm 0.3268 | time 465.3063 ms | tok/sec 1126758.9118
for step 3559 | loss 3.614294 | norm 0.3251 | time 465.0555 ms | tok/sec 1127366.6018
for step 3560 | loss 3.685509 | norm 0.4550 | time 466.7046 ms | tok/sec 1123382.9539
for step 3561 | loss 3.658908 | norm 0.3350 | time 465.9622 ms | tok/sec 1125172.8831
for step 3562 | loss 3.638015 | norm 0.3436 | time 465.8535 ms | tok/sec 1125435.4711
for step 3563 | loss 3.664782 | norm 0.3366 | time 465.7836 ms | tok/sec 1125604.2601
for step 3564 | loss 3.688224 | norm 0.3491 | time 466.7535 ms | tok/sec 1123265.3195
for step 3565 | loss 3.647384 | norm 0.3445 | time 465.6682 ms | tok/sec 1125883.1897
for step 3566 | loss 3.609230 | norm 0.3455 | time 465.4574 ms | tok/sec 1126392.9966
for step 3567 | loss 3.524980 | norm 0.3252 | time 464.8359 ms | tok/sec 1127899.1577
for step 3568 | loss 3.576480 | norm 0.3006 | time 465.5583 ms | tok/sec 1126148.9930
for step 3569 | loss 3.585161 | norm 0.3392 | time 465.5309 ms | tok/sec 1126215.3192
for step 3570 | loss 3.537253 | norm 0.3227 | time 465.9328 ms | tok/sec 1125243.7007
for step 3571 | loss 3.587320 | norm 0.3084 | time 464.8550 ms | tok/sec 1127852.8788
for step 3572 | loss 3.634768 | norm 0.3031 | time 465.6696 ms | tok/sec 1125879.7311
for step 3573 | loss 3.593556 | norm 0.2948 | time 463.4609 ms | tok/sec 1131245.3215
for step 3574 | loss 3.528662 | norm 0.2830 | time 464.7861 ms | tok/sec 1128020.0792
for step 3575 | loss 3.584209 | norm 0.2858 | time 464.3776 ms | tok/sec 1129012.1508
for step 3576 | loss 3.606068 | norm 0.2676 | time 465.6398 ms | tok/sec 1125951.7907
for step 3577 | loss 3.629218 | norm 0.2879 | time 465.4839 ms | tok/sec 1126328.9571
for step 3578 | loss 3.489134 | norm 0.2995 | time 465.1685 ms | tok/sec 1127092.7136
for step 3579 | loss 3.447271 | norm 0.3116 | time 464.9086 ms | tok/sec 1127722.7398
for step 3580 | loss 3.461543 | norm 0.3218 | time 465.4756 ms | tok/sec 1126349.1489
for step 3581 | loss 3.420614 | norm 0.3119 | time 465.0578 ms | tok/sec 1127360.8222
for step 3582 | loss 3.455504 | norm 0.3086 | time 465.4579 ms | tok/sec 1126391.8427
for step 3583 | loss 3.485651 | norm 0.3609 | time 465.1482 ms | tok/sec 1127141.8187
for step 3584 | loss 3.450973 | norm 0.3478 | time 465.2069 ms | tok/sec 1126999.7143
for step 3585 | loss 3.411882 | norm 0.3258 | time 465.4531 ms | tok/sec 1126403.3821
for step 3586 | loss 3.451013 | norm 0.3485 | time 465.9808 ms | tok/sec 1125127.9790
for step 3587 | loss 3.483369 | norm 0.3574 | time 465.2119 ms | tok/sec 1126987.5851
for step 3588 | loss 3.470498 | norm 0.3811 | time 464.8380 ms | tok/sec 1127893.9512
for step 3589 | loss 3.399115 | norm 0.3901 | time 465.4346 ms | tok/sec 1126448.3881
for step 3590 | loss 3.654459 | norm 0.3899 | time 465.3201 ms | tok/sec 1126725.4270
for step 3591 | loss 3.669135 | norm 0.3246 | time 465.5459 ms | tok/sec 1126178.9830
for step 3592 | loss 3.694211 | norm 0.3796 | time 465.3292 ms | tok/sec 1126703.4898
for step 3593 | loss 3.716049 | norm 0.3933 | time 465.5049 ms | tok/sec 1126278.1922
for step 3594 | loss 3.676156 | norm 0.3769 | time 464.8416 ms | tok/sec 1127885.2737
for step 3595 | loss 3.632217 | norm 0.3155 | time 465.1182 ms | tok/sec 1127214.6178
for step 3596 | loss 3.679998 | norm 0.3860 | time 464.8073 ms | tok/sec 1127968.5832
for step 3597 | loss 3.660876 | norm 0.3337 | time 465.1296 ms | tok/sec 1127186.8837
for step 3598 | loss 3.671022 | norm 0.3496 | time 465.5528 ms | tok/sec 1126162.2576
for step 3599 | loss 3.712272 | norm 0.3495 | time 464.8981 ms | tok/sec 1127748.1869
for step 3600 | loss 3.682409 | norm 0.3159 | time 465.2858 ms | tok/sec 1126808.5653
for step 3601 | loss 3.642420 | norm 0.3308 | time 465.1387 ms | tok/sec 1127164.9286
for step 3602 | loss 3.653692 | norm 0.3295 | time 464.8159 ms | tok/sec 1127947.7546
for step 3603 | loss 3.704714 | norm 0.3416 | time 465.1892 ms | tok/sec 1127042.4574
for step 3604 | loss 3.637643 | norm 0.3201 | time 465.5466 ms | tok/sec 1126177.2528
for step 3605 | loss 3.598350 | norm 0.3066 | time 464.9441 ms | tok/sec 1127636.5754
for step 3606 | loss 3.655789 | norm 0.3215 | time 464.7903 ms | tok/sec 1128009.6639
for step 3607 | loss 3.643466 | norm 0.2796 | time 464.9496 ms | tok/sec 1127623.2760
for step 3608 | loss 3.661828 | norm 0.3020 | time 465.3261 ms | tok/sec 1126710.9945
for step 3609 | loss 3.620571 | norm 0.3315 | time 465.6129 ms | tok/sec 1126016.9405
for step 3610 | loss 3.674253 | norm 0.3222 | time 465.2691 ms | tok/sec 1126848.9841
for step 3611 | loss 3.633371 | norm 0.3098 | time 465.1682 ms | tok/sec 1127093.2913
for step 3612 | loss 3.594942 | norm 0.3373 | time 464.2143 ms | tok/sec 1129409.3521
for step 3613 | loss 3.590448 | norm 0.3573 | time 465.4458 ms | tok/sec 1126421.2687
for step 3614 | loss 3.620538 | norm 0.3248 | time 464.8614 ms | tok/sec 1127837.2606
for step 3615 | loss 3.616779 | norm 0.3353 | time 466.2929 ms | tok/sec 1124374.9306
for step 3616 | loss 3.692481 | norm 0.3175 | time 464.8879 ms | tok/sec 1127773.0567
for step 3617 | loss 3.523433 | norm 0.3343 | time 464.8709 ms | tok/sec 1127814.1232
for step 3618 | loss 3.628488 | norm 0.3545 | time 465.4560 ms | tok/sec 1126396.4584
Will loading at 0 from edu_fineweb10B/edufineweb_train_000020.npy
for step 3619 | loss 3.555838 | norm 0.3559 | time 1627.5265 ms | tok/sec 322137.9148
for step 3620 | loss 3.612331 | norm 0.3073 | time 463.4435 ms | tok/sec 1131287.8053
for step 3621 | loss 3.607171 | norm 0.3426 | time 464.0074 ms | tok/sec 1129913.0688
for step 3622 | loss 3.617304 | norm 0.3122 | time 463.8708 ms | tok/sec 1130245.8377
for step 3623 | loss 3.569954 | norm 0.2867 | time 465.0311 ms | tok/sec 1127425.5571
for step 3624 | loss 3.473106 | norm 0.3275 | time 464.0982 ms | tok/sec 1129691.9121
for step 3625 | loss 3.486418 | norm 0.3162 | time 464.8769 ms | tok/sec 1127799.6628
for step 3626 | loss 3.410608 | norm 0.2960 | time 464.9198 ms | tok/sec 1127695.5590
for step 3627 | loss 3.417587 | norm 0.2923 | time 465.0517 ms | tok/sec 1127375.8493
for step 3628 | loss 3.406235 | norm 0.3016 | time 465.6382 ms | tok/sec 1125955.8263
for step 3629 | loss 3.421855 | norm 0.3464 | time 465.2970 ms | tok/sec 1126781.4285
for step 3630 | loss 3.431573 | norm 0.3706 | time 466.5964 ms | tok/sec 1123643.5587
for step 3631 | loss 3.427243 | norm 0.3861 | time 465.8301 ms | tok/sec 1125491.9205
for step 3632 | loss 3.477850 | norm 0.4078 | time 465.6765 ms | tok/sec 1125863.0146
for step 3633 | loss 3.372143 | norm 0.3946 | time 465.4510 ms | tok/sec 1126408.5749
for step 3634 | loss 3.404504 | norm 0.3387 | time 464.9034 ms | tok/sec 1127735.4632
for step 3635 | loss 3.482850 | norm 0.3416 | time 465.6279 ms | tok/sec 1125980.6171
for step 3636 | loss 3.623659 | norm 0.3568 | time 465.6651 ms | tok/sec 1125890.6835
for step 3637 | loss 3.701609 | norm 0.3434 | time 466.0816 ms | tok/sec 1124884.5231
for step 3638 | loss 3.638171 | norm 0.3699 | time 465.4191 ms | tok/sec 1126485.8958
for step 3639 | loss 3.661219 | norm 0.3991 | time 465.8554 ms | tok/sec 1125430.8633
for step 3640 | loss 3.756838 | norm 0.3447 | time 465.7421 ms | tok/sec 1125704.5206
for step 3641 | loss 3.644260 | norm 0.3827 | time 465.9660 ms | tok/sec 1125163.6717
for step 3642 | loss 3.635661 | norm 0.3892 | time 465.6031 ms | tok/sec 1126040.5808
for step 3643 | loss 3.677835 | norm 0.3622 | time 464.6339 ms | tok/sec 1128389.3685
for step 3644 | loss 3.659359 | norm 0.3378 | time 465.5287 ms | tok/sec 1126220.5103
for step 3645 | loss 3.700421 | norm 0.3491 | time 466.3551 ms | tok/sec 1124224.9017
for step 3646 | loss 3.653764 | norm 0.3650 | time 466.3084 ms | tok/sec 1124337.5633
for step 3647 | loss 3.630918 | norm 0.3154 | time 465.3840 ms | tok/sec 1126570.7301
for step 3648 | loss 3.625987 | norm 0.3147 | time 466.2812 ms | tok/sec 1124403.1014
for step 3649 | loss 3.659794 | norm 0.3037 | time 465.6522 ms | tok/sec 1125921.8128
for step 3650 | loss 3.591593 | norm 0.2911 | time 465.7800 ms | tok/sec 1125612.9026
for step 3651 | loss 3.605912 | norm 0.2764 | time 465.9309 ms | tok/sec 1125248.3071
for step 3652 | loss 3.605537 | norm 0.2776 | time 465.3165 ms | tok/sec 1126734.0867
for step 3653 | loss 3.638360 | norm 0.2857 | time 465.6715 ms | tok/sec 1125875.1196
for step 3654 | loss 3.612235 | norm 0.2894 | time 465.7772 ms | tok/sec 1125619.8166
for step 3655 | loss 3.647145 | norm 0.3187 | time 466.0945 ms | tok/sec 1124853.4512
for step 3656 | loss 3.585485 | norm 0.3844 | time 464.6103 ms | tok/sec 1128446.6937
for step 3657 | loss 3.662677 | norm 0.3562 | time 464.7379 ms | tok/sec 1128136.9754
for step 3658 | loss 3.614799 | norm 0.3445 | time 465.5824 ms | tok/sec 1126090.7477
for step 3659 | loss 3.613763 | norm 0.3132 | time 466.2230 ms | tok/sec 1124543.4016
for step 3660 | loss 3.580083 | norm 0.3137 | time 466.0616 ms | tok/sec 1124932.8606
for step 3661 | loss 3.562935 | norm 0.3085 | time 465.8251 ms | tok/sec 1125504.0176
for step 3662 | loss 3.614771 | norm 0.3056 | time 464.7441 ms | tok/sec 1128121.9280
for step 3663 | loss 3.585971 | norm 0.2923 | time 465.2600 ms | tok/sec 1126870.9270
for step 3664 | loss 3.589151 | norm 0.2904 | time 464.7849 ms | tok/sec 1128022.9724
for step 3665 | loss 3.534130 | norm 0.2799 | time 465.0829 ms | tok/sec 1127300.1399
for step 3666 | loss 3.575306 | norm 0.2999 | time 465.0736 ms | tok/sec 1127322.6782
for step 3667 | loss 3.612819 | norm 0.2978 | time 465.1320 ms | tok/sec 1127181.1060
for step 3668 | loss 3.541331 | norm 0.2872 | time 465.2295 ms | tok/sec 1126944.8462
for step 3669 | loss 3.517217 | norm 0.2836 | time 465.7879 ms | tok/sec 1125593.8894
for step 3670 | loss 3.543058 | norm 0.2774 | time 465.6701 ms | tok/sec 1125878.5782
for step 3671 | loss 3.492739 | norm 0.3020 | time 465.6007 ms | tok/sec 1126046.3469
for step 3672 | loss 3.438539 | norm 0.3293 | time 466.2602 ms | tok/sec 1124453.6975
for step 3673 | loss 3.403942 | norm 0.3233 | time 465.5259 ms | tok/sec 1126227.4318
for step 3674 | loss 3.352708 | norm 0.3404 | time 465.2400 ms | tok/sec 1126919.4354
for step 3675 | loss 3.417846 | norm 0.3967 | time 466.0673 ms | tok/sec 1124919.0494
for step 3676 | loss 3.439831 | norm 0.4706 | time 464.2782 ms | tok/sec 1129253.9175
for step 3677 | loss 3.491779 | norm 0.4209 | time 465.6579 ms | tok/sec 1125907.9774
for step 3678 | loss 3.421363 | norm 0.4003 | time 465.0168 ms | tok/sec 1127460.2397
for step 3679 | loss 3.451367 | norm 0.3697 | time 464.8595 ms | tok/sec 1127841.8882
for step 3680 | loss 3.488914 | norm 0.4021 | time 464.8359 ms | tok/sec 1127899.1577
for step 3681 | loss 3.464806 | norm 0.3607 | time 465.7719 ms | tok/sec 1125632.4926
for step 3682 | loss 3.424911 | norm 0.3749 | time 465.8766 ms | tok/sec 1125379.6033
for step 3683 | loss 3.576324 | norm 0.3256 | time 464.6730 ms | tok/sec 1128294.4184
for step 3684 | loss 3.688107 | norm 0.3172 | time 464.8116 ms | tok/sec 1127958.1688
for step 3685 | loss 3.642975 | norm 0.3196 | time 464.5727 ms | tok/sec 1128538.1944
for step 3686 | loss 3.619848 | norm 0.3435 | time 464.7982 ms | tok/sec 1127990.5697
for step 3687 | loss 3.676679 | norm 0.3467 | time 464.1747 ms | tok/sec 1129505.6503
for step 3688 | loss 3.578740 | norm 0.3416 | time 465.7750 ms | tok/sec 1125625.0022
for step 3689 | loss 3.657980 | norm 0.3248 | time 464.9286 ms | tok/sec 1127674.1623
for step 3690 | loss 3.642073 | norm 0.3531 | time 464.8683 ms | tok/sec 1127820.4859
for step 3691 | loss 3.715515 | norm 0.3835 | time 465.1835 ms | tok/sec 1127056.3207
for step 3692 | loss 3.634868 | norm 0.3727 | time 465.2452 ms | tok/sec 1126906.7304
for step 3693 | loss 3.656043 | norm 0.3234 | time 465.8923 ms | tok/sec 1125341.5934
for step 3694 | loss 3.630380 | norm 0.3391 | time 464.3867 ms | tok/sec 1128990.1244
for step 3695 | loss 3.611379 | norm 0.3248 | time 466.0792 ms | tok/sec 1124890.2773
for step 3696 | loss 3.659659 | norm 1.3141 | time 465.3742 ms | tok/sec 1126594.3937
for step 3697 | loss 3.585705 | norm 0.3907 | time 465.1845 ms | tok/sec 1127054.0101
for step 3698 | loss 3.644065 | norm 0.3739 | time 464.9806 ms | tok/sec 1127548.1116
for step 3699 | loss 3.638290 | norm 0.3379 | time 464.7696 ms | tok/sec 1128060.0064
for step 3700 | loss 3.621968 | norm 0.3608 | time 464.9057 ms | tok/sec 1127729.6798
for step 3701 | loss 3.639104 | norm 0.3446 | time 464.5746 ms | tok/sec 1128533.5611
for step 3702 | loss 3.639806 | norm 0.3271 | time 465.1067 ms | tok/sec 1127242.3533
for step 3703 | loss 3.676151 | norm 0.2970 | time 465.1432 ms | tok/sec 1127153.9513
for step 3704 | loss 3.657191 | norm 0.3162 | time 465.0176 ms | tok/sec 1127458.5055
for step 3705 | loss 3.681286 | norm 0.2779 | time 465.4582 ms | tok/sec 1126391.2657
for step 3706 | loss 3.575696 | norm 0.2999 | time 464.7536 ms | tok/sec 1128098.7789
for step 3707 | loss 3.597386 | norm 0.3385 | time 465.9030 ms | tok/sec 1125315.6790
for step 3708 | loss 3.544250 | norm 0.3392 | time 465.0881 ms | tok/sec 1127287.4263
for step 3709 | loss 3.587980 | norm 0.3212 | time 466.5644 ms | tok/sec 1123720.5004
for step 3710 | loss 3.533869 | norm 0.3033 | time 465.7209 ms | tok/sec 1125755.8101
for step 3711 | loss 3.564517 | norm 0.3198 | time 465.6701 ms | tok/sec 1125878.5782
for step 3712 | loss 3.571441 | norm 0.3016 | time 465.8811 ms | tok/sec 1125368.6608
for step 3713 | loss 3.566263 | norm 0.3239 | time 465.4343 ms | tok/sec 1126448.9651
for step 3714 | loss 3.563958 | norm 0.3057 | time 465.4398 ms | tok/sec 1126435.6937
for step 3715 | loss 3.557948 | norm 0.2905 | time 465.2014 ms | tok/sec 1127012.9990
for step 3716 | loss 3.625660 | norm 0.3081 | time 465.4756 ms | tok/sec 1126349.1489
for step 3717 | loss 3.605639 | norm 0.3537 | time 464.8304 ms | tok/sec 1127912.4636
for step 3718 | loss 3.427299 | norm 0.3327 | time 465.0414 ms | tok/sec 1127400.7027
for step 3719 | loss 3.504277 | norm 0.4004 | time 465.2755 ms | tok/sec 1126833.3937
for step 3720 | loss 3.459091 | norm 0.3652 | time 464.9255 ms | tok/sec 1127681.6800
for step 3721 | loss 3.438395 | norm 0.3301 | time 465.4324 ms | tok/sec 1126453.5813
for step 3722 | loss 3.419137 | norm 0.3223 | time 465.8139 ms | tok/sec 1125531.0928
for step 3723 | loss 3.473830 | norm 0.3688 | time 465.8744 ms | tok/sec 1125384.7867
for step 3724 | loss 3.512687 | norm 0.3442 | time 464.9765 ms | tok/sec 1127557.9402
for step 3725 | loss 3.444249 | norm 0.3869 | time 465.9245 ms | tok/sec 1125263.8537
for step 3726 | loss 3.484486 | norm 0.3695 | time 465.0464 ms | tok/sec 1127388.5648
for step 3727 | loss 3.414299 | norm 0.3637 | time 465.6236 ms | tok/sec 1125990.9950
for step 3728 | loss 3.379431 | norm 0.2881 | time 464.8478 ms | tok/sec 1127870.2330
for step 3729 | loss 3.406807 | norm 0.3428 | time 465.3614 ms | tok/sec 1126625.5619
for step 3730 | loss 3.604543 | norm 0.3581 | time 465.3981 ms | tok/sec 1126536.6794
for step 3731 | loss 3.718995 | norm 0.3138 | time 465.6000 ms | tok/sec 1126048.0768
for step 3732 | loss 3.633851 | norm 0.3527 | time 464.8576 ms | tok/sec 1127846.5158
for step 3733 | loss 3.683436 | norm 0.3125 | time 464.7079 ms | tok/sec 1128209.9031
for step 3734 | loss 3.738719 | norm 0.3158 | time 465.2433 ms | tok/sec 1126911.3503
for step 3735 | loss 3.718946 | norm 0.3465 | time 465.7276 ms | tok/sec 1125739.6736
for step 3736 | loss 3.695283 | norm 0.3503 | time 464.5085 ms | tok/sec 1128694.0114
for step 3737 | loss 3.688446 | norm 0.3034 | time 464.7267 ms | tok/sec 1128164.1775
for step 3738 | loss 3.623318 | norm 0.3120 | time 465.4679 ms | tok/sec 1126367.6107
for step 3739 | loss 3.670254 | norm 0.3857 | time 465.5397 ms | tok/sec 1126193.9786
for step 3740 | loss 3.598576 | norm 0.3482 | time 465.1122 ms | tok/sec 1127229.0632
for step 3741 | loss 3.613009 | norm 0.3644 | time 464.7181 ms | tok/sec 1128185.0140
for step 3742 | loss 3.617575 | norm 0.2978 | time 464.8266 ms | tok/sec 1127921.7201
for step 3743 | loss 3.637764 | norm 0.3203 | time 465.4410 ms | tok/sec 1126432.8087
for step 3744 | loss 3.582288 | norm 0.3274 | time 465.7133 ms | tok/sec 1125774.2524
for step 3745 | loss 3.627831 | norm 0.3141 | time 465.0679 ms | tok/sec 1127336.5485
for step 3746 | loss 3.562668 | norm 0.3106 | time 466.1102 ms | tok/sec 1124815.4767
for step 3747 | loss 3.619728 | norm 0.2894 | time 465.2128 ms | tok/sec 1126985.2748
for step 3748 | loss 3.618219 | norm 0.3161 | time 465.5852 ms | tok/sec 1126083.8279
for step 3749 | loss 3.607719 | norm 0.3230 | time 464.6447 ms | tok/sec 1128363.3135
validation loss 3.6255
HellaSwag accuracy: 2594/10042=0.2583
> Hello, I'm a language model, and I am a language model. Most languages will use all its own language - and many are still not able to use
> Hello, I'm a language model, a model language model, and the definition of the language:
|M.Sc.'s vocabulary|
|M
> Hello, I'm a language model, so I'll show you how to use it in a sentence as an introduction to the language and use it as part of
> Hello, I'm a language model, and I'm just trying to communicate what "I'm creating the English language barrier. He's the language barrier. I
> Hello, I'm a language model, but I can't even see it in terms of our own.
A: As with I said, there's no
> Hello, I'm a language model,” said Ms. Brown. “What you call a language model is very difficult.”
Mapping
> Hello, I'm a language model, I said, there are 3 types of words:
- words in a sentence
- words in a sentence that you
> Hello, I'm a language model, and you didn't even understand the exact structure in Google. When I made a speech, it turned into a map of
> Hello, I'm a language model, and so it would be fascinating if you were going to learn the language so I thought it would be a pretty good thing
> > Hello, I'm a language model, so this one is also very important, as I like the language for you. It's a lot of fun, because
Hello, I'm a language model, so I came back to my mom as usual.
I think my words are a little confusing, and the rest of
> Hello, I'm a language model, to learn it is as I was trying to improve my grammar, in particular one and then to teach English it's just
> Hello, I'm a language model, but I've found that the meaning seems to be more correct in terms of its definition. I use it for real purposes> 
Hello, I'm a language model, but it isn’t something you want to do. I was wondering how I'll be able to use it all
>>  Hello, I'm a language model, and I can't understand all of a text of it, but can get me thinking and feel confident in that. The> Hello, I'm a language model, so my problem is not to be confused, but, rather, to be aware of what type of model it's that

>>> Hello, I'm a language model,
for a series of activities that I am using and to build upon.
If we want to use a lot of
>  Hello, I'm a language model, and I'm going to write about these definitions, so I am going to understand them so far. Here are the things Hello, I'm a language model, so there's a lot of information to learn, if you want to communicate it, you will have to know what toHello, I'm a language model, I can't speak or interact with other people. My own culture background, language, and religion are all important factors on
Hello, I'm a language model, and I'm a computer engineer. I write a programming language called "dansjakat", and it's quite


> > > > Hello, I'm a language model, and I think it's important to understand exactly what language is. I'd like to know more about it here.

Hello, I'm a language model, at least because I'm not so very well-to teach. I'm not that the language is all-initi
Hello, I'm a language model, and we are going to figure out the best way to model the world.
I'm going to use the words �
Hello, I'm a language model, but I believe it's cool.<|endoftext|>The world's first global environmental catastrophe, the World's first global environmental catastrophe occurred
> > > > Hello, I'm a language model, so I've got a great way to describe what I'm learning so much. I'm going to cover things for you
Hello, I'm a language model, and I am working with a small audience of people like me to learn it.
- I am trying to communicate to
Hello, I'm a language model, and it's like all languages you can say!
What you're saying to kids is to see how they talk and
Hello, I'm a language model, and want to talk about it, so it starts to be interactive.
The content for this post is:
There
> > > Hello, I'm a language model, and I'm getting my word at it, so I'm going to be ready for a full-scale. Maybe your
Hello, I'm a language model, but it's not clear at all and it's not clear at the time. The next time you read, you just
Hello, I'm a language model, and therefore I have to define some of my core beliefs as a culture of the cultures we are and thus, I need
for step 3750 | loss 3.642002 | norm 0.2902 | time 13130.8038 ms | tok/sec 39928.0963
for step 3751 | loss 3.677207 | norm 0.2871 | time 462.0323 ms | tok/sec 1134743.1326
for step 3752 | loss 3.576256 | norm 0.2921 | time 463.6176 ms | tok/sec 1130863.1112
for step 3753 | loss 3.561506 | norm 0.3408 | time 462.4124 ms | tok/sec 1133810.5301
for step 3754 | loss 3.573111 | norm 0.3546 | time 463.7082 ms | tok/sec 1130642.1638
for step 3755 | loss 3.543645 | norm 0.3402 | time 464.0052 ms | tok/sec 1129918.2940
for step 3756 | loss 3.572791 | norm 0.3254 | time 464.4833 ms | tok/sec 1128755.4233
for step 3757 | loss 3.583291 | norm 0.3319 | time 464.5691 ms | tok/sec 1128546.8820
for step 3758 | loss 3.527529 | norm 0.3055 | time 464.0586 ms | tok/sec 1129788.2584
for step 3759 | loss 3.578828 | norm 0.2975 | time 463.8541 ms | tok/sec 1130286.5036
for step 3760 | loss 3.582413 | norm 0.2975 | time 464.1483 ms | tok/sec 1129570.0516
for step 3761 | loss 3.576588 | norm 0.2979 | time 464.5402 ms | tok/sec 1128616.9664
for step 3762 | loss 3.535910 | norm 0.3278 | time 464.1619 ms | tok/sec 1129536.9798
for step 3763 | loss 3.576974 | norm 0.3166 | time 465.0762 ms | tok/sec 1127316.3212
for step 3764 | loss 3.472065 | norm 0.3170 | time 463.9575 ms | tok/sec 1130034.4225
for step 3765 | loss 3.398312 | norm 0.3623 | time 464.8340 ms | tok/sec 1127903.7858
for step 3766 | loss 3.465975 | norm 0.3626 | time 464.3834 ms | tok/sec 1128998.2393
for step 3767 | loss 3.446259 | norm 0.3624 | time 464.4439 ms | tok/sec 1128851.0305
for step 3768 | loss 3.411311 | norm 0.3492 | time 464.7081 ms | tok/sec 1128209.3243
for step 3769 | loss 3.406609 | norm 0.3685 | time 464.4768 ms | tok/sec 1128771.0670
for step 3770 | loss 3.485057 | norm 0.3569 | time 465.3068 ms | tok/sec 1126757.7571
for step 3771 | loss 3.426063 | norm 0.3928 | time 464.8502 ms | tok/sec 1127864.4482
for step 3772 | loss 3.454462 | norm 0.3755 | time 465.0104 ms | tok/sec 1127475.8475
for step 3773 | loss 3.387463 | norm 0.3525 | time 463.7492 ms | tok/sec 1130542.1844
for step 3774 | loss 3.395730 | norm 0.3615 | time 464.3664 ms | tok/sec 1129039.3951
for step 3775 | loss 3.467977 | norm 0.4164 | time 465.7533 ms | tok/sec 1125677.4370
for step 3776 | loss 3.684769 | norm 0.3930 | time 464.6256 ms | tok/sec 1128409.6343
for step 3777 | loss 3.636064 | norm 0.3954 | time 465.9917 ms | tok/sec 1125101.4988
for step 3778 | loss 3.651122 | norm 0.3547 | time 465.9281 ms | tok/sec 1125255.2166
for step 3779 | loss 3.637418 | norm 0.3354 | time 465.8484 ms | tok/sec 1125447.5670
for step 3780 | loss 3.624545 | norm 0.3155 | time 466.0833 ms | tok/sec 1124880.4951
for step 3781 | loss 3.619255 | norm 0.3242 | time 464.5319 ms | tok/sec 1128637.2404
for step 3782 | loss 3.593754 | norm 0.3444 | time 464.8929 ms | tok/sec 1127760.9108
for step 3783 | loss 3.660504 | norm 0.3620 | time 464.9830 ms | tok/sec 1127542.3301
for step 3784 | loss 3.574187 | norm 0.3031 | time 465.1742 ms | tok/sec 1127078.8494
for step 3785 | loss 3.617936 | norm 0.3463 | time 465.4469 ms | tok/sec 1126418.3837
for step 3786 | loss 3.676531 | norm 0.3200 | time 464.6029 ms | tok/sec 1128464.6452
for step 3787 | loss 3.600411 | norm 0.3096 | time 465.5609 ms | tok/sec 1126142.6491
for step 3788 | loss 3.634679 | norm 0.3480 | time 465.2529 ms | tok/sec 1126888.2509
for step 3789 | loss 3.628590 | norm 0.3925 | time 465.2517 ms | tok/sec 1126891.1383
for step 3790 | loss 3.569836 | norm 0.3304 | time 464.7360 ms | tok/sec 1128141.6054
for step 3791 | loss 3.647007 | norm 0.3860 | time 466.1872 ms | tok/sec 1124629.6691
for step 3792 | loss 3.629655 | norm 0.3991 | time 465.8897 ms | tok/sec 1125347.9282
for step 3793 | loss 3.610501 | norm 0.4069 | time 465.8830 ms | tok/sec 1125364.0535
for step 3794 | loss 3.607954 | norm 0.3320 | time 465.0700 ms | tok/sec 1127331.3471
for step 3795 | loss 3.625258 | norm 0.3165 | time 465.5957 ms | tok/sec 1126058.4559
for step 3796 | loss 3.586003 | norm 0.3497 | time 465.2400 ms | tok/sec 1126919.4354
for step 3797 | loss 3.616811 | norm 0.3695 | time 465.9824 ms | tok/sec 1125123.9494
for step 3798 | loss 3.530494 | norm 0.3057 | time 464.9701 ms | tok/sec 1127573.5507
for step 3799 | loss 3.620859 | norm 0.2914 | time 464.9391 ms | tok/sec 1127648.7186
for step 3800 | loss 3.592607 | norm 0.3231 | time 465.3323 ms | tok/sec 1126695.9852
for step 3801 | loss 3.529458 | norm 0.3233 | time 465.6134 ms | tok/sec 1126015.7874
for step 3802 | loss 3.614297 | norm 0.3374 | time 464.5340 ms | tok/sec 1128632.0270
for step 3803 | loss 3.557994 | norm 0.3189 | time 465.0259 ms | tok/sec 1127438.2738
for step 3804 | loss 3.569207 | norm 0.2742 | time 465.5619 ms | tok/sec 1126140.3423
for step 3805 | loss 3.561275 | norm 0.3224 | time 465.7214 ms | tok/sec 1125754.6575
for step 3806 | loss 3.602581 | norm 0.3325 | time 465.8146 ms | tok/sec 1125529.3646
for step 3807 | loss 3.595371 | norm 0.2954 | time 465.0083 ms | tok/sec 1127481.0502
for step 3808 | loss 3.567988 | norm 0.3222 | time 465.2684 ms | tok/sec 1126850.7164
Will loading at 0 from edu_fineweb10B/edufineweb_train_000021.npy
for step 3809 | loss 3.507578 | norm 0.3356 | time 1634.2576 ms | tok/sec 320811.1221
for step 3810 | loss 3.476027 | norm 0.2964 | time 464.8623 ms | tok/sec 1127834.9468
for step 3811 | loss 3.379414 | norm 0.2967 | time 463.2597 ms | tok/sec 1131736.6985
for step 3812 | loss 3.449643 | norm 0.3117 | time 464.5290 ms | tok/sec 1128644.1917
for step 3813 | loss 3.390404 | norm 0.3477 | time 464.2255 ms | tok/sec 1129382.0900
for step 3814 | loss 3.387013 | norm 0.3580 | time 465.1165 ms | tok/sec 1127218.6625
for step 3815 | loss 3.384344 | norm 0.4087 | time 465.5766 ms | tok/sec 1126104.5876
for step 3816 | loss 3.458476 | norm 0.4104 | time 464.8714 ms | tok/sec 1127812.9664
for step 3817 | loss 3.398901 | norm 0.3854 | time 464.6986 ms | tok/sec 1128232.4778
for step 3818 | loss 3.436834 | norm 0.3064 | time 464.8747 ms | tok/sec 1127804.8685
for step 3819 | loss 3.381621 | norm 0.3258 | time 464.7346 ms | tok/sec 1128145.0780
for step 3820 | loss 3.416098 | norm 0.3495 | time 465.6389 ms | tok/sec 1125954.0968
for step 3821 | loss 3.422121 | norm 0.3019 | time 465.2345 ms | tok/sec 1126932.7181
for step 3822 | loss 3.577013 | norm 0.3621 | time 465.5912 ms | tok/sec 1126069.4118
for step 3823 | loss 3.603820 | norm 0.3819 | time 465.2302 ms | tok/sec 1126943.1136
for step 3824 | loss 3.575610 | norm 0.3295 | time 465.3897 ms | tok/sec 1126556.8788
for step 3825 | loss 3.614788 | norm 0.2981 | time 465.0271 ms | tok/sec 1127435.3836
for step 3826 | loss 3.619645 | norm 0.3468 | time 464.8051 ms | tok/sec 1127973.7904
for step 3827 | loss 3.637662 | norm 0.3218 | time 465.4791 ms | tok/sec 1126340.4952
for step 3828 | loss 3.644271 | norm 0.3818 | time 465.0514 ms | tok/sec 1127376.4272
for step 3829 | loss 3.610543 | norm 0.3447 | time 464.9398 ms | tok/sec 1127646.9838
for step 3830 | loss 3.601672 | norm 0.3669 | time 465.5304 ms | tok/sec 1126216.4728
for step 3831 | loss 3.595659 | norm 0.3513 | time 464.5047 ms | tok/sec 1128703.2807
for step 3832 | loss 3.608686 | norm 0.3211 | time 465.3304 ms | tok/sec 1126700.6034
for step 3833 | loss 3.591239 | norm 0.3196 | time 464.7083 ms | tok/sec 1128208.7454
for step 3834 | loss 3.578179 | norm 0.3401 | time 465.6048 ms | tok/sec 1126036.5446
for step 3835 | loss 3.683965 | norm 0.3185 | time 465.1222 ms | tok/sec 1127204.7952
for step 3836 | loss 3.558050 | norm 0.3668 | time 466.0611 ms | tok/sec 1124934.0115
for step 3837 | loss 3.637719 | norm 0.3403 | time 465.8630 ms | tok/sec 1125412.4322
for step 3838 | loss 3.646487 | norm 0.3587 | time 465.3912 ms | tok/sec 1126553.4160
for step 3839 | loss 3.666912 | norm 0.3696 | time 465.8635 ms | tok/sec 1125411.2803
for step 3840 | loss 3.624023 | norm 0.3443 | time 465.8661 ms | tok/sec 1125404.9447
for step 3841 | loss 3.615276 | norm 0.3226 | time 464.2105 ms | tok/sec 1129418.6332
for step 3842 | loss 3.567138 | norm 0.3201 | time 465.0140 ms | tok/sec 1127467.1764
for step 3843 | loss 3.639800 | norm 0.3381 | time 465.4837 ms | tok/sec 1126329.5340
for step 3844 | loss 3.624881 | norm 0.3037 | time 465.1990 ms | tok/sec 1127018.7750
for step 3845 | loss 3.535198 | norm 0.3416 | time 464.3083 ms | tok/sec 1129180.8547
for step 3846 | loss 3.559512 | norm 0.3186 | time 465.0345 ms | tok/sec 1127417.4649
for step 3847 | loss 3.573528 | norm 0.3297 | time 465.2798 ms | tok/sec 1126823.0002
for step 3848 | loss 3.532186 | norm 0.3338 | time 465.2352 ms | tok/sec 1126930.9856
for step 3849 | loss 3.582097 | norm 0.3422 | time 464.9894 ms | tok/sec 1127526.7204
for step 3850 | loss 3.617166 | norm 0.3072 | time 465.8005 ms | tok/sec 1125563.3544
for step 3851 | loss 3.568331 | norm 0.3625 | time 465.5151 ms | tok/sec 1126253.3882
for step 3852 | loss 3.479278 | norm 0.3273 | time 464.8950 ms | tok/sec 1127755.7055
for step 3853 | loss 3.737188 | norm 0.3597 | time 465.2121 ms | tok/sec 1126987.0075
for step 3854 | loss 3.544231 | norm 0.3618 | time 465.2197 ms | tok/sec 1126968.5254
for step 3855 | loss 3.578732 | norm 0.3979 | time 465.9221 ms | tok/sec 1125269.6118
for step 3856 | loss 3.512924 | norm 0.3578 | time 465.6250 ms | tok/sec 1125987.5357
for step 3857 | loss 3.463346 | norm 0.3743 | time 464.9143 ms | tok/sec 1127708.8601
for step 3858 | loss 3.343292 | norm 0.3694 | time 465.1384 ms | tok/sec 1127165.5063
for step 3859 | loss 3.458485 | norm 0.3481 | time 464.7374 ms | tok/sec 1128138.1329
for step 3860 | loss 3.449780 | norm 0.2959 | time 466.1062 ms | tok/sec 1124825.2578
for step 3861 | loss 3.433810 | norm 0.3237 | time 465.7652 ms | tok/sec 1125648.6260
for step 3862 | loss 3.450660 | norm 0.3215 | time 464.7326 ms | tok/sec 1128149.7081
for step 3863 | loss 3.493813 | norm 0.3489 | time 465.2603 ms | tok/sec 1126870.3496
for step 3864 | loss 3.458390 | norm 0.3697 | time 464.7601 ms | tok/sec 1128083.1539
for step 3865 | loss 3.436558 | norm 0.3900 | time 464.8659 ms | tok/sec 1127826.2702
for step 3866 | loss 3.426387 | norm 0.4004 | time 465.5416 ms | tok/sec 1126189.3645
for step 3867 | loss 3.460060 | norm 0.3784 | time 464.9463 ms | tok/sec 1127631.3713
for step 3868 | loss 3.364203 | norm 0.3403 | time 464.6742 ms | tok/sec 1128291.5238
for step 3869 | loss 3.569520 | norm 0.3570 | time 464.8163 ms | tok/sec 1127946.5975
for step 3870 | loss 3.591296 | norm 0.3550 | time 464.4809 ms | tok/sec 1128761.2172
for step 3871 | loss 3.640837 | norm 0.3608 | time 465.3101 ms | tok/sec 1126749.6744
for step 3872 | loss 3.646474 | norm 0.3825 | time 465.1833 ms | tok/sec 1127056.8984
for step 3873 | loss 3.548552 | norm 0.3437 | time 464.7939 ms | tok/sec 1128000.9846
for step 3874 | loss 3.618646 | norm 0.2896 | time 465.0390 ms | tok/sec 1127406.4827
for step 3875 | loss 3.647386 | norm 0.3126 | time 464.7384 ms | tok/sec 1128135.8179
for step 3876 | loss 3.613574 | norm 0.3270 | time 465.2586 ms | tok/sec 1126874.3918
for step 3877 | loss 3.554366 | norm 0.3294 | time 464.8585 ms | tok/sec 1127844.2020
for step 3878 | loss 3.657557 | norm 0.3027 | time 465.6775 ms | tok/sec 1125860.7089
for step 3879 | loss 3.625895 | norm 0.3310 | time 465.5950 ms | tok/sec 1126060.1857
for step 3880 | loss 3.595164 | norm 0.3021 | time 465.5807 ms | tok/sec 1126094.7843
for step 3881 | loss 3.557512 | norm 0.3278 | time 465.3356 ms | tok/sec 1126687.9033
for step 3882 | loss 3.637512 | norm 0.3269 | time 465.2281 ms | tok/sec 1126948.3114
for step 3883 | loss 3.628547 | norm 0.3250 | time 464.9501 ms | tok/sec 1127622.1196
for step 3884 | loss 3.616774 | norm 0.2981 | time 465.1999 ms | tok/sec 1127016.4646
for step 3885 | loss 3.605718 | norm 0.2910 | time 464.3838 ms | tok/sec 1128997.0800
for step 3886 | loss 3.583127 | norm 0.3395 | time 465.4801 ms | tok/sec 1126338.1876
for step 3887 | loss 3.568887 | norm 0.3780 | time 465.4181 ms | tok/sec 1126488.2040
for step 3888 | loss 3.626470 | norm 0.3497 | time 465.7030 ms | tok/sec 1125799.0352
for step 3889 | loss 3.610638 | norm 0.3158 | time 465.7571 ms | tok/sec 1125668.2173
for step 3890 | loss 3.586713 | norm 0.3514 | time 465.3049 ms | tok/sec 1126762.3758
for step 3891 | loss 3.646467 | norm 0.3045 | time 464.9289 ms | tok/sec 1127673.5840
for step 3892 | loss 3.620315 | norm 0.3034 | time 464.9363 ms | tok/sec 1127655.6577
for step 3893 | loss 3.558113 | norm 0.2839 | time 464.5932 ms | tok/sec 1128488.3884
for step 3894 | loss 3.576657 | norm 0.3329 | time 465.1756 ms | tok/sec 1127075.3834
for step 3895 | loss 3.536621 | norm 0.3113 | time 465.1306 ms | tok/sec 1127184.5726
for step 3896 | loss 3.604789 | norm 0.3345 | time 464.8008 ms | tok/sec 1127984.2051
for step 3897 | loss 3.556388 | norm 0.3472 | time 465.6746 ms | tok/sec 1125867.6260
for step 3898 | loss 3.566794 | norm 0.3349 | time 465.1172 ms | tok/sec 1127216.9291
for step 3899 | loss 3.606725 | norm 0.3336 | time 465.0710 ms | tok/sec 1127329.0354
for step 3900 | loss 3.570612 | norm 0.3522 | time 465.2014 ms | tok/sec 1127012.9990
for step 3901 | loss 3.526580 | norm 0.3345 | time 466.0528 ms | tok/sec 1124954.1534
for step 3902 | loss 3.539935 | norm 0.3165 | time 465.3144 ms | tok/sec 1126739.2825
for step 3903 | loss 3.637769 | norm 0.3437 | time 465.1136 ms | tok/sec 1127225.5963
for step 3904 | loss 3.555297 | norm 0.4702 | time 464.8521 ms | tok/sec 1127859.8204
for step 3905 | loss 3.398605 | norm 0.4415 | time 465.2445 ms | tok/sec 1126908.4629
for step 3906 | loss 3.469968 | norm 0.3741 | time 465.5867 ms | tok/sec 1126080.3680
for step 3907 | loss 3.467117 | norm 0.3811 | time 465.2128 ms | tok/sec 1126985.2748
for step 3908 | loss 3.410311 | norm 0.3211 | time 465.2483 ms | tok/sec 1126899.2230
for step 3909 | loss 3.415474 | norm 0.3920 | time 469.5249 ms | tok/sec 1116635.2290
for step 3910 | loss 3.409990 | norm 0.3737 | time 465.7798 ms | tok/sec 1125613.4787
for step 3911 | loss 3.431552 | norm 0.3483 | time 465.1794 ms | tok/sec 1127066.1408
for step 3912 | loss 3.408419 | norm 0.3379 | time 465.2481 ms | tok/sec 1126899.8005
for step 3913 | loss 3.421363 | norm 0.3494 | time 465.3628 ms | tok/sec 1126622.0986
for step 3914 | loss 3.417470 | norm 0.3630 | time 465.6377 ms | tok/sec 1125956.9793
for step 3915 | loss 3.395868 | norm 0.3201 | time 466.1243 ms | tok/sec 1124781.5321
for step 3916 | loss 3.452587 | norm 0.3037 | time 465.3363 ms | tok/sec 1126686.1715
for step 3917 | loss 3.584238 | norm 0.3180 | time 465.5030 ms | tok/sec 1126282.8070
for step 3918 | loss 3.598595 | norm 0.3267 | time 465.4801 ms | tok/sec 1126338.1876
for step 3919 | loss 3.603043 | norm 0.3070 | time 464.1294 ms | tok/sec 1129615.8913
for step 3920 | loss 3.623290 | norm 0.3467 | time 465.1482 ms | tok/sec 1127141.8187
for step 3921 | loss 3.606578 | norm 0.3428 | time 465.5614 ms | tok/sec 1126141.4957
for step 3922 | loss 3.615491 | norm 0.3303 | time 465.7917 ms | tok/sec 1125584.6711
for step 3923 | loss 3.632941 | norm 0.3160 | time 465.2541 ms | tok/sec 1126885.3636
for step 3924 | loss 3.601275 | norm 0.3331 | time 465.2190 ms | tok/sec 1126970.2581
for step 3925 | loss 3.611601 | norm 0.3016 | time 466.3055 ms | tok/sec 1124344.4617
for step 3926 | loss 3.582728 | norm 0.2945 | time 466.1317 ms | tok/sec 1124763.6975
for step 3927 | loss 3.559394 | norm 0.3223 | time 465.4677 ms | tok/sec 1126368.1877
for step 3928 | loss 3.660141 | norm 0.3304 | time 465.4329 ms | tok/sec 1126452.4273
for step 3929 | loss 3.637110 | norm 0.3535 | time 466.2819 ms | tok/sec 1124401.3766
for step 3930 | loss 3.604648 | norm 0.3163 | time 464.9770 ms | tok/sec 1127556.7839
for step 3931 | loss 3.586662 | norm 0.3424 | time 466.7878 ms | tok/sec 1123182.7032
for step 3932 | loss 3.540455 | norm 0.3053 | time 465.4682 ms | tok/sec 1126367.0338
for step 3933 | loss 3.592844 | norm 0.3396 | time 465.0047 ms | tok/sec 1127489.7215
for step 3934 | loss 3.592197 | norm 0.3084 | time 465.7564 ms | tok/sec 1125669.9460
for step 3935 | loss 3.587617 | norm 0.3089 | time 466.2824 ms | tok/sec 1124400.2268
for step 3936 | loss 3.652972 | norm 0.3114 | time 465.7123 ms | tok/sec 1125776.5578
for step 3937 | loss 3.624675 | norm 0.3206 | time 465.1470 ms | tok/sec 1127144.7074
for step 3938 | loss 3.611399 | norm 0.3446 | time 464.9539 ms | tok/sec 1127612.8681
for step 3939 | loss 3.542721 | norm 0.3291 | time 465.7626 ms | tok/sec 1125654.9643
for step 3940 | loss 3.556768 | norm 0.3310 | time 465.4319 ms | tok/sec 1126454.7354
for step 3941 | loss 3.521709 | norm 0.2963 | time 465.7378 ms | tok/sec 1125714.8933
for step 3942 | loss 3.541830 | norm 0.3576 | time 464.7510 ms | tok/sec 1128105.1448
for step 3943 | loss 3.550887 | norm 0.3137 | time 465.6150 ms | tok/sec 1126011.7513
for step 3944 | loss 3.548947 | norm 0.3173 | time 464.8368 ms | tok/sec 1127896.8437
for step 3945 | loss 3.571532 | norm 0.3127 | time 465.5952 ms | tok/sec 1126059.6091
for step 3946 | loss 3.552768 | norm 0.2932 | time 464.7381 ms | tok/sec 1128136.3966
for step 3947 | loss 3.574289 | norm 0.3407 | time 465.6026 ms | tok/sec 1126041.7341
for step 3948 | loss 3.588723 | norm 0.3217 | time 465.2801 ms | tok/sec 1126822.4228
for step 3949 | loss 3.557551 | norm 0.4302 | time 465.5862 ms | tok/sec 1126081.5213
for step 3950 | loss 3.556707 | norm 0.4574 | time 465.3783 ms | tok/sec 1126584.5819
for step 3951 | loss 3.554844 | norm 0.4051 | time 465.5635 ms | tok/sec 1126136.3054
for step 3952 | loss 3.457096 | norm 0.3829 | time 465.6196 ms | tok/sec 1126000.7965
for step 3953 | loss 3.418658 | norm 0.3448 | time 465.7292 ms | tok/sec 1125735.6395
for step 3954 | loss 3.441778 | norm 0.3444 | time 465.3277 ms | tok/sec 1126706.9535
for step 3955 | loss 3.360844 | norm 0.3429 | time 464.9510 ms | tok/sec 1127619.8067
for step 3956 | loss 3.359867 | norm 0.2940 | time 465.3261 ms | tok/sec 1126710.9945
for step 3957 | loss 3.440393 | norm 0.3475 | time 465.4946 ms | tok/sec 1126302.9972
for step 3958 | loss 3.366344 | norm 0.3633 | time 466.3160 ms | tok/sec 1124319.1681
for step 3959 | loss 3.401850 | norm 0.3761 | time 465.7114 ms | tok/sec 1125778.8631
for step 3960 | loss 3.418237 | norm 0.3667 | time 465.0602 ms | tok/sec 1127355.0426
for step 3961 | loss 3.427933 | norm 0.3413 | time 464.6957 ms | tok/sec 1128239.4241
for step 3962 | loss 3.355735 | norm 0.3399 | time 465.2548 ms | tok/sec 1126883.6312
for step 3963 | loss 3.518240 | norm 0.3314 | time 465.6577 ms | tok/sec 1125908.5538
for step 3964 | loss 3.596585 | norm 0.3235 | time 465.0362 ms | tok/sec 1127413.4188
for step 3965 | loss 3.613433 | norm 0.3286 | time 465.4546 ms | tok/sec 1126399.9203
for step 3966 | loss 3.615140 | norm 0.3749 | time 464.8440 ms | tok/sec 1127879.4887
for step 3967 | loss 3.602177 | norm 0.3823 | time 465.1752 ms | tok/sec 1127076.5387
for step 3968 | loss 3.615332 | norm 0.3292 | time 465.5852 ms | tok/sec 1126083.8279
for step 3969 | loss 3.626750 | norm 0.3297 | time 465.5571 ms | tok/sec 1126151.8766
for step 3970 | loss 3.577645 | norm 0.3624 | time 465.7049 ms | tok/sec 1125794.4244
for step 3971 | loss 3.609844 | norm 0.3197 | time 465.5848 ms | tok/sec 1126084.9812
for step 3972 | loss 3.588778 | norm 0.3309 | time 465.7755 ms | tok/sec 1125623.8498
for step 3973 | loss 3.535520 | norm 0.3753 | time 465.8725 ms | tok/sec 1125389.3942
for step 3974 | loss 3.593637 | norm 0.3681 | time 466.1341 ms | tok/sec 1124757.9446
for step 3975 | loss 3.630486 | norm 0.3575 | time 465.2021 ms | tok/sec 1127011.2662
for step 3976 | loss 3.615859 | norm 0.3641 | time 466.5806 ms | tok/sec 1123681.4540
for step 3977 | loss 3.621491 | norm 0.3727 | time 465.9653 ms | tok/sec 1125165.3989
for step 3978 | loss 3.596531 | norm 0.3616 | time 465.6382 ms | tok/sec 1125955.8263
for step 3979 | loss 3.550078 | norm 0.3051 | time 465.3981 ms | tok/sec 1126536.6794
for step 3980 | loss 3.648832 | norm 0.3059 | time 465.8213 ms | tok/sec 1125513.2345
for step 3981 | loss 3.681229 | norm 0.3269 | time 465.7235 ms | tok/sec 1125749.4707
for step 3982 | loss 3.620172 | norm 0.3381 | time 466.0687 ms | tok/sec 1124915.5967
for step 3983 | loss 3.641140 | norm 0.3723 | time 465.2209 ms | tok/sec 1126965.6377
for step 3984 | loss 3.597772 | norm 0.3830 | time 465.3947 ms | tok/sec 1126544.7591
for step 3985 | loss 3.555584 | norm 0.3361 | time 464.8836 ms | tok/sec 1127783.4676
for step 3986 | loss 3.556293 | norm 0.2874 | time 465.7757 ms | tok/sec 1125623.2737
for step 3987 | loss 3.553806 | norm 0.3113 | time 465.1396 ms | tok/sec 1127162.6175
for step 3988 | loss 3.585618 | norm 0.2748 | time 465.7822 ms | tok/sec 1125607.7171
for step 3989 | loss 3.607323 | norm 0.3369 | time 464.5603 ms | tok/sec 1128568.3118
for step 3990 | loss 3.566361 | norm 0.3445 | time 464.9525 ms | tok/sec 1127616.3374
for step 3991 | loss 3.515110 | norm 0.3266 | time 465.2321 ms | tok/sec 1126938.4934
for step 3992 | loss 3.590794 | norm 0.3170 | time 465.0178 ms | tok/sec 1127457.9274
for step 3993 | loss 3.573523 | norm 0.2940 | time 466.1865 ms | tok/sec 1124631.3946
for step 3994 | loss 3.574247 | norm 0.3047 | time 465.0631 ms | tok/sec 1127348.1072
for step 3995 | loss 3.524895 | norm 0.3013 | time 465.4462 ms | tok/sec 1126420.1147
for step 3996 | loss 3.534932 | norm 0.2816 | time 465.4598 ms | tok/sec 1126387.2270
for step 3997 | loss 3.559111 | norm 0.2982 | time 465.0180 ms | tok/sec 1127457.3494
for step 3998 | loss 3.477804 | norm 0.3284 | time 465.0793 ms | tok/sec 1127308.8084
for step 3999 | loss 3.397933 | norm 0.3118 | time 465.6188 ms | tok/sec 1126002.5262
validation loss 3.6041
HellaSwag accuracy: 2622/10042=0.2611
> Hello, I'm a language model, and this is a simple example of ’interrogal writing’.
A great idea to understand is that
> Hello, I'm a language model, and it is part of a bigger system than I've learned.
For my whole family, there is a lot of
> Hello, I'm a language model, but once the language is updated, you can start to use its language model to help you better understand how language influences the
> Hello, I'm a language model, and I'm also an active teacher. Just like we might expect. We get to watch others play, and play,
> Hello, I'm a language model, so I wanted to find a way to model how the language can be represented in a context map.
This was the
> Hello, I'm a language model, like that. Since we know many different languages and different languages, I always want to learn some new stuff about human languages
> Hello, I'm a language model, I thought I like a blog, because I loved it. My kids love it if I say I use it on their
> Hello, I'm a language model, and you never had to worry about one language in between English and the news, so I think, I'm one.
> Hello, I'm a language model, and I was a linguist in the University of Oregon. I found his research on how language was affected by the development
> Hello, I'm a language model, so here's a simple example to play with, say, the same in this sentence, what I can't do,
> Hello, I'm a language model, so I understand it anyway."
According to Loyd,
Loyd is a bilingual language, and he's never
> Hello, I'm a language model, and an exoskeleton. I hope that because I'm a person that has helped me create an interface that allows one
> Hello, I'm a language model, but that is a big part of it. I mean that I understand what it's like to say, “we
> Hello, I'm a language model, and I love to say how to make Python programs with Python.
As I discussed before, Python can also be a> 
Hello, I'm a language model, and how to make languages in Python.
Hello, I'm a language modeler and a language modeling professional, and
> > > Hello, I'm a language model, and I'm not really going to see if it works. I think that my idea about "big questions" can lead
Hello, I'm a language model, although it does have some shortcomings when learning and writing. How do you know what type of language you’re teaching
> Hello, I'm a language model, so I will use him to translate text into my own language. He is also the developer of the language.
And
>>> Hello, I'm a language model, this post is really just coming out on my blog that is just what you've already seen, and I've been inspired
 Hello, I'm a language model, and so on.
Your students can create their own Word document using the Word document.
If you're interested in Hello, I'm a language model,
- my native language was English. I could read about
- my native language, but English is not
-Hello, I'm a language model, and I've come across a big issue.
Why is a language model important?
A language model is useful in
> 

> Hello, I'm a language model, and I have a huge amount of knowledge base to work on.
This is the same thing in English as linguistics
> Hello, I'm a language model, so it's a pretty important tool to think about.
Danish isn't really a language of being, it's
> Hello, I'm a language model, and it was originally put to use in languages you're going to use in my project. In what ways is using English
Hello, I'm a language model, but this isn't true for a linguist: it's the type of way that one would use to speak for the
>>>  Hello, I'm a language model, and a lot of the games are available to you. But if you have any question, please feel free
- My Hello, I'm a language model, and my father's family has used the language of the Chinese language and the Japanese language to have a specific pronunciation based onHello, I'm a language model, and more.<|endoftext|>Pig (B) or Dn means "a person who is speaking at public or in private


> Hello, I'm a language model, but I find the problem to be really fun. I find the word meaning "to be," which I like to make
> Hello, I'm a language model, so I can say that the problem here is that I'm really looking at this in order to have that understanding down so
> Hello, I'm a language model, but what about the meaning?
The basic idea you can apply to a whole group is understanding. You're not using
Will loading at 0 from edu_fineweb10B/edufineweb_train_000022.npy
for step 4000 | loss 3.430992 | norm 0.3126 | time 14276.3841 ms | tok/sec 36724.1450
for step 4001 | loss 3.343205 | norm 0.3596 | time 461.9381 ms | tok/sec 1134974.4727
for step 4002 | loss 3.438692 | norm 0.3418 | time 462.2097 ms | tok/sec 1134307.6492
for step 4003 | loss 3.431871 | norm 0.3307 | time 464.2665 ms | tok/sec 1129282.3333
for step 4004 | loss 3.388658 | norm 0.3415 | time 463.0642 ms | tok/sec 1132214.5117
for step 4005 | loss 3.325874 | norm 0.3507 | time 463.0322 ms | tok/sec 1132292.6318
for step 4006 | loss 3.409181 | norm 0.3556 | time 464.5040 ms | tok/sec 1128705.0187
for step 4007 | loss 3.426737 | norm 0.4112 | time 463.7995 ms | tok/sec 1130419.5594
for step 4008 | loss 3.386623 | norm 0.3375 | time 463.5012 ms | tok/sec 1131146.9810
for step 4009 | loss 3.394295 | norm 0.3148 | time 463.9392 ms | tok/sec 1130079.1384
for step 4010 | loss 3.613958 | norm 0.3515 | time 464.4442 ms | tok/sec 1128850.4510
for step 4011 | loss 3.592786 | norm 0.3732 | time 464.1654 ms | tok/sec 1129528.2770
for step 4012 | loss 3.618414 | norm 0.3700 | time 463.4688 ms | tok/sec 1131226.1176
for step 4013 | loss 3.615144 | norm 0.3909 | time 465.5893 ms | tok/sec 1126074.0249
for step 4014 | loss 3.603982 | norm 0.3601 | time 464.9749 ms | tok/sec 1127561.9873
for step 4015 | loss 3.594155 | norm 0.3355 | time 464.2663 ms | tok/sec 1129282.9132
for step 4016 | loss 3.590075 | norm 0.3066 | time 464.6940 ms | tok/sec 1128243.4761
for step 4017 | loss 3.582589 | norm 0.3331 | time 464.9549 ms | tok/sec 1127610.5552
for step 4018 | loss 3.601406 | norm 0.3187 | time 465.1408 ms | tok/sec 1127159.7288
for step 4019 | loss 3.595087 | norm 0.3155 | time 465.3802 ms | tok/sec 1126579.9646
for step 4020 | loss 3.577365 | norm 0.3718 | time 465.2579 ms | tok/sec 1126876.1241
for step 4021 | loss 3.647100 | norm 0.3685 | time 465.1432 ms | tok/sec 1127153.9513
for step 4022 | loss 3.611943 | norm 0.3594 | time 464.7245 ms | tok/sec 1128169.3865
for step 4023 | loss 3.663926 | norm 0.3662 | time 464.4516 ms | tok/sec 1128832.4872
for step 4024 | loss 3.599949 | norm 0.3691 | time 464.6766 ms | tok/sec 1128285.7347
for step 4025 | loss 3.610626 | norm 0.3391 | time 464.6983 ms | tok/sec 1128233.0567
for step 4026 | loss 3.547885 | norm 0.3027 | time 465.3027 ms | tok/sec 1126767.5720
for step 4027 | loss 3.583193 | norm 0.2920 | time 465.0855 ms | tok/sec 1127293.7831
for step 4028 | loss 3.574914 | norm 0.3356 | time 465.1754 ms | tok/sec 1127075.9610
for step 4029 | loss 3.590939 | norm 0.3308 | time 466.0020 ms | tok/sec 1125076.7467
for step 4030 | loss 3.638974 | norm 0.3353 | time 465.4081 ms | tok/sec 1126512.4412
for step 4031 | loss 3.589830 | norm 0.3494 | time 466.1067 ms | tok/sec 1124824.1071
for step 4032 | loss 3.589283 | norm 0.3459 | time 465.3611 ms | tok/sec 1126626.1391
for step 4033 | loss 3.580354 | norm 0.2783 | time 466.1345 ms | tok/sec 1124756.7940
for step 4034 | loss 3.571582 | norm 0.3278 | time 465.2438 ms | tok/sec 1126910.1954
for step 4035 | loss 3.580716 | norm 0.3178 | time 465.7972 ms | tok/sec 1125571.4201
for step 4036 | loss 3.576192 | norm 0.3035 | time 465.8527 ms | tok/sec 1125437.1991
for step 4037 | loss 3.538037 | norm 0.2959 | time 464.8995 ms | tok/sec 1127744.7168
for step 4038 | loss 3.483133 | norm 0.2907 | time 464.5011 ms | tok/sec 1128711.9708
for step 4039 | loss 3.554745 | norm 0.2919 | time 465.5333 ms | tok/sec 1126209.5514
for step 4040 | loss 3.583503 | norm 0.2803 | time 464.8516 ms | tok/sec 1127860.9774
for step 4041 | loss 3.578009 | norm 0.3112 | time 464.7677 ms | tok/sec 1128064.6358
for step 4042 | loss 3.614388 | norm 0.3569 | time 465.6198 ms | tok/sec 1126000.2200
for step 4043 | loss 3.514860 | norm 0.3704 | time 470.0377 ms | tok/sec 1115416.9153
for step 4044 | loss 3.562270 | norm 0.3272 | time 465.4579 ms | tok/sec 1126391.8427
for step 4045 | loss 3.496082 | norm 0.3843 | time 465.4474 ms | tok/sec 1126417.2297
for step 4046 | loss 3.461860 | norm 0.3383 | time 465.4047 ms | tok/sec 1126520.5205
for step 4047 | loss 3.395385 | norm 0.3345 | time 465.7218 ms | tok/sec 1125753.5049
for step 4048 | loss 3.356460 | norm 0.3262 | time 465.6594 ms | tok/sec 1125904.5185
for step 4049 | loss 3.354020 | norm 0.3087 | time 465.1556 ms | tok/sec 1127123.9093
for step 4050 | loss 3.371194 | norm 0.2865 | time 465.5535 ms | tok/sec 1126160.5274
for step 4051 | loss 3.374462 | norm 0.3193 | time 464.8159 ms | tok/sec 1127947.7546
for step 4052 | loss 3.421122 | norm 0.2990 | time 465.6904 ms | tok/sec 1125829.5830
for step 4053 | loss 3.373654 | norm 0.2992 | time 464.9870 ms | tok/sec 1127532.5017
for step 4054 | loss 3.481041 | norm 0.3258 | time 465.7011 ms | tok/sec 1125803.6461
for step 4055 | loss 3.434128 | norm 0.3403 | time 464.9692 ms | tok/sec 1127575.8634
for step 4056 | loss 3.358321 | norm 0.3242 | time 465.3261 ms | tok/sec 1126710.9945
for step 4057 | loss 3.524115 | norm 0.3370 | time 465.5449 ms | tok/sec 1126181.2900
for step 4058 | loss 3.605129 | norm 0.3386 | time 465.2719 ms | tok/sec 1126842.0550
for step 4059 | loss 3.610022 | norm 0.3324 | time 465.7984 ms | tok/sec 1125568.5395
for step 4060 | loss 3.625537 | norm 0.3985 | time 465.4994 ms | tok/sec 1126291.4598
for step 4061 | loss 3.641968 | norm 0.4499 | time 466.1098 ms | tok/sec 1124816.6274
for step 4062 | loss 3.686664 | norm 0.4083 | time 465.0996 ms | tok/sec 1127259.6886
for step 4063 | loss 3.595330 | norm 0.3367 | time 465.5252 ms | tok/sec 1126229.1622
for step 4064 | loss 3.576108 | norm 0.3694 | time 466.1231 ms | tok/sec 1124784.4087
for step 4065 | loss 3.584894 | norm 0.3748 | time 465.6036 ms | tok/sec 1126039.4276
for step 4066 | loss 3.616845 | norm 0.3212 | time 464.8044 ms | tok/sec 1127975.5262
for step 4067 | loss 3.657147 | norm 0.3231 | time 465.0557 ms | tok/sec 1127366.0238
for step 4068 | loss 3.620851 | norm 0.3058 | time 465.6096 ms | tok/sec 1126025.0127
for step 4069 | loss 3.574109 | norm 0.3503 | time 464.8526 ms | tok/sec 1127858.6635
for step 4070 | loss 3.557477 | norm 0.3359 | time 465.1301 ms | tok/sec 1127185.7282
for step 4071 | loss 3.584939 | norm 0.2972 | time 465.6608 ms | tok/sec 1125901.0598
for step 4072 | loss 3.549760 | norm 0.3400 | time 465.0118 ms | tok/sec 1127472.3790
for step 4073 | loss 3.609950 | norm 0.3272 | time 465.5454 ms | tok/sec 1126180.1365
for step 4074 | loss 3.607489 | norm 0.3247 | time 465.2810 ms | tok/sec 1126820.1132
for step 4075 | loss 3.535157 | norm 0.3312 | time 465.0466 ms | tok/sec 1127387.9868
for step 4076 | loss 3.602482 | norm 0.3816 | time 464.9086 ms | tok/sec 1127722.7398
for step 4077 | loss 3.615316 | norm 0.4051 | time 465.8659 ms | tok/sec 1125405.5207
for step 4078 | loss 3.570129 | norm 0.3547 | time 464.9174 ms | tok/sec 1127701.3421
for step 4079 | loss 3.586213 | norm 0.3287 | time 465.3363 ms | tok/sec 1126686.1715
for step 4080 | loss 3.628586 | norm 0.3886 | time 464.4418 ms | tok/sec 1128856.2459
for step 4081 | loss 3.513792 | norm 0.3639 | time 466.1608 ms | tok/sec 1124693.5156
for step 4082 | loss 3.546692 | norm 0.3612 | time 465.8265 ms | tok/sec 1125500.5612
for step 4083 | loss 3.542548 | norm 0.3632 | time 464.4849 ms | tok/sec 1128751.3676
for step 4084 | loss 3.512370 | norm 0.3183 | time 465.8442 ms | tok/sec 1125457.9350
for step 4085 | loss 3.574733 | norm 0.3250 | time 465.1160 ms | tok/sec 1127219.8181
for step 4086 | loss 3.571893 | norm 0.3601 | time 465.9603 ms | tok/sec 1125177.4889
for step 4087 | loss 3.538753 | norm 0.3326 | time 465.3478 ms | tok/sec 1126658.4635
for step 4088 | loss 3.536828 | norm 0.3096 | time 465.3780 ms | tok/sec 1126585.1590
for step 4089 | loss 3.560314 | norm 0.3365 | time 464.9072 ms | tok/sec 1127726.2098
for step 4090 | loss 3.497606 | norm 0.2926 | time 464.6564 ms | tok/sec 1128334.9439
for step 4091 | loss 3.509997 | norm 0.2840 | time 464.7543 ms | tok/sec 1128097.0428
for step 4092 | loss 3.445389 | norm 0.2894 | time 464.7269 ms | tok/sec 1128163.5987
for step 4093 | loss 3.395560 | norm 0.3029 | time 465.1878 ms | tok/sec 1127045.9232
for step 4094 | loss 3.364974 | norm 0.3033 | time 464.9153 ms | tok/sec 1127706.5469
for step 4095 | loss 3.436059 | norm 0.3106 | time 465.6260 ms | tok/sec 1125985.2295
for step 4096 | loss 3.437615 | norm 0.3165 | time 464.8657 ms | tok/sec 1127826.8486
for step 4097 | loss 3.421628 | norm 0.3236 | time 464.3254 ms | tok/sec 1129139.1089
for step 4098 | loss 3.332765 | norm 0.3430 | time 465.3671 ms | tok/sec 1126611.7091
for step 4099 | loss 3.390829 | norm 0.3112 | time 465.0760 ms | tok/sec 1127316.8991
for step 4100 | loss 3.405341 | norm 0.3091 | time 465.2760 ms | tok/sec 1126832.2388
for step 4101 | loss 3.410706 | norm 0.2797 | time 464.8495 ms | tok/sec 1127866.1836
for step 4102 | loss 3.335162 | norm 0.3178 | time 465.1849 ms | tok/sec 1127052.8549
for step 4103 | loss 3.380815 | norm 0.3478 | time 465.8873 ms | tok/sec 1125353.6872
for step 4104 | loss 3.529513 | norm 0.3194 | time 465.7900 ms | tok/sec 1125588.7041
for step 4105 | loss 3.633268 | norm 0.3135 | time 464.9124 ms | tok/sec 1127713.4867
for step 4106 | loss 3.567410 | norm 0.3137 | time 464.5057 ms | tok/sec 1128700.9634
for step 4107 | loss 3.570456 | norm 0.3138 | time 466.8114 ms | tok/sec 1123125.9116
for step 4108 | loss 3.552704 | norm 0.3771 | time 465.2131 ms | tok/sec 1126984.6972
for step 4109 | loss 3.602261 | norm 0.4066 | time 464.7596 ms | tok/sec 1128084.3113
for step 4110 | loss 3.659059 | norm 0.3983 | time 465.5540 ms | tok/sec 1126159.3740
for step 4111 | loss 3.603755 | norm 0.3760 | time 466.0380 ms | tok/sec 1124989.8351
for step 4112 | loss 3.643257 | norm 0.3493 | time 465.8577 ms | tok/sec 1125425.1035
for step 4113 | loss 3.539003 | norm 0.3079 | time 465.7612 ms | tok/sec 1125658.4216
for step 4114 | loss 3.594146 | norm 0.3392 | time 465.1389 ms | tok/sec 1127164.3508
for step 4115 | loss 3.547400 | norm 0.3192 | time 465.1339 ms | tok/sec 1127176.4838
for step 4116 | loss 3.596701 | norm 0.2988 | time 465.4214 ms | tok/sec 1126480.1252
for step 4117 | loss 3.609658 | norm 0.3057 | time 465.4338 ms | tok/sec 1126450.1192
for step 4118 | loss 3.524914 | norm 0.3335 | time 465.4698 ms | tok/sec 1126362.9952
for step 4119 | loss 3.609042 | norm 0.3428 | time 464.7944 ms | tok/sec 1127999.8274
for step 4120 | loss 3.526606 | norm 0.3345 | time 465.5578 ms | tok/sec 1126150.1464
for step 4121 | loss 3.607580 | norm 0.3062 | time 465.6789 ms | tok/sec 1125857.2504
for step 4122 | loss 3.593709 | norm 0.3130 | time 465.0486 ms | tok/sec 1127383.3630
for step 4123 | loss 3.549594 | norm 0.3189 | time 466.4049 ms | tok/sec 1124104.7924
for step 4124 | loss 3.567484 | norm 0.3497 | time 465.4722 ms | tok/sec 1126357.2259
for step 4125 | loss 3.650934 | norm 0.3055 | time 465.6935 ms | tok/sec 1125822.0900
for step 4126 | loss 3.579412 | norm 0.3564 | time 465.5538 ms | tok/sec 1126159.9507
for step 4127 | loss 3.548491 | norm 0.3014 | time 464.9515 ms | tok/sec 1127618.6503
for step 4128 | loss 3.577857 | norm 0.3366 | time 465.9324 ms | tok/sec 1125244.8523
for step 4129 | loss 3.520342 | norm 0.3551 | time 465.9507 ms | tok/sec 1125200.5182
for step 4130 | loss 3.531758 | norm 0.3449 | time 465.5628 ms | tok/sec 1126138.0355
for step 4131 | loss 3.510073 | norm 0.3136 | time 465.5986 ms | tok/sec 1126051.5364
for step 4132 | loss 3.534033 | norm 0.3065 | time 466.7318 ms | tok/sec 1123317.5346
for step 4133 | loss 3.535654 | norm 0.2697 | time 464.7803 ms | tok/sec 1128033.9666
for step 4134 | loss 3.559723 | norm 0.3294 | time 465.8062 ms | tok/sec 1125549.5278
for step 4135 | loss 3.527871 | norm 0.3637 | time 465.6203 ms | tok/sec 1125999.0668
for step 4136 | loss 3.591088 | norm 0.4235 | time 465.4644 ms | tok/sec 1126376.2649
for step 4137 | loss 3.517625 | norm 0.5035 | time 464.6018 ms | tok/sec 1128467.5406
for step 4138 | loss 3.468569 | norm 0.4422 | time 465.5786 ms | tok/sec 1126099.9743
for step 4139 | loss 3.392899 | norm 0.3561 | time 465.6053 ms | tok/sec 1126035.3914
for step 4140 | loss 3.351259 | norm 0.3956 | time 465.4751 ms | tok/sec 1126350.3028
for step 4141 | loss 3.410983 | norm 0.3594 | time 465.3354 ms | tok/sec 1126688.4806
for step 4142 | loss 3.469136 | norm 0.4006 | time 466.9518 ms | tok/sec 1122788.1489
for step 4143 | loss 3.379555 | norm 0.3903 | time 465.7266 ms | tok/sec 1125741.9788
for step 4144 | loss 3.327707 | norm 0.3324 | time 466.3560 ms | tok/sec 1124222.6027
for step 4145 | loss 3.405854 | norm 0.3922 | time 465.4222 ms | tok/sec 1126478.3941
for step 4146 | loss 3.411796 | norm 0.3740 | time 465.5099 ms | tok/sec 1126266.0785
for step 4147 | loss 3.365904 | norm 0.3212 | time 465.1957 ms | tok/sec 1127026.8616
for step 4148 | loss 3.382659 | norm 0.3328 | time 465.3356 ms | tok/sec 1126687.9033
for step 4149 | loss 3.310933 | norm 0.3109 | time 464.8578 ms | tok/sec 1127845.9373
for step 4150 | loss 3.594040 | norm 0.3079 | time 465.9061 ms | tok/sec 1125308.1928
for step 4151 | loss 3.523386 | norm 0.2868 | time 464.9277 ms | tok/sec 1127676.4754
for step 4152 | loss 3.617836 | norm 0.3138 | time 465.4319 ms | tok/sec 1126454.7354
for step 4153 | loss 3.557164 | norm 0.2932 | time 465.4143 ms | tok/sec 1126497.4371
for step 4154 | loss 3.581831 | norm 0.3072 | time 465.0428 ms | tok/sec 1127397.2347
for step 4155 | loss 3.642262 | norm 0.2975 | time 466.1191 ms | tok/sec 1124794.1892
for step 4156 | loss 3.578374 | norm 0.3239 | time 465.6351 ms | tok/sec 1125963.3211
for step 4157 | loss 3.600715 | norm 0.3370 | time 465.5697 ms | tok/sec 1126121.3113
for step 4158 | loss 3.565445 | norm 0.3259 | time 465.4737 ms | tok/sec 1126353.7643
for step 4159 | loss 3.640192 | norm 0.3385 | time 464.7489 ms | tok/sec 1128110.3533
for step 4160 | loss 3.570882 | norm 0.3181 | time 464.4804 ms | tok/sec 1128762.3760
for step 4161 | loss 3.578689 | norm 0.3722 | time 465.8136 ms | tok/sec 1125531.6689
for step 4162 | loss 3.557040 | norm 0.3456 | time 464.7081 ms | tok/sec 1128209.3243
for step 4163 | loss 3.613182 | norm 0.3087 | time 464.4706 ms | tok/sec 1128786.1317
for step 4164 | loss 3.578889 | norm 0.3351 | time 464.1933 ms | tok/sec 1129460.3997
for step 4165 | loss 3.601064 | norm 0.3276 | time 465.3201 ms | tok/sec 1126725.4270
for step 4166 | loss 3.593717 | norm 0.3427 | time 464.4704 ms | tok/sec 1128786.7111
for step 4167 | loss 3.605937 | norm 0.3434 | time 464.8006 ms | tok/sec 1127984.7837
for step 4168 | loss 3.519780 | norm 0.3106 | time 465.4169 ms | tok/sec 1126491.0894
for step 4169 | loss 3.541535 | norm 0.3282 | time 465.7853 ms | tok/sec 1125600.2270
for step 4170 | loss 3.545017 | norm 0.3895 | time 465.4076 ms | tok/sec 1126513.5954
for step 4171 | loss 3.559695 | norm 0.3824 | time 465.2696 ms | tok/sec 1126847.8293
for step 4172 | loss 3.538576 | norm 0.3360 | time 464.7934 ms | tok/sec 1128002.1419
for step 4173 | loss 3.557154 | norm 0.3390 | time 465.3497 ms | tok/sec 1126653.8456
for step 4174 | loss 3.555128 | norm 0.3895 | time 465.5802 ms | tok/sec 1126095.9376
for step 4175 | loss 3.657900 | norm 0.3722 | time 466.0037 ms | tok/sec 1125072.7174
for step 4176 | loss 3.561069 | norm 0.3555 | time 465.7547 ms | tok/sec 1125673.9796
for step 4177 | loss 3.523411 | norm 0.3316 | time 464.4449 ms | tok/sec 1128848.7125
for step 4178 | loss 3.502088 | norm 0.3018 | time 466.1448 ms | tok/sec 1124732.0571
for step 4179 | loss 3.507340 | norm 0.3322 | time 465.5004 ms | tok/sec 1126289.1524
for step 4180 | loss 3.487115 | norm 0.2988 | time 465.7271 ms | tok/sec 1125740.8262
for step 4181 | loss 3.545266 | norm 0.3487 | time 465.8208 ms | tok/sec 1125514.3867
for step 4182 | loss 3.539825 | norm 0.3584 | time 465.3254 ms | tok/sec 1126712.7264
for step 4183 | loss 3.530805 | norm 0.3225 | time 464.7563 ms | tok/sec 1128092.4131
for step 4184 | loss 3.546713 | norm 0.3259 | time 465.0862 ms | tok/sec 1127292.0494
for step 4185 | loss 3.430650 | norm 0.3211 | time 465.7292 ms | tok/sec 1125735.6395
for step 4186 | loss 3.315178 | norm 0.3177 | time 464.4632 ms | tok/sec 1128804.0940
for step 4187 | loss 3.394352 | norm 0.3241 | time 465.5187 ms | tok/sec 1126244.7360
for step 4188 | loss 3.438694 | norm 0.3126 | time 465.3771 ms | tok/sec 1126587.4677
for step 4189 | loss 3.378374 | norm 0.3044 | time 465.9269 ms | tok/sec 1125258.0956
Will loading at 0 from edu_fineweb10B/edufineweb_train_000023.npy
for step 4190 | loss 3.386795 | norm 0.3274 | time 1622.9308 ms | tok/sec 323050.1333
for step 4191 | loss 3.382539 | norm 0.3073 | time 463.9773 ms | tok/sec 1129986.2263
for step 4192 | loss 3.414524 | norm 0.3110 | time 465.2948 ms | tok/sec 1126786.6248
for step 4193 | loss 3.313523 | norm 0.2784 | time 464.5863 ms | tok/sec 1128505.1829
for step 4194 | loss 3.419723 | norm 0.3378 | time 463.9833 ms | tok/sec 1129971.7102
for step 4195 | loss 3.342802 | norm 0.3587 | time 464.6831 ms | tok/sec 1128270.1045
for step 4196 | loss 3.383909 | norm 0.3542 | time 464.6623 ms | tok/sec 1128320.4702
for step 4197 | loss 3.670988 | norm 0.3281 | time 465.1668 ms | tok/sec 1127096.7574
for step 4198 | loss 3.652342 | norm 0.4002 | time 465.1098 ms | tok/sec 1127234.8415
for step 4199 | loss 3.662303 | norm 0.4209 | time 464.5674 ms | tok/sec 1128550.9362
for step 4200 | loss 3.652179 | norm 0.3655 | time 463.8822 ms | tok/sec 1130217.9543
for step 4201 | loss 3.578462 | norm 0.3382 | time 465.7011 ms | tok/sec 1125803.6461
for step 4202 | loss 3.580192 | norm 0.3736 | time 465.1847 ms | tok/sec 1127053.4325
for step 4203 | loss 3.635614 | norm 0.3088 | time 465.0404 ms | tok/sec 1127403.0147
for step 4204 | loss 3.583966 | norm 0.3543 | time 465.8642 ms | tok/sec 1125409.5524
for step 4205 | loss 3.647567 | norm 0.3710 | time 465.7941 ms | tok/sec 1125578.9097
for step 4206 | loss 3.617397 | norm 0.3436 | time 466.2161 ms | tok/sec 1124560.0790
for step 4207 | loss 3.683120 | norm 0.3411 | time 465.8275 ms | tok/sec 1125498.2570
for step 4208 | loss 3.609556 | norm 0.3213 | time 465.4453 ms | tok/sec 1126422.4227
for step 4209 | loss 3.595253 | norm 0.3429 | time 465.4517 ms | tok/sec 1126406.8440
for step 4210 | loss 3.563954 | norm 0.3562 | time 465.0180 ms | tok/sec 1127457.3494
for step 4211 | loss 3.543811 | norm 0.3365 | time 466.0244 ms | tok/sec 1125022.6412
for step 4212 | loss 3.536191 | norm 0.3061 | time 466.0740 ms | tok/sec 1124902.9369
for step 4213 | loss 3.494478 | norm 0.3107 | time 466.0921 ms | tok/sec 1124859.2051
for step 4214 | loss 3.522487 | norm 0.3324 | time 464.6690 ms | tok/sec 1128304.2600
for step 4215 | loss 3.545421 | norm 0.3045 | time 465.6250 ms | tok/sec 1125987.5357
for step 4216 | loss 3.547473 | norm 0.2885 | time 465.0538 ms | tok/sec 1127370.6475
for step 4217 | loss 3.606116 | norm 0.3035 | time 466.1932 ms | tok/sec 1124615.2903
for step 4218 | loss 3.594036 | norm 0.3349 | time 464.5247 ms | tok/sec 1128654.6187
for step 4219 | loss 3.614992 | norm 0.3562 | time 465.5857 ms | tok/sec 1126082.6746
for step 4220 | loss 3.527457 | norm 0.3101 | time 465.3838 ms | tok/sec 1126571.3073
for step 4221 | loss 3.528114 | norm 0.2997 | time 465.1279 ms | tok/sec 1127190.9282
for step 4222 | loss 3.537900 | norm 0.3485 | time 465.3416 ms | tok/sec 1126673.4718
for step 4223 | loss 3.566679 | norm 0.3270 | time 465.1792 ms | tok/sec 1127066.7184
for step 4224 | loss 3.549944 | norm 0.2990 | time 464.8404 ms | tok/sec 1127888.1661
for step 4225 | loss 3.568856 | norm 0.3209 | time 466.2738 ms | tok/sec 1124420.9245
for step 4226 | loss 3.497828 | norm 0.3568 | time 464.6366 ms | tok/sec 1128382.9994
for step 4227 | loss 3.517165 | norm 0.2967 | time 466.9633 ms | tok/sec 1122760.6322
for step 4228 | loss 3.560487 | norm 0.3318 | time 465.2781 ms | tok/sec 1126827.0421
for step 4229 | loss 3.569315 | norm 0.3356 | time 464.8507 ms | tok/sec 1127863.2913
for step 4230 | loss 3.567705 | norm 0.3251 | time 464.7238 ms | tok/sec 1128171.1229
for step 4231 | loss 3.487199 | norm 0.3977 | time 465.6379 ms | tok/sec 1125956.4028
for step 4232 | loss 3.389443 | norm 0.3692 | time 465.0540 ms | tok/sec 1127370.0696
for step 4233 | loss 3.393003 | norm 0.3252 | time 465.7793 ms | tok/sec 1125614.6311
for step 4234 | loss 3.356786 | norm 0.3288 | time 465.1668 ms | tok/sec 1127096.7574
for step 4235 | loss 3.351227 | norm 0.3462 | time 465.2562 ms | tok/sec 1126880.1664
for step 4236 | loss 3.390604 | norm 0.3387 | time 464.7930 ms | tok/sec 1128003.2991
for step 4237 | loss 3.384722 | norm 0.3250 | time 464.5493 ms | tok/sec 1128594.9555
for step 4238 | loss 3.331550 | norm 0.3131 | time 465.7681 ms | tok/sec 1125641.7116
for step 4239 | loss 3.438623 | norm 0.3645 | time 465.3642 ms | tok/sec 1126618.6355
for step 4240 | loss 3.402361 | norm 0.3497 | time 464.8404 ms | tok/sec 1127888.1661
for step 4241 | loss 3.344957 | norm 0.3549 | time 464.9653 ms | tok/sec 1127585.1144
for step 4242 | loss 3.314978 | norm 0.3296 | time 465.6007 ms | tok/sec 1126046.3469
for step 4243 | loss 3.522063 | norm 0.3271 | time 465.2257 ms | tok/sec 1126954.0867
for step 4244 | loss 3.622530 | norm 0.3552 | time 465.4806 ms | tok/sec 1126337.0337
for step 4245 | loss 3.604194 | norm 0.3579 | time 465.4953 ms | tok/sec 1126301.2666
for step 4246 | loss 3.553252 | norm 0.3920 | time 465.2779 ms | tok/sec 1126827.6195
for step 4247 | loss 3.633087 | norm 0.3352 | time 465.7896 ms | tok/sec 1125589.8564
for step 4248 | loss 3.581196 | norm 0.3415 | time 465.8141 ms | tok/sec 1125530.5167
for step 4249 | loss 3.621411 | norm 0.3625 | time 465.0998 ms | tok/sec 1127259.1108
validation loss 3.5849
HellaSwag accuracy: 2611/10042=0.2600
> Hello, I'm a language model, and I've been thinking of an inbuilt interface to model something like the MVC (for me, a language model
> Hello, I'm a language model, and you've tried to use that in place of other contexts. A lot of people now enjoy knowing how to use it
> Hello, I'm a language model, but no two languages are currently available.
I've found them on a search engine, so the word is pretty broad
> Hello, I'm a language model, and I'm really looking for a little game to go after your name. After that, I have a little time to
> Hello, I'm a language model, so I wanted to explain why I wanted you to use the words "Kantitlanguages" when working with you
> Hello, I'm a language model, now that I look at how many of us get in here. I get into things where they are most of the time
>>  Hello, I'm a language model, and my experience level is limited to some other languages. So if you have any idea of what is the world of theHello, I'm a language model, I’m not trying to write an algebraic expression (and I’m very, too, my very

> Hello, I'm a language model, and it could give some ideas for those that can apply
[tough to them, and we'll see an analogy> 
Hello, I'm a language model, but I couldn't really get to an expert in. I can't say anything about it, and then I can't
> > Hello, I'm a language model, and I like to think how we think or interact with other people? Are our minds changing differently or do we have new
Hello, I'm a language model, so I'll explain what I'll have to say, and explain the two things together. I hope that the people that
> > > Hello, I'm a language model, I think it's just plain to think about it - and that is what makes it so wonderful.
So, my
> Hello, I'm a language model, but how do you get to a solution? How well do you think the answers are, and what tools you can use
Hello, I'm a language model, but what will I expect?
I wanted to think about some of the important issues, first-generation programming, and
> Hello, I'm a language model, and I haven't got an answer yet—I want to look at why it will make you the better.
How
> Hello, I'm a language model, and I'll be in a little bit bit of my way to talk my ideas about how I could do a couple of
> Hello, I'm a language model, so I're in love with Language, and because I'm an educator, I'll do it in a way I do
> Hello, I'm a language model, using syntax to describe how a computer process produces a different "referred" type of output.
For example, suppose
> Hello, I'm a language model, but I am not a programmer or programming instructor but I am an interpreter, too. I am a beginner programmer so I
> Hello, I'm a language model, and in my own language, it's quite language-wise.
A good linguist (or just a little trick
> Hello, I'm a language model, and I've made a lot of notes on the actual language, which ones I've been using in my classes since my
Hello, I'm a language model, and I'll also try to describe it as a "structure."
Structure theory - A function for understanding

> > Hello, I'm a language model, so I wanted to talk about the same process I've been using in my classroom. I wanted to include a discussion as
Hello, I'm a language model, and I'm trying to use the most intuitive way I can visualize it I thought would. My answer and my point would
> Hello, I'm a language model, and so, I'm here because I've seen it in the movies and it's a lot of fun. We can
> Hello, I'm a language model, and you'll use just one method to simulate, for example, a simple version of the program
I'm writing and
> Hello, I'm a language model, but that doesn't have to be. They are used in a similar way. They are used to express a simple message> 
Hello, I'm a language model, and to write, I have written a few sentences that will be useful in the same way as we work.
We
> Hello, I'm a language model, and there's lots of language you can think of. So it's important for language designers and marketers to recognize and understand
> Hello, I'm a language model, and I'll explain other languages at hand. I'll explain how. I've used C++ for this, and now
> Hello, I'm a language model, this is why the whole school system is a big help for my son. If one has to create an entire school system
for step 4250 | loss 3.624913 | norm 0.3360 | time 13050.1480 ms | tok/sec 40174.8700
for step 4251 | loss 3.577223 | norm 0.4026 | time 462.6999 ms | tok/sec 1133105.9531
for step 4252 | loss 3.572110 | norm 0.4332 | time 464.7052 ms | tok/sec 1128216.2702
for step 4253 | loss 3.606088 | norm 0.3926 | time 462.8615 ms | tok/sec 1132710.2320
for step 4254 | loss 3.553607 | norm 0.4018 | time 463.2430 ms | tok/sec 1131777.4717
for step 4255 | loss 3.536158 | norm 0.3320 | time 462.9898 ms | tok/sec 1132396.4198
for step 4256 | loss 3.517144 | norm 0.3217 | time 463.3310 ms | tok/sec 1131562.5718
for step 4257 | loss 3.527529 | norm 0.3115 | time 463.6090 ms | tok/sec 1130884.0476
for step 4258 | loss 3.528015 | norm 0.2998 | time 464.5255 ms | tok/sec 1128652.8808
for step 4259 | loss 3.560105 | norm 0.2970 | time 463.9568 ms | tok/sec 1130036.1647
for step 4260 | loss 3.516644 | norm 0.2861 | time 464.4811 ms | tok/sec 1128760.6378
for step 4261 | loss 3.573975 | norm 0.3190 | time 464.4775 ms | tok/sec 1128769.3288
for step 4262 | loss 3.554849 | norm 0.3166 | time 465.1835 ms | tok/sec 1127056.3207
for step 4263 | loss 3.624661 | norm 0.3219 | time 464.2930 ms | tok/sec 1129217.9648
for step 4264 | loss 3.550680 | norm 0.2814 | time 464.5443 ms | tok/sec 1128607.1193
for step 4265 | loss 3.595427 | norm 0.2924 | time 464.5715 ms | tok/sec 1128541.0902
for step 4266 | loss 3.516594 | norm 0.2957 | time 465.0002 ms | tok/sec 1127500.7053
for step 4267 | loss 3.491800 | norm 0.3098 | time 465.0567 ms | tok/sec 1127363.7120
for step 4268 | loss 3.551534 | norm 0.3329 | time 464.1285 ms | tok/sec 1129618.2124
for step 4269 | loss 3.500691 | norm 0.3136 | time 464.8676 ms | tok/sec 1127822.2212
for step 4270 | loss 3.527759 | norm 0.2840 | time 465.3695 ms | tok/sec 1126605.9373
for step 4271 | loss 3.575205 | norm 0.2868 | time 465.4603 ms | tok/sec 1126386.0731
for step 4272 | loss 3.554530 | norm 0.3068 | time 465.7493 ms | tok/sec 1125687.2330
for step 4273 | loss 3.509830 | norm 0.3176 | time 465.2517 ms | tok/sec 1126891.1383
for step 4274 | loss 3.527696 | norm 0.3284 | time 464.0474 ms | tok/sec 1129815.5402
for step 4275 | loss 3.506548 | norm 0.3165 | time 465.7466 ms | tok/sec 1125693.5717
for step 4276 | loss 3.554487 | norm 0.3573 | time 464.8266 ms | tok/sec 1127921.7201
for step 4277 | loss 3.515403 | norm 0.3375 | time 465.0156 ms | tok/sec 1127463.1300
for step 4278 | loss 3.515765 | norm 0.3098 | time 465.1940 ms | tok/sec 1127030.9049
for step 4279 | loss 3.374493 | norm 0.3556 | time 465.6157 ms | tok/sec 1126010.0216
for step 4280 | loss 3.383898 | norm 0.3825 | time 465.3530 ms | tok/sec 1126645.7644
for step 4281 | loss 3.391915 | norm 0.3436 | time 464.7899 ms | tok/sec 1128010.8212
for step 4282 | loss 3.330108 | norm 0.3740 | time 464.8919 ms | tok/sec 1127763.2243
for step 4283 | loss 3.432261 | norm 0.3344 | time 465.3859 ms | tok/sec 1126566.1130
for step 4284 | loss 3.403700 | norm 0.3323 | time 465.6668 ms | tok/sec 1125886.6484
for step 4285 | loss 3.366992 | norm 0.2990 | time 463.7311 ms | tok/sec 1130586.3592
for step 4286 | loss 3.393987 | norm 0.2922 | time 466.1002 ms | tok/sec 1124839.6420
for step 4287 | loss 3.417142 | norm 0.2894 | time 465.0135 ms | tok/sec 1127468.3325
for step 4288 | loss 3.363962 | norm 0.3136 | time 464.3440 ms | tok/sec 1129093.8876
for step 4289 | loss 3.369440 | norm 0.2947 | time 465.3533 ms | tok/sec 1126645.1871
for step 4290 | loss 3.422380 | norm 0.3213 | time 464.9370 ms | tok/sec 1127653.9229
for step 4291 | loss 3.588189 | norm 0.2818 | time 465.0240 ms | tok/sec 1127442.8981
for step 4292 | loss 3.704303 | norm 0.3192 | time 464.7703 ms | tok/sec 1128058.2704
for step 4293 | loss 3.576854 | norm 0.3048 | time 464.5905 ms | tok/sec 1128494.7587
for step 4294 | loss 3.611882 | norm 0.3326 | time 465.3471 ms | tok/sec 1126660.1952
for step 4295 | loss 3.563673 | norm 0.3512 | time 465.3685 ms | tok/sec 1126608.2460
for step 4296 | loss 3.615996 | norm 0.3773 | time 465.3878 ms | tok/sec 1126561.4959
for step 4297 | loss 3.613022 | norm 0.3539 | time 464.9668 ms | tok/sec 1127581.6453
for step 4298 | loss 3.566049 | norm 0.3102 | time 464.7529 ms | tok/sec 1128100.5151
for step 4299 | loss 3.635375 | norm 0.3686 | time 464.4949 ms | tok/sec 1128727.0340
for step 4300 | loss 3.634772 | norm 0.3688 | time 465.0214 ms | tok/sec 1127449.2566
for step 4301 | loss 3.585728 | norm 0.3733 | time 464.8008 ms | tok/sec 1127984.2051
for step 4302 | loss 3.586868 | norm 0.3524 | time 465.6987 ms | tok/sec 1125809.4098
for step 4303 | loss 3.583103 | norm 0.3247 | time 465.4863 ms | tok/sec 1126323.1881
for step 4304 | loss 3.583554 | norm 0.3218 | time 464.8974 ms | tok/sec 1127749.9219
for step 4305 | loss 3.568565 | norm 0.3241 | time 464.7148 ms | tok/sec 1128193.1173
for step 4306 | loss 3.587225 | norm 0.3129 | time 464.9692 ms | tok/sec 1127575.8634
for step 4307 | loss 3.634985 | norm 0.3270 | time 465.3480 ms | tok/sec 1126657.8862
for step 4308 | loss 3.527622 | norm 0.2915 | time 465.1248 ms | tok/sec 1127198.4394
for step 4309 | loss 3.625318 | norm 0.3322 | time 465.1899 ms | tok/sec 1127040.7245
for step 4310 | loss 3.587725 | norm 0.3773 | time 465.4255 ms | tok/sec 1126470.3154
for step 4311 | loss 3.520210 | norm 0.3621 | time 465.5476 ms | tok/sec 1126174.9458
for step 4312 | loss 3.591316 | norm 0.3297 | time 466.0013 ms | tok/sec 1125078.4735
for step 4313 | loss 3.573343 | norm 0.3663 | time 464.7686 ms | tok/sec 1128062.3211
for step 4314 | loss 3.492785 | norm 0.3818 | time 465.4906 ms | tok/sec 1126312.8041
for step 4315 | loss 3.514199 | norm 0.3465 | time 465.3332 ms | tok/sec 1126693.6761
for step 4316 | loss 3.545732 | norm 0.3663 | time 465.9081 ms | tok/sec 1125303.5860
for step 4317 | loss 3.561655 | norm 0.3517 | time 465.3018 ms | tok/sec 1126769.8814
for step 4318 | loss 3.542623 | norm 0.3151 | time 465.5633 ms | tok/sec 1126136.8821
for step 4319 | loss 3.510209 | norm 0.3270 | time 465.6980 ms | tok/sec 1125811.1389
for step 4320 | loss 3.578932 | norm 0.2898 | time 465.1537 ms | tok/sec 1127128.5310
for step 4321 | loss 3.463341 | norm 0.3205 | time 464.7813 ms | tok/sec 1128031.6520
for step 4322 | loss 3.483198 | norm 0.2901 | time 465.5681 ms | tok/sec 1126125.3481
for step 4323 | loss 3.495735 | norm 0.2827 | time 466.5251 ms | tok/sec 1123815.2565
for step 4324 | loss 3.491798 | norm 0.3275 | time 465.2081 ms | tok/sec 1126996.8264
for step 4325 | loss 3.474957 | norm 0.2933 | time 465.3625 ms | tok/sec 1126622.6758
for step 4326 | loss 3.347375 | norm 0.2999 | time 465.3461 ms | tok/sec 1126662.5041
for step 4327 | loss 3.385676 | norm 0.3189 | time 464.7663 ms | tok/sec 1128068.1079
for step 4328 | loss 3.351203 | norm 0.3685 | time 464.5321 ms | tok/sec 1128636.6611
for step 4329 | loss 3.399113 | norm 0.3820 | time 466.5573 ms | tok/sec 1123737.7276
for step 4330 | loss 3.408385 | norm 0.3495 | time 465.3606 ms | tok/sec 1126627.2935
for step 4331 | loss 3.330456 | norm 0.3050 | time 465.1072 ms | tok/sec 1127241.1976
for step 4332 | loss 3.417078 | norm 0.3129 | time 465.3287 ms | tok/sec 1126704.6444
for step 4333 | loss 3.400584 | norm 0.3683 | time 465.6725 ms | tok/sec 1125872.8138
for step 4334 | loss 3.337993 | norm 0.3678 | time 465.1394 ms | tok/sec 1127163.1953
for step 4335 | loss 3.368510 | norm 0.3363 | time 466.1903 ms | tok/sec 1124622.1921
for step 4336 | loss 3.355388 | norm 0.3154 | time 465.8771 ms | tok/sec 1125378.4515
for step 4337 | loss 3.444904 | norm 0.3199 | time 464.7844 ms | tok/sec 1128024.1297
for step 4338 | loss 3.586060 | norm 0.3674 | time 465.8902 ms | tok/sec 1125346.7764
for step 4339 | loss 3.580712 | norm 0.3985 | time 465.5914 ms | tok/sec 1126068.8352
for step 4340 | loss 3.634652 | norm 0.3927 | time 465.9326 ms | tok/sec 1125244.2765
for step 4341 | loss 3.599026 | norm 0.3466 | time 465.2801 ms | tok/sec 1126822.4228
for step 4342 | loss 3.540203 | norm 0.3467 | time 465.7788 ms | tok/sec 1125615.7834
for step 4343 | loss 3.593170 | norm 0.3706 | time 465.5430 ms | tok/sec 1126185.9040
for step 4344 | loss 3.582474 | norm 0.3456 | time 465.5223 ms | tok/sec 1126236.0838
for step 4345 | loss 3.620387 | norm 0.3386 | time 466.1617 ms | tok/sec 1124691.2147
for step 4346 | loss 3.592697 | norm 0.3411 | time 465.2092 ms | tok/sec 1126993.9385
for step 4347 | loss 3.603886 | norm 0.3398 | time 464.9928 ms | tok/sec 1127518.6267
for step 4348 | loss 3.583393 | norm 0.3399 | time 464.8607 ms | tok/sec 1127838.9959
for step 4349 | loss 3.550521 | norm 0.3046 | time 465.9338 ms | tok/sec 1125241.3976
for step 4350 | loss 3.589331 | norm 0.3121 | time 464.8800 ms | tok/sec 1127792.1436
for step 4351 | loss 3.576826 | norm 0.2946 | time 464.7396 ms | tok/sec 1128132.9241
for step 4352 | loss 3.607937 | norm 0.2973 | time 465.8573 ms | tok/sec 1125426.2554
for step 4353 | loss 3.570489 | norm 0.2849 | time 465.7164 ms | tok/sec 1125766.7602
for step 4354 | loss 3.576151 | norm 0.3311 | time 466.1796 ms | tok/sec 1124648.0746
for step 4355 | loss 3.537623 | norm 0.3028 | time 465.6906 ms | tok/sec 1125829.0066
for step 4356 | loss 3.593834 | norm 0.3080 | time 466.5992 ms | tok/sec 1123636.6689
for step 4357 | loss 3.587476 | norm 0.3041 | time 465.6022 ms | tok/sec 1126042.8873
for step 4358 | loss 3.549703 | norm 0.2855 | time 464.1967 ms | tok/sec 1129452.2782
for step 4359 | loss 3.600380 | norm 0.3120 | time 465.3754 ms | tok/sec 1126591.5079
for step 4360 | loss 3.606137 | norm 0.3161 | time 465.0743 ms | tok/sec 1127320.9445
for step 4361 | loss 3.529421 | norm 0.2853 | time 465.5108 ms | tok/sec 1126263.7711
for step 4362 | loss 3.478303 | norm 0.2925 | time 465.4911 ms | tok/sec 1126311.6503
for step 4363 | loss 3.542598 | norm 0.3413 | time 465.7881 ms | tok/sec 1125593.3132
for step 4364 | loss 3.567072 | norm 0.3208 | time 465.5643 ms | tok/sec 1126134.5753
for step 4365 | loss 3.544092 | norm 0.3227 | time 466.5732 ms | tok/sec 1123699.2542
for step 4366 | loss 3.535981 | norm 0.3018 | time 465.8351 ms | tok/sec 1125479.8237
for step 4367 | loss 3.552511 | norm 0.3347 | time 464.9882 ms | tok/sec 1127529.6111
for step 4368 | loss 3.515765 | norm 0.3483 | time 465.1837 ms | tok/sec 1127055.7431
for step 4369 | loss 3.480552 | norm 0.3073 | time 465.7199 ms | tok/sec 1125758.1154
for step 4370 | loss 3.524591 | norm 0.3422 | time 465.6866 ms | tok/sec 1125838.8053
for step 4371 | loss 3.496832 | norm 0.3509 | time 465.2019 ms | tok/sec 1127011.8438
for step 4372 | loss 3.461890 | norm 0.3465 | time 465.1117 ms | tok/sec 1127230.2189
for step 4373 | loss 3.431521 | norm 0.3143 | time 466.3906 ms | tok/sec 1124139.2709
for step 4374 | loss 3.342032 | norm 0.3536 | time 465.3780 ms | tok/sec 1126585.1590
for step 4375 | loss 3.410524 | norm 0.3734 | time 464.9346 ms | tok/sec 1127659.7055
for step 4376 | loss 3.368099 | norm 0.3497 | time 464.9596 ms | tok/sec 1127598.9910
for step 4377 | loss 3.344248 | norm 0.3587 | time 465.7476 ms | tok/sec 1125691.2667
for step 4378 | loss 3.328049 | norm 0.3605 | time 465.7161 ms | tok/sec 1125767.3365
for step 4379 | loss 3.404616 | norm 0.3204 | time 464.2739 ms | tok/sec 1129264.3558
for step 4380 | loss 3.324971 | norm 0.3528 | time 465.6973 ms | tok/sec 1125812.8680
Will loading at 0 from edu_fineweb10B/edufineweb_train_000024.npy
for step 4381 | loss 3.395201 | norm 0.3765 | time 1493.1087 ms | tok/sec 351138.5224
for step 4382 | loss 3.458045 | norm 0.3576 | time 464.2231 ms | tok/sec 1129387.8903
for step 4383 | loss 3.372672 | norm 0.3506 | time 465.2469 ms | tok/sec 1126902.6880
for step 4384 | loss 3.414137 | norm 0.3266 | time 466.2664 ms | tok/sec 1124438.7482
for step 4385 | loss 3.602822 | norm 0.3746 | time 465.4055 ms | tok/sec 1126518.7892
for step 4386 | loss 3.597670 | norm 0.3766 | time 465.7726 ms | tok/sec 1125630.7640
for step 4387 | loss 3.621780 | norm 0.3568 | time 465.4276 ms | tok/sec 1126465.1220
for step 4388 | loss 3.602845 | norm 0.3075 | time 465.5743 ms | tok/sec 1126110.3543
for step 4389 | loss 3.615835 | norm 0.3573 | time 466.0161 ms | tok/sec 1125042.7862
for step 4390 | loss 3.601059 | norm 0.3495 | time 465.8506 ms | tok/sec 1125442.3830
for step 4391 | loss 3.559280 | norm 0.3679 | time 465.9903 ms | tok/sec 1125104.9527
for step 4392 | loss 3.582089 | norm 0.3897 | time 466.1772 ms | tok/sec 1124653.8264
for step 4393 | loss 3.609751 | norm 0.4115 | time 465.7927 ms | tok/sec 1125582.3666
for step 4394 | loss 3.576402 | norm 0.3682 | time 465.5659 ms | tok/sec 1126130.5384
for step 4395 | loss 3.528949 | norm 0.3444 | time 466.3625 ms | tok/sec 1124207.0848
for step 4396 | loss 3.587206 | norm 0.3722 | time 465.5411 ms | tok/sec 1126190.5181
for step 4397 | loss 3.531065 | norm 0.3415 | time 465.6103 ms | tok/sec 1126023.2830
for step 4398 | loss 3.553253 | norm 0.3114 | time 466.0795 ms | tok/sec 1124889.7019
for step 4399 | loss 3.512258 | norm 0.2766 | time 465.8418 ms | tok/sec 1125463.6951
for step 4400 | loss 3.572588 | norm 0.3466 | time 466.2261 ms | tok/sec 1124535.9257
for step 4401 | loss 3.547991 | norm 0.3120 | time 465.8377 ms | tok/sec 1125473.4874
for step 4402 | loss 3.543961 | norm 0.3388 | time 466.3181 ms | tok/sec 1124313.9945
for step 4403 | loss 3.612357 | norm 0.3031 | time 465.4882 ms | tok/sec 1126318.5730
for step 4404 | loss 3.498342 | norm 0.3563 | time 464.9467 ms | tok/sec 1127630.2148
for step 4405 | loss 3.547112 | norm 0.3735 | time 467.2978 ms | tok/sec 1121956.9373
for step 4406 | loss 3.579878 | norm 0.3381 | time 466.0285 ms | tok/sec 1125012.8567
for step 4407 | loss 3.549773 | norm 0.3151 | time 466.1255 ms | tok/sec 1124778.6555
for step 4408 | loss 3.544271 | norm 0.3194 | time 464.4804 ms | tok/sec 1128762.3760
for step 4409 | loss 3.537814 | norm 0.3040 | time 464.9172 ms | tok/sec 1127701.9204
for step 4410 | loss 3.573485 | norm 0.3194 | time 465.4450 ms | tok/sec 1126422.9997
for step 4411 | loss 3.508663 | norm 0.3044 | time 466.2948 ms | tok/sec 1124370.3314
for step 4412 | loss 3.600395 | norm 0.3030 | time 466.0795 ms | tok/sec 1124889.7019
for step 4413 | loss 3.594033 | norm 0.3559 | time 464.9327 ms | tok/sec 1127664.3316
for step 4414 | loss 3.514354 | norm 0.3484 | time 465.4849 ms | tok/sec 1126326.6495
for step 4415 | loss 3.492394 | norm 0.3050 | time 466.2347 ms | tok/sec 1124515.2238
for step 4416 | loss 3.550644 | norm 0.3264 | time 465.4624 ms | tok/sec 1126380.8805
for step 4417 | loss 3.469651 | norm 0.3308 | time 467.0913 ms | tok/sec 1122452.8810
for step 4418 | loss 3.522378 | norm 0.3075 | time 465.5135 ms | tok/sec 1126257.4260
for step 4419 | loss 3.521577 | norm 0.2808 | time 466.0296 ms | tok/sec 1125009.9789
for step 4420 | loss 3.370443 | norm 0.3452 | time 465.6260 ms | tok/sec 1125985.2295
for step 4421 | loss 3.390962 | norm 0.4743 | time 465.9622 ms | tok/sec 1125172.8831
for step 4422 | loss 3.406096 | norm 0.5439 | time 468.4763 ms | tok/sec 1119134.5327
for step 4423 | loss 3.413846 | norm 0.4237 | time 465.3926 ms | tok/sec 1126549.9532
for step 4424 | loss 3.337547 | norm 0.3654 | time 466.0208 ms | tok/sec 1125031.2747
for step 4425 | loss 3.375104 | norm 0.3577 | time 469.1715 ms | tok/sec 1117476.1748
for step 4426 | loss 3.453220 | norm 0.3457 | time 466.8353 ms | tok/sec 1123068.5522
for step 4427 | loss 3.356735 | norm 0.3365 | time 466.6626 ms | tok/sec 1123483.9669
for step 4428 | loss 3.346511 | norm 0.3111 | time 465.7350 ms | tok/sec 1125721.8086
for step 4429 | loss 3.405838 | norm 0.3274 | time 466.2235 ms | tok/sec 1124542.2515
for step 4430 | loss 3.339593 | norm 0.3187 | time 466.6529 ms | tok/sec 1123507.5010
for step 4431 | loss 3.403827 | norm 0.3306 | time 466.1899 ms | tok/sec 1124623.3424
for step 4432 | loss 3.608524 | norm 0.3512 | time 465.3797 ms | tok/sec 1126581.1189
for step 4433 | loss 3.581653 | norm 0.3524 | time 466.7246 ms | tok/sec 1123334.7495
for step 4434 | loss 3.596327 | norm 0.3752 | time 465.6510 ms | tok/sec 1125924.6952
for step 4435 | loss 3.603009 | norm 0.3192 | time 465.4953 ms | tok/sec 1126301.2666
for step 4436 | loss 3.636764 | norm 0.3583 | time 465.0927 ms | tok/sec 1127276.4467
for step 4437 | loss 3.545765 | norm 0.3270 | time 464.9148 ms | tok/sec 1127707.7035
for step 4438 | loss 3.582326 | norm 0.3194 | time 465.2448 ms | tok/sec 1126907.8854
for step 4439 | loss 3.563390 | norm 0.7202 | time 465.6415 ms | tok/sec 1125947.7551
for step 4440 | loss 3.594491 | norm 0.3430 | time 465.5416 ms | tok/sec 1126189.3645
for step 4441 | loss 3.595015 | norm 0.3634 | time 465.2021 ms | tok/sec 1127011.2662
for step 4442 | loss 3.566070 | norm 0.3431 | time 465.9364 ms | tok/sec 1125235.0640
for step 4443 | loss 3.517075 | norm 0.2965 | time 465.7936 ms | tok/sec 1125580.0620
for step 4444 | loss 3.562257 | norm 0.3434 | time 465.1794 ms | tok/sec 1127066.1408
for step 4445 | loss 3.618563 | norm 0.3055 | time 466.4648 ms | tok/sec 1123960.5803
for step 4446 | loss 3.592533 | norm 0.3351 | time 465.7941 ms | tok/sec 1125578.9097
for step 4447 | loss 3.612621 | norm 0.3651 | time 465.4214 ms | tok/sec 1126480.1252
for step 4448 | loss 3.555569 | norm 0.3154 | time 465.9886 ms | tok/sec 1125108.9822
for step 4449 | loss 3.580360 | norm 0.3107 | time 466.5656 ms | tok/sec 1123717.6292
for step 4450 | loss 3.560486 | norm 0.2879 | time 466.4569 ms | tok/sec 1123979.5383
for step 4451 | loss 3.573905 | norm 0.3066 | time 466.2611 ms | tok/sec 1124451.3975
for step 4452 | loss 3.568494 | norm 0.3007 | time 465.3976 ms | tok/sec 1126537.8337
for step 4453 | loss 3.556145 | norm 0.2802 | time 465.7595 ms | tok/sec 1125662.4551
for step 4454 | loss 3.493717 | norm 0.2987 | time 465.3895 ms | tok/sec 1126557.4559
for step 4455 | loss 3.467443 | norm 0.2858 | time 466.4166 ms | tok/sec 1124076.6366
for step 4456 | loss 3.488571 | norm 0.2961 | time 465.5421 ms | tok/sec 1126188.2110
for step 4457 | loss 3.493705 | norm 0.3540 | time 465.9586 ms | tok/sec 1125181.5189
for step 4458 | loss 3.503374 | norm 0.3262 | time 465.9729 ms | tok/sec 1125146.9765
for step 4459 | loss 3.516261 | norm 0.3299 | time 467.9322 ms | tok/sec 1120435.7657
for step 4460 | loss 3.490510 | norm 0.2856 | time 466.2578 ms | tok/sec 1124459.4473
for step 4461 | loss 3.505642 | norm 0.3330 | time 465.8923 ms | tok/sec 1125341.5934
for step 4462 | loss 3.513844 | norm 0.2970 | time 465.7252 ms | tok/sec 1125745.4366
for step 4463 | loss 3.525327 | norm 0.3308 | time 465.3752 ms | tok/sec 1126592.0850
for step 4464 | loss 3.518966 | norm 0.3146 | time 465.3256 ms | tok/sec 1126712.1491
for step 4465 | loss 3.514725 | norm 0.3153 | time 465.5383 ms | tok/sec 1126197.4392
for step 4466 | loss 3.334961 | norm 0.3461 | time 465.7199 ms | tok/sec 1125758.1154
for step 4467 | loss 3.355524 | norm 0.3914 | time 465.5230 ms | tok/sec 1126234.3534
for step 4468 | loss 3.287879 | norm 0.3817 | time 465.4996 ms | tok/sec 1126290.8830
for step 4469 | loss 3.390672 | norm 0.3987 | time 466.2275 ms | tok/sec 1124532.4754
for step 4470 | loss 3.348274 | norm 0.3809 | time 465.5547 ms | tok/sec 1126157.6438
for step 4471 | loss 3.299733 | norm 0.3319 | time 466.3002 ms | tok/sec 1124357.1090
for step 4472 | loss 3.369966 | norm 0.3688 | time 465.6320 ms | tok/sec 1125970.8160
for step 4473 | loss 3.351229 | norm 0.3287 | time 466.8639 ms | tok/sec 1122999.7286
for step 4474 | loss 3.508791 | norm 0.3993 | time 465.1840 ms | tok/sec 1127055.1654
for step 4475 | loss 3.383839 | norm 0.3718 | time 466.6378 ms | tok/sec 1123543.6650
for step 4476 | loss 3.349020 | norm 0.4746 | time 465.2152 ms | tok/sec 1126979.4991
for step 4477 | loss 3.477957 | norm 0.4821 | time 465.2693 ms | tok/sec 1126848.4067
for step 4478 | loss 3.608439 | norm 0.3782 | time 466.3086 ms | tok/sec 1124336.9885
for step 4479 | loss 3.574430 | norm 0.3708 | time 465.5135 ms | tok/sec 1126257.4260
for step 4480 | loss 3.602327 | norm 0.3888 | time 465.0178 ms | tok/sec 1127457.9274
for step 4481 | loss 3.636104 | norm 0.3322 | time 466.3258 ms | tok/sec 1124295.6000
for step 4482 | loss 3.616838 | norm 0.4891 | time 465.4305 ms | tok/sec 1126458.1975
for step 4483 | loss 3.632051 | norm 0.4864 | time 466.1751 ms | tok/sec 1124659.0031
for step 4484 | loss 3.550730 | norm 0.4130 | time 465.6973 ms | tok/sec 1125812.8680
for step 4485 | loss 3.633260 | norm 0.3896 | time 466.1553 ms | tok/sec 1124706.7459
for step 4486 | loss 3.664242 | norm 0.3986 | time 465.6775 ms | tok/sec 1125860.7089
for step 4487 | loss 3.562017 | norm 0.3890 | time 466.4364 ms | tok/sec 1124028.9472
for step 4488 | loss 3.667111 | norm 0.3932 | time 464.5164 ms | tok/sec 1128674.8940
for step 4489 | loss 3.585047 | norm 0.3105 | time 465.2503 ms | tok/sec 1126894.6032
for step 4490 | loss 3.541547 | norm 0.3353 | time 466.5966 ms | tok/sec 1123642.9845
for step 4491 | loss 3.563003 | norm 0.3211 | time 466.4919 ms | tok/sec 1123895.0938
for step 4492 | loss 3.544843 | norm 0.3231 | time 466.2154 ms | tok/sec 1124561.8042
for step 4493 | loss 3.522562 | norm 0.3190 | time 465.6973 ms | tok/sec 1125812.8680
for step 4494 | loss 3.604530 | norm 0.3512 | time 466.1512 ms | tok/sec 1124716.5251
for step 4495 | loss 3.554884 | norm 0.2987 | time 465.8673 ms | tok/sec 1125402.0650
for step 4496 | loss 3.544529 | norm 0.3170 | time 465.7469 ms | tok/sec 1125692.9955
for step 4497 | loss 3.570951 | norm 0.3355 | time 465.9631 ms | tok/sec 1125170.5803
for step 4498 | loss 3.584477 | norm 0.3330 | time 467.0699 ms | tok/sec 1122504.4476
for step 4499 | loss 3.644011 | norm 0.3087 | time 466.7938 ms | tok/sec 1123168.3614
validation loss 3.5662
HellaSwag accuracy: 2633/10042=0.2622
> Hello, I'm a language model, and I am a language model. Like the syntax I, when I use the syntax that I wrote, I can't
> Hello, I'm a language model, and the result is that the object itself is a very basic and stable object. Also what is done to the object is
> Hello, I'm a language model, so I'll do a lot of work on the project in another post and I'll be on to some other topics from
> Hello, I'm a language model, and I'm working with languages, and will be helping make sense of things - and sometimes using it to generate some of
> Hello, I'm a language model, but I also have to understand the concept's relationship with the verb. But there are no restrictions on which I can't
> Hello, I'm a language model, don't worry not that I was thinking that he would work.
That said, "The big one, too."
> Hello, I'm a language model, I see a series of books, so I'm not so familiar. I'm used to make it clearer. If you
> Hello, I'm a language model, and it used me to make a series of statements into small elements, some interesting, and perhaps confusing, but to be
> Hello, I'm a language model, and I'm going to give you a formalised description of the language from the starting point of course, but I really
> Hello, I'm a language model, when you think of language theory (see Figure 1). On the other hand, some languages, like Greek, have not
> Hello, I'm a language model, so my question is "what is an OED?" I'm looking at the question of how "to speak or read> 
Hello, I'm a language model, and I am doing this for 3 seconds now. However, I want to make sure I am not trying to learn how
> Hello, I'm a language model, and there is one example where there is one class of languages in the language that will make the argument a much better argument> 
Hello, I'm a language model, but it's a good representation of these processes.
One thing that was important about this article is that it's possible
>>  Hello, I'm a language model, but it may not fully understand some terms. If you're doing anything wrong with a bit of fancy, you can goHello, I'm a language model, I'll be learning. (Okay, so I will do that!) I can't do anything. I'll be reading

> > Hello, I'm a language model, and I'll be doing it with you tomorrow. But I'm not doing it well. I think it would be more
Hello, I'm a language model, this is because we have already been able to put language model into the language for language development,
and it is this
> Hello, I'm a language model, so I can just talk with my children in this way. So what do you do with this?
Let me tell
> > Hello, I'm a language model, and I just want my reader to do stuff without that. I'd start using the syntax/symbol expression to help
Hello, I'm a language model, and I'll work through a list of rules to help you understand how to design your model’s models. When
> Hello, I'm a language model, but don't know the language of something. So please read my post first!<|endoftext|>In our previous essay on the history
> Hello, I'm a language model, but I would need to start with the language structure, and we can start with the language structure.
I am so
> Hello, I'm a language model, and as a parent, I am interested in English. I am currently teaching my grammar at LPLT.<|endoftext|>As
> > Hello, I'm a language model, and so on (I'm not just talking to me about a language teacher). I'm not talking to your students when
Hello, I'm a language model, and I'm a teacher at a hospital.
It's how we apply programming in class to programming we do, the
> Hello, I'm a language model, so I learned to make sense of English but I went to my classroom after school. I am going to learn that,> 
Hello, I'm a language model, and this one is a method used to describe a text for a language.
I'm a language model, using this
> Hello, I'm a language model, but I don't think that they understand every word, but I am an individual, and that all we know is there> 
Hello, I'm a language model, and the way I describe the story is this:
"And I am very satisfied with it - and I hope to
> > Hello, I'm a language model, so this book is very useful for students to learn all of languages.
First, you use objects and objects in a
Hello, I'm a language model, and has a nice list of 5 things I want my reader to learn, and will use the game so I can learn
for step 4500 | loss 3.541271 | norm 0.3022 | time 17544.9011 ms | tok/sec 29882.6420
for step 4501 | loss 3.483153 | norm 0.2899 | time 461.5262 ms | tok/sec 1135987.6223
for step 4502 | loss 3.539440 | norm 0.2814 | time 462.2464 ms | tok/sec 1134217.5506
for step 4503 | loss 3.475111 | norm 0.2811 | time 462.5666 ms | tok/sec 1133432.4267
for step 4504 | loss 3.522640 | norm 0.3260 | time 464.9091 ms | tok/sec 1127721.5832
for step 4505 | loss 3.524302 | norm 0.3200 | time 462.5857 ms | tok/sec 1133385.6927
for step 4506 | loss 3.495669 | norm 0.3128 | time 463.2299 ms | tok/sec 1131809.5099
for step 4507 | loss 3.533394 | norm 0.3248 | time 463.2478 ms | tok/sec 1131765.8219
for step 4508 | loss 3.506403 | norm 0.2823 | time 463.8295 ms | tok/sec 1130346.3457
for step 4509 | loss 3.537879 | norm 0.3383 | time 464.4513 ms | tok/sec 1128833.0667
for step 4510 | loss 3.478185 | norm 0.3434 | time 463.9716 ms | tok/sec 1130000.1622
for step 4511 | loss 3.492597 | norm 0.3153 | time 464.5386 ms | tok/sec 1128621.0212
for step 4512 | loss 3.386011 | norm 0.3626 | time 464.1128 ms | tok/sec 1129656.5118
for step 4513 | loss 3.328631 | norm 0.3345 | time 464.1719 ms | tok/sec 1129512.6123
for step 4514 | loss 3.357425 | norm 0.3230 | time 463.9177 ms | tok/sec 1130131.4081
for step 4515 | loss 3.316063 | norm 0.3736 | time 463.9688 ms | tok/sec 1130007.1302
for step 4516 | loss 3.341238 | norm 0.3604 | time 465.0271 ms | tok/sec 1127435.3836
for step 4517 | loss 3.358798 | norm 0.3527 | time 464.8077 ms | tok/sec 1127967.4260
for step 4518 | loss 3.359800 | norm 0.3447 | time 464.6702 ms | tok/sec 1128301.3654
for step 4519 | loss 3.402993 | norm 0.3288 | time 465.2739 ms | tok/sec 1126837.4356
for step 4520 | loss 3.354870 | norm 0.3588 | time 465.0323 ms | tok/sec 1127422.6670
for step 4521 | loss 3.467289 | norm 0.3043 | time 465.2908 ms | tok/sec 1126796.4402
for step 4522 | loss 3.395208 | norm 0.3413 | time 464.7005 ms | tok/sec 1128227.8471
for step 4523 | loss 3.397384 | norm 0.3631 | time 465.0848 ms | tok/sec 1127295.5167
for step 4524 | loss 3.483817 | norm 0.3733 | time 465.2641 ms | tok/sec 1126861.1104
for step 4525 | loss 3.537621 | norm 0.3056 | time 465.6091 ms | tok/sec 1126026.1659
for step 4526 | loss 3.528814 | norm 0.3336 | time 465.0936 ms | tok/sec 1127274.1352
for step 4527 | loss 3.647875 | norm 0.2998 | time 464.6685 ms | tok/sec 1128305.4179
for step 4528 | loss 3.558280 | norm 0.3125 | time 464.3631 ms | tok/sec 1129047.5107
for step 4529 | loss 3.568453 | norm 0.3391 | time 466.4729 ms | tok/sec 1123941.0484
for step 4530 | loss 3.562366 | norm 0.3247 | time 464.4647 ms | tok/sec 1128800.6174
for step 4531 | loss 3.521797 | norm 0.3376 | time 463.3222 ms | tok/sec 1131584.1164
for step 4532 | loss 3.618709 | norm 0.3626 | time 464.0145 ms | tok/sec 1129895.6517
for step 4533 | loss 3.521631 | norm 0.4196 | time 464.3691 ms | tok/sec 1129033.0186
for step 4534 | loss 3.599166 | norm 0.4587 | time 463.9771 ms | tok/sec 1129986.8070
for step 4535 | loss 3.566925 | norm 0.3747 | time 464.8321 ms | tok/sec 1127908.4140
for step 4536 | loss 3.585026 | norm 0.4022 | time 464.8330 ms | tok/sec 1127906.0999
for step 4537 | loss 3.562960 | norm 0.3705 | time 465.3337 ms | tok/sec 1126692.5215
for step 4538 | loss 3.534978 | norm 0.3586 | time 464.8125 ms | tok/sec 1127955.8545
for step 4539 | loss 3.541322 | norm 0.3402 | time 465.0156 ms | tok/sec 1127463.1300
for step 4540 | loss 3.564833 | norm 0.3652 | time 465.3842 ms | tok/sec 1126570.1530
for step 4541 | loss 3.624801 | norm 0.3191 | time 465.0936 ms | tok/sec 1127274.1352
for step 4542 | loss 3.578709 | norm 0.3665 | time 465.6253 ms | tok/sec 1125986.9591
for step 4543 | loss 3.515200 | norm 0.3367 | time 465.7140 ms | tok/sec 1125772.5234
for step 4544 | loss 3.552731 | norm 0.3100 | time 466.1059 ms | tok/sec 1124825.8332
for step 4545 | loss 3.541963 | norm 0.3126 | time 466.0709 ms | tok/sec 1124910.4176
for step 4546 | loss 3.553269 | norm 0.3097 | time 465.5759 ms | tok/sec 1126106.3176
for step 4547 | loss 3.568020 | norm 0.3001 | time 465.4255 ms | tok/sec 1126470.3154
for step 4548 | loss 3.581256 | norm 0.2996 | time 465.5025 ms | tok/sec 1126283.9607
for step 4549 | loss 3.509212 | norm 0.3164 | time 465.3323 ms | tok/sec 1126695.9852
for step 4550 | loss 3.485003 | norm 0.3031 | time 465.4727 ms | tok/sec 1126356.0720
for step 4551 | loss 3.483802 | norm 0.3429 | time 465.1885 ms | tok/sec 1127044.1903
for step 4552 | loss 3.528862 | norm 0.3227 | time 466.5132 ms | tok/sec 1123843.9736
for step 4553 | loss 3.515886 | norm 0.2837 | time 464.7980 ms | tok/sec 1127991.1483
for step 4554 | loss 3.533369 | norm 0.3068 | time 465.5206 ms | tok/sec 1126240.1215
for step 4555 | loss 3.517293 | norm 0.3335 | time 465.9534 ms | tok/sec 1125194.1850
for step 4556 | loss 3.551370 | norm 0.2929 | time 465.0381 ms | tok/sec 1127408.7947
for step 4557 | loss 3.509924 | norm 0.3288 | time 466.6672 ms | tok/sec 1123473.0612
for step 4558 | loss 3.485214 | norm 0.3229 | time 464.8194 ms | tok/sec 1127939.0763
for step 4559 | loss 3.521618 | norm 0.3364 | time 465.3685 ms | tok/sec 1126608.2460
for step 4560 | loss 3.306327 | norm 0.3406 | time 466.0103 ms | tok/sec 1125056.6004
for step 4561 | loss 3.413854 | norm 0.3546 | time 466.2488 ms | tok/sec 1124481.2972
for step 4562 | loss 3.395449 | norm 0.3940 | time 466.2921 ms | tok/sec 1124376.6553
for step 4563 | loss 3.376060 | norm 0.3561 | time 465.9054 ms | tok/sec 1125309.9204
for step 4564 | loss 3.283778 | norm 0.3152 | time 466.0153 ms | tok/sec 1125044.5130
for step 4565 | loss 3.299176 | norm 0.2967 | time 465.5871 ms | tok/sec 1126079.2147
for step 4566 | loss 3.317391 | norm 0.3145 | time 465.6019 ms | tok/sec 1126043.4639
for step 4567 | loss 3.334715 | norm 0.3638 | time 465.5750 ms | tok/sec 1126108.6243
for step 4568 | loss 3.346063 | norm 0.3453 | time 467.0272 ms | tok/sec 1122607.0220
for step 4569 | loss 3.294414 | norm 0.3609 | time 466.3172 ms | tok/sec 1124316.2939
for step 4570 | loss 3.300712 | norm 0.3668 | time 465.7838 ms | tok/sec 1125603.6840
Will loading at 0 from edu_fineweb10B/edufineweb_train_000025.npy
for step 4571 | loss 3.432778 | norm 0.3423 | time 1486.9137 ms | tok/sec 352601.5038
for step 4572 | loss 3.551126 | norm 0.3286 | time 464.6287 ms | tok/sec 1128402.1069
for step 4573 | loss 3.532035 | norm 0.3581 | time 463.0599 ms | tok/sec 1132225.0048
for step 4574 | loss 3.506183 | norm 0.3011 | time 463.9974 ms | tok/sec 1129937.4536
for step 4575 | loss 3.550199 | norm 0.3186 | time 465.4210 ms | tok/sec 1126481.2793
for step 4576 | loss 3.634778 | norm 0.4031 | time 466.0676 ms | tok/sec 1124918.4740
for step 4577 | loss 3.549643 | norm 0.3711 | time 464.6599 ms | tok/sec 1128326.2596
for step 4578 | loss 3.549904 | norm 0.3237 | time 465.0295 ms | tok/sec 1127429.6033
for step 4579 | loss 3.562603 | norm 0.3376 | time 465.0133 ms | tok/sec 1127468.9106
for step 4580 | loss 3.510720 | norm 0.3666 | time 464.9298 ms | tok/sec 1127671.2709
for step 4581 | loss 3.591004 | norm 0.3361 | time 465.4133 ms | tok/sec 1126499.7454
for step 4582 | loss 3.516006 | norm 0.3291 | time 466.1481 ms | tok/sec 1124724.0034
for step 4583 | loss 3.573163 | norm 0.2913 | time 464.6125 ms | tok/sec 1128441.4821
for step 4584 | loss 3.561665 | norm 0.3215 | time 466.1717 ms | tok/sec 1124667.0558
for step 4585 | loss 3.564868 | norm 0.3127 | time 465.4596 ms | tok/sec 1126387.8040
for step 4586 | loss 3.540073 | norm 0.2703 | time 465.5812 ms | tok/sec 1126093.6310
for step 4587 | loss 3.511087 | norm 0.2880 | time 465.9774 ms | tok/sec 1125136.0385
for step 4588 | loss 3.591248 | norm 0.2870 | time 467.8216 ms | tok/sec 1120700.7162
for step 4589 | loss 3.608881 | norm 0.2871 | time 466.1593 ms | tok/sec 1124696.9670
for step 4590 | loss 3.587358 | norm 0.3011 | time 466.3739 ms | tok/sec 1124179.4985
for step 4591 | loss 3.476201 | norm 0.3121 | time 466.1973 ms | tok/sec 1124605.5129
for step 4592 | loss 3.601256 | norm 0.3339 | time 466.0053 ms | tok/sec 1125068.6881
for step 4593 | loss 3.550392 | norm 0.3209 | time 465.6990 ms | tok/sec 1125808.8334
for step 4594 | loss 3.548220 | norm 0.2914 | time 465.2967 ms | tok/sec 1126782.0059
for step 4595 | loss 3.490248 | norm 0.3004 | time 465.9371 ms | tok/sec 1125233.3366
for step 4596 | loss 3.519808 | norm 0.3072 | time 465.6858 ms | tok/sec 1125840.5345
for step 4597 | loss 3.547960 | norm 0.3630 | time 466.4040 ms | tok/sec 1124107.0909
for step 4598 | loss 3.515455 | norm 0.3550 | time 465.5144 ms | tok/sec 1126255.1187
for step 4599 | loss 3.521055 | norm 0.3997 | time 465.4152 ms | tok/sec 1126495.1288
for step 4600 | loss 3.497437 | norm 0.3456 | time 466.8903 ms | tok/sec 1122936.0743
for step 4601 | loss 3.507703 | norm 0.3435 | time 465.5831 ms | tok/sec 1126089.0177
for step 4602 | loss 3.476360 | norm 0.3162 | time 465.2567 ms | tok/sec 1126879.0114
for step 4603 | loss 3.515066 | norm 0.3276 | time 466.0065 ms | tok/sec 1125065.8101
for step 4604 | loss 3.501056 | norm 0.3309 | time 465.3227 ms | tok/sec 1126719.0767
for step 4605 | loss 3.492230 | norm 0.3031 | time 465.3296 ms | tok/sec 1126702.3352
for step 4606 | loss 3.530808 | norm 0.3325 | time 465.2157 ms | tok/sec 1126978.3440
for step 4607 | loss 3.357504 | norm 0.3523 | time 466.3954 ms | tok/sec 1124127.7779
for step 4608 | loss 3.362872 | norm 0.3211 | time 464.7577 ms | tok/sec 1128088.9409
for step 4609 | loss 3.323017 | norm 0.3063 | time 465.6036 ms | tok/sec 1126039.4276
for step 4610 | loss 3.341043 | norm 0.3214 | time 465.1172 ms | tok/sec 1127216.9291
for step 4611 | loss 3.335428 | norm 0.3353 | time 466.7058 ms | tok/sec 1123380.0844
for step 4612 | loss 3.292668 | norm 0.3507 | time 465.9388 ms | tok/sec 1125229.3062
for step 4613 | loss 3.380965 | norm 0.3428 | time 466.2857 ms | tok/sec 1124392.1779
for step 4614 | loss 3.366180 | norm 0.3246 | time 466.2678 ms | tok/sec 1124435.2984
for step 4615 | loss 3.378400 | norm 0.3008 | time 466.0165 ms | tok/sec 1125041.6351
for step 4616 | loss 3.342885 | norm 0.3270 | time 465.5602 ms | tok/sec 1126144.3793
for step 4617 | loss 3.333010 | norm 0.2967 | time 466.1226 ms | tok/sec 1124785.5593
for step 4618 | loss 3.515967 | norm 0.3001 | time 465.7233 ms | tok/sec 1125750.0470
for step 4619 | loss 3.548350 | norm 0.3378 | time 465.3659 ms | tok/sec 1126614.5951
for step 4620 | loss 3.560965 | norm 0.3212 | time 465.0705 ms | tok/sec 1127330.1912
for step 4621 | loss 3.567128 | norm 0.2860 | time 465.0826 ms | tok/sec 1127300.7178
for step 4622 | loss 3.595948 | norm 0.2998 | time 464.7932 ms | tok/sec 1128002.7205
for step 4623 | loss 3.564131 | norm 0.3483 | time 466.0294 ms | tok/sec 1125010.5545
for step 4624 | loss 3.542359 | norm 0.3478 | time 466.0962 ms | tok/sec 1124849.4235
for step 4625 | loss 3.580434 | norm 0.3317 | time 465.7609 ms | tok/sec 1125658.9978
for step 4626 | loss 3.529223 | norm 0.3249 | time 465.7171 ms | tok/sec 1125765.0312
for step 4627 | loss 3.508759 | norm 0.3684 | time 464.8583 ms | tok/sec 1127844.7804
for step 4628 | loss 3.591896 | norm 0.3361 | time 465.2698 ms | tok/sec 1126847.2518
for step 4629 | loss 3.560033 | norm 0.3431 | time 465.6799 ms | tok/sec 1125854.9447
for step 4630 | loss 3.596024 | norm 0.3382 | time 465.1680 ms | tok/sec 1127093.8689
for step 4631 | loss 3.562781 | norm 0.3290 | time 465.1413 ms | tok/sec 1127158.5733
for step 4632 | loss 3.576152 | norm 0.3166 | time 465.5094 ms | tok/sec 1126267.2321
for step 4633 | loss 3.550395 | norm 0.3772 | time 465.4987 ms | tok/sec 1126293.1904
for step 4634 | loss 3.534084 | norm 0.3664 | time 465.2255 ms | tok/sec 1126954.6643
for step 4635 | loss 3.582299 | norm 0.3653 | time 466.1312 ms | tok/sec 1124764.8481
for step 4636 | loss 3.564117 | norm 0.3409 | time 465.5130 ms | tok/sec 1126258.5797
for step 4637 | loss 3.573288 | norm 0.3262 | time 465.5824 ms | tok/sec 1126090.7477
for step 4638 | loss 3.506682 | norm 0.3417 | time 464.5183 ms | tok/sec 1128670.2596
for step 4639 | loss 3.535451 | norm 0.3328 | time 466.4094 ms | tok/sec 1124093.8747
for step 4640 | loss 3.567507 | norm 0.3312 | time 465.6308 ms | tok/sec 1125973.6987
for step 4641 | loss 3.529085 | norm 0.2922 | time 465.4853 ms | tok/sec 1126325.4957
for step 4642 | loss 3.509957 | norm 0.3413 | time 465.6367 ms | tok/sec 1125959.2854
for step 4643 | loss 3.522691 | norm 0.2883 | time 465.8411 ms | tok/sec 1125465.4232
for step 4644 | loss 3.499558 | norm 0.3346 | time 465.1897 ms | tok/sec 1127041.3021
for step 4645 | loss 3.458140 | norm 0.3201 | time 465.9774 ms | tok/sec 1125136.0385
for step 4646 | loss 3.524221 | norm 0.3342 | time 465.2715 ms | tok/sec 1126843.2098
for step 4647 | loss 3.523605 | norm 0.3256 | time 464.9649 ms | tok/sec 1127586.2707
for step 4648 | loss 3.513457 | norm 0.2965 | time 465.5678 ms | tok/sec 1126125.9248
for step 4649 | loss 3.512877 | norm 0.3280 | time 465.2698 ms | tok/sec 1126847.2518
for step 4650 | loss 3.518080 | norm 0.3150 | time 465.7328 ms | tok/sec 1125726.9952
for step 4651 | loss 3.503091 | norm 0.3355 | time 465.4524 ms | tok/sec 1126405.1131
for step 4652 | loss 3.505816 | norm 0.3391 | time 465.8606 ms | tok/sec 1125418.1918
for step 4653 | loss 3.309125 | norm 0.3406 | time 465.4300 ms | tok/sec 1126459.3516
for step 4654 | loss 3.287356 | norm 0.3797 | time 465.4026 ms | tok/sec 1126525.7144
for step 4655 | loss 3.292145 | norm 0.3607 | time 464.8654 ms | tok/sec 1127827.4271
for step 4656 | loss 3.319865 | norm 0.3483 | time 464.9887 ms | tok/sec 1127528.4548
for step 4657 | loss 3.309266 | norm 0.3323 | time 465.9901 ms | tok/sec 1125105.5283
for step 4658 | loss 3.311951 | norm 0.3495 | time 464.8979 ms | tok/sec 1127748.7652
for step 4659 | loss 3.358491 | norm 0.2994 | time 465.9770 ms | tok/sec 1125137.1898
for step 4660 | loss 3.374684 | norm 0.3046 | time 465.9581 ms | tok/sec 1125182.6704
for step 4661 | loss 3.300586 | norm 0.3453 | time 465.3168 ms | tok/sec 1126733.5094
for step 4662 | loss 3.376386 | norm 0.3697 | time 464.8125 ms | tok/sec 1127955.8545
for step 4663 | loss 3.371263 | norm 0.3532 | time 465.2851 ms | tok/sec 1126810.2974
for step 4664 | loss 3.257470 | norm 0.3559 | time 465.5819 ms | tok/sec 1126091.9010
for step 4665 | loss 3.575459 | norm 0.3980 | time 464.4094 ms | tok/sec 1128935.0623
for step 4666 | loss 3.547713 | norm 0.3637 | time 465.1155 ms | tok/sec 1127220.9738
for step 4667 | loss 3.551210 | norm 0.3415 | time 464.9591 ms | tok/sec 1127600.1474
for step 4668 | loss 3.553155 | norm 0.4033 | time 465.1880 ms | tok/sec 1127045.3456
for step 4669 | loss 3.540087 | norm 0.3669 | time 465.5261 ms | tok/sec 1126226.8550
for step 4670 | loss 3.569119 | norm 0.3582 | time 465.5712 ms | tok/sec 1126117.8512
for step 4671 | loss 3.567126 | norm 0.4176 | time 464.9322 ms | tok/sec 1127665.4882
for step 4672 | loss 3.540517 | norm 0.3895 | time 465.8332 ms | tok/sec 1125484.4320
for step 4673 | loss 3.622316 | norm 0.3871 | time 467.2565 ms | tok/sec 1122055.9765
for step 4674 | loss 3.564307 | norm 0.3660 | time 465.3780 ms | tok/sec 1126585.1590
for step 4675 | loss 3.503024 | norm 0.3627 | time 466.2158 ms | tok/sec 1124560.6541
for step 4676 | loss 3.527749 | norm 0.3018 | time 465.4820 ms | tok/sec 1126333.5723
for step 4677 | loss 3.593508 | norm 0.3164 | time 465.0235 ms | tok/sec 1127444.0542
for step 4678 | loss 3.492187 | norm 0.3491 | time 465.8115 ms | tok/sec 1125536.8537
for step 4679 | loss 3.569120 | norm 0.3025 | time 465.6703 ms | tok/sec 1125878.0018
for step 4680 | loss 3.541555 | norm 0.3293 | time 466.2936 ms | tok/sec 1124373.2059
for step 4681 | loss 3.547029 | norm 0.3049 | time 465.9967 ms | tok/sec 1125089.4104
for step 4682 | loss 3.547381 | norm 0.2807 | time 465.1089 ms | tok/sec 1127237.1528
for step 4683 | loss 3.539674 | norm 0.3107 | time 465.7428 ms | tok/sec 1125702.7918
for step 4684 | loss 3.548120 | norm 0.2877 | time 465.9486 ms | tok/sec 1125205.6999
for step 4685 | loss 3.531480 | norm 0.2940 | time 465.0559 ms | tok/sec 1127365.4458
for step 4686 | loss 3.533766 | norm 0.2860 | time 465.8358 ms | tok/sec 1125478.0957
for step 4687 | loss 3.522097 | norm 0.3036 | time 465.9755 ms | tok/sec 1125140.6439
for step 4688 | loss 3.458821 | norm 0.3118 | time 464.7996 ms | tok/sec 1127987.0981
for step 4689 | loss 3.516524 | norm 0.2811 | time 466.1660 ms | tok/sec 1124680.8608
for step 4690 | loss 3.494578 | norm 0.3056 | time 465.9166 ms | tok/sec 1125282.8557
for step 4691 | loss 3.428355 | norm 0.3541 | time 465.4670 ms | tok/sec 1126369.9185
for step 4692 | loss 3.476628 | norm 0.2995 | time 465.6489 ms | tok/sec 1125929.8836
for step 4693 | loss 3.523447 | norm 0.3342 | time 464.9918 ms | tok/sec 1127520.9392
for step 4694 | loss 3.527626 | norm 0.3277 | time 465.1322 ms | tok/sec 1127180.5282
for step 4695 | loss 3.466381 | norm 0.3410 | time 465.6262 ms | tok/sec 1125984.6530
for step 4696 | loss 3.486312 | norm 0.3343 | time 464.8089 ms | tok/sec 1127964.5331
for step 4697 | loss 3.470663 | norm 0.3337 | time 465.1251 ms | tok/sec 1127197.8617
for step 4698 | loss 3.473839 | norm 0.3172 | time 464.8218 ms | tok/sec 1127933.2908
for step 4699 | loss 3.405446 | norm 0.2998 | time 464.9339 ms | tok/sec 1127661.4403
for step 4700 | loss 3.355622 | norm 0.3297 | time 466.2180 ms | tok/sec 1124555.4783
for step 4701 | loss 3.337475 | norm 0.3652 | time 465.7791 ms | tok/sec 1125615.2072
for step 4702 | loss 3.381852 | norm 0.4206 | time 464.9370 ms | tok/sec 1127653.9229
for step 4703 | loss 3.282881 | norm 0.3809 | time 464.4051 ms | tok/sec 1128945.4947
for step 4704 | loss 3.338686 | norm 0.3621 | time 464.5157 ms | tok/sec 1128676.6319
for step 4705 | loss 3.347054 | norm 0.3808 | time 465.6303 ms | tok/sec 1125974.8517
for step 4706 | loss 3.352871 | norm 0.4509 | time 465.9486 ms | tok/sec 1125205.6999
for step 4707 | loss 3.378359 | norm 0.3759 | time 464.7303 ms | tok/sec 1128155.4958
for step 4708 | loss 3.387920 | norm 0.3315 | time 464.5426 ms | tok/sec 1128611.1740
for step 4709 | loss 3.389824 | norm 0.3675 | time 465.1325 ms | tok/sec 1127179.9504
for step 4710 | loss 3.284769 | norm 0.3035 | time 465.3184 ms | tok/sec 1126729.4682
for step 4711 | loss 3.518685 | norm 0.3657 | time 464.4837 ms | tok/sec 1128754.2645
for step 4712 | loss 3.508562 | norm 0.3329 | time 465.4126 ms | tok/sec 1126501.4767
for step 4713 | loss 3.583239 | norm 0.3106 | time 464.8149 ms | tok/sec 1127950.0689
for step 4714 | loss 3.499321 | norm 0.3347 | time 464.9241 ms | tok/sec 1127685.1497
for step 4715 | loss 3.630347 | norm 0.3376 | time 465.7309 ms | tok/sec 1125731.6055
for step 4716 | loss 3.613225 | norm 0.3450 | time 465.0736 ms | tok/sec 1127322.6782
for step 4717 | loss 3.573527 | norm 0.3175 | time 465.7173 ms | tok/sec 1125764.4549
for step 4718 | loss 3.594406 | norm 0.4214 | time 464.6146 ms | tok/sec 1128436.2705
for step 4719 | loss 3.591485 | norm 0.3858 | time 464.7732 ms | tok/sec 1128051.3263
for step 4720 | loss 3.552238 | norm 0.3874 | time 465.2152 ms | tok/sec 1126979.4991
for step 4721 | loss 3.506489 | norm 0.3604 | time 465.3459 ms | tok/sec 1126663.0814
for step 4722 | loss 3.498811 | norm 0.3274 | time 465.5039 ms | tok/sec 1126280.4995
for step 4723 | loss 3.565538 | norm 0.3062 | time 464.9634 ms | tok/sec 1127589.7399
for step 4724 | loss 3.559171 | norm 0.3038 | time 464.2453 ms | tok/sec 1129333.9494
for step 4725 | loss 3.490331 | norm 0.2721 | time 466.0394 ms | tok/sec 1124986.3819
for step 4726 | loss 3.545835 | norm 0.2887 | time 465.9429 ms | tok/sec 1125219.5181
for step 4727 | loss 3.618748 | norm 0.2942 | time 465.4822 ms | tok/sec 1126332.9954
for step 4728 | loss 3.541536 | norm 0.3537 | time 465.7154 ms | tok/sec 1125769.0655
for step 4729 | loss 3.530216 | norm 0.3725 | time 465.0991 ms | tok/sec 1127260.8444
for step 4730 | loss 3.569811 | norm 0.3298 | time 465.4658 ms | tok/sec 1126372.8032
for step 4731 | loss 3.552099 | norm 0.2844 | time 464.5772 ms | tok/sec 1128527.1904
for step 4732 | loss 3.574299 | norm 0.3195 | time 465.4560 ms | tok/sec 1126396.4584
for step 4733 | loss 3.514252 | norm 0.2901 | time 465.4143 ms | tok/sec 1126497.4371
for step 4734 | loss 3.527127 | norm 0.2918 | time 465.3411 ms | tok/sec 1126674.6263
for step 4735 | loss 3.466121 | norm 0.2911 | time 465.6019 ms | tok/sec 1126043.4639
for step 4736 | loss 3.477754 | norm 0.3107 | time 465.3492 ms | tok/sec 1126655.0000
for step 4737 | loss 3.515231 | norm 0.3336 | time 465.5111 ms | tok/sec 1126263.1943
for step 4738 | loss 3.501362 | norm 0.3107 | time 464.0346 ms | tok/sec 1129846.8868
for step 4739 | loss 3.484329 | norm 0.2748 | time 465.2727 ms | tok/sec 1126840.3227
for step 4740 | loss 3.510712 | norm 0.3132 | time 465.1275 ms | tok/sec 1127192.0838
for step 4741 | loss 3.540276 | norm 0.3109 | time 465.5159 ms | tok/sec 1126251.6578
for step 4742 | loss 3.494430 | norm 0.3210 | time 465.8086 ms | tok/sec 1125543.7668
for step 4743 | loss 3.493500 | norm 0.3111 | time 465.7481 ms | tok/sec 1125690.1142
for step 4744 | loss 3.505831 | norm 0.2777 | time 465.6155 ms | tok/sec 1126010.5982
for step 4745 | loss 3.335689 | norm 0.3111 | time 464.9391 ms | tok/sec 1127648.7186
for step 4746 | loss 3.324151 | norm 0.3259 | time 465.1728 ms | tok/sec 1127082.3154
for step 4747 | loss 3.359262 | norm 0.3505 | time 465.7302 ms | tok/sec 1125733.3343
for step 4748 | loss 3.402775 | norm 0.4048 | time 465.2784 ms | tok/sec 1126826.4647
for step 4749 | loss 3.381086 | norm 0.4224 | time 465.7185 ms | tok/sec 1125761.5733
validation loss 3.5524
HellaSwag accuracy: 2682/10042=0.2671
> Hello, I'm a language model, and I'm a language model. Each model, which is available in the Microsoft Word 2010-2010, is a model
> Hello, I'm a language model, a model, a model, a model, a model to be used to develop problem-solving and problem-s
> Hello, I'm a language model, but are very, very intuitive. I'm not a native speaker of this language. I'm trying to learn some vocabulary
> Hello, I'm a language model, and I'm an expert in language arts through the Internet research platform Coding Solutions. From early childhood, I'll be
> Hello, I'm a language model, so I know that to create a language diagram, it's not a linear way of structuring objects. However, it
> Hello, I'm a language model, looking to understand basic vocabulary and grammatical structure, syntax, and even to develop language model skills. As you probably already
> Hello, I'm a language model, I started researching an idea with a linguist, and I began exploring the concept of linguistics. A few ideas came
> Hello, I'm a language model, and it explains things that we don't need for each other... I got the code for this example, and I'll
> Hello, I'm a language model, and I love to learn something that all readers don't even know (e.g. because the word "language"),
> > Hello, I'm a language model, but also a language model.
Today, you do this:
Use it to help you learn language and speak flu
Hello, I'm a language model, and we are learning to describe and apply it in different languages and contexts. I'll see how the language used in each
> > > Hello, I'm a language model, and I'm going to go over to an overview of that approach to math—I really do my physics experiments and get
Hello, I'm a language model, so I love using grammar expressions. If you do use the expression of any of the above words, you can also give
Hello, I'm a language model, but I're trying to use it myself only for you. So you could use it to model your data, but this
> > > Hello, I'm a language model, and a very good way to explain and explain language. However, it's also a very good way to develop skills,
> > Hello, I'm a language model, in addition to the concept of non-linearity with respect for simplicity and a great deal of non-equilibrium states
Hello, I'm a language model, and is probably not a real-time language guru. I'm still using it and am in a really cool, low
> Hello, I'm a language model, so I've got to write an introductory vocabulary and a brief introduction to your grammar section. I was inspired by the concept
Hello, I'm a language model, but my goal is to understand the difference between a human language and a human language. I'm looking at how this might
> Hello, I'm a language model, and you are very helpful. For example, this topic is related to a number of languages (Gree, Russian,
> > Hello, I'm a language model, and I have a large amount of experience understanding how I can use it at home and abroad, but it also has a
> Hello, I'm a language model, but what is it that makes it possible to implement something new every time you work or write from there. This means that
Hello, I'm a language model, and I'm also part of my class. A lot of students really enjoy writing for me.
How long does it
> Hello, I'm a language model, and like I mentioned earlier, they have been quite helpful when it comes to the question of whether or not a language model
> Hello, I'm a language model, but I haven't heard the word yet. But I'm a speaker and we're using it to say 'a',
Hello, I'm a language model, and I use the program to describe things. I'm the language model.
- Write a sentence in the sentence "
> Hello, I'm a language model, and I'm teaching it in an introductory course at the University of Massachusetts, D.Tech Community College (UMSL).> 
Hello, I'm a language model, based on my Spanish grammar but my Spanish vocabulary is so complex that it is easier to remember and respond in Spanish than you
> Hello, I'm a language model, and I'm a bilingual learner. Some of my goals are to give a broad definition of concepts and concepts, but
> Hello, I'm a language model, no word-processing system. You really get to communicate clearly and consistently.
2. What is a multilingual environment
> Hello, I'm a language model, and I am passionate about teaching these core language skills in a way that both students and teachers can understand. I believe the
> Hello, I'm a language model, linguistics is a type of model by which students can learn about their natural language and its relation to one another. And
for step 4750 | loss 3.390095 | norm 0.3923 | time 12893.4131 ms | tok/sec 40663.2439
for step 4751 | loss 3.316371 | norm 0.3606 | time 462.7411 ms | tok/sec 1133004.9537
for step 4752 | loss 3.304380 | norm 0.2933 | time 462.7049 ms | tok/sec 1133093.6921
for step 4753 | loss 3.292850 | norm 0.3814 | time 463.4523 ms | tok/sec 1131266.2720
for step 4754 | loss 3.368279 | norm 0.3464 | time 463.6433 ms | tok/sec 1130800.3069
for step 4755 | loss 3.319022 | norm 0.3071 | time 463.6641 ms | tok/sec 1130749.7195
for step 4756 | loss 3.327080 | norm 0.3123 | time 463.8042 ms | tok/sec 1130407.9375
for step 4757 | loss 3.518903 | norm 0.3464 | time 464.6444 ms | tok/sec 1128363.8925
for step 4758 | loss 3.550942 | norm 0.3425 | time 463.9735 ms | tok/sec 1129995.5168
for step 4759 | loss 3.560577 | norm 0.3003 | time 464.1132 ms | tok/sec 1129655.3512
for step 4760 | loss 3.554189 | norm 0.3275 | time 464.2715 ms | tok/sec 1129270.1549
for step 4761 | loss 3.596850 | norm 0.3221 | time 465.1704 ms | tok/sec 1127088.0921
Will loading at 0 from edu_fineweb10B/edufineweb_train_000026.npy
for step 4762 | loss 3.511656 | norm 0.2953 | time 1400.9519 ms | tok/sec 374236.9842
for step 4763 | loss 3.594130 | norm 0.3203 | time 462.9781 ms | tok/sec 1132424.9940
for step 4764 | loss 3.569551 | norm 0.3344 | time 463.0892 ms | tok/sec 1132153.3058
for step 4765 | loss 3.565464 | norm 0.3546 | time 463.6145 ms | tok/sec 1130870.6715
for step 4766 | loss 3.548974 | norm 0.3276 | time 463.1596 ms | tok/sec 1131981.3817
for step 4767 | loss 3.506531 | norm 0.3335 | time 463.1615 ms | tok/sec 1131976.7201
for step 4768 | loss 3.545156 | norm 0.3184 | time 464.2437 ms | tok/sec 1129338.0093
for step 4769 | loss 3.514693 | norm 0.3223 | time 463.9971 ms | tok/sec 1129938.0342
for step 4770 | loss 3.527449 | norm 0.3204 | time 464.3023 ms | tok/sec 1129195.3506
for step 4771 | loss 3.547876 | norm 0.3067 | time 464.1287 ms | tok/sec 1129617.6321
for step 4772 | loss 3.560064 | norm 0.3465 | time 464.1290 ms | tok/sec 1129617.0518
for step 4773 | loss 3.510477 | norm 0.2893 | time 464.9711 ms | tok/sec 1127571.2380
for step 4774 | loss 3.503724 | norm 0.3240 | time 465.2717 ms | tok/sec 1126842.6324
for step 4775 | loss 3.590548 | norm 0.3301 | time 465.0109 ms | tok/sec 1127474.6913
for step 4776 | loss 3.511326 | norm 0.3296 | time 464.4310 ms | tok/sec 1128882.3236
for step 4777 | loss 3.492037 | norm 0.3109 | time 464.6959 ms | tok/sec 1128238.8453
for step 4778 | loss 3.535095 | norm 0.3250 | time 464.5660 ms | tok/sec 1128554.4113
for step 4779 | loss 3.559000 | norm 0.3333 | time 466.3141 ms | tok/sec 1124323.7668
for step 4780 | loss 3.486149 | norm 0.3216 | time 465.9352 ms | tok/sec 1125237.9429
for step 4781 | loss 3.502287 | norm 0.3448 | time 465.7423 ms | tok/sec 1125703.9443
for step 4782 | loss 3.476587 | norm 0.4039 | time 466.6529 ms | tok/sec 1123507.5010
for step 4783 | loss 3.517447 | norm 0.4547 | time 465.5087 ms | tok/sec 1126268.9627
for step 4784 | loss 3.501943 | norm 0.4335 | time 466.5916 ms | tok/sec 1123655.0418
for step 4785 | loss 3.537677 | norm 0.3251 | time 466.2035 ms | tok/sec 1124590.5595
for step 4786 | loss 3.543060 | norm 0.3624 | time 465.9176 ms | tok/sec 1125280.5524
for step 4787 | loss 3.474626 | norm 0.3568 | time 466.6913 ms | tok/sec 1123415.0924
for step 4788 | loss 3.494985 | norm 0.3332 | time 465.3418 ms | tok/sec 1126672.8946
for step 4789 | loss 3.489626 | norm 0.3154 | time 465.4233 ms | tok/sec 1126475.5088
for step 4790 | loss 3.467581 | norm 0.3506 | time 465.5073 ms | tok/sec 1126272.4237
for step 4791 | loss 3.509825 | norm 0.3192 | time 467.8686 ms | tok/sec 1120588.2111
for step 4792 | loss 3.375687 | norm 0.3356 | time 465.5607 ms | tok/sec 1126143.2259
for step 4793 | loss 3.387530 | norm 0.3657 | time 465.7464 ms | tok/sec 1125694.1480
for step 4794 | loss 3.286577 | norm 0.3501 | time 465.4708 ms | tok/sec 1126360.6875
for step 4795 | loss 3.325589 | norm 0.3464 | time 465.3227 ms | tok/sec 1126719.0767
for step 4796 | loss 3.331516 | norm 0.3585 | time 466.1350 ms | tok/sec 1124755.6434
for step 4797 | loss 3.250733 | norm 0.3369 | time 465.6746 ms | tok/sec 1125867.6260
for step 4798 | loss 3.394085 | norm 0.3408 | time 464.8767 ms | tok/sec 1127800.2412
for step 4799 | loss 3.366842 | norm 0.3520 | time 465.0059 ms | tok/sec 1127486.8310
for step 4800 | loss 3.355608 | norm 0.3636 | time 465.8840 ms | tok/sec 1125361.7499
for step 4801 | loss 3.376622 | norm 0.3286 | time 465.6754 ms | tok/sec 1125865.8967
for step 4802 | loss 3.382510 | norm 0.2925 | time 465.8637 ms | tok/sec 1125410.7043
for step 4803 | loss 3.348432 | norm 0.2890 | time 466.0535 ms | tok/sec 1124952.4269
for step 4804 | loss 3.531828 | norm 0.3166 | time 465.6508 ms | tok/sec 1125925.2717
for step 4805 | loss 3.583940 | norm 0.3982 | time 465.5898 ms | tok/sec 1126072.8716
for step 4806 | loss 3.547919 | norm 0.3814 | time 465.8875 ms | tok/sec 1125353.1113
for step 4807 | loss 3.590711 | norm 0.3555 | time 465.5609 ms | tok/sec 1126142.6491
for step 4808 | loss 3.593998 | norm 0.4240 | time 465.5581 ms | tok/sec 1126149.5697
for step 4809 | loss 3.547991 | norm 0.3902 | time 465.6894 ms | tok/sec 1125831.8886
for step 4810 | loss 3.539500 | norm 0.3465 | time 465.5030 ms | tok/sec 1126282.8070
for step 4811 | loss 3.512415 | norm 0.3342 | time 465.0438 ms | tok/sec 1127394.9227
for step 4812 | loss 3.509784 | norm 0.3379 | time 464.9439 ms | tok/sec 1127637.1537
for step 4813 | loss 3.521523 | norm 0.3432 | time 465.2081 ms | tok/sec 1126996.8264
for step 4814 | loss 3.561936 | norm 0.3442 | time 464.9174 ms | tok/sec 1127701.3421
for step 4815 | loss 3.525382 | norm 0.3379 | time 465.3425 ms | tok/sec 1126671.1628
for step 4816 | loss 3.512102 | norm 0.3197 | time 463.9947 ms | tok/sec 1129943.8403
for step 4817 | loss 3.547379 | norm 0.3110 | time 464.5872 ms | tok/sec 1128502.8664
for step 4818 | loss 3.519145 | norm 0.3294 | time 465.3041 ms | tok/sec 1126764.1079
for step 4819 | loss 3.599647 | norm 0.3467 | time 465.2576 ms | tok/sec 1126876.7016
for step 4820 | loss 3.477100 | norm 0.3559 | time 465.2247 ms | tok/sec 1126956.3969
for step 4821 | loss 3.506351 | norm 0.3114 | time 466.0773 ms | tok/sec 1124894.8808
for step 4822 | loss 3.531238 | norm 0.3249 | time 466.0566 ms | tok/sec 1124944.9456
for step 4823 | loss 3.508333 | norm 0.3407 | time 464.9079 ms | tok/sec 1127724.4748
for step 4824 | loss 3.548329 | norm 0.3215 | time 464.4840 ms | tok/sec 1128753.6851
for step 4825 | loss 3.548542 | norm 0.3329 | time 465.2274 ms | tok/sec 1126950.0440
for step 4826 | loss 3.622118 | norm 0.3155 | time 464.8380 ms | tok/sec 1127893.9512
for step 4827 | loss 3.480048 | norm 0.3247 | time 466.0823 ms | tok/sec 1124882.7968
for step 4828 | loss 3.457718 | norm 0.2947 | time 466.2280 ms | tok/sec 1124531.3252
for step 4829 | loss 3.514643 | norm 0.2771 | time 464.8261 ms | tok/sec 1127922.8771
for step 4830 | loss 3.579828 | norm 0.3229 | time 465.3637 ms | tok/sec 1126619.7898
for step 4831 | loss 3.517781 | norm 0.3145 | time 464.8862 ms | tok/sec 1127777.1054
for step 4832 | loss 3.549061 | norm 0.2836 | time 464.9365 ms | tok/sec 1127655.0794
for step 4833 | loss 3.467407 | norm 0.2848 | time 465.6014 ms | tok/sec 1126044.6171
for step 4834 | loss 3.563327 | norm 0.2836 | time 467.6447 ms | tok/sec 1121124.6691
for step 4835 | loss 3.432018 | norm 0.3118 | time 465.8687 ms | tok/sec 1125398.6093
for step 4836 | loss 3.472943 | norm 0.3186 | time 465.0655 ms | tok/sec 1127342.3278
for step 4837 | loss 3.500292 | norm 0.3023 | time 465.4326 ms | tok/sec 1126453.0043
for step 4838 | loss 3.469917 | norm 0.3058 | time 465.6253 ms | tok/sec 1125986.9591
for step 4839 | loss 3.334450 | norm 0.2812 | time 465.3387 ms | tok/sec 1126680.3989
for step 4840 | loss 3.371139 | norm 0.3464 | time 465.4763 ms | tok/sec 1126347.4182
for step 4841 | loss 3.312626 | norm 0.3916 | time 465.8802 ms | tok/sec 1125370.9645
for step 4842 | loss 3.364710 | norm 0.3885 | time 466.1641 ms | tok/sec 1124685.4625
for step 4843 | loss 3.351053 | norm 0.3410 | time 465.9355 ms | tok/sec 1125237.3671
for step 4844 | loss 3.276327 | norm 0.3027 | time 465.2598 ms | tok/sec 1126871.5045
for step 4845 | loss 3.310121 | norm 0.3570 | time 465.1225 ms | tok/sec 1127204.2174
for step 4846 | loss 3.287427 | norm 0.3618 | time 466.4719 ms | tok/sec 1123943.3463
for step 4847 | loss 3.321089 | norm 0.3285 | time 466.0816 ms | tok/sec 1124884.5231
for step 4848 | loss 3.308119 | norm 0.3401 | time 465.3866 ms | tok/sec 1126564.3816
for step 4849 | loss 3.270846 | norm 0.3190 | time 465.8992 ms | tok/sec 1125324.8928
for step 4850 | loss 3.297434 | norm 0.3491 | time 466.6839 ms | tok/sec 1123432.8842
for step 4851 | loss 3.542008 | norm 0.3686 | time 466.1732 ms | tok/sec 1124663.6046
for step 4852 | loss 3.524597 | norm 0.3896 | time 465.2116 ms | tok/sec 1126988.1627
for step 4853 | loss 3.509458 | norm 0.3522 | time 465.5120 ms | tok/sec 1126260.8870
for step 4854 | loss 3.591853 | norm 0.3430 | time 466.2373 ms | tok/sec 1124508.8983
for step 4855 | loss 3.587092 | norm 0.3289 | time 465.1442 ms | tok/sec 1127151.6403
for step 4856 | loss 3.503072 | norm 0.3327 | time 465.3835 ms | tok/sec 1126571.8844
for step 4857 | loss 3.541957 | norm 0.3079 | time 465.4515 ms | tok/sec 1126407.4210
for step 4858 | loss 3.531796 | norm 0.3255 | time 466.2170 ms | tok/sec 1124557.7786
for step 4859 | loss 3.561430 | norm 0.3218 | time 465.6003 ms | tok/sec 1126047.5001
for step 4860 | loss 3.527962 | norm 0.2993 | time 465.4713 ms | tok/sec 1126359.5336
for step 4861 | loss 3.538285 | norm 0.3491 | time 465.6656 ms | tok/sec 1125889.5306
for step 4862 | loss 3.553491 | norm 0.4031 | time 465.2641 ms | tok/sec 1126861.1104
for step 4863 | loss 3.555191 | norm 0.3549 | time 465.1213 ms | tok/sec 1127207.1064
for step 4864 | loss 3.538946 | norm 0.3090 | time 466.2757 ms | tok/sec 1124416.3250
for step 4865 | loss 3.583894 | norm 0.3152 | time 465.2052 ms | tok/sec 1127003.7574
for step 4866 | loss 3.629663 | norm 0.3357 | time 465.9066 ms | tok/sec 1125307.0411
for step 4867 | loss 3.580414 | norm 0.3636 | time 465.9767 ms | tok/sec 1125137.7655
for step 4868 | loss 3.576191 | norm 0.3171 | time 466.1694 ms | tok/sec 1124672.8078
for step 4869 | loss 3.588668 | norm 0.3198 | time 465.4641 ms | tok/sec 1126376.8418
for step 4870 | loss 3.465748 | norm 0.3312 | time 465.5054 ms | tok/sec 1126277.0385
for step 4871 | loss 3.531956 | norm 0.3016 | time 465.8682 ms | tok/sec 1125399.7612
for step 4872 | loss 3.549810 | norm 0.3083 | time 465.9572 ms | tok/sec 1125184.9733
for step 4873 | loss 3.560223 | norm 0.3280 | time 465.2793 ms | tok/sec 1126824.1551
for step 4874 | loss 3.526343 | norm 0.2919 | time 465.9975 ms | tok/sec 1125087.6835
for step 4875 | loss 3.535316 | norm 0.2889 | time 464.9758 ms | tok/sec 1127559.6747
for step 4876 | loss 3.500171 | norm 0.3019 | time 465.9379 ms | tok/sec 1125231.6093
for step 4877 | loss 3.499118 | norm 0.3026 | time 465.4682 ms | tok/sec 1126367.0338
for step 4878 | loss 3.518133 | norm 0.2835 | time 465.9312 ms | tok/sec 1125247.7313
for step 4879 | loss 3.470220 | norm 0.2831 | time 466.0497 ms | tok/sec 1124961.6348
for step 4880 | loss 3.482214 | norm 0.2798 | time 466.6498 ms | tok/sec 1123514.9632
for step 4881 | loss 3.481914 | norm 0.2938 | time 465.9905 ms | tok/sec 1125104.3770
for step 4882 | loss 3.459894 | norm 0.3003 | time 465.2965 ms | tok/sec 1126782.5832
for step 4883 | loss 3.428843 | norm 0.3042 | time 464.9196 ms | tok/sec 1127696.1373
for step 4884 | loss 3.499242 | norm 0.2879 | time 465.4884 ms | tok/sec 1126317.9961
for step 4885 | loss 3.439309 | norm 0.3205 | time 465.8089 ms | tok/sec 1125543.1907
for step 4886 | loss 3.381188 | norm 0.3930 | time 465.1191 ms | tok/sec 1127212.3066
for step 4887 | loss 3.289201 | norm 0.3821 | time 465.7612 ms | tok/sec 1125658.4216
for step 4888 | loss 3.350634 | norm 0.3924 | time 466.1751 ms | tok/sec 1124659.0031
for step 4889 | loss 3.356335 | norm 0.3872 | time 465.2724 ms | tok/sec 1126840.9001
for step 4890 | loss 3.270653 | norm 0.3925 | time 464.9119 ms | tok/sec 1127714.6433
for step 4891 | loss 3.303307 | norm 0.3951 | time 464.8917 ms | tok/sec 1127763.8027
for step 4892 | loss 3.368030 | norm 0.3740 | time 465.1895 ms | tok/sec 1127041.8798
for step 4893 | loss 3.322465 | norm 0.3711 | time 466.2490 ms | tok/sec 1124480.7222
for step 4894 | loss 3.403281 | norm 0.3587 | time 465.3146 ms | tok/sec 1126738.7052
for step 4895 | loss 3.351475 | norm 0.3275 | time 465.1973 ms | tok/sec 1127022.8183
for step 4896 | loss 3.308190 | norm 0.2873 | time 465.7876 ms | tok/sec 1125594.4655
for step 4897 | loss 3.426178 | norm 0.3947 | time 467.0279 ms | tok/sec 1122605.3028
for step 4898 | loss 3.552877 | norm 0.3441 | time 465.9350 ms | tok/sec 1125238.5186
for step 4899 | loss 3.531268 | norm 0.3309 | time 465.8620 ms | tok/sec 1125414.7360
for step 4900 | loss 3.596977 | norm 0.3759 | time 466.0254 ms | tok/sec 1125020.3389
for step 4901 | loss 3.508945 | norm 0.3898 | time 465.6539 ms | tok/sec 1125917.7774
for step 4902 | loss 3.568792 | norm 0.3364 | time 466.0754 ms | tok/sec 1124899.4842
for step 4903 | loss 3.552804 | norm 0.3252 | time 465.4212 ms | tok/sec 1126480.7023
for step 4904 | loss 3.574721 | norm 0.3392 | time 465.8985 ms | tok/sec 1125326.6204
for step 4905 | loss 3.610588 | norm 0.3625 | time 465.0795 ms | tok/sec 1127308.2305
for step 4906 | loss 3.529888 | norm 0.3336 | time 465.5738 ms | tok/sec 1126111.5077
for step 4907 | loss 3.544889 | norm 0.3178 | time 466.0923 ms | tok/sec 1124858.6297
for step 4908 | loss 3.519679 | norm 0.3164 | time 466.3103 ms | tok/sec 1124332.9645
for step 4909 | loss 3.529456 | norm 0.3301 | time 465.9390 ms | tok/sec 1125228.7304
for step 4910 | loss 3.539605 | norm 0.3252 | time 466.2344 ms | tok/sec 1124515.7988
for step 4911 | loss 3.543895 | norm 0.3265 | time 465.8828 ms | tok/sec 1125364.6294
for step 4912 | loss 3.557095 | norm 0.3691 | time 464.6382 ms | tok/sec 1128378.9464
for step 4913 | loss 3.527599 | norm 0.4039 | time 465.2977 ms | tok/sec 1126779.6964
for step 4914 | loss 3.551505 | norm 0.3535 | time 465.4622 ms | tok/sec 1126381.4574
for step 4915 | loss 3.562872 | norm 0.3695 | time 466.2137 ms | tok/sec 1124565.8299
for step 4916 | loss 3.635014 | norm 0.4282 | time 465.6217 ms | tok/sec 1125995.6075
for step 4917 | loss 3.526604 | norm 0.3798 | time 466.2516 ms | tok/sec 1124474.3972
for step 4918 | loss 3.612102 | norm 0.3394 | time 466.1489 ms | tok/sec 1124722.2776
for step 4919 | loss 3.505781 | norm 0.3416 | time 465.5731 ms | tok/sec 1126113.2377
for step 4920 | loss 3.529003 | norm 0.3181 | time 466.0695 ms | tok/sec 1124913.8703
for step 4921 | loss 3.505280 | norm 0.2979 | time 466.4159 ms | tok/sec 1124078.3603
for step 4922 | loss 3.479265 | norm 0.3116 | time 466.2693 ms | tok/sec 1124431.8486
for step 4923 | loss 3.487721 | norm 0.3219 | time 465.3625 ms | tok/sec 1126622.6758
for step 4924 | loss 3.455180 | norm 0.3153 | time 465.5836 ms | tok/sec 1126087.8644
for step 4925 | loss 3.462667 | norm 0.3102 | time 465.7283 ms | tok/sec 1125737.9447
for step 4926 | loss 3.507654 | norm 0.3205 | time 465.5411 ms | tok/sec 1126190.5181
for step 4927 | loss 3.497334 | norm 0.2885 | time 465.8608 ms | tok/sec 1125417.6159
for step 4928 | loss 3.473245 | norm 0.3334 | time 465.1194 ms | tok/sec 1127211.7288
for step 4929 | loss 3.505327 | norm 0.3119 | time 465.8601 ms | tok/sec 1125419.3438
for step 4930 | loss 3.426145 | norm 0.3332 | time 465.7841 ms | tok/sec 1125603.1078
for step 4931 | loss 3.463869 | norm 0.3216 | time 464.8399 ms | tok/sec 1127889.3231
for step 4932 | loss 3.496315 | norm 0.3278 | time 465.0607 ms | tok/sec 1127353.8867
for step 4933 | loss 3.296854 | norm 0.3471 | time 464.9041 ms | tok/sec 1127733.7282
for step 4934 | loss 3.334681 | norm 0.2972 | time 464.7062 ms | tok/sec 1128213.9549
for step 4935 | loss 3.345567 | norm 0.3347 | time 465.4839 ms | tok/sec 1126328.9571
for step 4936 | loss 3.352760 | norm 0.3421 | time 465.3027 ms | tok/sec 1126767.5720
for step 4937 | loss 3.357133 | norm 0.3571 | time 465.5352 ms | tok/sec 1126204.9372
for step 4938 | loss 3.334944 | norm 0.3708 | time 464.9329 ms | tok/sec 1127663.7534
for step 4939 | loss 3.310857 | norm 0.3500 | time 465.4980 ms | tok/sec 1126294.9210
for step 4940 | loss 3.297218 | norm 0.3078 | time 465.6193 ms | tok/sec 1126001.3731
for step 4941 | loss 3.327071 | norm 0.3406 | time 464.9816 ms | tok/sec 1127545.7990
for step 4942 | loss 3.361215 | norm 0.3364 | time 465.3807 ms | tok/sec 1126578.8103
for step 4943 | loss 3.301806 | norm 0.3057 | time 465.9388 ms | tok/sec 1125229.3062
for step 4944 | loss 3.465064 | norm 0.3480 | time 465.8449 ms | tok/sec 1125456.2070
for step 4945 | loss 3.532965 | norm 0.3366 | time 465.9865 ms | tok/sec 1125114.1631
for step 4946 | loss 3.556230 | norm 0.3136 | time 465.3223 ms | tok/sec 1126720.2313
for step 4947 | loss 3.560664 | norm 0.3354 | time 465.4686 ms | tok/sec 1126365.8799
for step 4948 | loss 3.470397 | norm 0.3447 | time 465.4748 ms | tok/sec 1126350.8797
for step 4949 | loss 3.528911 | norm 0.3266 | time 464.9935 ms | tok/sec 1127516.8923
for step 4950 | loss 3.540578 | norm 0.3182 | time 465.5571 ms | tok/sec 1126151.8766
for step 4951 | loss 3.622069 | norm 0.3814 | time 465.9166 ms | tok/sec 1125282.8557
Will loading at 0 from edu_fineweb10B/edufineweb_train_000027.npy
for step 4952 | loss 3.560685 | norm 0.3765 | time 1415.1604 ms | tok/sec 370479.5538
for step 4953 | loss 3.557768 | norm 0.3164 | time 464.9425 ms | tok/sec 1127640.6231
for step 4954 | loss 3.604193 | norm 0.3722 | time 465.3409 ms | tok/sec 1126675.2036
for step 4955 | loss 3.511217 | norm 0.3383 | time 465.4744 ms | tok/sec 1126352.0336
for step 4956 | loss 3.653852 | norm 0.3278 | time 464.3888 ms | tok/sec 1128984.9078
for step 4957 | loss 3.547831 | norm 0.3789 | time 465.6079 ms | tok/sec 1126029.0489
for step 4958 | loss 3.567433 | norm 0.3647 | time 464.9262 ms | tok/sec 1127679.9451
for step 4959 | loss 3.562604 | norm 0.3089 | time 464.5042 ms | tok/sec 1128704.4394
for step 4960 | loss 3.579207 | norm 0.3568 | time 465.5702 ms | tok/sec 1126120.1579
for step 4961 | loss 3.535834 | norm 0.3410 | time 465.7178 ms | tok/sec 1125763.3022
for step 4962 | loss 3.565575 | norm 0.3263 | time 464.7968 ms | tok/sec 1127994.0413
for step 4963 | loss 3.561433 | norm 0.2917 | time 465.9421 ms | tok/sec 1125221.2454
for step 4964 | loss 3.584758 | norm 0.3547 | time 464.3307 ms | tok/sec 1129126.3538
for step 4965 | loss 3.471530 | norm 0.3322 | time 465.6861 ms | tok/sec 1125839.9581
for step 4966 | loss 3.547182 | norm 0.3262 | time 466.1498 ms | tok/sec 1124719.9766
for step 4967 | loss 3.565376 | norm 0.3447 | time 464.9029 ms | tok/sec 1127736.6199
for step 4968 | loss 3.494612 | norm 0.3469 | time 464.9565 ms | tok/sec 1127606.5077
for step 4969 | loss 3.480896 | norm 0.3494 | time 465.8654 ms | tok/sec 1125406.6726
for step 4970 | loss 3.473787 | norm 0.3490 | time 465.9665 ms | tok/sec 1125162.5203
for step 4971 | loss 3.501701 | norm 0.3692 | time 465.6484 ms | tok/sec 1125931.0366
for step 4972 | loss 3.474817 | norm 0.3423 | time 465.6587 ms | tok/sec 1125906.2479
for step 4973 | loss 3.508695 | norm 0.2875 | time 464.6416 ms | tok/sec 1128370.8404
for step 4974 | loss 3.429260 | norm 0.3064 | time 466.4466 ms | tok/sec 1124004.2422
for step 4975 | loss 3.501334 | norm 0.2923 | time 466.7246 ms | tok/sec 1123334.7495
for step 4976 | loss 3.509145 | norm 0.2686 | time 465.1556 ms | tok/sec 1127123.9093
for step 4977 | loss 3.549920 | norm 0.2794 | time 465.5657 ms | tok/sec 1126131.1151
for step 4978 | loss 3.439060 | norm 0.2744 | time 465.5783 ms | tok/sec 1126100.5509
for step 4979 | loss 3.328188 | norm 0.2693 | time 465.4856 ms | tok/sec 1126324.9188
for step 4980 | loss 3.278317 | norm 0.2869 | time 466.3479 ms | tok/sec 1124242.1443
for step 4981 | loss 3.285475 | norm 0.3024 | time 464.7775 ms | tok/sec 1128040.9104
for step 4982 | loss 3.291347 | norm 0.2966 | time 465.8051 ms | tok/sec 1125552.4083
for step 4983 | loss 3.258265 | norm 0.3569 | time 464.6871 ms | tok/sec 1128260.2634
for step 4984 | loss 3.238891 | norm 0.4214 | time 465.1742 ms | tok/sec 1127078.8494
for step 4985 | loss 3.334183 | norm 0.4437 | time 465.3831 ms | tok/sec 1126573.0387
for step 4986 | loss 3.257489 | norm 0.3914 | time 464.6006 ms | tok/sec 1128470.4361
for step 4987 | loss 3.366305 | norm 0.3955 | time 465.4629 ms | tok/sec 1126379.7266
for step 4988 | loss 3.254535 | norm 0.3760 | time 465.4691 ms | tok/sec 1126364.7260
for step 4989 | loss 3.328606 | norm 0.3445 | time 465.2228 ms | tok/sec 1126961.0173
for step 4990 | loss 3.441288 | norm 0.3665 | time 465.7934 ms | tok/sec 1125580.6381
for step 4991 | loss 3.570880 | norm 0.3371 | time 464.8762 ms | tok/sec 1127801.3980
for step 4992 | loss 3.624936 | norm 0.3351 | time 465.9660 ms | tok/sec 1125163.6717
for step 4993 | loss 3.513802 | norm 0.3571 | time 465.0378 ms | tok/sec 1127409.3727
for step 4994 | loss 3.514787 | norm 0.3039 | time 464.8356 ms | tok/sec 1127899.7362
for step 4995 | loss 3.566653 | norm 0.3711 | time 465.0180 ms | tok/sec 1127457.3494
for step 4996 | loss 3.575570 | norm 0.3815 | time 465.2133 ms | tok/sec 1126984.1197
for step 4997 | loss 3.590068 | norm 0.3735 | time 465.5740 ms | tok/sec 1126110.9310
for step 4998 | loss 3.567773 | norm 0.3583 | time 465.1904 ms | tok/sec 1127039.5693
for step 4999 | loss 3.527684 | norm 0.3978 | time 465.7049 ms | tok/sec 1125794.4244
validation loss 3.5297
HellaSwag accuracy: 2676/10042=0.2665
> Hello, I'm a language model, and I'm a linguistic model. Why do I have all of the languages out there and want some of them?

> Hello, I'm a language model, a model, and a model to understand English. I would like to work with Python the next time you're done with
> Hello, I'm a language model, so some languages, like English, are not as good for understanding. But my goal is to provide some examples for both
> Hello, I'm a language model, and I'm also an actor in the ICT classroom teaching method. My research is looking at what the language model is
> Hello, I'm a language model, but I'd like to get a lot of fun, because I'm just one of those good languages (which I'll
> Hello, I'm a language model, at that point. And I just understand how to be interested in the field and that we have a long-term interest
> Hello, I'm a language model, I prefer to define what they mean when it comes to grammar or spelling. I am trying to find clear, concrete ways
>> >  Hello, I'm a language model, so this post is the first time I made the project, and then you'll have to do this next year. I> Hello, I'm a language model, and I know that it's easy to forget them.
So lets understand this, because I want to explain it more
Hello, I'm a language model, and it only uses language in a group (the more primitive model languages the better).
The basic idea is to get

Hello, I'm a language model, and I'm a native speaker. I’ve always been a linguistic computer!
And today is a great day
> > Hello, I'm a language model, but now I'm interested in what could be called something for a particular programming language, or some kind of a language,
> Hello, I'm a language model, and in order that I could make my work happen. But this is just one piece, I'm the "word"
> > Hello, I'm a language model, language guide to help you improve your performance.
2) You know how to translate an English sentence into English, then
> > Hello, I'm a language model, and I'm not interested, because language is the main part of language construction since we need to look the way to be
Hello, I'm a language model, but I didn't mind writing your paper. (I'm also trying to write in the language model, but it turns
> Hello, I'm a language model, but it seems that everyone is using something that will help them understand some of a great aspect of the language.
Here
Hello, I'm a language model,
the language is just right for you, and my
own statement, it's just to make sure you're an
> > Hello, I'm a language model, and I've taken the language. For me, the language is really what I want to do. I've just taken
> > Hello, I'm a language model, and a very good way to write a program for a web site, but I'm not a software developer. But if
> Hello, I'm a language model, and to whom I'm a linguist. Let's talk about language theory and literature on some of the other issues to
> Hello, I'm a language model, let's find it. That's a very interesting idea for me, because my father has been involved with the project,
Hello, I'm a language model, and I have a big idea about what exactly it all means. I mean, you can say that you know which language
> Hello, I'm a language model, and we've always been around the world, but now, it's still in many languages. I'm looking at all
Hello, I'm a language model, but it's a great first step (to create a world of language modeling). I want to introduce it to the field
> Hello, I'm a language model, and you've got a lot of fun playing at the same time. I don't know about to go to one particular
> Hello, I'm a language model, so my friend has made me realize, I have a good grasp on what we're going to do. That'll help
> Hello, I'm a language model, meaning that to use is to find or process information around the Internet in many ways.
So what do I do now
> Hello, I'm a language model, and why I have learned the other languages, but that was not my experience.
But when I've got the computer
> Hello, I'm a language model, and I have had my own languages, and I'm a language model. So, if our language programming team really looks
Hello, I'm a language model, but I've always wondered how I want to get a good deal out of a bunch of people to speak a language and
> Hello, I'm a language model, so we like to start off with a nice "model" to help us know if you find this (though it's
for step 5000 | loss 3.498209 | norm 0.3347 | time 12855.2969 ms | tok/sec 40783.8112
for step 5001 | loss 3.574636 | norm 0.3101 | time 462.8563 ms | tok/sec 1132723.0681
for step 5002 | loss 3.540723 | norm 0.3155 | time 463.7353 ms | tok/sec 1130575.8964
for step 5003 | loss 3.540705 | norm 0.3292 | time 463.1639 ms | tok/sec 1131970.8931
for step 5004 | loss 3.530236 | norm 0.3218 | time 462.7786 ms | tok/sec 1132913.3109
for step 5005 | loss 3.576917 | norm 0.3048 | time 464.1228 ms | tok/sec 1129632.1391
for step 5006 | loss 3.579589 | norm 0.3353 | time 463.5308 ms | tok/sec 1131074.8366
for step 5007 | loss 3.551593 | norm 0.3361 | time 463.3009 ms | tok/sec 1131635.9431
for step 5008 | loss 3.636318 | norm 0.3352 | time 463.7623 ms | tok/sec 1130510.2180
for step 5009 | loss 3.558676 | norm 0.3680 | time 464.4313 ms | tok/sec 1128881.7441
for step 5010 | loss 3.494724 | norm 0.2917 | time 465.8008 ms | tok/sec 1125562.7783
for step 5011 | loss 3.540129 | norm 0.3149 | time 464.0570 ms | tok/sec 1129792.3216
for step 5012 | loss 3.568641 | norm 0.3292 | time 464.1550 ms | tok/sec 1129553.8056
for step 5013 | loss 3.522765 | norm 0.3325 | time 465.0018 ms | tok/sec 1127496.6586
for step 5014 | loss 3.444202 | norm 0.3031 | time 464.7636 ms | tok/sec 1128074.4734
for step 5015 | loss 3.454625 | norm 0.3325 | time 464.2551 ms | tok/sec 1129310.1706
for step 5016 | loss 3.497838 | norm 0.4306 | time 465.0450 ms | tok/sec 1127392.0327
for step 5017 | loss 3.490858 | norm 0.3693 | time 463.7587 ms | tok/sec 1130518.9359
for step 5018 | loss 3.524872 | norm 0.3724 | time 465.3089 ms | tok/sec 1126752.5611
for step 5019 | loss 3.442733 | norm 0.3606 | time 465.4610 ms | tok/sec 1126384.3422
for step 5020 | loss 3.460003 | norm 0.3235 | time 464.4952 ms | tok/sec 1128726.4546
for step 5021 | loss 3.522051 | norm 0.3501 | time 464.6544 ms | tok/sec 1128339.5756
for step 5022 | loss 3.502452 | norm 0.3411 | time 464.3581 ms | tok/sec 1129059.6842
for step 5023 | loss 3.471603 | norm 0.3035 | time 465.8346 ms | tok/sec 1125480.9758
for step 5024 | loss 3.459914 | norm 0.2991 | time 465.1101 ms | tok/sec 1127234.2636
for step 5025 | loss 3.372996 | norm 0.3245 | time 464.5221 ms | tok/sec 1128660.9909
for step 5026 | loss 3.266914 | norm 0.3265 | time 464.9720 ms | tok/sec 1127568.9253
for step 5027 | loss 3.253214 | norm 0.3860 | time 465.1201 ms | tok/sec 1127209.9954
for step 5028 | loss 3.297418 | norm 0.4547 | time 465.1468 ms | tok/sec 1127145.2852
for step 5029 | loss 3.236484 | norm 0.4366 | time 465.5552 ms | tok/sec 1126156.4903
for step 5030 | loss 3.251928 | norm 0.3540 | time 464.8125 ms | tok/sec 1127955.8545
for step 5031 | loss 3.279061 | norm 0.3354 | time 465.2371 ms | tok/sec 1126926.3655
for step 5032 | loss 3.288527 | norm 0.3193 | time 466.0468 ms | tok/sec 1124968.5409
for step 5033 | loss 3.276496 | norm 0.3341 | time 465.4255 ms | tok/sec 1126470.3154
for step 5034 | loss 3.285861 | norm 0.3102 | time 465.3480 ms | tok/sec 1126657.8862
for step 5035 | loss 3.304041 | norm 0.3207 | time 465.2627 ms | tok/sec 1126864.5750
for step 5036 | loss 3.303240 | norm 0.3429 | time 465.3356 ms | tok/sec 1126687.9033
for step 5037 | loss 3.538073 | norm 0.3368 | time 464.7534 ms | tok/sec 1128099.3576
for step 5038 | loss 3.555269 | norm 0.3105 | time 465.4334 ms | tok/sec 1126451.2732
for step 5039 | loss 3.604191 | norm 0.3280 | time 466.2082 ms | tok/sec 1124579.0573
for step 5040 | loss 3.584584 | norm 0.3179 | time 465.8217 ms | tok/sec 1125512.0824
for step 5041 | loss 3.582590 | norm 0.3422 | time 465.5657 ms | tok/sec 1126131.1151
for step 5042 | loss 3.568729 | norm 0.3354 | time 464.7443 ms | tok/sec 1128121.3493
for step 5043 | loss 3.468968 | norm 0.3080 | time 464.6871 ms | tok/sec 1128260.2634
for step 5044 | loss 3.515171 | norm 0.3101 | time 465.5285 ms | tok/sec 1126221.0871
for step 5045 | loss 3.503151 | norm 0.3042 | time 465.1258 ms | tok/sec 1127196.1283
for step 5046 | loss 3.575817 | norm 0.3187 | time 465.2646 ms | tok/sec 1126859.9555
for step 5047 | loss 3.531738 | norm 0.3213 | time 465.5125 ms | tok/sec 1126259.7333
for step 5048 | loss 3.536826 | norm 0.3082 | time 464.8409 ms | tok/sec 1127887.0092
for step 5049 | loss 3.549596 | norm 0.3245 | time 464.9734 ms | tok/sec 1127565.4563
for step 5050 | loss 3.519279 | norm 0.3150 | time 464.7524 ms | tok/sec 1128101.6725
for step 5051 | loss 3.534938 | norm 0.3253 | time 466.1255 ms | tok/sec 1124778.6555
for step 5052 | loss 3.514251 | norm 0.3336 | time 465.1475 ms | tok/sec 1127143.5520
for step 5053 | loss 3.521036 | norm 0.3307 | time 464.2448 ms | tok/sec 1129335.1093
for step 5054 | loss 3.607012 | norm 0.3278 | time 465.3931 ms | tok/sec 1126548.7990
for step 5055 | loss 3.533548 | norm 0.3308 | time 465.0335 ms | tok/sec 1127419.7769
for step 5056 | loss 3.582666 | norm 0.3212 | time 465.0207 ms | tok/sec 1127450.9908
for step 5057 | loss 3.452822 | norm 0.3655 | time 465.4233 ms | tok/sec 1126475.5088
for step 5058 | loss 3.612071 | norm 0.3690 | time 464.4208 ms | tok/sec 1128907.2435
for step 5059 | loss 3.546454 | norm 0.3659 | time 465.7311 ms | tok/sec 1125731.0292
for step 5060 | loss 3.466740 | norm 0.3673 | time 465.1909 ms | tok/sec 1127038.4140
for step 5061 | loss 3.431681 | norm 0.3523 | time 465.3850 ms | tok/sec 1126568.4216
for step 5062 | loss 3.476200 | norm 0.3158 | time 465.7104 ms | tok/sec 1125781.1685
for step 5063 | loss 3.499271 | norm 0.3327 | time 465.7412 ms | tok/sec 1125706.8256
for step 5064 | loss 3.488191 | norm 0.3113 | time 465.5943 ms | tok/sec 1126061.9156
for step 5065 | loss 3.481424 | norm 0.3293 | time 465.5881 ms | tok/sec 1126076.9081
for step 5066 | loss 3.499362 | norm 0.3042 | time 465.4601 ms | tok/sec 1126386.6501
for step 5067 | loss 3.496787 | norm 0.3075 | time 464.5972 ms | tok/sec 1128478.5435
for step 5068 | loss 3.483713 | norm 0.3166 | time 464.8561 ms | tok/sec 1127849.9865
for step 5069 | loss 3.447003 | norm 0.2999 | time 465.0695 ms | tok/sec 1127332.5030
for step 5070 | loss 3.439169 | norm 0.3085 | time 465.2319 ms | tok/sec 1126939.0709
for step 5071 | loss 3.472699 | norm 0.3030 | time 464.8621 ms | tok/sec 1127835.5252
for step 5072 | loss 3.257862 | norm 0.3561 | time 465.6837 ms | tok/sec 1125845.7221
for step 5073 | loss 3.197194 | norm 0.4629 | time 465.6470 ms | tok/sec 1125934.4956
for step 5074 | loss 3.264237 | norm 0.4762 | time 464.7937 ms | tok/sec 1128001.5633
for step 5075 | loss 3.264568 | norm 0.4153 | time 465.4517 ms | tok/sec 1126406.8440
for step 5076 | loss 3.281696 | norm 0.3595 | time 465.3828 ms | tok/sec 1126573.6159
for step 5077 | loss 3.237822 | norm 0.3371 | time 465.3678 ms | tok/sec 1126609.9776
for step 5078 | loss 3.290074 | norm 0.3475 | time 464.9901 ms | tok/sec 1127524.9861
for step 5079 | loss 3.245952 | norm 0.3542 | time 465.4274 ms | tok/sec 1126465.6990
for step 5080 | loss 3.309424 | norm 0.3373 | time 465.2495 ms | tok/sec 1126896.3356
for step 5081 | loss 3.300069 | norm 0.3053 | time 465.8520 ms | tok/sec 1125438.9271
for step 5082 | loss 3.290117 | norm 0.3274 | time 466.0435 ms | tok/sec 1124976.5980
for step 5083 | loss 3.519654 | norm 0.2961 | time 465.0040 ms | tok/sec 1127491.4557
for step 5084 | loss 3.531272 | norm 0.3195 | time 465.0657 ms | tok/sec 1127341.7499
for step 5085 | loss 3.548567 | norm 0.3520 | time 465.4691 ms | tok/sec 1126364.7260
for step 5086 | loss 3.546338 | norm 0.3207 | time 466.3463 ms | tok/sec 1124246.1677
for step 5087 | loss 3.576571 | norm 0.3357 | time 465.2796 ms | tok/sec 1126823.5777
for step 5088 | loss 3.549775 | norm 0.2910 | time 465.0333 ms | tok/sec 1127420.3550
for step 5089 | loss 3.543693 | norm 0.2880 | time 464.9553 ms | tok/sec 1127609.3988
for step 5090 | loss 3.560872 | norm 0.3341 | time 465.0502 ms | tok/sec 1127379.3171
for step 5091 | loss 3.525433 | norm 0.3543 | time 464.6003 ms | tok/sec 1128471.0152
for step 5092 | loss 3.537588 | norm 0.3722 | time 465.0407 ms | tok/sec 1127402.4367
for step 5093 | loss 3.587759 | norm 0.3116 | time 464.7543 ms | tok/sec 1128097.0428
for step 5094 | loss 3.563215 | norm 0.3793 | time 465.1806 ms | tok/sec 1127063.2525
for step 5095 | loss 3.575849 | norm 0.3480 | time 465.6363 ms | tok/sec 1125960.4385
for step 5096 | loss 3.547232 | norm 0.3099 | time 465.0793 ms | tok/sec 1127308.8084
for step 5097 | loss 3.552361 | norm 0.3175 | time 465.6129 ms | tok/sec 1126016.9405
for step 5098 | loss 3.573897 | norm 0.3918 | time 464.5302 ms | tok/sec 1128641.2953
for step 5099 | loss 3.542363 | norm 0.3704 | time 465.7447 ms | tok/sec 1125698.1817
for step 5100 | loss 3.551208 | norm 0.3778 | time 466.1496 ms | tok/sec 1124720.5519
for step 5101 | loss 3.576163 | norm 0.4035 | time 465.4052 ms | tok/sec 1126519.3663
for step 5102 | loss 3.518379 | norm 0.3663 | time 465.5886 ms | tok/sec 1126075.7548
for step 5103 | loss 3.650137 | norm 0.3625 | time 464.8769 ms | tok/sec 1127799.6628
for step 5104 | loss 3.531308 | norm 0.4208 | time 465.3773 ms | tok/sec 1126586.8905
for step 5105 | loss 3.553312 | norm 0.4399 | time 465.7142 ms | tok/sec 1125771.9471
for step 5106 | loss 3.547716 | norm 0.3670 | time 465.7521 ms | tok/sec 1125680.3181
for step 5107 | loss 3.490426 | norm 0.3313 | time 464.5746 ms | tok/sec 1128533.5611
for step 5108 | loss 3.565606 | norm 0.3746 | time 465.3914 ms | tok/sec 1126552.8388
for step 5109 | loss 3.448850 | norm 0.3977 | time 465.2669 ms | tok/sec 1126854.1811
for step 5110 | loss 3.498540 | norm 0.3632 | time 465.5263 ms | tok/sec 1126226.2782
for step 5111 | loss 3.473830 | norm 0.3212 | time 466.4152 ms | tok/sec 1124080.0841
for step 5112 | loss 3.498121 | norm 0.3668 | time 465.0035 ms | tok/sec 1127492.6119
for step 5113 | loss 3.475456 | norm 0.3012 | time 465.6281 ms | tok/sec 1125980.0406
for step 5114 | loss 3.437346 | norm 0.3477 | time 465.8456 ms | tok/sec 1125454.4790
for step 5115 | loss 3.510473 | norm 0.3171 | time 465.2522 ms | tok/sec 1126889.9834
for step 5116 | loss 3.458013 | norm 0.3266 | time 465.4205 ms | tok/sec 1126482.4334
for step 5117 | loss 3.489395 | norm 0.3011 | time 465.7540 ms | tok/sec 1125675.7083
for step 5118 | loss 3.351382 | norm 0.3462 | time 465.9429 ms | tok/sec 1125219.5181
for step 5119 | loss 3.370960 | norm 0.3174 | time 464.8457 ms | tok/sec 1127875.4393
for step 5120 | loss 3.333266 | norm 0.3141 | time 465.6510 ms | tok/sec 1125924.6952
for step 5121 | loss 3.232770 | norm 0.3229 | time 464.6862 ms | tok/sec 1128262.5789
for step 5122 | loss 3.232999 | norm 0.2914 | time 465.4336 ms | tok/sec 1126450.6962
for step 5123 | loss 3.273283 | norm 0.2695 | time 464.4911 ms | tok/sec 1128736.3038
for step 5124 | loss 3.238387 | norm 0.2926 | time 464.8211 ms | tok/sec 1127935.0265
for step 5125 | loss 3.232847 | norm 0.3148 | time 465.3487 ms | tok/sec 1126656.1545
for step 5126 | loss 3.261765 | norm 0.3045 | time 465.6203 ms | tok/sec 1125999.0668
for step 5127 | loss 3.310552 | norm 0.2779 | time 464.8910 ms | tok/sec 1127765.5378
for step 5128 | loss 3.223043 | norm 0.2783 | time 465.2140 ms | tok/sec 1126982.3869
for step 5129 | loss 3.250294 | norm 0.3105 | time 465.5173 ms | tok/sec 1126248.1968
for step 5130 | loss 3.531067 | norm 0.4161 | time 465.4760 ms | tok/sec 1126347.9951
for step 5131 | loss 3.570915 | norm 0.4175 | time 466.8014 ms | tok/sec 1123150.0043
for step 5132 | loss 3.582787 | norm 0.4041 | time 465.3943 ms | tok/sec 1126545.9133
for step 5133 | loss 3.523127 | norm 0.3857 | time 465.3106 ms | tok/sec 1126748.5197
for step 5134 | loss 3.500806 | norm 0.3608 | time 465.5457 ms | tok/sec 1126179.5598
for step 5135 | loss 3.533727 | norm 0.3487 | time 465.7550 ms | tok/sec 1125673.4033
for step 5136 | loss 3.556915 | norm 0.3897 | time 466.0261 ms | tok/sec 1125018.6123
for step 5137 | loss 3.533564 | norm 0.3179 | time 465.4095 ms | tok/sec 1126508.9787
for step 5138 | loss 3.565547 | norm 0.3238 | time 465.3807 ms | tok/sec 1126578.8103
for step 5139 | loss 3.575590 | norm 0.3457 | time 464.8521 ms | tok/sec 1127859.8204
for step 5140 | loss 3.492331 | norm 0.3304 | time 464.9234 ms | tok/sec 1127686.8846
for step 5141 | loss 3.512840 | norm 0.3064 | time 465.3656 ms | tok/sec 1126615.1723
for step 5142 | loss 3.582516 | norm 0.4033 | time 465.1456 ms | tok/sec 1127148.1739
Will loading at 0 from edu_fineweb10B/edufineweb_train_000028.npy
for step 5143 | loss 3.553080 | norm 0.3882 | time 1401.0255 ms | tok/sec 374217.3054
for step 5144 | loss 3.527061 | norm 0.3352 | time 463.9776 ms | tok/sec 1129985.6457
for step 5145 | loss 3.540764 | norm 0.3130 | time 465.2061 ms | tok/sec 1127001.4471
for step 5146 | loss 3.547322 | norm 0.3415 | time 465.3606 ms | tok/sec 1126627.2935
for step 5147 | loss 3.516683 | norm 0.3331 | time 464.2720 ms | tok/sec 1129268.9951
for step 5148 | loss 3.533276 | norm 0.3051 | time 465.5383 ms | tok/sec 1126197.4392
for step 5149 | loss 3.553354 | norm 0.3242 | time 464.5255 ms | tok/sec 1128652.8808
for step 5150 | loss 3.535755 | norm 0.3175 | time 465.3790 ms | tok/sec 1126582.8504
for step 5151 | loss 3.550716 | norm 0.3217 | time 465.2781 ms | tok/sec 1126827.0421
for step 5152 | loss 3.518882 | norm 0.3670 | time 465.4679 ms | tok/sec 1126367.6107
for step 5153 | loss 3.496667 | norm 0.3491 | time 465.1902 ms | tok/sec 1127040.1469
for step 5154 | loss 3.491333 | norm 0.3001 | time 465.5793 ms | tok/sec 1126098.2443
for step 5155 | loss 3.473783 | norm 0.2865 | time 466.5849 ms | tok/sec 1123671.1187
for step 5156 | loss 3.469032 | norm 0.2907 | time 465.1749 ms | tok/sec 1127077.1163
for step 5157 | loss 3.486090 | norm 0.2983 | time 465.1639 ms | tok/sec 1127103.6897
for step 5158 | loss 3.427265 | norm 0.3099 | time 465.2774 ms | tok/sec 1126828.7743
for step 5159 | loss 3.478275 | norm 0.2956 | time 466.0528 ms | tok/sec 1124954.1534
for step 5160 | loss 3.466833 | norm 0.3176 | time 465.6267 ms | tok/sec 1125983.4999
for step 5161 | loss 3.506867 | norm 0.3047 | time 465.6069 ms | tok/sec 1126031.3552
for step 5162 | loss 3.456900 | norm 0.2907 | time 466.0554 ms | tok/sec 1124947.8230
for step 5163 | loss 3.486373 | norm 0.3480 | time 465.6146 ms | tok/sec 1126012.9045
for step 5164 | loss 3.300962 | norm 0.4078 | time 466.1977 ms | tok/sec 1124604.3626
for step 5165 | loss 3.257336 | norm 0.4578 | time 465.7938 ms | tok/sec 1125579.4859
for step 5166 | loss 3.251091 | norm 0.4538 | time 466.3875 ms | tok/sec 1124146.7415
for step 5167 | loss 3.338492 | norm 0.3945 | time 465.9483 ms | tok/sec 1125206.2757
for step 5168 | loss 3.207789 | norm 0.3324 | time 466.3386 ms | tok/sec 1124264.5606
for step 5169 | loss 3.287146 | norm 0.3758 | time 466.3150 ms | tok/sec 1124321.4674
for step 5170 | loss 3.236264 | norm 0.3130 | time 465.7741 ms | tok/sec 1125627.3069
for step 5171 | loss 3.266600 | norm 0.3527 | time 464.9401 ms | tok/sec 1127646.4056
for step 5172 | loss 3.256386 | norm 0.3172 | time 465.9910 ms | tok/sec 1125103.2257
for step 5173 | loss 3.319578 | norm 0.3257 | time 465.6134 ms | tok/sec 1126015.7874
for step 5174 | loss 3.271483 | norm 0.3388 | time 465.5752 ms | tok/sec 1126108.0476
for step 5175 | loss 3.357705 | norm 0.3165 | time 465.7862 ms | tok/sec 1125597.9224
for step 5176 | loss 3.465096 | norm 0.3682 | time 465.2431 ms | tok/sec 1126911.9278
for step 5177 | loss 3.580927 | norm 0.3787 | time 464.8144 ms | tok/sec 1127951.2260
for step 5178 | loss 3.508930 | norm 0.3570 | time 465.5204 ms | tok/sec 1126240.6983
for step 5179 | loss 3.498660 | norm 0.3283 | time 465.4527 ms | tok/sec 1126404.5361
for step 5180 | loss 3.566884 | norm 0.3754 | time 465.6310 ms | tok/sec 1125973.1221
for step 5181 | loss 3.506004 | norm 0.3730 | time 465.3938 ms | tok/sec 1126547.0676
for step 5182 | loss 3.464495 | norm 0.3557 | time 466.6660 ms | tok/sec 1123475.9311
for step 5183 | loss 3.555094 | norm 0.3448 | time 465.4698 ms | tok/sec 1126362.9952
for step 5184 | loss 3.559699 | norm 0.4218 | time 465.4853 ms | tok/sec 1126325.4957
for step 5185 | loss 3.638637 | norm 0.4497 | time 464.5946 ms | tok/sec 1128484.9137
for step 5186 | loss 3.552073 | norm 0.4467 | time 465.4799 ms | tok/sec 1126338.7645
for step 5187 | loss 3.501423 | norm 0.3669 | time 465.1926 ms | tok/sec 1127034.3706
for step 5188 | loss 3.557832 | norm 0.3723 | time 465.8732 ms | tok/sec 1125387.6664
for step 5189 | loss 3.542522 | norm 0.3582 | time 466.1288 ms | tok/sec 1124770.6012
for step 5190 | loss 3.509805 | norm 0.3954 | time 464.4225 ms | tok/sec 1128903.1867
for step 5191 | loss 3.495865 | norm 0.3226 | time 465.8072 ms | tok/sec 1125547.2234
for step 5192 | loss 3.501507 | norm 0.3357 | time 466.0699 ms | tok/sec 1124912.7194
for step 5193 | loss 3.500422 | norm 0.3288 | time 465.3616 ms | tok/sec 1126624.9847
for step 5194 | loss 3.555341 | norm 0.3172 | time 465.2724 ms | tok/sec 1126840.9001
for step 5195 | loss 3.574661 | norm 0.3319 | time 465.5352 ms | tok/sec 1126204.9372
for step 5196 | loss 3.508471 | norm 0.3701 | time 465.0638 ms | tok/sec 1127346.3734
for step 5197 | loss 3.604221 | norm 0.3539 | time 464.8297 ms | tok/sec 1127914.1992
for step 5198 | loss 3.553772 | norm 0.3380 | time 464.9460 ms | tok/sec 1127631.9495
for step 5199 | loss 3.503256 | norm 0.3354 | time 464.9832 ms | tok/sec 1127541.7520
for step 5200 | loss 3.504066 | norm 0.3171 | time 465.5406 ms | tok/sec 1126191.6716
for step 5201 | loss 3.477583 | norm 0.3299 | time 465.0464 ms | tok/sec 1127388.5648
for step 5202 | loss 3.534642 | norm 0.3181 | time 465.3230 ms | tok/sec 1126718.4994
for step 5203 | loss 3.454622 | norm 0.3363 | time 465.2209 ms | tok/sec 1126965.6377
for step 5204 | loss 3.492688 | norm 0.3392 | time 465.3478 ms | tok/sec 1126658.4635
for step 5205 | loss 3.440906 | norm 0.3155 | time 466.2106 ms | tok/sec 1124573.3062
for step 5206 | loss 3.420419 | norm 0.3094 | time 465.5097 ms | tok/sec 1126266.6553
for step 5207 | loss 3.483653 | norm 0.3182 | time 466.3382 ms | tok/sec 1124265.7102
for step 5208 | loss 3.471238 | norm 0.3072 | time 464.7486 ms | tok/sec 1128110.9321
for step 5209 | loss 3.435425 | norm 0.2905 | time 465.8444 ms | tok/sec 1125457.3590
for step 5210 | loss 3.480692 | norm 0.2880 | time 465.9472 ms | tok/sec 1125209.1544
for step 5211 | loss 3.332012 | norm 0.3199 | time 465.7841 ms | tok/sec 1125603.1078
for step 5212 | loss 3.314018 | norm 0.3593 | time 465.0095 ms | tok/sec 1127478.1598
for step 5213 | loss 3.245167 | norm 0.3632 | time 465.3673 ms | tok/sec 1126611.1319
for step 5214 | loss 3.249430 | norm 0.3617 | time 465.0984 ms | tok/sec 1127262.5779
for step 5215 | loss 3.309642 | norm 0.3026 | time 464.7837 ms | tok/sec 1128025.8656
for step 5216 | loss 3.240514 | norm 0.3083 | time 465.5344 ms | tok/sec 1126206.6675
for step 5217 | loss 3.283604 | norm 0.3209 | time 465.1434 ms | tok/sec 1127153.3735
for step 5218 | loss 3.259695 | norm 0.3215 | time 465.8258 ms | tok/sec 1125502.2894
for step 5219 | loss 3.289255 | norm 0.3177 | time 464.6842 ms | tok/sec 1128267.2100
for step 5220 | loss 3.282658 | norm 0.3011 | time 465.9250 ms | tok/sec 1125262.7021
for step 5221 | loss 3.273596 | norm 0.3089 | time 465.4219 ms | tok/sec 1126478.9711
for step 5222 | loss 3.365764 | norm 0.3185 | time 464.5500 ms | tok/sec 1128593.2178
for step 5223 | loss 3.496307 | norm 0.3725 | time 465.1263 ms | tok/sec 1127194.9727
for step 5224 | loss 3.500557 | norm 0.3240 | time 465.0128 ms | tok/sec 1127470.0668
for step 5225 | loss 3.474186 | norm 0.3430 | time 465.4908 ms | tok/sec 1126312.2272
for step 5226 | loss 3.493378 | norm 0.3015 | time 465.8964 ms | tok/sec 1125331.8033
for step 5227 | loss 3.498355 | norm 0.3265 | time 465.4171 ms | tok/sec 1126490.5123
for step 5228 | loss 3.530550 | norm 0.2935 | time 465.8945 ms | tok/sec 1125336.4104
for step 5229 | loss 3.523463 | norm 0.2942 | time 465.6315 ms | tok/sec 1125971.9690
for step 5230 | loss 3.491199 | norm 0.3176 | time 465.7671 ms | tok/sec 1125644.0164
for step 5231 | loss 3.451184 | norm 0.3362 | time 465.1046 ms | tok/sec 1127247.5538
for step 5232 | loss 3.516701 | norm 0.2920 | time 465.2393 ms | tok/sec 1126921.1679
for step 5233 | loss 3.547777 | norm 0.3192 | time 466.6624 ms | tok/sec 1123484.5409
for step 5234 | loss 3.562911 | norm 0.3386 | time 465.4377 ms | tok/sec 1126440.8868
for step 5235 | loss 3.526876 | norm 0.3877 | time 465.6923 ms | tok/sec 1125824.9719
for step 5236 | loss 3.507579 | norm 0.3766 | time 465.3471 ms | tok/sec 1126660.1952
for step 5237 | loss 3.524259 | norm 0.3569 | time 465.5030 ms | tok/sec 1126282.8070
for step 5238 | loss 3.505172 | norm 0.3255 | time 464.7973 ms | tok/sec 1127992.8841
for step 5239 | loss 3.554466 | norm 0.3423 | time 464.6008 ms | tok/sec 1128469.8570
for step 5240 | loss 3.443964 | norm 0.3223 | time 464.5946 ms | tok/sec 1128484.9137
for step 5241 | loss 3.509019 | norm 0.3445 | time 465.2421 ms | tok/sec 1126914.2378
for step 5242 | loss 3.513306 | norm 0.3266 | time 465.4965 ms | tok/sec 1126298.3822
for step 5243 | loss 3.526555 | norm 0.3159 | time 465.2686 ms | tok/sec 1126850.1390
for step 5244 | loss 3.526790 | norm 0.3537 | time 465.4877 ms | tok/sec 1126319.7268
for step 5245 | loss 3.553131 | norm 0.3129 | time 466.7695 ms | tok/sec 1123226.8785
for step 5246 | loss 3.465634 | norm 0.3122 | time 464.9208 ms | tok/sec 1127693.2458
for step 5247 | loss 3.435588 | norm 0.3047 | time 465.0784 ms | tok/sec 1127311.1200
for step 5248 | loss 3.435131 | norm 0.2849 | time 465.3268 ms | tok/sec 1126709.2627
for step 5249 | loss 3.512739 | norm 0.3817 | time 465.3649 ms | tok/sec 1126616.9039
validation loss 3.5199
HellaSwag accuracy: 2724/10042=0.2713
> Hello, I'm a language model, and a member of the public education arena of Singapore, with strong language skills.
My book The Language of the Language
> Hello, I'm a language model, a model of data analysis. You may call it a new language model, because you didn't talk to the person who
> Hello, I'm a language model, so some people have a lot of language to talk about in, so if you're a language programmer or someone interested in
> Hello, I'm a language model, and I'm interested to try and build as a language model by itself. With this website, you can find language models
> Hello, I'm a language model, but I can't get this right, that is, I'm not doing it without going beyond the main topic, and
> Hello, I'm a language model, especially in the medical field. But here's a way to do that…
This is the language model for every medical
> Hello, I'm a language model, I speak English without having had a great experience with it. And that's it. The language model comes from. And
> Hello, I'm a language model, and the result you've learned is in all aspects of the system,
- An interactive system will allow you to see
> Hello, I'm a language model, and I don't have it all that interesting from the video. What has this been so good to do is to look
> > Hello, I'm a language model, so don't be confused. I already have some models but it's never going to change. I really need to understand
Hello, I'm a language model, and I know that there's a lot different ways of understanding the language being spoken within your context, and that's important
> Hello, I'm a language model, but I need to say:
When you look at the real thing, you have a certain amount of information about what
> Hello, I'm a language model, to communicate a language that can use in daily life. Please consider reading this article before speaking to your students. I would> > 
Hello, I'm a language model, so we've been teaching a lot of Java programming at a very very young age.
I love Python. What I
Hello, I'm a language model, so do you want to make your programming easier to understand.
My language models want to give you more control over how
> > > Hello, I'm a language model, so I'll add some new and exciting pieces to it. I want to have some fun with it. I have lots
Hello, I'm a language model, they use a series of finite elements that can be written using finite elements. I hope you'll be able learn how to
Hello, I'm a language model, since if you have problems and have an idea when it is needed, I have written the first book (i.e
>>> >  Hello, I'm a language model, with many different languages I see more, and I don't even hear the languages right away.
I am a person>  Hello, I'm a language model, and I'm going to write the names of my words, sentences and functions being spoken of on the basis of our pronunciationHello, I'm a language model, based on the language and not a noun. There are many different types of language that I'm a language model. But
Hello, I'm a language model, so I chose this post as my final unit. I'm not the programmer, but I want to help the programmer learn

Hello, I'm a language model, but it has to catch up to many languages in the world. That's just one type of language model.
It

>> >  Hello, I'm a language model, then, how we've spoken, how to define our words, and how to describe it in all the languages we work> Hello, I'm a language model, but I don't think I will write things down in the future. Let's get started! Now the time is here
Hello, I'm a language model, or you are a language expert who can implement some very simple language models (or even just one). I hope you enjoyed

Hello, I'm a language model, and we've been working on building all of our language models. We've been working on working on our language and how
> > > Hello, I'm a language model, so the user can define a language using multiple languages simultaneously. We've been a few weeks for all the fun and learning
Hello, I'm a language model, so, how to play a role in programming and programming languages, in this way, I'm going to show them the
Hello, I'm a language model, and it will be amazing to try to help you to get around the basics of using the language right from the first day
> Hello, I'm a language model, and a great resource for learning language, pronunciation, and other language related topics, including vocabulary formation, conjunctival design> 
Hello, I'm a language model, and is now using the syntax I am writing for. If I were a language model, how would I write the language
for step 5250 | loss 3.468995 | norm 0.4007 | time 12883.4736 ms | tok/sec 40694.6150
for step 5251 | loss 3.467353 | norm 0.3584 | time 461.7679 ms | tok/sec 1135392.8813
for step 5252 | loss 3.454793 | norm 0.2910 | time 462.1699 ms | tok/sec 1134405.3697
for step 5253 | loss 3.400203 | norm 0.3131 | time 462.8279 ms | tok/sec 1132792.5052
for step 5254 | loss 3.468347 | norm 0.3123 | time 462.9161 ms | tok/sec 1132576.6364
for step 5255 | loss 3.414272 | norm 0.3140 | time 463.2237 ms | tok/sec 1131824.6558
for step 5256 | loss 3.473774 | norm 0.3130 | time 463.0897 ms | tok/sec 1132152.1400
for step 5257 | loss 3.277844 | norm 0.3790 | time 462.9493 ms | tok/sec 1132495.5610
for step 5258 | loss 3.322922 | norm 0.4873 | time 463.0504 ms | tok/sec 1132248.3236
for step 5259 | loss 3.302962 | norm 0.4731 | time 463.6710 ms | tok/sec 1130732.8581
for step 5260 | loss 3.280528 | norm 0.4334 | time 463.9142 ms | tok/sec 1130140.1202
for step 5261 | loss 3.236234 | norm 0.3919 | time 463.8004 ms | tok/sec 1130417.2350
for step 5262 | loss 3.222394 | norm 0.4579 | time 464.2456 ms | tok/sec 1129333.3694
for step 5263 | loss 3.220255 | norm 0.4254 | time 464.4475 ms | tok/sec 1128842.3383
for step 5264 | loss 3.272528 | norm 0.3272 | time 464.1719 ms | tok/sec 1129512.6123
for step 5265 | loss 3.246365 | norm 0.3328 | time 463.6154 ms | tok/sec 1130868.3453
for step 5266 | loss 3.241245 | norm 0.3334 | time 464.5972 ms | tok/sec 1128478.5435
for step 5267 | loss 3.229592 | norm 0.3158 | time 465.2073 ms | tok/sec 1126998.5591
for step 5268 | loss 3.383721 | norm 0.3489 | time 464.0434 ms | tok/sec 1129825.4084
for step 5269 | loss 3.529431 | norm 0.3100 | time 465.2560 ms | tok/sec 1126880.7438
for step 5270 | loss 3.499010 | norm 0.3026 | time 465.4393 ms | tok/sec 1126436.8477
for step 5271 | loss 3.490334 | norm 0.3327 | time 464.2811 ms | tok/sec 1129246.9587
for step 5272 | loss 3.512364 | norm 0.3116 | time 464.7417 ms | tok/sec 1128127.7154
for step 5273 | loss 3.469658 | norm 0.3354 | time 465.6093 ms | tok/sec 1126025.5893
for step 5274 | loss 3.617831 | norm 0.3705 | time 465.5542 ms | tok/sec 1126158.7972
for step 5275 | loss 3.460352 | norm 0.3619 | time 465.0564 ms | tok/sec 1127364.2899
for step 5276 | loss 3.528089 | norm 0.3241 | time 464.9963 ms | tok/sec 1127509.9550
for step 5277 | loss 3.531452 | norm 0.3555 | time 465.0149 ms | tok/sec 1127464.8642
for step 5278 | loss 3.405596 | norm 0.3669 | time 465.0395 ms | tok/sec 1127405.3267
for step 5279 | loss 3.541151 | norm 0.3529 | time 464.8776 ms | tok/sec 1127797.9276
for step 5280 | loss 3.540509 | norm 0.3214 | time 465.5421 ms | tok/sec 1126188.2110
for step 5281 | loss 3.595330 | norm 0.3287 | time 465.0569 ms | tok/sec 1127363.1340
for step 5282 | loss 3.551839 | norm 0.3564 | time 464.9742 ms | tok/sec 1127563.7218
for step 5283 | loss 3.552805 | norm 0.3522 | time 466.1338 ms | tok/sec 1124758.5199
for step 5284 | loss 3.535126 | norm 0.3326 | time 464.2010 ms | tok/sec 1129441.8364
for step 5285 | loss 3.562686 | norm 0.3230 | time 465.1742 ms | tok/sec 1127078.8494
for step 5286 | loss 3.520814 | norm 0.3510 | time 464.5140 ms | tok/sec 1128680.6871
for step 5287 | loss 3.579685 | norm 0.3676 | time 465.1649 ms | tok/sec 1127101.3789
for step 5288 | loss 3.597633 | norm 0.2908 | time 465.0056 ms | tok/sec 1127487.4091
for step 5289 | loss 3.551048 | norm 0.3412 | time 466.3017 ms | tok/sec 1124353.6597
for step 5290 | loss 3.553759 | norm 0.3178 | time 465.8837 ms | tok/sec 1125362.3258
for step 5291 | loss 3.503697 | norm 0.2809 | time 465.8277 ms | tok/sec 1125497.6810
for step 5292 | loss 3.459064 | norm 0.3308 | time 465.7750 ms | tok/sec 1125625.0022
for step 5293 | loss 3.481177 | norm 0.3223 | time 465.8325 ms | tok/sec 1125486.1601
for step 5294 | loss 3.503873 | norm 0.2900 | time 465.2357 ms | tok/sec 1126929.8306
for step 5295 | loss 3.415854 | norm 0.3144 | time 465.9078 ms | tok/sec 1125304.1618
for step 5296 | loss 3.464395 | norm 0.3057 | time 466.7752 ms | tok/sec 1123213.1092
for step 5297 | loss 3.462788 | norm 0.2955 | time 464.3710 ms | tok/sec 1129028.3813
for step 5298 | loss 3.444175 | norm 0.2865 | time 464.7913 ms | tok/sec 1128007.3494
for step 5299 | loss 3.421323 | norm 0.2891 | time 465.4117 ms | tok/sec 1126503.7850
for step 5300 | loss 3.439441 | norm 0.3145 | time 464.6921 ms | tok/sec 1128248.1071
for step 5301 | loss 3.479228 | norm 0.3106 | time 465.8232 ms | tok/sec 1125508.6260
for step 5302 | loss 3.478765 | norm 0.3029 | time 465.5013 ms | tok/sec 1126286.8449
for step 5303 | loss 3.356205 | norm 0.3370 | time 465.3692 ms | tok/sec 1126606.5144
for step 5304 | loss 3.245649 | norm 0.3366 | time 465.0440 ms | tok/sec 1127394.3447
for step 5305 | loss 3.216623 | norm 0.3771 | time 465.4274 ms | tok/sec 1126465.6990
for step 5306 | loss 3.259257 | norm 0.3839 | time 464.9906 ms | tok/sec 1127523.8298
for step 5307 | loss 3.302926 | norm 0.3674 | time 464.6480 ms | tok/sec 1128355.2078
for step 5308 | loss 3.259981 | norm 0.3818 | time 464.9684 ms | tok/sec 1127577.5980
for step 5309 | loss 3.260005 | norm 0.3958 | time 465.1520 ms | tok/sec 1127132.5751
for step 5310 | loss 3.261197 | norm 0.4028 | time 464.6173 ms | tok/sec 1128429.9008
for step 5311 | loss 3.272535 | norm 0.3781 | time 465.0352 ms | tok/sec 1127415.7308
for step 5312 | loss 3.336112 | norm 0.3724 | time 464.5920 ms | tok/sec 1128491.2839
for step 5313 | loss 3.307342 | norm 0.3524 | time 465.6019 ms | tok/sec 1126043.4639
for step 5314 | loss 3.304815 | norm 0.3289 | time 464.8600 ms | tok/sec 1127840.7313
for step 5315 | loss 3.513646 | norm 0.3653 | time 465.5240 ms | tok/sec 1126232.0462
for step 5316 | loss 3.593743 | norm 0.3594 | time 465.2619 ms | tok/sec 1126866.3074
for step 5317 | loss 3.454834 | norm 0.3977 | time 465.1484 ms | tok/sec 1127141.2410
for step 5318 | loss 3.479408 | norm 0.4675 | time 465.3635 ms | tok/sec 1126620.3670
for step 5319 | loss 3.551991 | norm 0.3658 | time 465.6098 ms | tok/sec 1126024.4361
for step 5320 | loss 3.563454 | norm 0.3981 | time 465.1558 ms | tok/sec 1127123.3316
for step 5321 | loss 3.545891 | norm 0.3711 | time 465.0757 ms | tok/sec 1127317.4770
for step 5322 | loss 3.548721 | norm 0.3546 | time 464.8407 ms | tok/sec 1127887.5876
for step 5323 | loss 3.501332 | norm 0.3860 | time 464.6633 ms | tok/sec 1128318.1544
for step 5324 | loss 3.554075 | norm 0.3614 | time 465.4551 ms | tok/sec 1126398.7663
for step 5325 | loss 3.586653 | norm 0.3394 | time 465.0123 ms | tok/sec 1127471.2229
for step 5326 | loss 3.471213 | norm 0.3634 | time 465.6899 ms | tok/sec 1125830.7358
for step 5327 | loss 3.550998 | norm 0.3302 | time 465.2100 ms | tok/sec 1126992.2057
for step 5328 | loss 3.493314 | norm 0.3650 | time 465.2972 ms | tok/sec 1126780.8512
for step 5329 | loss 3.551611 | norm 0.3272 | time 464.8669 ms | tok/sec 1127823.9565
for step 5330 | loss 3.509099 | norm 0.3315 | time 464.8199 ms | tok/sec 1127937.9192
for step 5331 | loss 3.548462 | norm 0.3290 | time 464.4878 ms | tok/sec 1128744.4150
for step 5332 | loss 3.542955 | norm 0.3214 | time 465.6465 ms | tok/sec 1125935.6485
Will loading at 0 from edu_fineweb10B/edufineweb_train_000029.npy
for step 5333 | loss 3.529142 | norm 0.3171 | time 1406.7907 ms | tok/sec 372683.7173
for step 5334 | loss 3.541866 | norm 0.2985 | time 462.3256 ms | tok/sec 1134023.3607
for step 5335 | loss 3.478677 | norm 0.3257 | time 463.8567 ms | tok/sec 1130280.1130
for step 5336 | loss 3.537222 | norm 0.3324 | time 464.3934 ms | tok/sec 1128973.8950
for step 5337 | loss 3.495600 | norm 0.3375 | time 464.1135 ms | tok/sec 1129654.7709
for step 5338 | loss 3.536838 | norm 0.3383 | time 466.1214 ms | tok/sec 1124788.4359
for step 5339 | loss 3.425156 | norm 0.3804 | time 464.6761 ms | tok/sec 1128286.8925
for step 5340 | loss 3.484695 | norm 0.3084 | time 464.9885 ms | tok/sec 1127529.0330
for step 5341 | loss 3.440266 | norm 0.3349 | time 464.8581 ms | tok/sec 1127845.3589
for step 5342 | loss 3.420879 | norm 0.3311 | time 464.4105 ms | tok/sec 1128932.1644
for step 5343 | loss 3.433206 | norm 0.3460 | time 464.6330 ms | tok/sec 1128391.6846
for step 5344 | loss 3.412325 | norm 0.3473 | time 465.1215 ms | tok/sec 1127206.5286
for step 5345 | loss 3.491098 | norm 0.3521 | time 464.8063 ms | tok/sec 1127970.8975
for step 5346 | loss 3.517986 | norm 0.3348 | time 464.5476 ms | tok/sec 1128599.0101
for step 5347 | loss 3.493913 | norm 0.3160 | time 464.5970 ms | tok/sec 1128479.1226
for step 5348 | loss 3.504502 | norm 0.3454 | time 465.0249 ms | tok/sec 1127440.5860
for step 5349 | loss 3.434582 | norm 0.3340 | time 465.6682 ms | tok/sec 1125883.1897
for step 5350 | loss 3.277503 | norm 0.3099 | time 465.2350 ms | tok/sec 1126931.5631
for step 5351 | loss 3.242611 | norm 0.3916 | time 464.1876 ms | tok/sec 1129474.3226
for step 5352 | loss 3.242825 | norm 0.3622 | time 465.9286 ms | tok/sec 1125254.0650
for step 5353 | loss 3.270334 | norm 0.3652 | time 465.9262 ms | tok/sec 1125259.8230
for step 5354 | loss 3.234752 | norm 0.3433 | time 466.0504 ms | tok/sec 1124959.9083
for step 5355 | loss 3.294377 | norm 0.3437 | time 465.5378 ms | tok/sec 1126198.5927
for step 5356 | loss 3.225806 | norm 0.3512 | time 465.1814 ms | tok/sec 1127061.5196
for step 5357 | loss 3.247483 | norm 0.3718 | time 465.3895 ms | tok/sec 1126557.4559
for step 5358 | loss 3.244155 | norm 0.3363 | time 466.0044 ms | tok/sec 1125070.9905
for step 5359 | loss 3.287223 | norm 0.3490 | time 466.1076 ms | tok/sec 1124821.8056
for step 5360 | loss 3.265269 | norm 0.3308 | time 465.3087 ms | tok/sec 1126753.1384
for step 5361 | loss 3.358056 | norm 0.3158 | time 466.1222 ms | tok/sec 1124786.7099
for step 5362 | loss 3.547458 | norm 0.3252 | time 466.1686 ms | tok/sec 1124674.5335
for step 5363 | loss 3.642174 | norm 0.3760 | time 466.4755 ms | tok/sec 1123934.7294
for step 5364 | loss 3.544856 | norm 0.4078 | time 465.3771 ms | tok/sec 1126587.4677
for step 5365 | loss 3.524385 | norm 0.4094 | time 465.5471 ms | tok/sec 1126176.0993
for step 5366 | loss 3.581778 | norm 0.4098 | time 465.1911 ms | tok/sec 1127037.8364
for step 5367 | loss 3.555966 | norm 0.3955 | time 464.7667 ms | tok/sec 1128066.9505
for step 5368 | loss 3.480343 | norm 0.3727 | time 464.7272 ms | tok/sec 1128163.0199
for step 5369 | loss 3.512073 | norm 0.3632 | time 465.7853 ms | tok/sec 1125600.2270
for step 5370 | loss 3.501175 | norm 0.3468 | time 464.9770 ms | tok/sec 1127556.7839
for step 5371 | loss 3.490765 | norm 0.3835 | time 465.7066 ms | tok/sec 1125790.3900
for step 5372 | loss 3.482771 | norm 0.3025 | time 465.7238 ms | tok/sec 1125748.8944
for step 5373 | loss 3.442983 | norm 0.3935 | time 466.2282 ms | tok/sec 1124530.7502
for step 5374 | loss 3.490213 | norm 0.3883 | time 464.8771 ms | tok/sec 1127799.0844
for step 5375 | loss 3.537766 | norm 0.3114 | time 465.5395 ms | tok/sec 1126194.5554
for step 5376 | loss 3.482054 | norm 0.3255 | time 466.3491 ms | tok/sec 1124239.2705
for step 5377 | loss 3.518679 | norm 0.3290 | time 465.4367 ms | tok/sec 1126443.1949
for step 5378 | loss 3.511200 | norm 0.3052 | time 466.0082 ms | tok/sec 1125061.7808
for step 5379 | loss 3.514682 | norm 0.3975 | time 465.0946 ms | tok/sec 1127271.8237
for step 5380 | loss 3.553274 | norm 0.3732 | time 465.2801 ms | tok/sec 1126822.4228
for step 5381 | loss 3.562432 | norm 0.3695 | time 465.7648 ms | tok/sec 1125649.7785
for step 5382 | loss 3.548308 | norm 0.3473 | time 465.6413 ms | tok/sec 1125948.3316
for step 5383 | loss 3.539464 | norm 0.3736 | time 465.2011 ms | tok/sec 1127013.5766
for step 5384 | loss 3.519682 | norm 0.3558 | time 465.6479 ms | tok/sec 1125932.1896
for step 5385 | loss 3.480709 | norm 0.2837 | time 465.2190 ms | tok/sec 1126970.2581
for step 5386 | loss 3.442653 | norm 0.3207 | time 465.3821 ms | tok/sec 1126575.3473
for step 5387 | loss 3.430842 | norm 0.3124 | time 465.4033 ms | tok/sec 1126523.9831
for step 5388 | loss 3.415071 | norm 0.2939 | time 465.1575 ms | tok/sec 1127119.2876
for step 5389 | loss 3.451756 | norm 0.3199 | time 467.0169 ms | tok/sec 1122631.6656
for step 5390 | loss 3.417721 | norm 0.3144 | time 466.4893 ms | tok/sec 1123901.4123
for step 5391 | loss 3.508612 | norm 0.3523 | time 464.5460 ms | tok/sec 1128603.0647
for step 5392 | loss 3.506736 | norm 0.3049 | time 465.8642 ms | tok/sec 1125409.5524
for step 5393 | loss 3.478756 | norm 0.3029 | time 465.7109 ms | tok/sec 1125780.0158
for step 5394 | loss 3.489273 | norm 0.3087 | time 465.6677 ms | tok/sec 1125884.3426
for step 5395 | loss 3.463424 | norm 0.2656 | time 465.8802 ms | tok/sec 1125370.9645
for step 5396 | loss 3.321519 | norm 0.3245 | time 465.5159 ms | tok/sec 1126251.6578
for step 5397 | loss 3.204155 | norm 0.3538 | time 465.9498 ms | tok/sec 1125202.8212
for step 5398 | loss 3.263639 | norm 0.3599 | time 465.0202 ms | tok/sec 1127452.1469
for step 5399 | loss 3.245932 | norm 0.3670 | time 465.2653 ms | tok/sec 1126858.2231
for step 5400 | loss 3.168586 | norm 0.3528 | time 465.7464 ms | tok/sec 1125694.1480
for step 5401 | loss 3.246592 | norm 0.3373 | time 465.2114 ms | tok/sec 1126988.7403
for step 5402 | loss 3.280129 | norm 0.3698 | time 465.4853 ms | tok/sec 1126325.4957
for step 5403 | loss 3.268542 | norm 0.3879 | time 465.5807 ms | tok/sec 1126094.7843
for step 5404 | loss 3.259789 | norm 0.3665 | time 465.5380 ms | tok/sec 1126198.0160
for step 5405 | loss 3.209616 | norm 0.4321 | time 465.6746 ms | tok/sec 1125867.6260
for step 5406 | loss 3.216692 | norm 0.4820 | time 465.6944 ms | tok/sec 1125819.7845
for step 5407 | loss 3.297474 | norm 0.3787 | time 465.3497 ms | tok/sec 1126653.8456
for step 5408 | loss 3.491642 | norm 0.3706 | time 465.3227 ms | tok/sec 1126719.0767
for step 5409 | loss 3.542473 | norm 0.3873 | time 465.5676 ms | tok/sec 1126126.5015
for step 5410 | loss 3.449300 | norm 0.3176 | time 465.8637 ms | tok/sec 1125410.7043
for step 5411 | loss 3.578127 | norm 0.3604 | time 465.1704 ms | tok/sec 1127088.0921
for step 5412 | loss 3.429686 | norm 0.6019 | time 465.2302 ms | tok/sec 1126943.1136
for step 5413 | loss 3.507518 | norm 0.4218 | time 466.0542 ms | tok/sec 1124950.7004
for step 5414 | loss 3.500573 | norm 0.3698 | time 465.5306 ms | tok/sec 1126215.8960
for step 5415 | loss 3.535631 | norm 0.3870 | time 465.1577 ms | tok/sec 1127118.7099
for step 5416 | loss 3.542589 | norm 0.3246 | time 465.1275 ms | tok/sec 1127192.0838
for step 5417 | loss 3.514276 | norm 0.3535 | time 465.6141 ms | tok/sec 1126014.0576
for step 5418 | loss 3.535956 | norm 0.3598 | time 465.9407 ms | tok/sec 1125224.7000
for step 5419 | loss 3.550997 | norm 0.3591 | time 465.7207 ms | tok/sec 1125756.3864
for step 5420 | loss 3.523091 | norm 0.3472 | time 465.9061 ms | tok/sec 1125308.1928
for step 5421 | loss 3.553080 | norm 0.2935 | time 465.2655 ms | tok/sec 1126857.6457
for step 5422 | loss 3.504948 | norm 0.3451 | time 465.5008 ms | tok/sec 1126287.9987
for step 5423 | loss 3.512295 | norm 0.3576 | time 465.3246 ms | tok/sec 1126714.4583
for step 5424 | loss 3.532634 | norm 0.3512 | time 465.7958 ms | tok/sec 1125574.8768
for step 5425 | loss 3.575611 | norm 0.3588 | time 466.5143 ms | tok/sec 1123841.1019
for step 5426 | loss 3.514910 | norm 0.3415 | time 465.9240 ms | tok/sec 1125265.0053
for step 5427 | loss 3.486439 | norm 0.3089 | time 466.4629 ms | tok/sec 1123965.1761
for step 5428 | loss 3.551262 | norm 0.3419 | time 465.1377 ms | tok/sec 1127167.2396
for step 5429 | loss 3.547817 | norm 0.3219 | time 466.0628 ms | tok/sec 1124929.9832
for step 5430 | loss 3.487512 | norm 0.4099 | time 464.9620 ms | tok/sec 1127593.2091
for step 5431 | loss 3.490717 | norm 0.3619 | time 464.8702 ms | tok/sec 1127815.8585
for step 5432 | loss 3.456659 | norm 0.3246 | time 465.2436 ms | tok/sec 1126910.7729
for step 5433 | loss 3.476522 | norm 0.3357 | time 465.2767 ms | tok/sec 1126830.5066
for step 5434 | loss 3.492112 | norm 0.3022 | time 465.3687 ms | tok/sec 1126607.6688
for step 5435 | loss 3.442559 | norm 0.3282 | time 465.7271 ms | tok/sec 1125740.8262
for step 5436 | loss 3.473567 | norm 0.3026 | time 465.2851 ms | tok/sec 1126810.2974
for step 5437 | loss 3.502188 | norm 0.2810 | time 465.3721 ms | tok/sec 1126599.5883
for step 5438 | loss 3.489115 | norm 0.2872 | time 466.0916 ms | tok/sec 1124860.3559
for step 5439 | loss 3.442247 | norm 0.2728 | time 465.4875 ms | tok/sec 1126320.3036
for step 5440 | loss 3.410195 | norm 0.3011 | time 464.6664 ms | tok/sec 1128310.6283
for step 5441 | loss 3.449731 | norm 0.2595 | time 463.7594 ms | tok/sec 1130517.1923
for step 5442 | loss 3.359123 | norm 0.2888 | time 465.0464 ms | tok/sec 1127388.5648
for step 5443 | loss 3.216560 | norm 0.2768 | time 465.3780 ms | tok/sec 1126585.1590
for step 5444 | loss 3.224162 | norm 0.3140 | time 465.2467 ms | tok/sec 1126903.2654
for step 5445 | loss 3.298712 | norm 0.3432 | time 464.9842 ms | tok/sec 1127539.4394
for step 5446 | loss 3.256887 | norm 0.3817 | time 465.2517 ms | tok/sec 1126891.1383
for step 5447 | loss 3.213366 | norm 0.3764 | time 464.2789 ms | tok/sec 1129252.1778
for step 5448 | loss 3.306631 | norm 0.3852 | time 465.1628 ms | tok/sec 1127106.5781
for step 5449 | loss 3.180572 | norm 0.3619 | time 465.7352 ms | tok/sec 1125721.2324
for step 5450 | loss 3.248112 | norm 0.4127 | time 465.9004 ms | tok/sec 1125322.0135
for step 5451 | loss 3.260571 | norm 0.4085 | time 465.9767 ms | tok/sec 1125137.7655
for step 5452 | loss 3.268994 | norm 0.3633 | time 465.0581 ms | tok/sec 1127360.2442
for step 5453 | loss 3.257798 | norm 0.3620 | time 464.8864 ms | tok/sec 1127776.5270
for step 5454 | loss 3.466523 | norm 0.3498 | time 465.2383 ms | tok/sec 1126923.4779
for step 5455 | loss 3.487793 | norm 0.3865 | time 465.1961 ms | tok/sec 1127025.7063
for step 5456 | loss 3.636281 | norm 0.3859 | time 465.1198 ms | tok/sec 1127210.5732
for step 5457 | loss 3.472062 | norm 0.3584 | time 464.8852 ms | tok/sec 1127779.4189
for step 5458 | loss 3.571313 | norm 0.3274 | time 465.6453 ms | tok/sec 1125938.5310
for step 5459 | loss 3.484003 | norm 0.3800 | time 465.0187 ms | tok/sec 1127455.6152
for step 5460 | loss 3.498885 | norm 0.3439 | time 465.0528 ms | tok/sec 1127372.9594
for step 5461 | loss 3.503621 | norm 0.3091 | time 465.2214 ms | tok/sec 1126964.4826
for step 5462 | loss 3.527476 | norm 0.3135 | time 465.4362 ms | tok/sec 1126444.3489
for step 5463 | loss 3.597087 | norm 0.3538 | time 465.2960 ms | tok/sec 1126783.7380
for step 5464 | loss 3.507671 | norm 0.3160 | time 465.4973 ms | tok/sec 1126296.6516
for step 5465 | loss 3.526565 | norm 0.3331 | time 465.7550 ms | tok/sec 1125673.4033
for step 5466 | loss 3.476594 | norm 0.3157 | time 465.0426 ms | tok/sec 1127397.8127
for step 5467 | loss 3.528693 | norm 0.3350 | time 465.4794 ms | tok/sec 1126339.9183
for step 5468 | loss 3.525941 | norm 0.3047 | time 465.4975 ms | tok/sec 1126296.0747
for step 5469 | loss 3.529206 | norm 0.3398 | time 465.3032 ms | tok/sec 1126766.4173
for step 5470 | loss 3.558074 | norm 0.3124 | time 465.3630 ms | tok/sec 1126621.5214
for step 5471 | loss 3.559879 | norm 0.3146 | time 465.8079 ms | tok/sec 1125545.4951
for step 5472 | loss 3.490474 | norm 0.3059 | time 464.8206 ms | tok/sec 1127936.1836
for step 5473 | loss 3.553164 | norm 0.2862 | time 465.1811 ms | tok/sec 1127062.0972
for step 5474 | loss 3.587630 | norm 0.2968 | time 465.5042 ms | tok/sec 1126279.9227
for step 5475 | loss 3.495244 | norm 0.3269 | time 464.5109 ms | tok/sec 1128688.2182
for step 5476 | loss 3.583404 | norm 0.3084 | time 465.7278 ms | tok/sec 1125739.0973
for step 5477 | loss 3.482497 | norm 0.3239 | time 465.6193 ms | tok/sec 1126001.3731
for step 5478 | loss 3.425121 | norm 0.3080 | time 464.8182 ms | tok/sec 1127941.9691
for step 5479 | loss 3.495075 | norm 0.3728 | time 465.1403 ms | tok/sec 1127160.8843
for step 5480 | loss 3.456801 | norm 0.3625 | time 464.3896 ms | tok/sec 1128983.1689
for step 5481 | loss 3.450238 | norm 0.3533 | time 465.3795 ms | tok/sec 1126581.6961
for step 5482 | loss 3.444735 | norm 0.3008 | time 464.6454 ms | tok/sec 1128361.5766
for step 5483 | loss 3.462840 | norm 0.3300 | time 464.8077 ms | tok/sec 1127967.4260
for step 5484 | loss 3.499803 | norm 0.2918 | time 465.3754 ms | tok/sec 1126591.5079
for step 5485 | loss 3.412513 | norm 0.3157 | time 465.0869 ms | tok/sec 1127290.3157
for step 5486 | loss 3.439621 | norm 0.3364 | time 466.0535 ms | tok/sec 1124952.4269
for step 5487 | loss 3.473139 | norm 0.3324 | time 465.2050 ms | tok/sec 1127004.3350
for step 5488 | loss 3.390780 | norm 0.3201 | time 466.2192 ms | tok/sec 1124552.6029
for step 5489 | loss 3.252263 | norm 0.3346 | time 464.5925 ms | tok/sec 1128490.1257
for step 5490 | loss 3.296079 | norm 0.3856 | time 464.9978 ms | tok/sec 1127506.4863
for step 5491 | loss 3.273826 | norm 0.3650 | time 465.3080 ms | tok/sec 1126754.8704
for step 5492 | loss 3.233357 | norm 0.3589 | time 464.9987 ms | tok/sec 1127504.1739
for step 5493 | loss 3.262675 | norm 0.3185 | time 466.0206 ms | tok/sec 1125031.8503
for step 5494 | loss 3.270278 | norm 0.3498 | time 464.9704 ms | tok/sec 1127572.9726
for step 5495 | loss 3.243340 | norm 0.3594 | time 465.1477 ms | tok/sec 1127142.9742
for step 5496 | loss 3.216902 | norm 0.3286 | time 464.5836 ms | tok/sec 1128511.5534
for step 5497 | loss 3.306896 | norm 0.3165 | time 464.8018 ms | tok/sec 1127981.8907
for step 5498 | loss 3.239526 | norm 0.3162 | time 464.1128 ms | tok/sec 1129656.5118
for step 5499 | loss 3.224404 | norm 0.3010 | time 465.2333 ms | tok/sec 1126935.6057
validation loss 3.5092
HellaSwag accuracy: 2724/10042=0.2713
> Hello, I'm a language model, and I'm a language model. Why do you use Java without Java?
Java provides a variety of Java programming languages
> Hello, I'm a language model, a model of my language. I use three different tools or models to illustrate each of your characters, but I'm not
> Hello, I'm a language model, but some other factors that I'm interested in are:
• Language model: I'm not talking about language as an
> Hello, I'm a language model, and I'm interested to add a language filter to my head before making decisions about the behavior of each language. If you
> Hello, I'm a language model, and I really like the way the data doesn't fit in your mind. And I'll see this, right? It
> > Hello, I'm a language model, so when I'm trying to learn another language. (My example was going to be an English speaker with no English background
Hello, I'm a language model, so I don't need to be a lot of pressure to understand the fundamentals of language syntax without creating a detailed model of
> Hello, I'm a language model, so I also recommend building languages that make sense if you're writing in languages that are difficult to understand.
- So
> Hello, I'm a language model, now it's probably easier to see when you go through this.
Here, a second one is called an xer> 
Hello, I'm a language model, and am aware of the other two terms that matter.
These are some other terminology terms that I know lots. This
> Hello, I'm a language model, I usually come across languages other than English or with a single subject and I tend to get myself writing all the content I
> Hello, I'm a language model, and it kinda has an infinite number of letters! As you continue reading these, I'll show you the difference between an
> Hello, I'm a language model, and I'm a person at the top class.
How to use Word AI algorithm?
Do you need to follow
> Hello, I'm a language model, then I'm a language model in one of my world?
You're a language model and I love using a very
>>>  Hello, I'm a language model, but this isn't true.
Let's start with the basics of English. In this article, I'm going to Hello, I'm a language model, and I am pretty good at translating any document into functional language. I would love to know how to use the tools ofHello, I'm a language model, and that is one of the things that you're really going to do, but in the end, I don't understand


>> >  Hello, I'm a language model, but I're really confused about the differences the model creates. The most often used model is the model used in the classroom> Hello, I'm a language model, I want to give you a detailed view on the different languages and uses the various idioms and idioms to describe some
Hello, I'm a language model, and I'm trying to do the above-source code for programming:
2d, 1m3cm3cm

Hello, I'm a language model, so I'm not afraid to look too near anything. I'm a professional language model, so I'm not afraid that
> > > > Hello, I'm a language model, and I think that we can understand it right now!
I'm using the concepts of the real world as a metaphor
Hello, I'm a language model, and I'm not really the one for your language.
Yes, here is an outline of the words you need to
Hello, I'm a language model, so I know I won't forget everything.
I'm a software engineer. And to be honest, I'm going
Hello, I'm a language model, so I've got a lot of it, but I'm gonna do. And the best part here is that I am
> > > > Hello, I'm a language model, but if you are wondering if this is the perfect place to find.
There are no rules about it. Some people
Hello, I'm a language model, and this post is a topic all the way to your knowledge. I'm not a language model, but I just love
Hello, I'm a language model, since I was talking about my language model, but you have to explain that really very little, at least for the reason
Hello, I'm a language model, but in a language model. I'm always learning. I think I'm always looking for my knowledge, so I try
> > Hello, I'm a language model, and I'm not afraid to be confused. I'm not sure how to react. But understanding the general idea of why
Hello, I'm a language model, and you have to choose the kind of framework you want to achieve. I mean, I want to understand how and why
> Hello, I'm a language model, and even if we try to develop a model to describe how language can change.
But for now, let's look
for step 5500 | loss 3.380775 | norm 0.2864 | time 12771.6558 ms | tok/sec 41050.9027
for step 5501 | loss 3.530192 | norm 0.3216 | time 462.6687 ms | tok/sec 1133182.4443
for step 5502 | loss 3.549312 | norm 0.3382 | time 462.3849 ms | tok/sec 1133877.7619
for step 5503 | loss 3.526273 | norm 0.3257 | time 463.8073 ms | tok/sec 1130400.3835
for step 5504 | loss 3.524617 | norm 0.3165 | time 463.2359 ms | tok/sec 1131794.9468
for step 5505 | loss 3.516354 | norm 0.2975 | time 463.8431 ms | tok/sec 1130313.2284
for step 5506 | loss 3.499749 | norm 0.3124 | time 464.6780 ms | tok/sec 1128282.2613
for step 5507 | loss 3.591975 | norm 0.3262 | time 463.4871 ms | tok/sec 1131181.3109
for step 5508 | loss 3.518917 | norm 0.3837 | time 464.1318 ms | tok/sec 1129610.0886
for step 5509 | loss 3.497241 | norm 0.3739 | time 463.8894 ms | tok/sec 1130200.5278
for step 5510 | loss 3.515422 | norm 0.3407 | time 465.2801 ms | tok/sec 1126822.4228
for step 5511 | loss 3.532074 | norm 0.3350 | time 464.5357 ms | tok/sec 1128627.9722
for step 5512 | loss 3.507601 | norm 0.3518 | time 464.1712 ms | tok/sec 1129514.3528
for step 5513 | loss 3.526180 | norm 0.3465 | time 464.4730 ms | tok/sec 1128780.3375
for step 5514 | loss 3.512097 | norm 0.3261 | time 464.7853 ms | tok/sec 1128021.8151
for step 5515 | loss 3.548462 | norm 0.3428 | time 465.1582 ms | tok/sec 1127117.5545
for step 5516 | loss 3.445753 | norm 0.3346 | time 465.0576 ms | tok/sec 1127361.4001
for step 5517 | loss 3.489342 | norm 0.3364 | time 463.1810 ms | tok/sec 1131928.9407
for step 5518 | loss 3.514688 | norm 0.3215 | time 464.7806 ms | tok/sec 1128033.3880
for step 5519 | loss 3.559337 | norm 0.3078 | time 464.5753 ms | tok/sec 1128531.8236
for step 5520 | loss 3.528697 | norm 0.3230 | time 464.8461 ms | tok/sec 1127874.2824
for step 5521 | loss 3.500833 | norm 0.3000 | time 465.7669 ms | tok/sec 1125644.5926
for step 5522 | loss 3.501036 | norm 0.3059 | time 464.7894 ms | tok/sec 1128011.9784
for step 5523 | loss 3.451644 | norm 0.3381 | time 464.6370 ms | tok/sec 1128381.8414
Will loading at 0 from edu_fineweb10B/edufineweb_train_000030.npy
for step 5524 | loss 3.447133 | norm 0.3415 | time 1414.1026 ms | tok/sec 370756.7025
for step 5525 | loss 3.476641 | norm 0.3324 | time 464.3486 ms | tok/sec 1129082.8728
for step 5526 | loss 3.479268 | norm 0.3211 | time 464.4063 ms | tok/sec 1128942.5968
for step 5527 | loss 3.476676 | norm 0.3412 | time 465.2078 ms | tok/sec 1126997.4040
for step 5528 | loss 3.451119 | norm 0.2971 | time 464.6974 ms | tok/sec 1128235.3721
for step 5529 | loss 3.496249 | norm 0.3313 | time 464.2026 ms | tok/sec 1129437.7758
for step 5530 | loss 3.478250 | norm 0.3683 | time 465.3001 ms | tok/sec 1126773.9228
for step 5531 | loss 3.451716 | norm 0.3765 | time 465.4744 ms | tok/sec 1126352.0336
for step 5532 | loss 3.463612 | norm 0.4076 | time 464.8464 ms | tok/sec 1127873.7039
for step 5533 | loss 3.438624 | norm 0.4257 | time 465.7884 ms | tok/sec 1125592.7371
for step 5534 | loss 3.299853 | norm 0.3728 | time 464.7775 ms | tok/sec 1128040.9104
for step 5535 | loss 3.270531 | norm 0.3970 | time 464.9181 ms | tok/sec 1127699.6072
for step 5536 | loss 3.262236 | norm 0.3676 | time 466.6152 ms | tok/sec 1123598.2024
for step 5537 | loss 3.256910 | norm 0.3482 | time 466.9008 ms | tok/sec 1122910.8439
for step 5538 | loss 3.295651 | norm 0.3691 | time 467.2701 ms | tok/sec 1122023.3431
for step 5539 | loss 3.223385 | norm 0.4611 | time 466.0420 ms | tok/sec 1124980.0512
for step 5540 | loss 3.275678 | norm 0.5416 | time 465.6265 ms | tok/sec 1125984.0764
for step 5541 | loss 3.229627 | norm 0.4718 | time 466.1174 ms | tok/sec 1124798.2165
for step 5542 | loss 3.236949 | norm 0.3145 | time 466.0406 ms | tok/sec 1124983.5043
for step 5543 | loss 3.257774 | norm 0.3689 | time 465.3738 ms | tok/sec 1126595.5480
for step 5544 | loss 3.225294 | norm 0.3324 | time 466.9452 ms | tok/sec 1122804.2009
for step 5545 | loss 3.239038 | norm 0.3231 | time 466.0995 ms | tok/sec 1124841.3681
for step 5546 | loss 3.437342 | norm 0.3277 | time 465.6215 ms | tok/sec 1125996.1840
for step 5547 | loss 3.495251 | norm 0.3168 | time 465.7152 ms | tok/sec 1125769.6418
for step 5548 | loss 3.486512 | norm 0.3211 | time 465.6484 ms | tok/sec 1125931.0366
for step 5549 | loss 3.514763 | norm 0.3247 | time 465.4438 ms | tok/sec 1126425.8847
for step 5550 | loss 3.499334 | norm 0.3428 | time 466.2652 ms | tok/sec 1124441.6230
for step 5551 | loss 3.449808 | norm 0.3633 | time 465.3285 ms | tok/sec 1126705.2217
for step 5552 | loss 3.517491 | norm 0.3653 | time 465.8594 ms | tok/sec 1125421.0717
for step 5553 | loss 3.506212 | norm 0.3573 | time 466.0182 ms | tok/sec 1125037.6060
for step 5554 | loss 3.535311 | norm 0.3717 | time 466.2657 ms | tok/sec 1124440.4731
for step 5555 | loss 3.524684 | norm 0.3921 | time 465.1983 ms | tok/sec 1127020.5078
for step 5556 | loss 3.510772 | norm 0.3407 | time 465.3547 ms | tok/sec 1126641.7238
for step 5557 | loss 3.573282 | norm 0.3451 | time 464.4949 ms | tok/sec 1128727.0340
for step 5558 | loss 3.520811 | norm 0.3285 | time 464.6637 ms | tok/sec 1128316.9965
for step 5559 | loss 3.545246 | norm 0.3289 | time 465.6377 ms | tok/sec 1125956.9793
for step 5560 | loss 3.550961 | norm 0.3561 | time 465.2472 ms | tok/sec 1126902.1105
for step 5561 | loss 3.518541 | norm 0.3115 | time 465.6034 ms | tok/sec 1126040.0042
for step 5562 | loss 3.518832 | norm 0.3402 | time 465.1620 ms | tok/sec 1127108.3112
for step 5563 | loss 3.485330 | norm 0.2977 | time 465.7350 ms | tok/sec 1125721.8086
for step 5564 | loss 3.513126 | norm 0.2934 | time 464.6623 ms | tok/sec 1128320.4702
for step 5565 | loss 3.564248 | norm 0.3457 | time 465.3962 ms | tok/sec 1126541.2964
for step 5566 | loss 3.496264 | norm 0.3307 | time 465.2386 ms | tok/sec 1126922.9004
for step 5567 | loss 3.569013 | norm 0.3126 | time 466.5384 ms | tok/sec 1123783.0951
for step 5568 | loss 3.515928 | norm 0.3635 | time 465.6329 ms | tok/sec 1125968.5099
for step 5569 | loss 3.400067 | norm 0.3231 | time 465.0648 ms | tok/sec 1127344.0616
for step 5570 | loss 3.450112 | norm 0.2945 | time 465.1773 ms | tok/sec 1127071.3397
for step 5571 | loss 3.441432 | norm 0.3195 | time 466.6548 ms | tok/sec 1123502.9089
for step 5572 | loss 3.424586 | norm 0.2839 | time 464.8669 ms | tok/sec 1127823.9565
for step 5573 | loss 3.459939 | norm 0.3008 | time 465.6692 ms | tok/sec 1125880.8840
for step 5574 | loss 3.434700 | norm 0.3312 | time 465.9789 ms | tok/sec 1125132.5844
for step 5575 | loss 3.400646 | norm 0.3049 | time 465.4806 ms | tok/sec 1126337.0337
for step 5576 | loss 3.471254 | norm 0.3093 | time 466.6722 ms | tok/sec 1123461.0078
for step 5577 | loss 3.472022 | norm 0.3480 | time 465.7443 ms | tok/sec 1125699.3342
for step 5578 | loss 3.454847 | norm 0.3281 | time 464.7319 ms | tok/sec 1128151.4444
for step 5579 | loss 3.473492 | norm 0.3293 | time 466.6200 ms | tok/sec 1123586.7204
for step 5580 | loss 3.262201 | norm 0.3205 | time 465.3559 ms | tok/sec 1126638.8377
for step 5581 | loss 3.218893 | norm 0.3699 | time 465.0729 ms | tok/sec 1127324.4120
for step 5582 | loss 3.263597 | norm 0.3249 | time 465.7638 ms | tok/sec 1125652.0833
for step 5583 | loss 3.216072 | norm 0.3627 | time 465.4875 ms | tok/sec 1126320.3036
for step 5584 | loss 3.263505 | norm 0.3799 | time 465.7428 ms | tok/sec 1125702.7918
for step 5585 | loss 3.201855 | norm 0.3513 | time 465.1239 ms | tok/sec 1127200.7506
for step 5586 | loss 3.246467 | norm 0.3341 | time 466.4693 ms | tok/sec 1123949.6653
for step 5587 | loss 3.240217 | norm 0.3027 | time 465.6634 ms | tok/sec 1125894.7187
for step 5588 | loss 3.211026 | norm 0.2852 | time 466.0754 ms | tok/sec 1124899.4842
for step 5589 | loss 3.236657 | norm 0.2993 | time 465.7583 ms | tok/sec 1125665.3362
for step 5590 | loss 3.287833 | norm 0.2991 | time 465.4946 ms | tok/sec 1126302.9972
for step 5591 | loss 3.365169 | norm 0.3352 | time 465.5137 ms | tok/sec 1126256.8492
for step 5592 | loss 3.465241 | norm 0.3674 | time 465.9300 ms | tok/sec 1125250.6102
for step 5593 | loss 3.492833 | norm 0.3501 | time 465.1973 ms | tok/sec 1127022.8183
for step 5594 | loss 3.484974 | norm 0.2999 | time 466.1481 ms | tok/sec 1124724.0034
for step 5595 | loss 3.520838 | norm 0.3660 | time 466.2786 ms | tok/sec 1124409.4257
for step 5596 | loss 3.516802 | norm 0.3368 | time 465.5793 ms | tok/sec 1126098.2443
for step 5597 | loss 3.524219 | norm 0.3142 | time 466.1639 ms | tok/sec 1124686.0377
for step 5598 | loss 3.508077 | norm 0.3443 | time 466.1810 ms | tok/sec 1124644.6235
for step 5599 | loss 3.479710 | norm 0.3500 | time 466.3422 ms | tok/sec 1124255.9389
for step 5600 | loss 3.519765 | norm 0.3285 | time 466.4729 ms | tok/sec 1123941.0484
for step 5601 | loss 3.502706 | norm 0.3485 | time 465.7495 ms | tok/sec 1125686.6568
for step 5602 | loss 3.517315 | norm 0.3457 | time 465.8468 ms | tok/sec 1125451.5990
for step 5603 | loss 3.557160 | norm 0.3396 | time 465.2212 ms | tok/sec 1126965.0601
for step 5604 | loss 3.471591 | norm 0.3419 | time 466.2044 ms | tok/sec 1124588.2591
for step 5605 | loss 3.569863 | norm 0.3483 | time 465.5550 ms | tok/sec 1126157.0671
for step 5606 | loss 3.511960 | norm 0.3329 | time 466.2635 ms | tok/sec 1124445.6478
for step 5607 | loss 3.554030 | norm 0.3372 | time 465.9951 ms | tok/sec 1125093.4399
for step 5608 | loss 3.528507 | norm 0.3068 | time 464.9508 ms | tok/sec 1127620.3849
for step 5609 | loss 3.463920 | norm 0.3245 | time 465.3933 ms | tok/sec 1126548.2218
for step 5610 | loss 3.483380 | norm 0.3056 | time 465.5149 ms | tok/sec 1126253.9650
for step 5611 | loss 3.504375 | norm 0.3243 | time 465.9152 ms | tok/sec 1125286.3107
for step 5612 | loss 3.522372 | norm 0.3292 | time 465.0753 ms | tok/sec 1127318.6328
for step 5613 | loss 3.492446 | norm 0.3192 | time 466.2945 ms | tok/sec 1124370.9063
for step 5614 | loss 3.503332 | norm 0.3274 | time 466.1624 ms | tok/sec 1124689.4890
for step 5615 | loss 3.428047 | norm 0.3567 | time 465.2445 ms | tok/sec 1126908.4629
for step 5616 | loss 3.430499 | norm 0.3710 | time 466.1362 ms | tok/sec 1124752.7670
for step 5617 | loss 3.435205 | norm 0.2916 | time 465.6405 ms | tok/sec 1125950.0612
for step 5618 | loss 3.486042 | norm 0.3168 | time 467.1426 ms | tok/sec 1122329.7132
for step 5619 | loss 3.471811 | norm 0.3546 | time 465.6053 ms | tok/sec 1126035.3914
for step 5620 | loss 3.468166 | norm 0.2659 | time 465.3857 ms | tok/sec 1126566.6901
for step 5621 | loss 3.441382 | norm 0.3207 | time 464.9832 ms | tok/sec 1127541.7520
for step 5622 | loss 3.491419 | norm 0.2853 | time 464.8097 ms | tok/sec 1127962.7974
for step 5623 | loss 3.458915 | norm 0.2820 | time 465.5039 ms | tok/sec 1126280.4995
for step 5624 | loss 3.481035 | norm 0.2806 | time 465.9159 ms | tok/sec 1125284.5832
for step 5625 | loss 3.208251 | norm 0.3899 | time 466.0077 ms | tok/sec 1125062.9320
for step 5626 | loss 3.211353 | norm 0.5010 | time 465.8582 ms | tok/sec 1125423.9515
for step 5627 | loss 3.176739 | norm 0.4544 | time 466.1455 ms | tok/sec 1124730.3313
for step 5628 | loss 3.240736 | norm 0.3913 | time 465.0154 ms | tok/sec 1127463.7080
for step 5629 | loss 3.206339 | norm 0.3589 | time 466.1732 ms | tok/sec 1124663.6046
for step 5630 | loss 3.250040 | norm 0.4271 | time 465.7297 ms | tok/sec 1125734.4869
for step 5631 | loss 3.250462 | norm 0.3616 | time 466.4278 ms | tok/sec 1124049.6312
for step 5632 | loss 3.287554 | norm 0.3405 | time 465.3962 ms | tok/sec 1126541.2964
for step 5633 | loss 3.236664 | norm 0.3447 | time 465.4331 ms | tok/sec 1126451.8502
for step 5634 | loss 3.196874 | norm 0.3185 | time 465.7404 ms | tok/sec 1125708.5544
for step 5635 | loss 3.187145 | norm 0.3548 | time 465.0342 ms | tok/sec 1127418.0429
for step 5636 | loss 3.411267 | norm 0.3276 | time 465.0440 ms | tok/sec 1127394.3447
for step 5637 | loss 3.559753 | norm 0.3307 | time 466.0740 ms | tok/sec 1124902.9369
for step 5638 | loss 3.519660 | norm 0.3459 | time 464.9329 ms | tok/sec 1127663.7534
for step 5639 | loss 3.465144 | norm 0.3317 | time 465.8315 ms | tok/sec 1125488.4643
for step 5640 | loss 3.502481 | norm 0.3293 | time 465.8642 ms | tok/sec 1125409.5524
for step 5641 | loss 3.513436 | norm 0.3244 | time 465.4057 ms | tok/sec 1126518.2121
for step 5642 | loss 3.485009 | norm 0.3335 | time 465.1396 ms | tok/sec 1127162.6175
for step 5643 | loss 3.473236 | norm 0.3212 | time 465.5743 ms | tok/sec 1126110.3543
for step 5644 | loss 3.490777 | norm 0.2848 | time 465.1384 ms | tok/sec 1127165.5063
for step 5645 | loss 3.556792 | norm 0.3264 | time 465.7242 ms | tok/sec 1125747.7418
for step 5646 | loss 3.499762 | norm 0.3494 | time 465.5890 ms | tok/sec 1126074.6016
for step 5647 | loss 3.537439 | norm 0.3810 | time 465.9369 ms | tok/sec 1125233.9124
for step 5648 | loss 3.494136 | norm 0.3746 | time 465.2629 ms | tok/sec 1126863.9976
for step 5649 | loss 3.531010 | norm 0.4292 | time 465.3068 ms | tok/sec 1126757.7571
for step 5650 | loss 3.554193 | norm 0.4007 | time 467.0250 ms | tok/sec 1122612.1799
for step 5651 | loss 3.481111 | norm 0.3589 | time 466.2609 ms | tok/sec 1124451.9725
for step 5652 | loss 3.514277 | norm 0.3558 | time 465.5631 ms | tok/sec 1126137.4588
for step 5653 | loss 3.487701 | norm 0.3681 | time 464.1731 ms | tok/sec 1129509.7114
for step 5654 | loss 3.532780 | norm 0.3254 | time 465.0507 ms | tok/sec 1127378.1612
for step 5655 | loss 3.500489 | norm 0.3243 | time 465.5454 ms | tok/sec 1126180.1365
for step 5656 | loss 3.486401 | norm 0.3129 | time 465.6265 ms | tok/sec 1125984.0764
for step 5657 | loss 3.541364 | norm 0.3270 | time 465.8813 ms | tok/sec 1125368.0849
for step 5658 | loss 3.539425 | norm 0.3207 | time 465.6305 ms | tok/sec 1125974.2752
for step 5659 | loss 3.443488 | norm 0.3732 | time 465.9142 ms | tok/sec 1125288.6141
for step 5660 | loss 3.477587 | norm 0.3981 | time 466.1324 ms | tok/sec 1124761.9717
for step 5661 | loss 3.450805 | norm 0.3866 | time 465.0085 ms | tok/sec 1127480.4721
for step 5662 | loss 3.455711 | norm 0.3308 | time 465.2452 ms | tok/sec 1126906.7304
for step 5663 | loss 3.482832 | norm 0.3545 | time 465.1899 ms | tok/sec 1127040.7245
for step 5664 | loss 3.428349 | norm 0.3247 | time 464.5863 ms | tok/sec 1128505.1829
for step 5665 | loss 3.421570 | norm 0.3345 | time 465.2295 ms | tok/sec 1126944.8462
for step 5666 | loss 3.440984 | norm 0.3289 | time 465.4629 ms | tok/sec 1126379.7266
for step 5667 | loss 3.460842 | norm 0.3436 | time 465.4593 ms | tok/sec 1126388.3809
for step 5668 | loss 3.405418 | norm 0.3122 | time 465.5917 ms | tok/sec 1126068.2586
for step 5669 | loss 3.461761 | norm 0.3315 | time 465.0445 ms | tok/sec 1127393.1887
for step 5670 | loss 3.292882 | norm 0.3847 | time 464.6192 ms | tok/sec 1128425.2684
for step 5671 | loss 3.265748 | norm 0.4427 | time 465.6363 ms | tok/sec 1125960.4385
for step 5672 | loss 3.292224 | norm 0.4363 | time 464.8645 ms | tok/sec 1127829.7408
for step 5673 | loss 3.270921 | norm 0.3525 | time 464.8199 ms | tok/sec 1127937.9192
for step 5674 | loss 3.289161 | norm 0.3976 | time 466.2170 ms | tok/sec 1124557.7786
for step 5675 | loss 3.244200 | norm 0.4037 | time 465.5433 ms | tok/sec 1126185.3273
for step 5676 | loss 3.295526 | norm 0.3867 | time 465.0745 ms | tok/sec 1127320.3666
for step 5677 | loss 3.198547 | norm 0.3425 | time 464.8290 ms | tok/sec 1127915.9348
for step 5678 | loss 3.229703 | norm 0.3198 | time 465.8415 ms | tok/sec 1125464.2711
for step 5679 | loss 3.186054 | norm 0.3193 | time 465.4660 ms | tok/sec 1126372.2263
for step 5680 | loss 3.238077 | norm 0.3337 | time 465.1585 ms | tok/sec 1127116.9768
for step 5681 | loss 3.325490 | norm 0.3102 | time 465.4479 ms | tok/sec 1126416.0757
for step 5682 | loss 3.524313 | norm 0.3101 | time 464.0303 ms | tok/sec 1129857.3361
for step 5683 | loss 3.440196 | norm 0.4074 | time 464.8652 ms | tok/sec 1127828.0055
for step 5684 | loss 3.534227 | norm 0.3800 | time 465.7247 ms | tok/sec 1125746.5892
for step 5685 | loss 3.474715 | norm 0.3270 | time 465.1411 ms | tok/sec 1127159.1510
for step 5686 | loss 3.512673 | norm 0.3020 | time 465.1668 ms | tok/sec 1127096.7574
for step 5687 | loss 3.501215 | norm 0.3367 | time 465.2719 ms | tok/sec 1126842.0550
for step 5688 | loss 3.500842 | norm 0.3241 | time 465.2636 ms | tok/sec 1126862.2652
for step 5689 | loss 3.543658 | norm 0.3146 | time 465.1735 ms | tok/sec 1127080.5824
for step 5690 | loss 3.551781 | norm 0.3446 | time 465.1582 ms | tok/sec 1127117.5545
for step 5691 | loss 3.612419 | norm 0.5324 | time 465.7216 ms | tok/sec 1125754.0812
for step 5692 | loss 3.553760 | norm 0.4237 | time 465.7733 ms | tok/sec 1125629.0355
for step 5693 | loss 3.484175 | norm 0.3480 | time 465.5817 ms | tok/sec 1126092.4777
for step 5694 | loss 3.518001 | norm 0.3321 | time 466.1398 ms | tok/sec 1124744.1378
for step 5695 | loss 3.491189 | norm 0.3390 | time 465.4591 ms | tok/sec 1126388.9579
for step 5696 | loss 3.535769 | norm 0.3314 | time 464.5846 ms | tok/sec 1128509.2369
for step 5697 | loss 3.500310 | norm 0.2959 | time 465.1024 ms | tok/sec 1127252.7544
for step 5698 | loss 3.555085 | norm 0.3374 | time 465.1818 ms | tok/sec 1127060.3643
for step 5699 | loss 3.449458 | norm 0.3088 | time 465.7986 ms | tok/sec 1125567.9633
for step 5700 | loss 3.486735 | norm 0.3008 | time 466.6929 ms | tok/sec 1123411.0750
for step 5701 | loss 3.529381 | norm 0.3015 | time 465.5428 ms | tok/sec 1126186.4808
for step 5702 | loss 3.503244 | norm 0.3155 | time 466.1994 ms | tok/sec 1124600.3367
for step 5703 | loss 3.521483 | norm 0.3286 | time 466.1422 ms | tok/sec 1124738.3850
for step 5704 | loss 3.528983 | norm 0.3509 | time 465.4934 ms | tok/sec 1126305.8815
for step 5705 | loss 3.452102 | norm 0.3628 | time 465.6806 ms | tok/sec 1125853.2155
for step 5706 | loss 3.448296 | norm 0.3168 | time 465.1799 ms | tok/sec 1127064.9855
for step 5707 | loss 3.479586 | norm 0.3280 | time 465.4276 ms | tok/sec 1126465.1220
for step 5708 | loss 3.383291 | norm 0.3586 | time 464.8447 ms | tok/sec 1127877.7533
for step 5709 | loss 3.426504 | norm 0.2981 | time 465.6017 ms | tok/sec 1126044.0405
for step 5710 | loss 3.496346 | norm 0.3460 | time 465.7910 ms | tok/sec 1125586.3995
for step 5711 | loss 3.446414 | norm 0.2941 | time 465.4565 ms | tok/sec 1126395.3045
for step 5712 | loss 3.472308 | norm 0.3123 | time 465.7803 ms | tok/sec 1125612.3264
for step 5713 | loss 3.492602 | norm 0.2878 | time 464.8335 ms | tok/sec 1127904.9429
Will loading at 0 from edu_fineweb10B/edufineweb_train_000031.npy
for step 5714 | loss 3.479484 | norm 0.2951 | time 1395.1705 ms | tok/sec 375787.7756
for step 5715 | loss 3.342866 | norm 0.3403 | time 462.8487 ms | tok/sec 1132741.7394
for step 5716 | loss 3.149082 | norm 0.3579 | time 465.9705 ms | tok/sec 1125152.7334
for step 5717 | loss 3.279641 | norm 0.3815 | time 465.8442 ms | tok/sec 1125457.9350
for step 5718 | loss 3.261717 | norm 0.3464 | time 465.3094 ms | tok/sec 1126751.4064
for step 5719 | loss 3.220338 | norm 0.3757 | time 465.3726 ms | tok/sec 1126598.4339
for step 5720 | loss 3.219250 | norm 0.3777 | time 464.5762 ms | tok/sec 1128529.5070
for step 5721 | loss 3.218821 | norm 0.3729 | time 465.1225 ms | tok/sec 1127204.2174
for step 5722 | loss 3.249335 | norm 0.3557 | time 465.2267 ms | tok/sec 1126951.7766
for step 5723 | loss 3.195139 | norm 0.3197 | time 465.8258 ms | tok/sec 1125502.2894
for step 5724 | loss 3.275030 | norm 0.3342 | time 465.9588 ms | tok/sec 1125180.9432
for step 5725 | loss 3.304460 | norm 0.3413 | time 465.1487 ms | tok/sec 1127140.6633
for step 5726 | loss 3.182736 | norm 0.3313 | time 466.7406 ms | tok/sec 1123296.3037
for step 5727 | loss 3.400772 | norm 0.3619 | time 465.5113 ms | tok/sec 1126262.6175
for step 5728 | loss 3.542501 | norm 0.3433 | time 465.3442 ms | tok/sec 1126667.1221
for step 5729 | loss 3.519390 | norm 0.3552 | time 465.6155 ms | tok/sec 1126010.5982
for step 5730 | loss 3.522341 | norm 0.3887 | time 465.2805 ms | tok/sec 1126821.2680
for step 5731 | loss 3.523743 | norm 0.4080 | time 465.0435 ms | tok/sec 1127395.5007
for step 5732 | loss 3.530770 | norm 0.3715 | time 465.7006 ms | tok/sec 1125804.7989
for step 5733 | loss 3.563253 | norm 0.4017 | time 466.2867 ms | tok/sec 1124389.8782
for step 5734 | loss 3.493805 | norm 0.3930 | time 465.0698 ms | tok/sec 1127331.9250
for step 5735 | loss 3.512887 | norm 0.3169 | time 465.0505 ms | tok/sec 1127378.7391
for step 5736 | loss 3.512891 | norm 0.3402 | time 465.4958 ms | tok/sec 1126300.1128
for step 5737 | loss 3.510368 | norm 0.3100 | time 466.0568 ms | tok/sec 1124944.3701
for step 5738 | loss 3.452940 | norm 0.3349 | time 466.3165 ms | tok/sec 1124318.0184
for step 5739 | loss 3.499721 | norm 0.3105 | time 465.3573 ms | tok/sec 1126635.3744
for step 5740 | loss 3.588203 | norm 0.3280 | time 465.2870 ms | tok/sec 1126805.6783
for step 5741 | loss 3.441558 | norm 0.3582 | time 465.6539 ms | tok/sec 1125917.7774
for step 5742 | loss 3.509025 | norm 0.3588 | time 465.8134 ms | tok/sec 1125532.2450
for step 5743 | loss 3.580468 | norm 0.3649 | time 465.4772 ms | tok/sec 1126345.1105
for step 5744 | loss 3.470559 | norm 0.3229 | time 465.4591 ms | tok/sec 1126388.9579
for step 5745 | loss 3.500229 | norm 0.3400 | time 465.7471 ms | tok/sec 1125692.4192
for step 5746 | loss 3.523312 | norm 0.3355 | time 465.4670 ms | tok/sec 1126369.9185
for step 5747 | loss 3.481892 | norm 0.3124 | time 465.9293 ms | tok/sec 1125252.3376
for step 5748 | loss 3.479794 | norm 0.3011 | time 465.6577 ms | tok/sec 1125908.5538
for step 5749 | loss 3.502182 | norm 0.3051 | time 466.2559 ms | tok/sec 1124464.0472
validation loss 3.4871
HellaSwag accuracy: 2710/10042=0.2699
> Hello, I'm a language model, so there is no way to explain linguists who write more grammar.
To start you have to understand a few things
> Hello, I'm a language model, and you'll want to learn something more in the second sentence. Here's one of your favorite songs:
- "
> Hello, I'm a language model, so have you ever wondered what the difference between a language model and a computer model is. So how is it defined in
> Hello, I'm a language model, and I'm very excited to learn this more.
Well, maybe a small amount of energy to use. It's
> Hello, I'm a language model, but I also am interested in the way these words came into existence. This is so the story here is one of my
> Hello, I'm a language model, based on the current status of all the languages (nazi, etc.). It makes you feel like learning something online.
> Hello, I'm a language model, I actually believe so because there are two types of languages. You might think that if you had two other languages on the
> Hello, I'm a language model, and the entire text has the same grammar: you might confuse grammar and clarity with meaning. I was really confused. The
> Hello, I'm a language model, so that I can find a lot of resources for language models. In fact, it's a pretty good place to learn
> Hello, I'm a language model, in practice, it wasn't very difficult to talk about a model system, I'd just use the phrase of a very
> Hello, I'm a language model, but it works like simple language (not complicated), and it's still nice. It'll be interesting to see how a
> > > Hello, I'm a language model, and this article looks at the concept of a language model in general. I've talked about the language model's main focus
> Hello, I'm a language model, an object that's more interesting than the language itself...I'm very curious and it looks very difficult. I've done
Hello, I'm a language model, and I'm trying to make the distinction between a fully functional and an ever-evolving problem. How does I decide
> Hello, I'm a language model,
as a kid in other countries, but I know how much a child understands English. However, I've learned in
> Hello, I'm a language model, so I asked him some questions. You, I talked to me with Dr. James R. Clark and I asked him
Hello, I'm a language model, and a language model. Do it correctly, though, since I've been using it for a very long time! It
> > > Hello, I'm a language model, and I don't need a math background I am trying to understand. In fact, I'm sure you will find a
Hello, I'm a language model, but I don't want to say much about it, but I don't like the idea that all languages are complex,
Hello, I'm a language model, and you have to spend a lot of time reading it, trying to figure out how to interpret any text, the main
> > > Hello, I'm a language model, so what's going on in a language model?
I think of language as a field of study, with two main
Hello, I'm a language model, so, instead of trying to define a lot of definitions to make sense of everything in your dictionary definition. For instance,
Hello, I'm a language model, and to me, I used a similar language for all kinds of other languages. Let's call it E. Here,
> Hello, I'm a language model, and am looking to see how many words or phrases are in a single word (for me?)
After my talk showed
> Hello, I'm a language model, and I can't understand this very well to learn, but I understand so quickly that we can see that this is very
> Hello, I'm a language model, but there's a lot of people say that they did an awful lot. The problem, they didn't have a clue
> Hello, I'm a language model, but I thought of one or two of the language models that can make the difference. I've been using the term '
> Hello, I'm a language model, so now I'm going to be saying this out loud.
First, you should read all of the languages you'd
> Hello, I'm a language model, and I'm a teacher, and I need to explain everything in a particular way on my own, and then I'll
> Hello, I'm a language model, what do you mean by a machine is machine translation? Thanks for visiting.
Do you think you would want to go
> Hello, I'm a language model, so I've gotten a lot of me out of my head. If you're a language model, you're pretty,
> Hello, I'm a language model, but it's not quite like the above diagram.
We're talking about the first section of the spectrum. As the
for step 5750 | loss 3.434449 | norm 0.3178 | time 18384.0175 ms | tok/sec 28518.6848
for step 5751 | loss 3.425966 | norm 0.2717 | time 462.8518 ms | tok/sec 1132734.1542
for step 5752 | loss 3.472063 | norm 0.3295 | time 463.4230 ms | tok/sec 1131337.8587
for step 5753 | loss 3.414924 | norm 0.3297 | time 466.4245 ms | tok/sec 1124057.6752
for step 5754 | loss 3.442602 | norm 0.3201 | time 463.9075 ms | tok/sec 1130156.3832
for step 5755 | loss 3.403095 | norm 0.2892 | time 463.3758 ms | tok/sec 1131453.1149
for step 5756 | loss 3.413803 | norm 0.2809 | time 463.5057 ms | tok/sec 1131135.9260
for step 5757 | loss 3.450750 | norm 0.3250 | time 463.7561 ms | tok/sec 1130525.3291
for step 5758 | loss 3.462094 | norm 0.3417 | time 463.7306 ms | tok/sec 1130587.5217
for step 5759 | loss 3.426265 | norm 0.3050 | time 464.0596 ms | tok/sec 1129785.9366
for step 5760 | loss 3.456116 | norm 0.3070 | time 463.4411 ms | tok/sec 1131293.6252
for step 5761 | loss 3.228157 | norm 0.3141 | time 463.4500 ms | tok/sec 1131272.0918
for step 5762 | loss 3.174736 | norm 0.3901 | time 464.7098 ms | tok/sec 1128205.2725
for step 5763 | loss 3.212187 | norm 0.4148 | time 464.4933 ms | tok/sec 1128731.0895
for step 5764 | loss 3.234065 | norm 0.3343 | time 463.9916 ms | tok/sec 1129951.3882
for step 5765 | loss 3.198856 | norm 0.3449 | time 464.3996 ms | tok/sec 1128958.8253
for step 5766 | loss 3.209513 | norm 0.3920 | time 464.1364 ms | tok/sec 1129599.0636
for step 5767 | loss 3.235564 | norm 0.3301 | time 464.5705 ms | tok/sec 1128543.4069
for step 5768 | loss 3.200023 | norm 0.2974 | time 465.4853 ms | tok/sec 1126325.4957
for step 5769 | loss 3.205072 | norm 0.3197 | time 464.9804 ms | tok/sec 1127548.6897
for step 5770 | loss 3.223776 | norm 0.3174 | time 464.9062 ms | tok/sec 1127728.5232
for step 5771 | loss 3.217356 | norm 0.3395 | time 465.0550 ms | tok/sec 1127367.7577
for step 5772 | loss 3.286112 | norm 0.3427 | time 465.0841 ms | tok/sec 1127297.2504
for step 5773 | loss 3.447191 | norm 0.3227 | time 465.7450 ms | tok/sec 1125697.6055
for step 5774 | loss 3.466739 | norm 0.3195 | time 465.5237 ms | tok/sec 1126232.6230
for step 5775 | loss 3.483388 | norm 0.3103 | time 465.4043 ms | tok/sec 1126521.6747
for step 5776 | loss 3.460817 | norm 0.2895 | time 464.8123 ms | tok/sec 1127956.4331
for step 5777 | loss 3.428894 | norm 0.3104 | time 465.9970 ms | tok/sec 1125088.8348
for step 5778 | loss 3.465288 | norm 0.3018 | time 465.7791 ms | tok/sec 1125615.2072
for step 5779 | loss 3.411708 | norm 0.3065 | time 466.1884 ms | tok/sec 1124626.7933
for step 5780 | loss 3.467541 | norm 0.3342 | time 465.7261 ms | tok/sec 1125743.1314
for step 5781 | loss 3.511308 | norm 0.2697 | time 466.0549 ms | tok/sec 1124948.9740
for step 5782 | loss 3.465837 | norm 0.2994 | time 465.7013 ms | tok/sec 1125803.0698
for step 5783 | loss 3.474476 | norm 0.3294 | time 468.2910 ms | tok/sec 1119577.2511
for step 5784 | loss 3.442518 | norm 0.3049 | time 466.2788 ms | tok/sec 1124408.8508
for step 5785 | loss 3.474414 | norm 0.3003 | time 465.9820 ms | tok/sec 1125125.1007
for step 5786 | loss 3.495623 | norm 0.3249 | time 466.1880 ms | tok/sec 1124627.9436
for step 5787 | loss 3.456227 | norm 0.3064 | time 465.6372 ms | tok/sec 1125958.1324
for step 5788 | loss 3.466648 | norm 0.3002 | time 466.5771 ms | tok/sec 1123690.0669
for step 5789 | loss 3.475922 | norm 0.3108 | time 465.4074 ms | tok/sec 1126514.1725
for step 5790 | loss 3.447538 | norm 0.3217 | time 466.3308 ms | tok/sec 1124283.5289
for step 5791 | loss 3.475752 | norm 0.3542 | time 465.9421 ms | tok/sec 1125221.2454
for step 5792 | loss 3.491857 | norm 0.3135 | time 467.1063 ms | tok/sec 1122416.7872
for step 5793 | loss 3.469240 | norm 0.3450 | time 466.1863 ms | tok/sec 1124631.9698
for step 5794 | loss 3.464559 | norm 0.3344 | time 467.0861 ms | tok/sec 1122465.4857
for step 5795 | loss 3.503227 | norm 0.3205 | time 466.4986 ms | tok/sec 1123879.0105
for step 5796 | loss 3.431256 | norm 0.3663 | time 467.8943 ms | tok/sec 1120526.5428
for step 5797 | loss 3.481650 | norm 0.3296 | time 466.1789 ms | tok/sec 1124649.8001
for step 5798 | loss 3.472486 | norm 0.3101 | time 466.2876 ms | tok/sec 1124387.5786
for step 5799 | loss 3.459316 | norm 0.3346 | time 465.9431 ms | tok/sec 1125218.9423
for step 5800 | loss 3.426774 | norm 0.3233 | time 465.8980 ms | tok/sec 1125327.7722
for step 5801 | loss 3.448411 | norm 0.3105 | time 466.6121 ms | tok/sec 1123605.6659
for step 5802 | loss 3.482565 | norm 0.3272 | time 465.4412 ms | tok/sec 1126432.2317
for step 5803 | loss 3.438611 | norm 0.3046 | time 466.3072 ms | tok/sec 1124340.4377
for step 5804 | loss 3.431093 | norm 0.3384 | time 464.5243 ms | tok/sec 1128655.7773
for step 5805 | loss 3.388340 | norm 0.3356 | time 465.8005 ms | tok/sec 1125563.3544
for step 5806 | loss 3.367958 | norm 0.3765 | time 465.7259 ms | tok/sec 1125743.7077
for step 5807 | loss 3.136930 | norm 0.3335 | time 465.0738 ms | tok/sec 1127322.1003
for step 5808 | loss 3.300973 | norm 0.4351 | time 465.9984 ms | tok/sec 1125085.3810
for step 5809 | loss 3.206619 | norm 0.4870 | time 466.3322 ms | tok/sec 1124280.0801
for step 5810 | loss 3.191595 | norm 0.4705 | time 466.1510 ms | tok/sec 1124717.1004
for step 5811 | loss 3.269251 | norm 0.4620 | time 464.9763 ms | tok/sec 1127558.5184
for step 5812 | loss 3.245203 | norm 0.3682 | time 466.5067 ms | tok/sec 1123859.4815
for step 5813 | loss 3.227209 | norm 0.3519 | time 465.4057 ms | tok/sec 1126518.2121
for step 5814 | loss 3.216970 | norm 0.3601 | time 466.5155 ms | tok/sec 1123838.2301
for step 5815 | loss 3.276238 | norm 0.3438 | time 466.4116 ms | tok/sec 1124088.7032
for step 5816 | loss 3.211041 | norm 0.3028 | time 466.2633 ms | tok/sec 1124446.2228
for step 5817 | loss 3.232510 | norm 0.3264 | time 465.8086 ms | tok/sec 1125543.7668
for step 5818 | loss 3.526007 | norm 0.3172 | time 465.9603 ms | tok/sec 1125177.4889
for step 5819 | loss 3.474099 | norm 0.3569 | time 465.9145 ms | tok/sec 1125288.0382
for step 5820 | loss 3.499881 | norm 0.3345 | time 466.8276 ms | tok/sec 1123086.9066
for step 5821 | loss 3.519455 | norm 0.3735 | time 465.5550 ms | tok/sec 1126157.0671
for step 5822 | loss 3.445061 | norm 0.4021 | time 466.2521 ms | tok/sec 1124473.2472
for step 5823 | loss 3.495881 | norm 0.3419 | time 465.3704 ms | tok/sec 1126603.6285
for step 5824 | loss 3.477552 | norm 0.3672 | time 466.9456 ms | tok/sec 1122803.0543
for step 5825 | loss 3.464087 | norm 0.3534 | time 466.0065 ms | tok/sec 1125065.8101
for step 5826 | loss 3.443356 | norm 0.3209 | time 465.9512 ms | tok/sec 1125199.3667
for step 5827 | loss 3.489023 | norm 0.3236 | time 466.0912 ms | tok/sec 1124861.5067
for step 5828 | loss 3.472898 | norm 0.3180 | time 466.5680 ms | tok/sec 1123711.8870
for step 5829 | loss 3.480332 | norm 0.3170 | time 466.4681 ms | tok/sec 1123952.5377
for step 5830 | loss 3.484401 | norm 0.3324 | time 467.0672 ms | tok/sec 1122510.7505
for step 5831 | loss 3.522313 | norm 0.3310 | time 466.0580 ms | tok/sec 1124941.4927
for step 5832 | loss 3.466278 | norm 0.3242 | time 466.1188 ms | tok/sec 1124794.7645
for step 5833 | loss 3.489590 | norm 0.3385 | time 465.3075 ms | tok/sec 1126756.0251
for step 5834 | loss 3.476196 | norm 0.3038 | time 466.0623 ms | tok/sec 1124931.1341
for step 5835 | loss 3.411844 | norm 0.3066 | time 466.4154 ms | tok/sec 1124079.5095
for step 5836 | loss 3.481514 | norm 0.3374 | time 465.8275 ms | tok/sec 1125498.2570
for step 5837 | loss 3.400774 | norm 0.3008 | time 464.8082 ms | tok/sec 1127966.2689
for step 5838 | loss 3.461424 | norm 0.3038 | time 466.2769 ms | tok/sec 1124413.4503
for step 5839 | loss 3.452933 | norm 0.3152 | time 465.5385 ms | tok/sec 1126196.8624
for step 5840 | loss 3.462276 | norm 0.3069 | time 466.0008 ms | tok/sec 1125079.6248
for step 5841 | loss 3.468789 | norm 0.3328 | time 465.3659 ms | tok/sec 1126614.5951
for step 5842 | loss 3.432270 | norm 0.3123 | time 465.9147 ms | tok/sec 1125287.4624
for step 5843 | loss 3.470270 | norm 0.2978 | time 465.3645 ms | tok/sec 1126618.0583
for step 5844 | loss 3.353428 | norm 0.2992 | time 465.2507 ms | tok/sec 1126893.4482
for step 5845 | loss 3.457705 | norm 0.3094 | time 466.3851 ms | tok/sec 1124152.4882
for step 5846 | loss 3.466053 | norm 0.3166 | time 466.5189 ms | tok/sec 1123830.1892
for step 5847 | loss 3.473082 | norm 0.3098 | time 466.1479 ms | tok/sec 1124724.5787
for step 5848 | loss 3.451785 | norm 0.3406 | time 466.5844 ms | tok/sec 1123672.2670
for step 5849 | loss 3.398261 | norm 0.6316 | time 466.2836 ms | tok/sec 1124397.3522
for step 5850 | loss 3.474354 | norm 0.3900 | time 465.4717 ms | tok/sec 1126358.3798
for step 5851 | loss 3.405987 | norm 0.3737 | time 465.0617 ms | tok/sec 1127351.5749
for step 5852 | loss 3.300989 | norm 0.3341 | time 465.1515 ms | tok/sec 1127133.7305
for step 5853 | loss 3.226111 | norm 0.3159 | time 464.6108 ms | tok/sec 1128445.5355
for step 5854 | loss 3.241108 | norm 0.4295 | time 465.4081 ms | tok/sec 1126512.4412
for step 5855 | loss 3.235119 | norm 0.3958 | time 465.0507 ms | tok/sec 1127378.1612
for step 5856 | loss 3.202785 | norm 0.3418 | time 464.7946 ms | tok/sec 1127999.2488
for step 5857 | loss 3.275209 | norm 0.3487 | time 464.6668 ms | tok/sec 1128309.4704
for step 5858 | loss 3.166200 | norm 0.3433 | time 465.7159 ms | tok/sec 1125767.9128
for step 5859 | loss 3.288223 | norm 0.3838 | time 466.1884 ms | tok/sec 1124626.7933
for step 5860 | loss 3.203573 | norm 0.3774 | time 465.0624 ms | tok/sec 1127349.8411
for step 5861 | loss 3.216337 | norm 0.3580 | time 465.2050 ms | tok/sec 1127004.3350
for step 5862 | loss 3.158830 | norm 0.3122 | time 464.0346 ms | tok/sec 1129846.8868
for step 5863 | loss 3.276518 | norm 0.3855 | time 465.3692 ms | tok/sec 1126606.5144
for step 5864 | loss 3.479489 | norm 0.3430 | time 465.0972 ms | tok/sec 1127265.4672
for step 5865 | loss 3.469070 | norm 0.3276 | time 464.9322 ms | tok/sec 1127665.4882
for step 5866 | loss 3.448918 | norm 0.3404 | time 465.8797 ms | tok/sec 1125372.1163
for step 5867 | loss 3.477636 | norm 0.3686 | time 465.2207 ms | tok/sec 1126966.2152
for step 5868 | loss 3.545741 | norm 0.4057 | time 465.1308 ms | tok/sec 1127183.9948
for step 5869 | loss 3.487592 | norm 0.3948 | time 466.2549 ms | tok/sec 1124466.3472
for step 5870 | loss 3.524451 | norm 0.3249 | time 465.4124 ms | tok/sec 1126502.0537
for step 5871 | loss 3.461028 | norm 0.3441 | time 465.7104 ms | tok/sec 1125781.1685
for step 5872 | loss 3.531038 | norm 0.3425 | time 465.5652 ms | tok/sec 1126132.2685
for step 5873 | loss 3.484473 | norm 0.3312 | time 465.1532 ms | tok/sec 1127129.6865
for step 5874 | loss 3.532967 | norm 0.3412 | time 464.7634 ms | tok/sec 1128075.0521
for step 5875 | loss 3.486604 | norm 0.3187 | time 465.1086 ms | tok/sec 1127237.7306
for step 5876 | loss 3.527482 | norm 0.3377 | time 465.1034 ms | tok/sec 1127250.4431
for step 5877 | loss 3.517745 | norm 0.3425 | time 464.2525 ms | tok/sec 1129316.5502
for step 5878 | loss 3.487926 | norm 0.3212 | time 465.3223 ms | tok/sec 1126720.2313
for step 5879 | loss 3.506454 | norm 0.3156 | time 464.4132 ms | tok/sec 1128925.7892
for step 5880 | loss 3.462203 | norm 0.3001 | time 465.3053 ms | tok/sec 1126761.2211
for step 5881 | loss 3.485599 | norm 0.3237 | time 465.8251 ms | tok/sec 1125504.0176
for step 5882 | loss 3.519374 | norm 0.3424 | time 465.4291 ms | tok/sec 1126461.6598
for step 5883 | loss 3.470473 | norm 0.3588 | time 465.1730 ms | tok/sec 1127081.7377
for step 5884 | loss 3.450332 | norm 0.3101 | time 466.3155 ms | tok/sec 1124320.3177
for step 5885 | loss 3.490678 | norm 0.2985 | time 464.5989 ms | tok/sec 1128474.4898
for step 5886 | loss 3.425951 | norm 0.2833 | time 464.7708 ms | tok/sec 1128057.1130
for step 5887 | loss 3.419280 | norm 0.2845 | time 465.1580 ms | tok/sec 1127118.1322
for step 5888 | loss 3.349147 | norm 0.2989 | time 464.7505 ms | tok/sec 1128106.3023
for step 5889 | loss 3.462338 | norm 0.2891 | time 464.8180 ms | tok/sec 1127942.5476
for step 5890 | loss 3.421168 | norm 0.3322 | time 465.5297 ms | tok/sec 1126218.2031
for step 5891 | loss 3.498289 | norm 0.3063 | time 466.2318 ms | tok/sec 1124522.1243
for step 5892 | loss 3.444832 | norm 0.2704 | time 465.4744 ms | tok/sec 1126352.0336
for step 5893 | loss 3.450089 | norm 0.2885 | time 465.0583 ms | tok/sec 1127359.6663
for step 5894 | loss 3.405900 | norm 0.2699 | time 464.9603 ms | tok/sec 1127597.2564
for step 5895 | loss 3.436883 | norm 0.3031 | time 464.9453 ms | tok/sec 1127633.6842
for step 5896 | loss 3.410593 | norm 0.2810 | time 465.3692 ms | tok/sec 1126606.5144
for step 5897 | loss 3.422285 | norm 0.3068 | time 465.8828 ms | tok/sec 1125364.6294
for step 5898 | loss 3.510966 | norm 0.3268 | time 465.0528 ms | tok/sec 1127372.9594
for step 5899 | loss 3.497768 | norm 0.3344 | time 466.0652 ms | tok/sec 1124924.2286
for step 5900 | loss 3.481074 | norm 0.3656 | time 465.8017 ms | tok/sec 1125560.4738
for step 5901 | loss 3.518134 | norm 0.3547 | time 465.2023 ms | tok/sec 1127010.6886
for step 5902 | loss 3.439930 | norm 0.3974 | time 465.3704 ms | tok/sec 1126603.6285
for step 5903 | loss 3.515854 | norm 0.3620 | time 465.2267 ms | tok/sec 1126951.7766
for step 5904 | loss 3.456140 | norm 0.3438 | time 464.9031 ms | tok/sec 1127736.0416
Will loading at 0 from edu_fineweb10B/edufineweb_train_000032.npy
for step 5905 | loss 3.531520 | norm 0.4309 | time 1401.1879 ms | tok/sec 374173.9429
for step 5906 | loss 3.446802 | norm 0.4249 | time 464.2992 ms | tok/sec 1129202.8885
for step 5907 | loss 3.427411 | norm 0.3271 | time 465.0896 ms | tok/sec 1127283.9590
for step 5908 | loss 3.515306 | norm 0.3448 | time 466.2468 ms | tok/sec 1124485.8973
for step 5909 | loss 3.444344 | norm 0.3326 | time 464.6676 ms | tok/sec 1128307.7336
for step 5910 | loss 3.484659 | norm 0.3070 | time 464.8812 ms | tok/sec 1127789.2516
for step 5911 | loss 3.516522 | norm 0.3337 | time 465.1089 ms | tok/sec 1127237.1528
for step 5912 | loss 3.463524 | norm 0.3347 | time 464.9920 ms | tok/sec 1127520.3611
for step 5913 | loss 3.502729 | norm 0.3477 | time 465.8537 ms | tok/sec 1125434.8952
for step 5914 | loss 3.466996 | norm 0.3395 | time 465.4768 ms | tok/sec 1126346.2644
for step 5915 | loss 3.523743 | norm 0.3642 | time 465.2278 ms | tok/sec 1126948.8889
for step 5916 | loss 3.490275 | norm 0.3496 | time 464.9844 ms | tok/sec 1127538.8613
for step 5917 | loss 3.469839 | norm 0.3615 | time 468.0657 ms | tok/sec 1120116.1646
for step 5918 | loss 3.429029 | norm 0.3205 | time 465.8711 ms | tok/sec 1125392.8498
for step 5919 | loss 3.474993 | norm 0.3214 | time 465.1258 ms | tok/sec 1127196.1283
for step 5920 | loss 3.493272 | norm 0.3559 | time 466.2001 ms | tok/sec 1124598.6113
for step 5921 | loss 3.435349 | norm 0.3910 | time 465.7226 ms | tok/sec 1125751.7759
for step 5922 | loss 3.401934 | norm 0.3709 | time 465.6074 ms | tok/sec 1126030.2020
for step 5923 | loss 3.442197 | norm 0.3513 | time 465.7443 ms | tok/sec 1125699.3342
for step 5924 | loss 3.466851 | norm 0.3163 | time 465.3442 ms | tok/sec 1126667.1221
for step 5925 | loss 3.428155 | norm 0.3369 | time 466.0800 ms | tok/sec 1124888.5510
for step 5926 | loss 3.449530 | norm 0.3021 | time 465.3087 ms | tok/sec 1126753.1384
for step 5927 | loss 3.432738 | norm 0.3047 | time 465.7595 ms | tok/sec 1125662.4551
for step 5928 | loss 3.427405 | norm 0.3456 | time 465.8842 ms | tok/sec 1125361.1739
for step 5929 | loss 3.496679 | norm 0.2999 | time 465.1771 ms | tok/sec 1127071.9174
for step 5930 | loss 3.409840 | norm 0.3205 | time 465.7085 ms | tok/sec 1125785.7792
for step 5931 | loss 3.433691 | norm 0.2897 | time 465.3950 ms | tok/sec 1126544.1820
for step 5932 | loss 3.617673 | norm 0.3577 | time 466.1245 ms | tok/sec 1124780.9568
for step 5933 | loss 3.489823 | norm 0.3688 | time 466.1953 ms | tok/sec 1124610.1140
for step 5934 | loss 3.467103 | norm 0.3388 | time 465.9956 ms | tok/sec 1125092.2886
for step 5935 | loss 3.484102 | norm 0.3087 | time 465.2719 ms | tok/sec 1126842.0550
for step 5936 | loss 3.531510 | norm 0.3562 | time 465.2488 ms | tok/sec 1126898.0681
for step 5937 | loss 3.481680 | norm 0.4038 | time 465.7376 ms | tok/sec 1125715.4696
for step 5938 | loss 3.455115 | norm 0.3296 | time 465.6098 ms | tok/sec 1126024.4361
for step 5939 | loss 3.449950 | norm 0.3649 | time 465.3833 ms | tok/sec 1126572.4616
for step 5940 | loss 3.477084 | norm 0.3135 | time 465.0586 ms | tok/sec 1127359.0883
for step 5941 | loss 3.547719 | norm 0.3737 | time 465.3780 ms | tok/sec 1126585.1590
for step 5942 | loss 3.516268 | norm 0.3614 | time 466.0568 ms | tok/sec 1124944.3701
for step 5943 | loss 3.482533 | norm 0.3464 | time 465.0419 ms | tok/sec 1127399.5467
for step 5944 | loss 3.496123 | norm 0.3741 | time 465.1012 ms | tok/sec 1127255.6437
for step 5945 | loss 3.412182 | norm 0.3858 | time 465.0733 ms | tok/sec 1127323.2562
for step 5946 | loss 3.534939 | norm 0.3372 | time 465.2236 ms | tok/sec 1126959.2846
for step 5947 | loss 3.442039 | norm 0.3479 | time 465.0183 ms | tok/sec 1127456.7713
for step 5948 | loss 3.490713 | norm 0.3265 | time 465.2541 ms | tok/sec 1126885.3636
for step 5949 | loss 3.477628 | norm 0.2880 | time 464.2465 ms | tok/sec 1129331.0495
for step 5950 | loss 3.478169 | norm 0.3032 | time 464.9832 ms | tok/sec 1127541.7520
for step 5951 | loss 3.541404 | norm 0.2941 | time 464.9661 ms | tok/sec 1127583.3798
for step 5952 | loss 3.454352 | norm 0.3071 | time 465.4226 ms | tok/sec 1126477.2399
for step 5953 | loss 3.525547 | norm 0.2850 | time 465.3943 ms | tok/sec 1126545.9133
for step 5954 | loss 3.474027 | norm 0.3617 | time 465.3242 ms | tok/sec 1126715.6129
for step 5955 | loss 3.468416 | norm 0.3332 | time 465.7903 ms | tok/sec 1125588.1279
for step 5956 | loss 3.450100 | norm 0.3742 | time 464.8218 ms | tok/sec 1127933.2908
for step 5957 | loss 3.436634 | norm 0.3748 | time 465.9209 ms | tok/sec 1125272.4909
for step 5958 | loss 3.414647 | norm 0.3019 | time 464.5338 ms | tok/sec 1128632.6063
for step 5959 | loss 3.493725 | norm 0.3560 | time 464.7748 ms | tok/sec 1128047.2757
for step 5960 | loss 3.474620 | norm 0.4068 | time 464.0014 ms | tok/sec 1129927.5834
for step 5961 | loss 3.457220 | norm 0.3677 | time 464.8898 ms | tok/sec 1127768.4297
for step 5962 | loss 3.437789 | norm 0.2904 | time 465.0812 ms | tok/sec 1127304.1852
for step 5963 | loss 3.448596 | norm 0.3119 | time 465.8802 ms | tok/sec 1125370.9645
for step 5964 | loss 3.444801 | norm 0.2910 | time 465.3187 ms | tok/sec 1126728.8909
for step 5965 | loss 3.475071 | norm 0.2681 | time 465.2936 ms | tok/sec 1126789.5117
for step 5966 | loss 3.598254 | norm 0.2857 | time 464.6809 ms | tok/sec 1128275.3145
for step 5967 | loss 3.448605 | norm 0.2979 | time 465.5030 ms | tok/sec 1126282.8070
for step 5968 | loss 3.450287 | norm 0.2861 | time 465.7609 ms | tok/sec 1125658.9978
for step 5969 | loss 3.532604 | norm 0.3170 | time 465.5826 ms | tok/sec 1126090.1710
for step 5970 | loss 3.519059 | norm 0.3053 | time 466.0959 ms | tok/sec 1124849.9989
for step 5971 | loss 3.476250 | norm 0.3235 | time 465.0240 ms | tok/sec 1127442.8981
for step 5972 | loss 3.462673 | norm 0.3142 | time 466.1725 ms | tok/sec 1124665.3302
for step 5973 | loss 3.517524 | norm 0.3265 | time 465.3294 ms | tok/sec 1126702.9125
for step 5974 | loss 3.498081 | norm 0.3948 | time 465.3881 ms | tok/sec 1126560.9187
for step 5975 | loss 3.469939 | norm 0.3420 | time 464.8056 ms | tok/sec 1127972.6333
for step 5976 | loss 3.513573 | norm 0.3180 | time 465.2238 ms | tok/sec 1126958.7071
for step 5977 | loss 3.509378 | norm 0.3789 | time 465.1804 ms | tok/sec 1127063.8302
for step 5978 | loss 3.409033 | norm 0.3588 | time 465.0819 ms | tok/sec 1127302.4515
for step 5979 | loss 3.496621 | norm 0.3184 | time 464.8247 ms | tok/sec 1127926.3483
for step 5980 | loss 3.533505 | norm 0.3651 | time 464.8857 ms | tok/sec 1127778.2621
for step 5981 | loss 3.472614 | norm 0.3449 | time 465.6596 ms | tok/sec 1125903.9421
for step 5982 | loss 3.451177 | norm 0.3635 | time 465.1923 ms | tok/sec 1127034.9482
for step 5983 | loss 3.511003 | norm 0.3228 | time 464.4837 ms | tok/sec 1128754.2645
for step 5984 | loss 3.490475 | norm 0.3298 | time 464.9835 ms | tok/sec 1127541.1738
for step 5985 | loss 3.434341 | norm 0.3314 | time 465.6515 ms | tok/sec 1125923.5422
for step 5986 | loss 3.450253 | norm 0.4229 | time 465.0240 ms | tok/sec 1127442.8981
for step 5987 | loss 3.459982 | norm 0.4179 | time 465.5192 ms | tok/sec 1126243.5823
for step 5988 | loss 3.336250 | norm 0.3103 | time 465.0028 ms | tok/sec 1127494.3462
for step 5989 | loss 3.431005 | norm 0.3626 | time 464.6204 ms | tok/sec 1128422.3732
for step 5990 | loss 3.468083 | norm 0.3443 | time 464.7398 ms | tok/sec 1128132.3454
for step 5991 | loss 3.428835 | norm 0.3525 | time 464.6523 ms | tok/sec 1128344.7863
for step 5992 | loss 3.339508 | norm 0.3360 | time 464.5531 ms | tok/sec 1128585.6880
for step 5993 | loss 3.456573 | norm 0.3138 | time 465.5881 ms | tok/sec 1126076.9081
for step 5994 | loss 3.373488 | norm 0.3198 | time 465.0714 ms | tok/sec 1127327.8795
for step 5995 | loss 3.422610 | norm 0.3194 | time 465.5728 ms | tok/sec 1126113.8144
for step 5996 | loss 3.467685 | norm 0.3168 | time 464.9510 ms | tok/sec 1127619.8067
for step 5997 | loss 3.420928 | norm 0.3318 | time 465.0109 ms | tok/sec 1127474.6913
for step 5998 | loss 3.436537 | norm 0.3012 | time 464.9529 ms | tok/sec 1127615.1809
for step 5999 | loss 3.425970 | norm 0.3229 | time 465.1737 ms | tok/sec 1127080.0047
validation loss 3.4751
HellaSwag accuracy: 2705/10042=0.2694
> Hello, I'm a language model, and it is a lot easier to code in this manner. What I'm very excited about is, I can't say
> Hello, I'm a language model, and the definition of the word has the same meaning. We use the word, but is able to understand it more easily
> Hello, I'm a language model, so I can understand the language. I'm not a languagemodel myself but I'm a language model and a linguistic
> Hello, I'm a language model, and I'm just a theoretical physicist. "I'm at the moment in human history (however, I have a
> Hello, I'm a language model, and I think that is pretty simple. Since what I'm talking about is an example of a computer model that I want
> Hello, I'm a language model, but a lot of my fellow linguists are interested in studying how a complex set of words that can function in a real
> Hello, I'm a language model, but I'd be wondering if you know what they're talking about and if you could learn more about them. I could>
 Hello, I'm a language model, but I also have a problem with a list of phrases that can be derived from a text like "n," "d
> Hello, I'm a language model, and will be a little puzzled if you can start with a specific formula to get answers in Java, Java, C#
> Hello, I'm a language model, however the language as it is not understood has always been derived from the native languages we now use. We've looked at
> Hello, I'm a language model, I wrote a paper in "The Effect of Defining the Basic of the Language, which I have worked on here,
> Hello, I'm a language model, and it covers how you can use tools, tools and examples to understand our world.
The main idea is that the
> Hello, I'm a language model, one whose syntax is more accurate. For instance, "The language of a given game is
"The game is one>>
 Hello, I'm a language model, so if the language model is pretty good, it could help you predict your future, and it doesn't necessarily match that Hello, I'm a language model, and my students always have a language model, and I've always been there for my students.
I can't remember> 

Hello, I'm a language model, and I'll be in a much better school, as well.
A lot of my friends are interested in going to
> > > > Hello, I'm a language model, and I'm not talking the language at all, so I can't wait to go out the door but I can.
Hello, I'm a language model, learning more about languages such as Java, and if you are learning and applying this as well.
Java isn't the
Hello, I'm a language model, so I chose that because it's fairly specific.
As an example, I used a web search for the web site
Hello, I'm a language model, so I'll use it. The program that will be running on, I don't get a lot of experience to run
> > > > Hello, I'm a language model, and this one is a representation from the perspective of a simple language. I'm not a language model, but there may
Hello, I'm a language model, but it turns me an object of art.
The only thing I know of is that I can do it. This
Hello, I'm a language model, but I've always thought that they had.
The programming language for an example code is a way of organizing words together
Hello, I'm a language model, and I'll admit my first word of the week is "I've got it". But all of it was all too
> > > Hello, I'm a language model, and the most exciting part of Python is developing features such as features for Python for general purpose. By the way, there
Hello, I'm a language model, though. What does it mean?
I am very much interested in the field of programming, to understand how the model
Hello, I'm a language model, so my research is on how to model Python for C, or Python for C. The data center for the model is
> Hello, I'm a language model, and its very name is ROOF, the first-person-in-the-mathematics language, which
> Hello, I'm a language model, and I use the word for the model code.
Let's say that the object is represented as being "a new
> Hello, I'm a language model, this refers to a method of data comparison.
(Thanks for sharing this post at Facebook)
My first comment (
> Hello, I'm a language model, and I am one of the founders of The Language Profile.
So in this case, I'm an expert language model
> Hello, I'm a language model, but it's not going to be really perfect.
Now that's one of the main reasons why I'm called '
for step 6000 | loss 3.477994 | norm 0.3183 | time 12842.6416 ms | tok/sec 40824.0000
for step 6001 | loss 3.445310 | norm 0.2894 | time 462.1429 ms | tok/sec 1134471.5016
for step 6002 | loss 3.512980 | norm 0.3438 | time 462.8868 ms | tok/sec 1132648.3890
for step 6003 | loss 3.484522 | norm 0.2869 | time 462.9724 ms | tok/sec 1132438.9901
for step 6004 | loss 3.499282 | norm 0.3526 | time 463.5398 ms | tok/sec 1131052.7297
for step 6005 | loss 3.544762 | norm 0.3881 | time 463.7058 ms | tok/sec 1130647.9771
for step 6006 | loss 3.430752 | norm 0.3817 | time 463.3887 ms | tok/sec 1131421.6791
for step 6007 | loss 3.495271 | norm 0.4007 | time 463.1598 ms | tok/sec 1131980.7990
for step 6008 | loss 3.411419 | norm 0.4394 | time 464.1650 ms | tok/sec 1129529.4373
for step 6009 | loss 3.486218 | norm 0.3928 | time 464.1387 ms | tok/sec 1129593.2611
for step 6010 | loss 3.503681 | norm 0.3619 | time 463.6357 ms | tok/sec 1130818.9148
for step 6011 | loss 3.510715 | norm 0.3840 | time 464.2408 ms | tok/sec 1129344.9692
for step 6012 | loss 3.507352 | norm 0.3596 | time 463.4638 ms | tok/sec 1131238.3382
for step 6013 | loss 3.499272 | norm 0.3424 | time 463.5389 ms | tok/sec 1131055.0567
for step 6014 | loss 3.426024 | norm 0.3440 | time 463.8462 ms | tok/sec 1130305.6756
for step 6015 | loss 3.486946 | norm 0.3555 | time 463.6817 ms | tok/sec 1130706.6948
for step 6016 | loss 3.474229 | norm 0.2738 | time 464.5798 ms | tok/sec 1128520.8197
for step 6017 | loss 3.439203 | norm 0.3239 | time 464.9141 ms | tok/sec 1127709.4384
for step 6018 | loss 3.470956 | norm 0.3039 | time 464.5259 ms | tok/sec 1128651.7223
for step 6019 | loss 3.483615 | norm 0.3005 | time 465.0240 ms | tok/sec 1127442.8981
for step 6020 | loss 3.447609 | norm 0.3407 | time 464.2558 ms | tok/sec 1129308.4307
for step 6021 | loss 3.418109 | norm 0.3967 | time 464.6981 ms | tok/sec 1128233.6356
for step 6022 | loss 3.459723 | norm 0.4270 | time 465.1332 ms | tok/sec 1127178.2171
for step 6023 | loss 3.431664 | norm 0.3757 | time 465.5266 ms | tok/sec 1126225.7014
for step 6024 | loss 3.376615 | norm 0.3423 | time 464.1933 ms | tok/sec 1129460.3997
for step 6025 | loss 3.487524 | norm 0.3583 | time 465.1508 ms | tok/sec 1127135.4637
for step 6026 | loss 3.413899 | norm 0.3422 | time 465.1175 ms | tok/sec 1127216.3513
for step 6027 | loss 3.402484 | norm 0.3208 | time 465.6224 ms | tok/sec 1125993.8778
for step 6028 | loss 3.436465 | norm 0.3192 | time 464.8783 ms | tok/sec 1127796.1924
for step 6029 | loss 3.404066 | norm 0.2907 | time 465.1926 ms | tok/sec 1127034.3706
for step 6030 | loss 3.411219 | norm 0.3450 | time 465.5867 ms | tok/sec 1126080.3680
for step 6031 | loss 3.429198 | norm 0.3210 | time 465.6508 ms | tok/sec 1125925.2717
for step 6032 | loss 3.402651 | norm 0.3481 | time 465.9343 ms | tok/sec 1125240.2460
for step 6033 | loss 3.426395 | norm 0.2780 | time 465.3201 ms | tok/sec 1126725.4270
for step 6034 | loss 3.556556 | norm 0.3773 | time 465.1208 ms | tok/sec 1127208.2620
for step 6035 | loss 3.476416 | norm 0.4471 | time 465.3852 ms | tok/sec 1126567.8444
for step 6036 | loss 3.501415 | norm 0.4097 | time 464.9112 ms | tok/sec 1127716.3783
for step 6037 | loss 3.433727 | norm 0.3390 | time 465.0481 ms | tok/sec 1127384.5189
for step 6038 | loss 3.471956 | norm 0.3954 | time 464.8955 ms | tok/sec 1127754.5488
for step 6039 | loss 3.509336 | norm 0.3859 | time 465.7047 ms | tok/sec 1125795.0008
for step 6040 | loss 3.488311 | norm 0.3499 | time 464.9849 ms | tok/sec 1127537.7050
for step 6041 | loss 3.424791 | norm 0.3374 | time 464.5340 ms | tok/sec 1128632.0270
for step 6042 | loss 3.503342 | norm 0.3569 | time 465.4808 ms | tok/sec 1126336.4568
for step 6043 | loss 3.469727 | norm 0.3232 | time 465.9467 ms | tok/sec 1125210.3060
for step 6044 | loss 3.500780 | norm 0.3363 | time 464.9053 ms | tok/sec 1127730.8365
for step 6045 | loss 3.490008 | norm 0.3358 | time 464.9360 ms | tok/sec 1127656.2359
for step 6046 | loss 3.439346 | norm 0.3463 | time 466.1148 ms | tok/sec 1124804.5452
for step 6047 | loss 3.468460 | norm 0.3582 | time 465.7853 ms | tok/sec 1125600.2270
for step 6048 | loss 3.457158 | norm 0.3789 | time 465.5499 ms | tok/sec 1126169.1784
for step 6049 | loss 3.445319 | norm 0.3462 | time 464.8497 ms | tok/sec 1127865.6052
for step 6050 | loss 3.428189 | norm 0.3112 | time 465.8408 ms | tok/sec 1125465.9992
for step 6051 | loss 3.488531 | norm 0.3503 | time 464.7021 ms | tok/sec 1128223.7951
for step 6052 | loss 3.475606 | norm 0.3377 | time 464.9312 ms | tok/sec 1127667.8013
for step 6053 | loss 3.461126 | norm 0.3223 | time 465.6875 ms | tok/sec 1125836.4997
for step 6054 | loss 3.548567 | norm 0.3589 | time 465.6518 ms | tok/sec 1125922.9657
for step 6055 | loss 3.465884 | norm 0.3233 | time 465.2693 ms | tok/sec 1126848.4067
for step 6056 | loss 3.485708 | norm 0.3384 | time 466.0635 ms | tok/sec 1124928.2568
for step 6057 | loss 3.399601 | norm 0.3171 | time 464.8786 ms | tok/sec 1127795.6140
for step 6058 | loss 3.429302 | norm 0.3111 | time 464.8068 ms | tok/sec 1127969.7403
for step 6059 | loss 3.406913 | norm 0.3331 | time 465.1785 ms | tok/sec 1127068.4514
for step 6060 | loss 3.417515 | norm 0.3072 | time 464.6366 ms | tok/sec 1128382.9994
for step 6061 | loss 3.419457 | norm 0.3042 | time 465.1833 ms | tok/sec 1127056.8984
for step 6062 | loss 3.460618 | norm 0.2903 | time 464.4961 ms | tok/sec 1128724.1372
for step 6063 | loss 3.430134 | norm 0.3273 | time 464.3948 ms | tok/sec 1128970.4174
for step 6064 | loss 3.431950 | norm 0.3184 | time 465.0934 ms | tok/sec 1127274.7130
for step 6065 | loss 3.377346 | norm 0.3025 | time 464.7624 ms | tok/sec 1128077.3669
for step 6066 | loss 3.383538 | norm 0.2937 | time 465.9150 ms | tok/sec 1125286.8866
for step 6067 | loss 3.408763 | norm 0.2936 | time 465.1175 ms | tok/sec 1127216.3513
for step 6068 | loss 3.449949 | norm 0.3115 | time 466.0764 ms | tok/sec 1124897.1825
for step 6069 | loss 3.505302 | norm 0.3029 | time 464.6573 ms | tok/sec 1128332.6281
for step 6070 | loss 3.447855 | norm 0.2851 | time 463.7873 ms | tok/sec 1130449.1962
for step 6071 | loss 3.469504 | norm 0.3099 | time 465.0550 ms | tok/sec 1127367.7577
for step 6072 | loss 3.529790 | norm 0.3265 | time 464.7493 ms | tok/sec 1128109.1959
for step 6073 | loss 3.497902 | norm 0.3562 | time 465.5313 ms | tok/sec 1126214.1656
for step 6074 | loss 3.472019 | norm 0.3038 | time 465.2066 ms | tok/sec 1127000.2919
for step 6075 | loss 3.503427 | norm 0.3191 | time 465.1196 ms | tok/sec 1127211.1510
for step 6076 | loss 3.483155 | norm 0.3340 | time 464.8919 ms | tok/sec 1127763.2243
for step 6077 | loss 3.479148 | norm 0.3228 | time 465.4610 ms | tok/sec 1126384.3422
for step 6078 | loss 3.440214 | norm 0.3102 | time 465.6420 ms | tok/sec 1125946.6021
for step 6079 | loss 3.509810 | norm 0.3309 | time 465.0991 ms | tok/sec 1127260.8444
for step 6080 | loss 3.465010 | norm 0.2921 | time 464.8812 ms | tok/sec 1127789.2516
for step 6081 | loss 3.469541 | norm 0.3193 | time 464.4730 ms | tok/sec 1128780.3375
for step 6082 | loss 3.458033 | norm 0.3246 | time 465.3230 ms | tok/sec 1126718.4994
for step 6083 | loss 3.474329 | norm 0.3029 | time 465.0505 ms | tok/sec 1127378.7391
for step 6084 | loss 3.434546 | norm 0.2978 | time 465.7958 ms | tok/sec 1125574.8768
for step 6085 | loss 3.480571 | norm 0.2870 | time 466.4435 ms | tok/sec 1124011.7110
for step 6086 | loss 3.520401 | norm 0.3187 | time 465.4946 ms | tok/sec 1126302.9972
for step 6087 | loss 3.469613 | norm 0.3140 | time 465.5473 ms | tok/sec 1126175.5225
for step 6088 | loss 3.503592 | norm 0.3150 | time 465.1790 ms | tok/sec 1127067.2961
for step 6089 | loss 3.484401 | norm 0.2904 | time 465.3118 ms | tok/sec 1126745.6331
for step 6090 | loss 3.449341 | norm 0.3123 | time 466.1562 ms | tok/sec 1124704.4450
for step 6091 | loss 3.406190 | norm 0.2882 | time 464.9909 ms | tok/sec 1127523.2517
for step 6092 | loss 3.420957 | norm 0.2850 | time 465.5433 ms | tok/sec 1126185.3273
for step 6093 | loss 3.417226 | norm 0.3016 | time 464.9584 ms | tok/sec 1127601.8821
for step 6094 | loss 3.408814 | norm 0.2897 | time 464.9065 ms | tok/sec 1127727.9448
Will loading at 0 from edu_fineweb10B/edufineweb_train_000033.npy
for step 6095 | loss 3.403773 | norm 0.2962 | time 1453.8291 ms | tok/sec 360625.6182
for step 6096 | loss 3.395692 | norm 0.3103 | time 463.3913 ms | tok/sec 1131415.2757
for step 6097 | loss 3.410625 | norm 0.3001 | time 463.6827 ms | tok/sec 1130704.3692
for step 6098 | loss 3.422892 | norm 0.3233 | time 464.4549 ms | tok/sec 1128824.3747
for step 6099 | loss 3.440431 | norm 0.2961 | time 465.2979 ms | tok/sec 1126779.1191
for step 6100 | loss 3.444146 | norm 0.3055 | time 464.6950 ms | tok/sec 1128241.1607
for step 6101 | loss 3.417021 | norm 0.3209 | time 465.0800 ms | tok/sec 1127307.0747
for step 6102 | loss 3.489445 | norm 0.2978 | time 464.9477 ms | tok/sec 1127627.9019
for step 6103 | loss 3.447820 | norm 0.3270 | time 465.0474 ms | tok/sec 1127386.2529
for step 6104 | loss 3.488716 | norm 0.3477 | time 464.4439 ms | tok/sec 1128851.0305
for step 6105 | loss 3.444402 | norm 0.3576 | time 464.9503 ms | tok/sec 1127621.5414
for step 6106 | loss 3.508268 | norm 0.3507 | time 465.5619 ms | tok/sec 1126140.3423
for step 6107 | loss 3.517654 | norm 0.3468 | time 464.6394 ms | tok/sec 1128376.0514
for step 6108 | loss 3.423565 | norm 0.3360 | time 466.1944 ms | tok/sec 1124612.4146
for step 6109 | loss 3.479141 | norm 0.3440 | time 465.4503 ms | tok/sec 1126410.3059
for step 6110 | loss 3.566995 | norm 0.3388 | time 465.2774 ms | tok/sec 1126828.7743
for step 6111 | loss 3.475113 | norm 0.3646 | time 464.6006 ms | tok/sec 1128470.4361
for step 6112 | loss 3.481848 | norm 0.3576 | time 465.0652 ms | tok/sec 1127342.9058
for step 6113 | loss 3.487800 | norm 0.3514 | time 466.1143 ms | tok/sec 1124805.6959
for step 6114 | loss 3.502844 | norm 0.3495 | time 465.2016 ms | tok/sec 1127012.4214
for step 6115 | loss 3.514794 | norm 0.3146 | time 466.1260 ms | tok/sec 1124777.5049
for step 6116 | loss 3.482607 | norm 0.3416 | time 465.8380 ms | tok/sec 1125472.9114
for step 6117 | loss 3.463472 | norm 0.3581 | time 465.0548 ms | tok/sec 1127368.3357
for step 6118 | loss 3.469539 | norm 0.3088 | time 466.4738 ms | tok/sec 1123938.7506
for step 6119 | loss 3.474773 | norm 0.3115 | time 465.2491 ms | tok/sec 1126897.4906
for step 6120 | loss 3.538523 | norm 0.3396 | time 470.9070 ms | tok/sec 1113357.9033
for step 6121 | loss 3.459910 | norm 0.3344 | time 465.5282 ms | tok/sec 1126221.6639
for step 6122 | loss 3.485285 | norm 0.3407 | time 465.3220 ms | tok/sec 1126720.8086
for step 6123 | loss 3.432599 | norm 0.3456 | time 465.2164 ms | tok/sec 1126976.6113
for step 6124 | loss 3.441373 | norm 0.3456 | time 465.7462 ms | tok/sec 1125694.7242
for step 6125 | loss 3.368464 | norm 0.3069 | time 465.8327 ms | tok/sec 1125485.5841
for step 6126 | loss 3.433507 | norm 0.3332 | time 465.1048 ms | tok/sec 1127246.9760
for step 6127 | loss 3.456054 | norm 0.3086 | time 465.0083 ms | tok/sec 1127481.0502
for step 6128 | loss 3.383222 | norm 0.3382 | time 465.0397 ms | tok/sec 1127404.7487
for step 6129 | loss 3.337270 | norm 0.3010 | time 465.6389 ms | tok/sec 1125954.0968
for step 6130 | loss 3.415457 | norm 0.3403 | time 465.4243 ms | tok/sec 1126473.2006
for step 6131 | loss 3.452617 | norm 0.3413 | time 465.5523 ms | tok/sec 1126163.4111
for step 6132 | loss 3.372058 | norm 0.3349 | time 465.7080 ms | tok/sec 1125786.9319
for step 6133 | loss 3.462755 | norm 0.3033 | time 465.1797 ms | tok/sec 1127065.5631
for step 6134 | loss 3.477504 | norm 0.3557 | time 465.2495 ms | tok/sec 1126896.3356
for step 6135 | loss 3.425315 | norm 0.3456 | time 465.2598 ms | tok/sec 1126871.5045
for step 6136 | loss 3.423699 | norm 0.3162 | time 464.6645 ms | tok/sec 1128315.2597
for step 6137 | loss 3.505326 | norm 0.3336 | time 465.4567 ms | tok/sec 1126394.7275
for step 6138 | loss 3.531621 | norm 0.3544 | time 465.8539 ms | tok/sec 1125434.3192
for step 6139 | loss 3.514512 | norm 0.3325 | time 464.2236 ms | tok/sec 1129386.7303
for step 6140 | loss 3.464325 | norm 0.3599 | time 465.2619 ms | tok/sec 1126866.3074
for step 6141 | loss 3.431512 | norm 0.3195 | time 464.7298 ms | tok/sec 1128156.6534
for step 6142 | loss 3.498314 | norm 0.3282 | time 464.8182 ms | tok/sec 1127941.9691
for step 6143 | loss 3.476931 | norm 0.3123 | time 465.0948 ms | tok/sec 1127271.2458
for step 6144 | loss 3.461521 | norm 0.2978 | time 464.3538 ms | tok/sec 1129070.1190
for step 6145 | loss 3.465286 | norm 0.2985 | time 465.2083 ms | tok/sec 1126996.2488
for step 6146 | loss 3.481640 | norm 0.3185 | time 464.6900 ms | tok/sec 1128253.3169
for step 6147 | loss 3.476778 | norm 0.3337 | time 465.1575 ms | tok/sec 1127119.2876
for step 6148 | loss 3.441592 | norm 0.3116 | time 464.5500 ms | tok/sec 1128593.2178
for step 6149 | loss 3.487808 | norm 0.3129 | time 465.7514 ms | tok/sec 1125682.0468
for step 6150 | loss 3.492488 | norm 0.3552 | time 464.2782 ms | tok/sec 1129253.9175
for step 6151 | loss 3.482829 | norm 0.2974 | time 465.3728 ms | tok/sec 1126597.8567
for step 6152 | loss 3.455834 | norm 0.3668 | time 465.8425 ms | tok/sec 1125461.9671
for step 6153 | loss 3.504330 | norm 0.3830 | time 464.7274 ms | tok/sec 1128162.4411
for step 6154 | loss 3.511730 | norm 0.3057 | time 465.1396 ms | tok/sec 1127162.6175
for step 6155 | loss 3.530577 | norm 0.3883 | time 465.2755 ms | tok/sec 1126833.3937
for step 6156 | loss 3.504169 | norm 0.3789 | time 465.7300 ms | tok/sec 1125733.9106
for step 6157 | loss 3.476791 | norm 0.3539 | time 465.0338 ms | tok/sec 1127419.1989
for step 6158 | loss 3.450698 | norm 0.3947 | time 465.6129 ms | tok/sec 1126016.9405
for step 6159 | loss 3.386312 | norm 0.3589 | time 465.0841 ms | tok/sec 1127297.2504
for step 6160 | loss 3.391978 | norm 0.3427 | time 465.4891 ms | tok/sec 1126316.2654
for step 6161 | loss 3.388328 | norm 0.3285 | time 465.5035 ms | tok/sec 1126281.6533
for step 6162 | loss 3.410507 | norm 0.3293 | time 465.3323 ms | tok/sec 1126695.9852
for step 6163 | loss 3.398014 | norm 0.3449 | time 466.0449 ms | tok/sec 1124973.1450
for step 6164 | loss 3.391175 | norm 0.3149 | time 465.1954 ms | tok/sec 1127027.4392
for step 6165 | loss 3.465867 | norm 0.3199 | time 465.1828 ms | tok/sec 1127058.0537
for step 6166 | loss 3.415918 | norm 0.3161 | time 465.5473 ms | tok/sec 1126175.5225
for step 6167 | loss 3.376886 | norm 0.3302 | time 465.4164 ms | tok/sec 1126492.2435
for step 6168 | loss 3.381746 | norm 0.3141 | time 465.0614 ms | tok/sec 1127352.1529
for step 6169 | loss 3.394512 | norm 0.3065 | time 465.4689 ms | tok/sec 1126365.3030
for step 6170 | loss 3.486620 | norm 0.3480 | time 465.6351 ms | tok/sec 1125963.3211
for step 6171 | loss 3.436758 | norm 0.2911 | time 464.6599 ms | tok/sec 1128326.2596
for step 6172 | loss 3.480489 | norm 0.3285 | time 465.3871 ms | tok/sec 1126563.2273
for step 6173 | loss 3.505787 | norm 0.3221 | time 465.0509 ms | tok/sec 1127377.5832
for step 6174 | loss 3.476532 | norm 0.3485 | time 464.6733 ms | tok/sec 1128293.8395
for step 6175 | loss 3.526022 | norm 0.3561 | time 464.9272 ms | tok/sec 1127677.6320
for step 6176 | loss 3.455319 | norm 0.3651 | time 465.9288 ms | tok/sec 1125253.4892
for step 6177 | loss 3.457040 | norm 0.3678 | time 465.4040 ms | tok/sec 1126522.2518
for step 6178 | loss 3.454581 | norm 0.4586 | time 465.4472 ms | tok/sec 1126417.8067
for step 6179 | loss 3.486428 | norm 0.5021 | time 465.7500 ms | tok/sec 1125685.5043
for step 6180 | loss 3.536617 | norm 0.4086 | time 465.3666 ms | tok/sec 1126612.8635
for step 6181 | loss 3.475385 | norm 0.4139 | time 465.5774 ms | tok/sec 1126102.8576
for step 6182 | loss 3.493004 | norm 0.4314 | time 465.4384 ms | tok/sec 1126439.1558
for step 6183 | loss 3.462329 | norm 0.3895 | time 465.4312 ms | tok/sec 1126456.4665
for step 6184 | loss 3.492781 | norm 0.3389 | time 465.0357 ms | tok/sec 1127414.5748
for step 6185 | loss 3.484303 | norm 0.3497 | time 465.9059 ms | tok/sec 1125308.7686
for step 6186 | loss 3.437061 | norm 0.3403 | time 464.6695 ms | tok/sec 1128303.1022
for step 6187 | loss 3.440812 | norm 0.3304 | time 464.7410 ms | tok/sec 1128129.4516
for step 6188 | loss 3.446559 | norm 0.3495 | time 467.2263 ms | tok/sec 1122128.6926
for step 6189 | loss 3.517793 | norm 0.3393 | time 465.5526 ms | tok/sec 1126162.8343
for step 6190 | loss 3.411584 | norm 0.2943 | time 466.2619 ms | tok/sec 1124449.6726
for step 6191 | loss 3.601427 | norm 0.3133 | time 465.9483 ms | tok/sec 1125206.2757
for step 6192 | loss 3.438061 | norm 0.3552 | time 465.4520 ms | tok/sec 1126406.2670
for step 6193 | loss 3.397550 | norm 0.3496 | time 465.1194 ms | tok/sec 1127211.7288
for step 6194 | loss 3.417903 | norm 0.3346 | time 466.7866 ms | tok/sec 1123185.5717
for step 6195 | loss 3.392296 | norm 0.3424 | time 465.7536 ms | tok/sec 1125676.8607
for step 6196 | loss 3.464798 | norm 0.3824 | time 465.3621 ms | tok/sec 1126623.8303
for step 6197 | loss 3.442751 | norm 0.3218 | time 465.9994 ms | tok/sec 1125083.0785
for step 6198 | loss 3.388149 | norm 0.3125 | time 465.5144 ms | tok/sec 1126255.1187
for step 6199 | loss 3.374532 | norm 0.3702 | time 465.4274 ms | tok/sec 1126465.6990
for step 6200 | loss 3.385081 | norm 0.3481 | time 465.4548 ms | tok/sec 1126399.3433
for step 6201 | loss 3.415192 | norm 0.3896 | time 465.7893 ms | tok/sec 1125590.4325
for step 6202 | loss 3.388363 | norm 0.3632 | time 466.0764 ms | tok/sec 1124897.1825
for step 6203 | loss 3.427977 | norm 0.3674 | time 465.8284 ms | tok/sec 1125495.9528
for step 6204 | loss 3.457655 | norm 0.2991 | time 465.3213 ms | tok/sec 1126722.5405
for step 6205 | loss 3.416630 | norm 0.3262 | time 465.3485 ms | tok/sec 1126656.7317
for step 6206 | loss 3.473499 | norm 0.3608 | time 464.8066 ms | tok/sec 1127970.3189
for step 6207 | loss 3.479960 | norm 0.3121 | time 466.2528 ms | tok/sec 1124471.5222
for step 6208 | loss 3.488745 | norm 0.3255 | time 465.5781 ms | tok/sec 1126101.1276
for step 6209 | loss 3.469458 | norm 0.3418 | time 465.3971 ms | tok/sec 1126538.9879
for step 6210 | loss 3.424140 | norm 0.3063 | time 465.3277 ms | tok/sec 1126706.9535
for step 6211 | loss 3.489596 | norm 0.3181 | time 465.5843 ms | tok/sec 1126086.1345
for step 6212 | loss 3.453879 | norm 0.3584 | time 464.4573 ms | tok/sec 1128818.5802
for step 6213 | loss 3.515721 | norm 0.3514 | time 465.7435 ms | tok/sec 1125701.0630
for step 6214 | loss 3.496019 | norm 0.3639 | time 465.4112 ms | tok/sec 1126504.9391
for step 6215 | loss 3.453100 | norm 0.3701 | time 465.5838 ms | tok/sec 1126087.2878
for step 6216 | loss 3.470006 | norm 0.3253 | time 465.2209 ms | tok/sec 1126965.6377
for step 6217 | loss 3.429636 | norm 0.3104 | time 464.7243 ms | tok/sec 1128169.9653
for step 6218 | loss 3.446037 | norm 0.3129 | time 465.1999 ms | tok/sec 1127016.4646
for step 6219 | loss 3.477595 | norm 0.3428 | time 465.9958 ms | tok/sec 1125091.7130
for step 6220 | loss 3.464568 | norm 0.3458 | time 465.1334 ms | tok/sec 1127177.6393
for step 6221 | loss 3.456564 | norm 0.3343 | time 465.5571 ms | tok/sec 1126151.8766
for step 6222 | loss 3.438693 | norm 0.3064 | time 464.6139 ms | tok/sec 1128438.0077
for step 6223 | loss 3.452861 | norm 0.3188 | time 464.9949 ms | tok/sec 1127513.4237
for step 6224 | loss 3.464660 | norm 0.3264 | time 465.5337 ms | tok/sec 1126208.3978
for step 6225 | loss 3.448060 | norm 0.3106 | time 465.1015 ms | tok/sec 1127255.0658
for step 6226 | loss 3.431684 | norm 0.3049 | time 465.5125 ms | tok/sec 1126259.7333
for step 6227 | loss 3.411560 | norm 0.2835 | time 465.3916 ms | tok/sec 1126552.2617
for step 6228 | loss 3.406720 | norm 0.3141 | time 466.1136 ms | tok/sec 1124807.4219
for step 6229 | loss 3.384184 | norm 0.3035 | time 465.6987 ms | tok/sec 1125809.4098
for step 6230 | loss 3.473100 | norm 0.2573 | time 466.3320 ms | tok/sec 1124280.6549
for step 6231 | loss 3.458236 | norm 0.2835 | time 465.8897 ms | tok/sec 1125347.9282
for step 6232 | loss 3.388919 | norm 0.2885 | time 465.7841 ms | tok/sec 1125603.1078
for step 6233 | loss 3.498911 | norm 0.3027 | time 464.8530 ms | tok/sec 1127857.5066
for step 6234 | loss 3.393721 | norm 0.3635 | time 465.5163 ms | tok/sec 1126250.5041
for step 6235 | loss 3.420745 | norm 0.3495 | time 465.1117 ms | tok/sec 1127230.2189
for step 6236 | loss 3.376999 | norm 0.3186 | time 464.2344 ms | tok/sec 1129360.6292
for step 6237 | loss 3.496482 | norm 0.3204 | time 465.1861 ms | tok/sec 1127049.9667
for step 6238 | loss 3.376809 | norm 0.3546 | time 465.2553 ms | tok/sec 1126882.4762
for step 6239 | loss 3.490449 | norm 0.3596 | time 465.1206 ms | tok/sec 1127208.8398
for step 6240 | loss 3.468442 | norm 0.3419 | time 465.6281 ms | tok/sec 1125980.0406
for step 6241 | loss 3.510400 | norm 0.3541 | time 465.9796 ms | tok/sec 1125130.8574
for step 6242 | loss 3.424047 | norm 0.3484 | time 465.2648 ms | tok/sec 1126859.3780
for step 6243 | loss 3.443236 | norm 0.3112 | time 464.5710 ms | tok/sec 1128542.2486
for step 6244 | loss 3.564857 | norm 0.3204 | time 465.0624 ms | tok/sec 1127349.8411
for step 6245 | loss 3.453784 | norm 0.3515 | time 465.4334 ms | tok/sec 1126451.2732
for step 6246 | loss 3.450673 | norm 0.3832 | time 465.4970 ms | tok/sec 1126297.2285
for step 6247 | loss 3.447362 | norm 0.3805 | time 465.2114 ms | tok/sec 1126988.7403
for step 6248 | loss 3.495620 | norm 0.3365 | time 464.8554 ms | tok/sec 1127851.7219
for step 6249 | loss 3.438356 | norm 0.3880 | time 464.6447 ms | tok/sec 1128363.3135
validation loss 3.4611
HellaSwag accuracy: 2702/10042=0.2691
> Hello, I'm a language model, and a problem solver, and just as we are just building a model of the relationship between our model and the model
> Hello, I'm a language model, a model, a model, a model, a model which is simple, like the famous computer library, a model,
> Hello, I'm a language model, so "it's a language model" is a good thing in my mind.
I have two different types of words
> Hello, I'm a language model, and I'm very happy by the way
I'll always admit that a single language approach only takes a few more

> Hello, I'm a language model, but I also have some good ideas. There's a lot of general grammar rules, a way to express these things,
> Hello, I'm a language model, who wrote a much longer piece to his music, this type of poem isn't that good, but there is great value
> Hello, I'm a language model, I guess. :) And so, my favorite case is the fact that I have always found a language book that works.
> > Hello, I'm a language model, and I can't figure out why some very young people just want to "learn" English.<|endoftext|>In this article:
Hello, I'm a language model, and the process involves several steps. And you are to me using a couple of additional steps, each with a different set
> Hello, I'm a language model, so now I'm going to show you how it takes time and it was time to get used to your model.

> Hello, I'm a language model, but I did a lot of model writing.<|endoftext|>From the Web's viewpoint, the internet is a public domain, no
> Hello, I'm a language model, and will be working on my model for a full-time learner.<|endoftext|>Graphene-Nano is perhaps
> Hello, I'm a language model, and this one uses the idea that languages can be understood and modeled in ways that have such a great impact when it comes
> Hello, I'm a language model, so I prefer using a more 'soft pointer' mode. The object may be a bit different than this.
This
> > Hello, I'm a language model, one, and the best books to get you started... I have one of the best books I've ever seen. They
> Hello, I'm a language model, but I've used it to talk it so well, I know, this is my way to think of it. And
> > Hello, I'm a language model, and I'm a good person. I thought it would be perfect for future talks since that's basically the way I can
> Hello, I'm a language model, and I want to talk to you about syntax, so I'm going to talk about how you will be writing a new
> Hello, I'm a language model, so this tutorial is very simple. You have a nice explanation of your programming, so you can learn more about it from
> Hello, I'm a language model, and I'm not really going to just look up my results on a book--you want to watch videos on Netflix without
> Hello, I'm a language model, so what's the point?
A friend of mine who likes to talk to my friends was wondering what would be the
Hello, I'm a language model, really. You can do anything by choosing from a title level, such as "You are going to find your favorite songs
> > Hello, I'm a language model, so we have to use it for both our home and business applications. So far, I've been experimenting with building an
> Hello, I'm a language model, just fine. But let's say we are making a map. One is coming off a little bit closer when you're
Hello, I'm a language model, and a very simple one. My "I" is for "the mother" and "the mother" is as much
> Hello, I'm a language model, and I am actually searching for things related to the 'language'. So how do you find out what a 'language is
> > Hello, I'm a language model, and I want to use the term 'informal' to refer to an individual, though I tend to label its
> Hello, I'm a language model, and I want you to understand things so that you can understand them very quickly and effectively," said Chisholmian
Hello, I'm a language model, and it was originally just a grammar model.<|endoftext|>Our research suggests that a new wave of innovation leads to the expansion of
Hello, I'm a language model, however, I'm still quite the beginner. For example, if I start with a good tutorial tutorial, I'll begin
> > Hello, I'm a language model, learning how to construct a script, and I'll go to the section on HTML4 for the beginning. I'm very
Hello, I'm a language model, and of course you can say something like: "A simple way of creating a function." A simple equation, a simple
for step 6250 | loss 3.503858 | norm 0.3713 | time 12717.7122 ms | tok/sec 41225.0249
for step 6251 | loss 3.492556 | norm 0.3461 | time 462.7304 ms | tok/sec 1133031.2235
for step 6252 | loss 3.501925 | norm 0.3535 | time 463.5601 ms | tok/sec 1131003.2832
for step 6253 | loss 3.464560 | norm 0.3163 | time 463.4786 ms | tok/sec 1131202.2591
for step 6254 | loss 3.472766 | norm 0.3893 | time 462.5535 ms | tok/sec 1133464.5586
for step 6255 | loss 3.395948 | norm 0.3830 | time 463.4597 ms | tok/sec 1131248.2313
for step 6256 | loss 3.467098 | norm 0.3913 | time 463.5708 ms | tok/sec 1130977.1074
for step 6257 | loss 3.491872 | norm 0.3528 | time 464.7157 ms | tok/sec 1128190.8021
for step 6258 | loss 3.480268 | norm 0.3533 | time 463.9266 ms | tok/sec 1130109.9189
for step 6259 | loss 3.439955 | norm 0.3152 | time 464.6952 ms | tok/sec 1128240.5818
for step 6260 | loss 3.461135 | norm 0.2894 | time 463.8660 ms | tok/sec 1130257.4562
for step 6261 | loss 3.396651 | norm 0.3558 | time 464.3836 ms | tok/sec 1128997.6597
for step 6262 | loss 3.368366 | norm 0.2883 | time 464.2677 ms | tok/sec 1129279.4336
for step 6263 | loss 3.382457 | norm 0.2871 | time 464.1488 ms | tok/sec 1129568.8912
for step 6264 | loss 3.419163 | norm 0.3505 | time 464.9704 ms | tok/sec 1127572.9726
for step 6265 | loss 3.396310 | norm 0.3078 | time 464.4051 ms | tok/sec 1128945.4947
for step 6266 | loss 3.369573 | norm 0.2709 | time 465.2250 ms | tok/sec 1126955.8194
for step 6267 | loss 3.329311 | norm 0.3051 | time 464.5553 ms | tok/sec 1128580.4751
for step 6268 | loss 3.412811 | norm 0.2935 | time 463.7594 ms | tok/sec 1130517.1923
for step 6269 | loss 3.394978 | norm 0.2857 | time 466.7022 ms | tok/sec 1123388.6927
for step 6270 | loss 3.385236 | norm 0.3049 | time 465.4281 ms | tok/sec 1126463.9679
for step 6271 | loss 3.391845 | norm 0.3447 | time 465.3490 ms | tok/sec 1126655.5773
for step 6272 | loss 3.423079 | norm 0.2974 | time 465.2488 ms | tok/sec 1126898.0681
for step 6273 | loss 3.472809 | norm 0.3332 | time 464.9339 ms | tok/sec 1127661.4403
for step 6274 | loss 3.607772 | norm 0.3618 | time 465.1875 ms | tok/sec 1127046.5008
for step 6275 | loss 3.518909 | norm 0.4376 | time 465.2159 ms | tok/sec 1126977.7664
for step 6276 | loss 3.485101 | norm 0.4208 | time 465.5108 ms | tok/sec 1126263.7711
for step 6277 | loss 3.468621 | norm 0.3484 | time 465.5316 ms | tok/sec 1126213.5889
for step 6278 | loss 3.476474 | norm 0.3426 | time 465.2271 ms | tok/sec 1126950.6215
for step 6279 | loss 3.533952 | norm 0.3875 | time 466.0618 ms | tok/sec 1124932.2851
for step 6280 | loss 3.471898 | norm 0.3283 | time 466.3875 ms | tok/sec 1124146.7415
for step 6281 | loss 3.474052 | norm 0.3123 | time 464.0267 ms | tok/sec 1129866.0440
for step 6282 | loss 3.476056 | norm 0.3043 | time 464.6475 ms | tok/sec 1128356.3657
for step 6283 | loss 3.464537 | norm 0.3171 | time 465.0755 ms | tok/sec 1127318.0549
for step 6284 | loss 3.518940 | norm 0.3115 | time 465.2731 ms | tok/sec 1126839.1679
for step 6285 | loss 3.449790 | norm 0.3465 | time 465.0011 ms | tok/sec 1127498.3929
Will loading at 0 from edu_fineweb10B/edufineweb_train_000034.npy
for step 6286 | loss 3.506224 | norm 0.3219 | time 1419.7474 ms | tok/sec 369282.6044
for step 6287 | loss 3.450980 | norm 0.2928 | time 463.6233 ms | tok/sec 1130849.1541
for step 6288 | loss 3.466009 | norm 0.3206 | time 464.2742 ms | tok/sec 1129263.7758
for step 6289 | loss 3.495853 | norm 0.3159 | time 464.8762 ms | tok/sec 1127801.3980
for step 6290 | loss 3.468532 | norm 0.2972 | time 463.7539 ms | tok/sec 1130530.5600
for step 6291 | loss 3.488039 | norm 0.3399 | time 464.5278 ms | tok/sec 1128647.0880
for step 6292 | loss 3.445678 | norm 0.3296 | time 464.5264 ms | tok/sec 1128650.5637
for step 6293 | loss 3.419648 | norm 0.3728 | time 464.5216 ms | tok/sec 1128662.1494
for step 6294 | loss 3.393508 | norm 0.3544 | time 464.7820 ms | tok/sec 1128029.9161
for step 6295 | loss 3.379282 | norm 0.2927 | time 464.3307 ms | tok/sec 1129126.3538
for step 6296 | loss 3.432247 | norm 0.3062 | time 464.3109 ms | tok/sec 1129174.4767
for step 6297 | loss 3.334483 | norm 0.3245 | time 463.9311 ms | tok/sec 1130098.8842
for step 6298 | loss 3.383265 | norm 0.2987 | time 464.7264 ms | tok/sec 1128164.7563
for step 6299 | loss 3.358722 | norm 0.3115 | time 466.1920 ms | tok/sec 1124618.1660
for step 6300 | loss 3.368108 | norm 0.3028 | time 465.9972 ms | tok/sec 1125088.2592
for step 6301 | loss 3.422947 | norm 0.3018 | time 466.3849 ms | tok/sec 1124153.0629
for step 6302 | loss 3.392646 | norm 0.3030 | time 463.9587 ms | tok/sec 1130031.5190
for step 6303 | loss 3.434014 | norm 0.2980 | time 465.3957 ms | tok/sec 1126542.4506
for step 6304 | loss 3.472531 | norm 0.3035 | time 465.1077 ms | tok/sec 1127240.0419
for step 6305 | loss 3.449928 | norm 0.3363 | time 465.3182 ms | tok/sec 1126730.0455
for step 6306 | loss 3.450562 | norm 0.3105 | time 465.9657 ms | tok/sec 1125164.2475
for step 6307 | loss 3.445846 | norm 0.3321 | time 464.1874 ms | tok/sec 1129474.9027
for step 6308 | loss 3.448139 | norm 0.3564 | time 465.1804 ms | tok/sec 1127063.8302
for step 6309 | loss 3.525321 | norm 0.3749 | time 466.0325 ms | tok/sec 1125003.0724
for step 6310 | loss 3.406804 | norm 0.3010 | time 465.6863 ms | tok/sec 1125839.3817
for step 6311 | loss 3.434906 | norm 0.3233 | time 465.6911 ms | tok/sec 1125827.8539
for step 6312 | loss 3.407095 | norm 0.3419 | time 465.5292 ms | tok/sec 1126219.3567
for step 6313 | loss 3.465643 | norm 0.3243 | time 464.8585 ms | tok/sec 1127844.2020
for step 6314 | loss 3.460817 | norm 0.4207 | time 464.3645 ms | tok/sec 1129044.0325
for step 6315 | loss 3.446475 | norm 0.3511 | time 464.2975 ms | tok/sec 1129206.9475
for step 6316 | loss 3.463135 | norm 0.3460 | time 465.2913 ms | tok/sec 1126795.2854
for step 6317 | loss 3.355904 | norm 0.3960 | time 465.0979 ms | tok/sec 1127263.7336
for step 6318 | loss 3.508830 | norm 0.3192 | time 465.3599 ms | tok/sec 1126629.0251
for step 6319 | loss 3.434376 | norm 0.3586 | time 464.7772 ms | tok/sec 1128041.4891
for step 6320 | loss 3.480169 | norm 0.3895 | time 465.4243 ms | tok/sec 1126473.2006
for step 6321 | loss 3.478927 | norm 0.3241 | time 465.4408 ms | tok/sec 1126433.3857
for step 6322 | loss 3.516149 | norm 0.3396 | time 466.3041 ms | tok/sec 1124347.9110
for step 6323 | loss 3.483730 | norm 0.3853 | time 465.5631 ms | tok/sec 1126137.4588
for step 6324 | loss 3.445769 | norm 0.3251 | time 465.8108 ms | tok/sec 1125538.5819
for step 6325 | loss 3.496686 | norm 0.3404 | time 465.1222 ms | tok/sec 1127204.7952
for step 6326 | loss 3.477028 | norm 0.3299 | time 465.4534 ms | tok/sec 1126402.8051
for step 6327 | loss 3.488699 | norm 0.3203 | time 464.7713 ms | tok/sec 1128055.9557
for step 6328 | loss 3.437929 | norm 0.3090 | time 465.2851 ms | tok/sec 1126810.2974
for step 6329 | loss 3.475334 | norm 0.3149 | time 465.0440 ms | tok/sec 1127394.3447
for step 6330 | loss 3.383906 | norm 0.3281 | time 464.9270 ms | tok/sec 1127678.2103
for step 6331 | loss 3.331799 | norm 0.3181 | time 465.8880 ms | tok/sec 1125351.9595
for step 6332 | loss 3.380294 | norm 0.3368 | time 465.1845 ms | tok/sec 1127054.0101
for step 6333 | loss 3.420518 | norm 0.3147 | time 464.9730 ms | tok/sec 1127566.6127
for step 6334 | loss 3.452172 | norm 0.3061 | time 464.0994 ms | tok/sec 1129689.0103
for step 6335 | loss 3.454030 | norm 0.3160 | time 464.6137 ms | tok/sec 1128438.5867
for step 6336 | loss 3.430510 | norm 0.3412 | time 464.5391 ms | tok/sec 1128619.8627
for step 6337 | loss 3.353693 | norm 0.2897 | time 465.2145 ms | tok/sec 1126981.2318
for step 6338 | loss 3.408045 | norm 0.3158 | time 465.2808 ms | tok/sec 1126820.6906
for step 6339 | loss 3.386700 | norm 0.2924 | time 465.7514 ms | tok/sec 1125682.0468
for step 6340 | loss 3.413153 | norm 0.3666 | time 465.1048 ms | tok/sec 1127246.9760
for step 6341 | loss 3.469830 | norm 0.3263 | time 465.4648 ms | tok/sec 1126375.1110
for step 6342 | loss 3.398266 | norm 0.3485 | time 464.1418 ms | tok/sec 1129585.7179
for step 6343 | loss 3.394252 | norm 0.3067 | time 464.1783 ms | tok/sec 1129496.9480
for step 6344 | loss 3.440564 | norm 0.3263 | time 464.8421 ms | tok/sec 1127884.1167
for step 6345 | loss 3.424094 | norm 0.3707 | time 464.4322 ms | tok/sec 1128879.4260
for step 6346 | loss 3.444511 | norm 0.2928 | time 465.1096 ms | tok/sec 1127235.4193
for step 6347 | loss 3.442130 | norm 0.3340 | time 464.7312 ms | tok/sec 1128153.1807
for step 6348 | loss 3.408983 | norm 0.3060 | time 464.8962 ms | tok/sec 1127752.8137
for step 6349 | loss 3.455926 | norm 0.3155 | time 464.9589 ms | tok/sec 1127600.7257
for step 6350 | loss 3.425002 | norm 0.3082 | time 465.6274 ms | tok/sec 1125981.7702
for step 6351 | loss 3.437856 | norm 0.3072 | time 465.0867 ms | tok/sec 1127290.8936
for step 6352 | loss 3.480747 | norm 0.3140 | time 465.5709 ms | tok/sec 1126118.4279
for step 6353 | loss 3.491289 | norm 0.2855 | time 465.5089 ms | tok/sec 1126268.3858
for step 6354 | loss 3.455725 | norm 0.3340 | time 464.6420 ms | tok/sec 1128369.6824
for step 6355 | loss 3.487124 | norm 0.3352 | time 465.0538 ms | tok/sec 1127370.6475
for step 6356 | loss 3.426919 | norm 0.3146 | time 464.6990 ms | tok/sec 1128231.3201
for step 6357 | loss 3.499303 | norm 0.3228 | time 464.8697 ms | tok/sec 1127817.0153
for step 6358 | loss 3.448314 | norm 0.3027 | time 465.5035 ms | tok/sec 1126281.6533
for step 6359 | loss 3.488841 | norm 0.3114 | time 464.2835 ms | tok/sec 1129241.1598
for step 6360 | loss 3.444934 | norm 0.3405 | time 464.7748 ms | tok/sec 1128047.2757
for step 6361 | loss 3.538952 | norm 0.3739 | time 465.6899 ms | tok/sec 1125830.7358
for step 6362 | loss 3.418844 | norm 0.3694 | time 464.6337 ms | tok/sec 1128389.9475
for step 6363 | loss 3.450209 | norm 0.3501 | time 465.5607 ms | tok/sec 1126143.2259
for step 6364 | loss 3.419904 | norm 0.3515 | time 465.4012 ms | tok/sec 1126529.1770
for step 6365 | loss 3.467669 | norm 0.3379 | time 464.8139 ms | tok/sec 1127952.3831
for step 6366 | loss 3.410654 | norm 0.3501 | time 465.1625 ms | tok/sec 1127107.1558
for step 6367 | loss 3.413635 | norm 0.3258 | time 464.8774 ms | tok/sec 1127798.5060
for step 6368 | loss 3.523219 | norm 0.3405 | time 465.5480 ms | tok/sec 1126173.7923
for step 6369 | loss 3.400316 | norm 0.3479 | time 465.4422 ms | tok/sec 1126429.9237
for step 6370 | loss 3.378917 | norm 0.3589 | time 464.6835 ms | tok/sec 1128268.9467
for step 6371 | loss 3.469748 | norm 0.3247 | time 464.6490 ms | tok/sec 1128352.8919
for step 6372 | loss 3.399186 | norm 0.3308 | time 465.6107 ms | tok/sec 1126022.1298
for step 6373 | loss 3.392693 | norm 0.3385 | time 464.7043 ms | tok/sec 1128218.5856
for step 6374 | loss 3.369419 | norm 0.3513 | time 465.3080 ms | tok/sec 1126754.8704
for step 6375 | loss 3.381294 | norm 0.3401 | time 464.3769 ms | tok/sec 1129013.8897
for step 6376 | loss 3.466619 | norm 0.3463 | time 465.9772 ms | tok/sec 1125136.6142
for step 6377 | loss 3.402853 | norm 0.3177 | time 464.7985 ms | tok/sec 1127989.9911
for step 6378 | loss 3.464224 | norm 0.3663 | time 465.0960 ms | tok/sec 1127268.3565
for step 6379 | loss 3.436223 | norm 0.3384 | time 465.0965 ms | tok/sec 1127267.2008
for step 6380 | loss 3.482452 | norm 0.3392 | time 465.7888 ms | tok/sec 1125591.5848
for step 6381 | loss 3.406075 | norm 0.3289 | time 465.3997 ms | tok/sec 1126532.6397
for step 6382 | loss 3.593727 | norm 0.4098 | time 465.0428 ms | tok/sec 1127397.2347
for step 6383 | loss 3.401279 | norm 0.4749 | time 465.6157 ms | tok/sec 1126010.0216
for step 6384 | loss 3.458672 | norm 0.4632 | time 464.7021 ms | tok/sec 1128223.7951
for step 6385 | loss 3.441076 | norm 0.3756 | time 465.6036 ms | tok/sec 1126039.4276
for step 6386 | loss 3.463696 | norm 0.3649 | time 465.1492 ms | tok/sec 1127139.5078
for step 6387 | loss 3.520531 | norm 0.4130 | time 464.7968 ms | tok/sec 1127994.0413
for step 6388 | loss 3.440356 | norm 0.3258 | time 465.8053 ms | tok/sec 1125551.8322
for step 6389 | loss 3.575833 | norm 0.3733 | time 465.6320 ms | tok/sec 1125970.8160
for step 6390 | loss 3.435153 | norm 0.4041 | time 464.8132 ms | tok/sec 1127954.1188
for step 6391 | loss 3.486941 | norm 0.3570 | time 465.0178 ms | tok/sec 1127457.9274
for step 6392 | loss 3.476931 | norm 0.3075 | time 464.7133 ms | tok/sec 1128196.5902
for step 6393 | loss 3.424614 | norm 0.3499 | time 465.5221 ms | tok/sec 1126236.6606
for step 6394 | loss 3.474950 | norm 0.3312 | time 464.7059 ms | tok/sec 1128214.5337
for step 6395 | loss 3.442724 | norm 0.2879 | time 464.7014 ms | tok/sec 1128225.5317
for step 6396 | loss 3.467206 | norm 0.3469 | time 464.3621 ms | tok/sec 1129049.8294
for step 6397 | loss 3.515104 | norm 0.2942 | time 464.9327 ms | tok/sec 1127664.3316
for step 6398 | loss 3.457560 | norm 0.3215 | time 465.2066 ms | tok/sec 1127000.2919
for step 6399 | loss 3.419823 | norm 0.3499 | time 465.9388 ms | tok/sec 1125229.3062
for step 6400 | loss 3.372323 | norm 0.2981 | time 465.5890 ms | tok/sec 1126074.6016
for step 6401 | loss 3.433470 | norm 0.3422 | time 465.3282 ms | tok/sec 1126705.7989
for step 6402 | loss 3.419858 | norm 0.3507 | time 464.9155 ms | tok/sec 1127705.9685
for step 6403 | loss 3.435620 | norm 0.3182 | time 465.0066 ms | tok/sec 1127485.0968
for step 6404 | loss 3.399928 | norm 0.3247 | time 464.2134 ms | tok/sec 1129411.6724
for step 6405 | loss 3.420794 | norm 0.2885 | time 464.8349 ms | tok/sec 1127901.4718
for step 6406 | loss 3.431608 | norm 0.2839 | time 465.2140 ms | tok/sec 1126982.3869
for step 6407 | loss 3.392252 | norm 0.3489 | time 464.7002 ms | tok/sec 1128228.4259
for step 6408 | loss 3.429686 | norm 0.3133 | time 464.5936 ms | tok/sec 1128487.2301
for step 6409 | loss 3.455164 | norm 0.3359 | time 464.6599 ms | tok/sec 1128326.2596
for step 6410 | loss 3.390542 | norm 0.3395 | time 464.7598 ms | tok/sec 1128083.7326
for step 6411 | loss 3.382107 | norm 0.2950 | time 465.6696 ms | tok/sec 1125879.7311
for step 6412 | loss 3.418792 | norm 0.3260 | time 465.1563 ms | tok/sec 1127122.1762
for step 6413 | loss 3.394544 | norm 0.3478 | time 465.1845 ms | tok/sec 1127054.0101
for step 6414 | loss 3.536771 | norm 0.3614 | time 464.8852 ms | tok/sec 1127779.4189
for step 6415 | loss 3.473495 | norm 0.3460 | time 464.9916 ms | tok/sec 1127521.5173
for step 6416 | loss 3.497141 | norm 0.3284 | time 464.9465 ms | tok/sec 1127630.7930
for step 6417 | loss 3.489927 | norm 0.3423 | time 465.1093 ms | tok/sec 1127235.9971
for step 6418 | loss 3.457311 | norm 0.3467 | time 465.9917 ms | tok/sec 1125101.4988
for step 6419 | loss 3.436169 | norm 0.3315 | time 465.5039 ms | tok/sec 1126280.4995
for step 6420 | loss 3.441201 | norm 0.3415 | time 465.0083 ms | tok/sec 1127481.0502
for step 6421 | loss 3.402115 | norm 0.2985 | time 466.2220 ms | tok/sec 1124545.7019
for step 6422 | loss 3.450885 | norm 0.3204 | time 465.8816 ms | tok/sec 1125367.5090
for step 6423 | loss 3.445567 | norm 0.3213 | time 466.3808 ms | tok/sec 1124162.8324
for step 6424 | loss 3.443275 | norm 0.3063 | time 465.3239 ms | tok/sec 1126716.1902
for step 6425 | loss 3.438588 | norm 0.3373 | time 464.6280 ms | tok/sec 1128403.8440
for step 6426 | loss 3.538776 | norm 0.3591 | time 464.5457 ms | tok/sec 1128603.6439
for step 6427 | loss 3.513465 | norm 0.3136 | time 465.5693 ms | tok/sec 1126122.4647
for step 6428 | loss 3.456515 | norm 0.3125 | time 465.1964 ms | tok/sec 1127025.1287
for step 6429 | loss 3.460960 | norm 0.2983 | time 464.7322 ms | tok/sec 1128150.8657
for step 6430 | loss 3.512618 | norm 0.3514 | time 465.7867 ms | tok/sec 1125596.7701
for step 6431 | loss 3.497735 | norm 0.3254 | time 466.4257 ms | tok/sec 1124054.8024
for step 6432 | loss 3.470305 | norm 0.3018 | time 465.0378 ms | tok/sec 1127409.3727
for step 6433 | loss 3.385865 | norm 0.3222 | time 465.6696 ms | tok/sec 1125879.7311
for step 6434 | loss 3.413871 | norm 0.3295 | time 464.9348 ms | tok/sec 1127659.1273
for step 6435 | loss 3.385031 | norm 0.2998 | time 465.7288 ms | tok/sec 1125736.7921
for step 6436 | loss 3.382163 | norm 0.2954 | time 466.1610 ms | tok/sec 1124692.9404
for step 6437 | loss 3.375469 | norm 0.3049 | time 465.5089 ms | tok/sec 1126268.3858
for step 6438 | loss 3.418336 | norm 0.3143 | time 465.4069 ms | tok/sec 1126515.3267
for step 6439 | loss 3.369212 | norm 0.2917 | time 466.0053 ms | tok/sec 1125068.6881
for step 6440 | loss 3.368068 | norm 0.3067 | time 465.5015 ms | tok/sec 1126286.2681
for step 6441 | loss 3.491632 | norm 0.3173 | time 465.5015 ms | tok/sec 1126286.2681
for step 6442 | loss 3.412276 | norm 0.3287 | time 465.8046 ms | tok/sec 1125553.5605
for step 6443 | loss 3.441711 | norm 0.3377 | time 465.6589 ms | tok/sec 1125905.6715
for step 6444 | loss 3.376278 | norm 0.3407 | time 465.5273 ms | tok/sec 1126223.9710
for step 6445 | loss 3.370223 | norm 0.3049 | time 466.2318 ms | tok/sec 1124522.1243
for step 6446 | loss 3.448638 | norm 0.3130 | time 465.9805 ms | tok/sec 1125128.5547
for step 6447 | loss 3.438437 | norm 0.3210 | time 465.0271 ms | tok/sec 1127435.3836
for step 6448 | loss 3.501688 | norm 0.3176 | time 465.4016 ms | tok/sec 1126528.0228
for step 6449 | loss 3.447937 | norm 0.3019 | time 465.2059 ms | tok/sec 1127002.0247
for step 6450 | loss 3.430504 | norm 0.3009 | time 466.3446 ms | tok/sec 1124250.1911
for step 6451 | loss 3.474287 | norm 0.3223 | time 465.6618 ms | tok/sec 1125898.7539
for step 6452 | loss 3.448395 | norm 0.3318 | time 464.9193 ms | tok/sec 1127696.7156
for step 6453 | loss 3.446719 | norm 0.2857 | time 467.6807 ms | tok/sec 1121038.3670
for step 6454 | loss 3.467690 | norm 0.3389 | time 465.0340 ms | tok/sec 1127418.6209
for step 6455 | loss 3.456631 | norm 0.3063 | time 465.3730 ms | tok/sec 1126597.2796
for step 6456 | loss 3.434923 | norm 0.3310 | time 465.6532 ms | tok/sec 1125919.5068
for step 6457 | loss 3.522115 | norm 0.3542 | time 465.5445 ms | tok/sec 1126182.4435
for step 6458 | loss 3.478672 | norm 0.3891 | time 466.0349 ms | tok/sec 1124997.3170
for step 6459 | loss 3.456432 | norm 0.3499 | time 466.2070 ms | tok/sec 1124581.9328
for step 6460 | loss 3.485363 | norm 0.3730 | time 465.2832 ms | tok/sec 1126814.9166
for step 6461 | loss 3.422615 | norm 0.3328 | time 466.2607 ms | tok/sec 1124452.5475
for step 6462 | loss 3.469898 | norm 0.3303 | time 465.2429 ms | tok/sec 1126912.5053
for step 6463 | loss 3.473162 | norm 0.3205 | time 464.9048 ms | tok/sec 1127731.9932
for step 6464 | loss 3.477334 | norm 0.3552 | time 465.3723 ms | tok/sec 1126599.0111
for step 6465 | loss 3.484800 | norm 0.3420 | time 464.9155 ms | tok/sec 1127705.9685
for step 6466 | loss 3.564265 | norm 0.3451 | time 464.9494 ms | tok/sec 1127623.8543
for step 6467 | loss 3.448156 | norm 0.3601 | time 464.7875 ms | tok/sec 1128016.6074
for step 6468 | loss 3.490666 | norm 0.3945 | time 465.6096 ms | tok/sec 1126025.0127
for step 6469 | loss 3.358071 | norm 0.3903 | time 465.2591 ms | tok/sec 1126873.2368
for step 6470 | loss 3.406388 | norm 0.3776 | time 465.3442 ms | tok/sec 1126667.1221
for step 6471 | loss 3.415326 | norm 0.3559 | time 465.2371 ms | tok/sec 1126926.3655
for step 6472 | loss 3.370654 | norm 0.3054 | time 465.2152 ms | tok/sec 1126979.4991
for step 6473 | loss 3.392081 | norm 0.3568 | time 465.3161 ms | tok/sec 1126735.2413
for step 6474 | loss 3.419758 | norm 0.3757 | time 465.2555 ms | tok/sec 1126881.8988
for step 6475 | loss 3.389962 | norm 0.3237 | time 465.3287 ms | tok/sec 1126704.6444
Will loading at 0 from edu_fineweb10B/edufineweb_train_000035.npy
for step 6476 | loss 3.441714 | norm 0.3705 | time 1439.9054 ms | tok/sec 364112.8078
for step 6477 | loss 3.379966 | norm 0.3390 | time 463.1433 ms | tok/sec 1132021.0071
for step 6478 | loss 3.394547 | norm 0.3204 | time 464.6771 ms | tok/sec 1128284.5769
for step 6479 | loss 3.361462 | norm 0.3260 | time 465.5743 ms | tok/sec 1126110.3543
for step 6480 | loss 3.437844 | norm 0.3336 | time 465.1570 ms | tok/sec 1127120.4430
for step 6481 | loss 3.441485 | norm 0.3293 | time 464.7496 ms | tok/sec 1128108.6172
for step 6482 | loss 3.512189 | norm 0.3790 | time 464.1478 ms | tok/sec 1129571.2121
for step 6483 | loss 3.496764 | norm 0.3612 | time 464.6523 ms | tok/sec 1128344.7863
for step 6484 | loss 3.445453 | norm 0.3578 | time 464.5507 ms | tok/sec 1128591.4801
for step 6485 | loss 3.435880 | norm 0.3158 | time 464.4213 ms | tok/sec 1128906.0844
for step 6486 | loss 3.430788 | norm 0.3175 | time 464.8850 ms | tok/sec 1127779.9973
for step 6487 | loss 3.454022 | norm 0.3271 | time 464.5061 ms | tok/sec 1128699.8047
for step 6488 | loss 3.440389 | norm 0.3351 | time 465.0881 ms | tok/sec 1127287.4263
for step 6489 | loss 3.488284 | norm 0.3240 | time 465.1251 ms | tok/sec 1127197.8617
for step 6490 | loss 3.424245 | norm 0.3577 | time 465.7507 ms | tok/sec 1125683.7756
for step 6491 | loss 3.427304 | norm 0.3541 | time 465.8775 ms | tok/sec 1125377.2996
for step 6492 | loss 3.515918 | norm 0.3226 | time 464.8890 ms | tok/sec 1127770.1648
for step 6493 | loss 3.535052 | norm 0.3834 | time 465.2371 ms | tok/sec 1126926.3655
for step 6494 | loss 3.427583 | norm 0.3423 | time 465.0874 ms | tok/sec 1127289.1600
for step 6495 | loss 3.435194 | norm 0.3130 | time 465.2717 ms | tok/sec 1126842.6324
for step 6496 | loss 3.459121 | norm 0.3033 | time 464.9689 ms | tok/sec 1127576.4416
for step 6497 | loss 3.485384 | norm 0.3346 | time 465.2677 ms | tok/sec 1126852.4487
for step 6498 | loss 3.426220 | norm 0.3273 | time 465.5848 ms | tok/sec 1126084.9812
for step 6499 | loss 3.433349 | norm 0.3350 | time 463.9447 ms | tok/sec 1130065.7813
validation loss 3.4458
HellaSwag accuracy: 2700/10042=0.2689
> Hello, I'm a language model, and we have a list of different modules and each has a code that we create. So if this is not a programming
> Hello, I'm a language model, a model for communication.
So here's a question with a more familiar and relevant thread. Did I ever get to
> Hello, I'm a language model, but only for some people.
I'm a language model at least a few years ago and will probably have more fun
> Hello, I'm a language model, and I'm just going to look at every language: 'This is going to be done today because I'm gonna be
> Hello, I'm a language model, so I want to keep in mind the difference of perspective.
I was surprised to find another thread about my favorite programming
> Hello, I'm a language model, then, I still think of some kind of universal languages today.
Thanks for taking part.
Thank you for the
> Hello, I'm a language model, I always do talk to your friends at a high school. It is a very conversational time where things are constantly being
> Hello, I'm a language model, and it must help me understand the process for the task, then it must also help me to communicate the message or information
> Hello, I'm a language model, and I can't tell you all you need to know about it or a couple of words about it: "I want
> Hello, I'm a language model, but now I'm not a language in the language area - it's much less a model than the reality of a language
> Hello, I'm a language model, so I tried a game using the .NET tool. I did it by using the .NET framework.
The basic
>>  Hello, I'm a language model, but that doesn't really help me out very much. I had this one because the programming language for the program was simpleHello, I'm a language model, but when I do I don't have any concrete ideas. I also think it helps my audience understand who I am,

> > > Hello, I'm a language model, and so on…
"The next step in this process is to see how many words are there in different languages and
Hello, I'm a language model, yet this is the key factor in creating an effective framework to implement with your audience.
Your audience is the one at
Hello, I'm a language model, although in most languages around the world there are numerous dialects that will be confused or confusing, I was actually trying to
> > > > Hello, I'm a language model, and I'm a big one, and I am the language developer, what else I know, and when you ask him
Hello, I'm a language model, but I mean the same thing. Now is the place where the world language is used. The language used is the language
> Hello, I'm a language model, and I don't even have to be fluent in it.
I would like to see this book, in several languages
Hello, I'm a language model, so this doesn’t cause any problems.
I'll tell you to go the other way and tell you,
> Hello, I'm a language model, and I hope that it could be useful both for my son's language skill development by working with these models in a second
> > > Hello, I'm a language model, and this will be a different problem. I hope that the answer will be improved.
But this is not how we
> Hello, I'm a language model, so I have the ability to learn one language and then you can practice both at school and in class.
*In
Hello, I'm a language model, but there is no evidence for that.
What is a good explanation? This is a good explanation on this: I
Hello, I'm a language model, in that you don't understand what you are going to be using in your research study and I wish you all had been
> Hello, I'm a language model, here is what I am doing for grammar and grammar. "I would like to get to this point" and "you
> > Hello, I'm a language model, and you are talking to someone whom I could relate to. And I am using 'I have just learned how people do
> Hello, I'm a language model, but what actually happens to a language when I look at a particular language? You don't look a lot alike, but
Hello, I'm a language model, and am using it for a long time. It's a very intuitive way to model a foreign language learning context. Now
> Hello, I'm a language model, but I am so caught up with both languages. :)
- If not, then you can't do it: if
Hello, I'm a language model, and he's trying to talk like a human being who uses the world's language to make words. Like the English language
> Hello, I'm a language model, so this is a lot of fun work. Thanks for your feedback!
I have a class set up to provide us
for step 6500 | loss 3.467891 | norm 0.3666 | time 12830.3204 ms | tok/sec 40863.2041
for step 6501 | loss 3.458639 | norm 0.3503 | time 461.7972 ms | tok/sec 1135320.7805
for step 6502 | loss 3.458764 | norm 0.3357 | time 460.6848 ms | tok/sec 1138062.3519
for step 6503 | loss 3.440365 | norm 0.3236 | time 462.5559 ms | tok/sec 1133458.7163
for step 6504 | loss 3.395001 | norm 0.2844 | time 462.7602 ms | tok/sec 1132958.2549
for step 6505 | loss 3.398267 | norm 0.3237 | time 462.7438 ms | tok/sec 1132998.5324
for step 6506 | loss 3.416854 | norm 0.3043 | time 462.6372 ms | tok/sec 1133259.5299
for step 6507 | loss 3.357693 | norm 0.2943 | time 462.3077 ms | tok/sec 1134067.2231
for step 6508 | loss 3.376985 | norm 0.3161 | time 463.4771 ms | tok/sec 1131205.7505
for step 6509 | loss 3.395242 | norm 0.2787 | time 462.4500 ms | tok/sec 1133718.1725
for step 6510 | loss 3.484165 | norm 0.3839 | time 463.3882 ms | tok/sec 1131422.8434
for step 6511 | loss 3.389586 | norm 0.4525 | time 463.5172 ms | tok/sec 1131107.9986
for step 6512 | loss 3.372759 | norm 0.4100 | time 463.6588 ms | tok/sec 1130762.5113
for step 6513 | loss 3.434165 | norm 0.3821 | time 464.1325 ms | tok/sec 1129608.3478
for step 6514 | loss 3.418595 | norm 0.3682 | time 463.6011 ms | tok/sec 1130903.2399
for step 6515 | loss 3.436583 | norm 0.3789 | time 462.9796 ms | tok/sec 1132421.4951
for step 6516 | loss 3.485048 | norm 0.3668 | time 463.9490 ms | tok/sec 1130055.3282
for step 6517 | loss 3.422549 | norm 0.3842 | time 463.3508 ms | tok/sec 1131514.2452
for step 6518 | loss 3.469420 | norm 0.3832 | time 464.2889 ms | tok/sec 1129227.8225
for step 6519 | loss 3.462614 | norm 0.3249 | time 464.1786 ms | tok/sec 1129496.3679
for step 6520 | loss 3.522508 | norm 0.3745 | time 464.3695 ms | tok/sec 1129031.8593
for step 6521 | loss 3.485127 | norm 0.3616 | time 464.7708 ms | tok/sec 1128057.1130
for step 6522 | loss 3.445104 | norm 0.3510 | time 464.6828 ms | tok/sec 1128270.6834
for step 6523 | loss 3.454080 | norm 0.3116 | time 465.2801 ms | tok/sec 1126822.4228
for step 6524 | loss 3.458261 | norm 0.3276 | time 464.9715 ms | tok/sec 1127570.0817
for step 6525 | loss 3.424087 | norm 0.2846 | time 464.9181 ms | tok/sec 1127699.6072
for step 6526 | loss 3.472507 | norm 0.3625 | time 464.9851 ms | tok/sec 1127537.1268
for step 6527 | loss 3.515055 | norm 0.3760 | time 464.4773 ms | tok/sec 1128769.9082
for step 6528 | loss 3.506754 | norm 0.3478 | time 464.9577 ms | tok/sec 1127603.6167
for step 6529 | loss 3.380555 | norm 0.3371 | time 465.1935 ms | tok/sec 1127032.0601
for step 6530 | loss 3.489649 | norm 0.3303 | time 465.0018 ms | tok/sec 1127496.6586
for step 6531 | loss 3.452121 | norm 0.3105 | time 464.1402 ms | tok/sec 1129589.7796
for step 6532 | loss 3.444339 | norm 0.3455 | time 464.4444 ms | tok/sec 1128849.8715
for step 6533 | loss 3.457299 | norm 0.3086 | time 465.1487 ms | tok/sec 1127140.6633
for step 6534 | loss 3.467485 | norm 0.3031 | time 464.9227 ms | tok/sec 1127688.6195
for step 6535 | loss 3.478002 | norm 0.2937 | time 464.6327 ms | tok/sec 1128392.2636
for step 6536 | loss 3.405823 | norm 0.3087 | time 465.7278 ms | tok/sec 1125739.0973
for step 6537 | loss 3.413227 | norm 0.3213 | time 464.9029 ms | tok/sec 1127736.6199
for step 6538 | loss 3.404282 | norm 0.2957 | time 465.3792 ms | tok/sec 1126582.2732
for step 6539 | loss 3.360611 | norm 0.3392 | time 465.4675 ms | tok/sec 1126368.7646
for step 6540 | loss 3.390127 | norm 0.3012 | time 465.2605 ms | tok/sec 1126869.7721
for step 6541 | loss 3.423136 | norm 0.2922 | time 464.4606 ms | tok/sec 1128810.4679
for step 6542 | loss 3.449604 | norm 0.3307 | time 464.9127 ms | tok/sec 1127712.9083
for step 6543 | loss 3.363595 | norm 0.2918 | time 465.4162 ms | tok/sec 1126492.8206
for step 6544 | loss 3.411694 | norm 0.3168 | time 464.2510 ms | tok/sec 1129320.0300
for step 6545 | loss 3.421315 | norm 0.2872 | time 465.3943 ms | tok/sec 1126545.9133
for step 6546 | loss 3.459342 | norm 0.2965 | time 465.2162 ms | tok/sec 1126977.1888
for step 6547 | loss 3.397952 | norm 0.3373 | time 464.6208 ms | tok/sec 1128421.2151
for step 6548 | loss 3.428037 | norm 0.3304 | time 464.9079 ms | tok/sec 1127724.4748
for step 6549 | loss 3.415430 | norm 0.3006 | time 465.1906 ms | tok/sec 1127038.9916
for step 6550 | loss 3.460272 | norm 0.3704 | time 464.5309 ms | tok/sec 1128639.5575
for step 6551 | loss 3.460202 | norm 0.3528 | time 464.6680 ms | tok/sec 1128306.5757
for step 6552 | loss 3.454354 | norm 0.3351 | time 464.8137 ms | tok/sec 1127952.9617
for step 6553 | loss 3.437810 | norm 0.3422 | time 465.0018 ms | tok/sec 1127496.6586
for step 6554 | loss 3.463575 | norm 0.3074 | time 464.7930 ms | tok/sec 1128003.2991
for step 6555 | loss 3.467483 | norm 0.3844 | time 464.7551 ms | tok/sec 1128095.3067
for step 6556 | loss 3.447306 | norm 0.3472 | time 465.2627 ms | tok/sec 1126864.5750
for step 6557 | loss 3.481038 | norm 0.3880 | time 464.4635 ms | tok/sec 1128803.5146
for step 6558 | loss 3.461755 | norm 0.3953 | time 464.8416 ms | tok/sec 1127885.2737
for step 6559 | loss 3.475034 | norm 0.3807 | time 465.0788 ms | tok/sec 1127309.9642
for step 6560 | loss 3.448265 | norm 0.3805 | time 464.5348 ms | tok/sec 1128630.2892
for step 6561 | loss 3.424228 | norm 0.3562 | time 464.5929 ms | tok/sec 1128488.9675
for step 6562 | loss 3.496089 | norm 0.3551 | time 464.6978 ms | tok/sec 1128234.2144
for step 6563 | loss 3.440961 | norm 0.3533 | time 464.4425 ms | tok/sec 1128854.5074
for step 6564 | loss 3.444794 | norm 0.3544 | time 464.5927 ms | tok/sec 1128489.5466
for step 6565 | loss 3.448857 | norm 0.3055 | time 464.3471 ms | tok/sec 1129086.3511
for step 6566 | loss 3.476194 | norm 0.3018 | time 465.4160 ms | tok/sec 1126493.3976
for step 6567 | loss 3.441468 | norm 0.3315 | time 464.2773 ms | tok/sec 1129256.2371
for step 6568 | loss 3.467139 | norm 0.3115 | time 465.2424 ms | tok/sec 1126913.6603
for step 6569 | loss 3.496198 | norm 0.3434 | time 465.5581 ms | tok/sec 1126149.5697
for step 6570 | loss 3.479795 | norm 0.3089 | time 464.7713 ms | tok/sec 1128055.9557
for step 6571 | loss 3.481489 | norm 0.3205 | time 465.1074 ms | tok/sec 1127240.6198
for step 6572 | loss 3.400640 | norm 0.3235 | time 464.9458 ms | tok/sec 1127632.5278
for step 6573 | loss 3.429370 | norm 0.3249 | time 465.9858 ms | tok/sec 1125115.8901
for step 6574 | loss 3.459352 | norm 0.3283 | time 465.5247 ms | tok/sec 1126230.3158
for step 6575 | loss 3.433257 | norm 0.3526 | time 465.2650 ms | tok/sec 1126858.8006
for step 6576 | loss 3.397063 | norm 0.3288 | time 465.0760 ms | tok/sec 1127316.8991
for step 6577 | loss 3.387201 | norm 0.3464 | time 465.3132 ms | tok/sec 1126742.1691
for step 6578 | loss 3.433711 | norm 0.3285 | time 465.7052 ms | tok/sec 1125793.8481
for step 6579 | loss 3.389605 | norm 0.3696 | time 465.0817 ms | tok/sec 1127303.0294
for step 6580 | loss 3.418206 | norm 0.3245 | time 465.0073 ms | tok/sec 1127483.3625
for step 6581 | loss 3.371974 | norm 0.3382 | time 464.7419 ms | tok/sec 1128127.1367
for step 6582 | loss 3.395391 | norm 0.3923 | time 465.4343 ms | tok/sec 1126448.9651
for step 6583 | loss 3.405743 | norm 0.3396 | time 465.2023 ms | tok/sec 1127010.6886
for step 6584 | loss 3.404557 | norm 0.3769 | time 464.5007 ms | tok/sec 1128713.1295
for step 6585 | loss 3.418634 | norm 0.4499 | time 465.7035 ms | tok/sec 1125797.8825
for step 6586 | loss 3.419907 | norm 0.3509 | time 464.7763 ms | tok/sec 1128043.8037
for step 6587 | loss 3.435480 | norm 0.4225 | time 464.9873 ms | tok/sec 1127531.9236
for step 6588 | loss 3.417533 | norm 0.3685 | time 464.9506 ms | tok/sec 1127620.9631
for step 6589 | loss 3.445564 | norm 0.3689 | time 465.4305 ms | tok/sec 1126458.1975
for step 6590 | loss 3.422646 | norm 0.3565 | time 465.5592 ms | tok/sec 1126146.6861
for step 6591 | loss 3.515966 | norm 0.3485 | time 465.6141 ms | tok/sec 1126014.0576
for step 6592 | loss 3.428757 | norm 0.3473 | time 466.1751 ms | tok/sec 1124659.0031
for step 6593 | loss 3.478246 | norm 0.3807 | time 465.0514 ms | tok/sec 1127376.4272
for step 6594 | loss 3.468561 | norm 0.3302 | time 464.9277 ms | tok/sec 1127676.4754
for step 6595 | loss 3.429322 | norm 0.3612 | time 464.1044 ms | tok/sec 1129676.8231
for step 6596 | loss 3.477214 | norm 0.3498 | time 465.8320 ms | tok/sec 1125487.3122
for step 6597 | loss 3.421534 | norm 0.3176 | time 465.9178 ms | tok/sec 1125279.9766
for step 6598 | loss 3.413726 | norm 0.3237 | time 465.4005 ms | tok/sec 1126530.9083
for step 6599 | loss 3.524622 | norm 0.3344 | time 464.8609 ms | tok/sec 1127838.4175
for step 6600 | loss 3.461059 | norm 0.2983 | time 465.1079 ms | tok/sec 1127239.4641
for step 6601 | loss 3.510142 | norm 0.3464 | time 464.5786 ms | tok/sec 1128523.7155
for step 6602 | loss 3.450376 | norm 0.3457 | time 464.5541 ms | tok/sec 1128583.3711
for step 6603 | loss 3.389881 | norm 0.3276 | time 464.2293 ms | tok/sec 1129372.8095
for step 6604 | loss 3.402951 | norm 0.3050 | time 464.8068 ms | tok/sec 1127969.7403
for step 6605 | loss 3.476997 | norm 0.2766 | time 464.8802 ms | tok/sec 1127791.5652
for step 6606 | loss 3.407191 | norm 0.2756 | time 465.0490 ms | tok/sec 1127382.2070
for step 6607 | loss 3.456745 | norm 0.2857 | time 465.2569 ms | tok/sec 1126878.4340
for step 6608 | loss 3.390188 | norm 0.3010 | time 465.5240 ms | tok/sec 1126232.0462
for step 6609 | loss 3.361361 | norm 0.2961 | time 464.8409 ms | tok/sec 1127887.0092
for step 6610 | loss 3.418646 | norm 0.2964 | time 464.6294 ms | tok/sec 1128400.3698
for step 6611 | loss 3.439940 | norm 0.2842 | time 465.1992 ms | tok/sec 1127018.1974
for step 6612 | loss 3.383630 | norm 0.2835 | time 465.5976 ms | tok/sec 1126053.8429
for step 6613 | loss 3.389214 | norm 0.3578 | time 464.5360 ms | tok/sec 1128627.3929
for step 6614 | loss 3.395880 | norm 0.3303 | time 465.4133 ms | tok/sec 1126499.7454
for step 6615 | loss 3.399333 | norm 0.3071 | time 465.6699 ms | tok/sec 1125879.1546
for step 6616 | loss 3.382413 | norm 0.3180 | time 465.4157 ms | tok/sec 1126493.9747
for step 6617 | loss 3.406512 | norm 0.3366 | time 465.1520 ms | tok/sec 1127132.5751
for step 6618 | loss 3.372750 | norm 0.2961 | time 464.9868 ms | tok/sec 1127533.0799
for step 6619 | loss 3.461790 | norm 0.3343 | time 465.4078 ms | tok/sec 1126513.0183
for step 6620 | loss 3.415170 | norm 0.3418 | time 465.3647 ms | tok/sec 1126617.4811
for step 6621 | loss 3.440771 | norm 0.3160 | time 464.7479 ms | tok/sec 1128112.6683
for step 6622 | loss 3.508453 | norm 0.3570 | time 464.1767 ms | tok/sec 1129501.0091
for step 6623 | loss 3.499636 | norm 0.3177 | time 464.6649 ms | tok/sec 1128314.1019
for step 6624 | loss 3.475844 | norm 0.3237 | time 466.3694 ms | tok/sec 1124190.4179
for step 6625 | loss 3.477578 | norm 0.3311 | time 464.2403 ms | tok/sec 1129346.1291
for step 6626 | loss 3.451493 | norm 0.3424 | time 464.5016 ms | tok/sec 1128710.8121
for step 6627 | loss 3.404083 | norm 0.3828 | time 464.1290 ms | tok/sec 1129617.0518
for step 6628 | loss 3.469418 | norm 0.3339 | time 464.8833 ms | tok/sec 1127784.0460
for step 6629 | loss 3.443664 | norm 0.3099 | time 465.5576 ms | tok/sec 1126150.7231
for step 6630 | loss 3.483105 | norm 0.3509 | time 463.8147 ms | tok/sec 1130382.3703
for step 6631 | loss 3.471470 | norm 0.3216 | time 464.8690 ms | tok/sec 1127818.7506
for step 6632 | loss 3.464597 | norm 0.3258 | time 464.3042 ms | tok/sec 1129190.7119
for step 6633 | loss 3.455475 | norm 0.3314 | time 465.2462 ms | tok/sec 1126904.4204
for step 6634 | loss 3.486586 | norm 0.3096 | time 464.1569 ms | tok/sec 1129549.1639
for step 6635 | loss 3.510120 | norm 0.2994 | time 464.6409 ms | tok/sec 1128372.5774
for step 6636 | loss 3.431499 | norm 0.3738 | time 463.9840 ms | tok/sec 1129969.9683
for step 6637 | loss 3.468549 | norm 0.4404 | time 464.8092 ms | tok/sec 1127963.9545
for step 6638 | loss 3.439891 | norm 0.3836 | time 465.2779 ms | tok/sec 1126827.6195
for step 6639 | loss 3.455572 | norm 0.3513 | time 465.1270 ms | tok/sec 1127193.2393
for step 6640 | loss 3.415311 | norm 0.3850 | time 463.9511 ms | tok/sec 1130050.1017
for step 6641 | loss 3.421484 | norm 0.3212 | time 464.7241 ms | tok/sec 1128170.5441
for step 6642 | loss 3.411788 | norm 0.3449 | time 464.6363 ms | tok/sec 1128383.5784
for step 6643 | loss 3.394049 | norm 0.3310 | time 465.5814 ms | tok/sec 1126093.0543
for step 6644 | loss 3.403606 | norm 0.3257 | time 465.0791 ms | tok/sec 1127309.3863
for step 6645 | loss 3.440801 | norm 0.3845 | time 464.5958 ms | tok/sec 1128482.0181
for step 6646 | loss 3.363495 | norm 0.3702 | time 464.8855 ms | tok/sec 1127778.8405
for step 6647 | loss 3.414112 | norm 0.2908 | time 464.3917 ms | tok/sec 1128977.9523
for step 6648 | loss 3.415228 | norm 0.3297 | time 465.2328 ms | tok/sec 1126936.7608
for step 6649 | loss 3.408295 | norm 0.3186 | time 465.1082 ms | tok/sec 1127238.8863
for step 6650 | loss 3.422998 | norm 0.3190 | time 465.1113 ms | tok/sec 1127231.3745
for step 6651 | loss 3.416545 | norm 0.3208 | time 464.6256 ms | tok/sec 1128409.6343
for step 6652 | loss 3.438609 | norm 0.3163 | time 464.0548 ms | tok/sec 1129797.5457
for step 6653 | loss 3.482053 | norm 0.3171 | time 464.7083 ms | tok/sec 1128208.7454
for step 6654 | loss 3.440531 | norm 0.3485 | time 464.7346 ms | tok/sec 1128145.0780
for step 6655 | loss 3.487909 | norm 0.3096 | time 465.1585 ms | tok/sec 1127116.9768
for step 6656 | loss 3.441428 | norm 0.3933 | time 465.5836 ms | tok/sec 1126087.8644
for step 6657 | loss 3.489233 | norm 0.4855 | time 464.5793 ms | tok/sec 1128521.9780
for step 6658 | loss 3.419751 | norm 0.4135 | time 464.5145 ms | tok/sec 1128679.5285
for step 6659 | loss 3.505835 | norm 0.3930 | time 464.3173 ms | tok/sec 1129158.8218
for step 6660 | loss 3.468922 | norm 0.4876 | time 464.8638 ms | tok/sec 1127831.4761
for step 6661 | loss 3.560528 | norm 0.3975 | time 464.2467 ms | tok/sec 1129330.4695
for step 6662 | loss 3.466124 | norm 0.4091 | time 464.3836 ms | tok/sec 1128997.6597
for step 6663 | loss 3.494019 | norm 0.3759 | time 464.7315 ms | tok/sec 1128152.6020
for step 6664 | loss 3.463061 | norm 0.3380 | time 464.5557 ms | tok/sec 1128579.3167
for step 6665 | loss 3.495923 | norm 0.3835 | time 465.1041 ms | tok/sec 1127248.7095
for step 6666 | loss 3.439804 | norm 0.3353 | time 465.3094 ms | tok/sec 1126751.4064
Will loading at 0 from edu_fineweb10B/edufineweb_train_000036.npy
for step 6667 | loss 3.449705 | norm 0.3392 | time 1439.2989 ms | tok/sec 364266.2491
for step 6668 | loss 3.441635 | norm 0.3237 | time 464.4244 ms | tok/sec 1128898.5504
for step 6669 | loss 3.448477 | norm 0.3224 | time 464.0582 ms | tok/sec 1129789.4193
for step 6670 | loss 3.454879 | norm 0.3301 | time 463.9318 ms | tok/sec 1130097.1419
for step 6671 | loss 3.480537 | norm 0.3115 | time 464.3033 ms | tok/sec 1129193.0312
for step 6672 | loss 3.471643 | norm 0.3372 | time 464.0722 ms | tok/sec 1129755.1738
for step 6673 | loss 3.451382 | norm 0.3321 | time 463.8596 ms | tok/sec 1130273.1416
for step 6674 | loss 3.434840 | norm 0.2947 | time 464.9398 ms | tok/sec 1127646.9838
for step 6675 | loss 3.380832 | norm 0.3114 | time 464.6268 ms | tok/sec 1128406.7391
for step 6676 | loss 3.399397 | norm 0.3189 | time 464.7300 ms | tok/sec 1128156.0746
for step 6677 | loss 3.395081 | norm 0.3166 | time 464.6416 ms | tok/sec 1128370.8404
for step 6678 | loss 3.424398 | norm 0.3199 | time 465.4875 ms | tok/sec 1126320.3036
for step 6679 | loss 3.443454 | norm 0.3281 | time 465.3609 ms | tok/sec 1126626.7163
for step 6680 | loss 3.403578 | norm 0.3278 | time 464.7169 ms | tok/sec 1128187.9080
for step 6681 | loss 3.362124 | norm 0.2777 | time 464.7741 ms | tok/sec 1128049.0117
for step 6682 | loss 3.385945 | norm 0.2960 | time 463.7854 ms | tok/sec 1130453.8452
for step 6683 | loss 3.369995 | norm 0.2758 | time 464.1440 ms | tok/sec 1129580.4958
for step 6684 | loss 3.408991 | norm 0.3064 | time 465.3022 ms | tok/sec 1126768.7267
for step 6685 | loss 3.430089 | norm 0.2679 | time 464.5913 ms | tok/sec 1128493.0213
for step 6686 | loss 3.394471 | norm 0.3046 | time 464.2386 ms | tok/sec 1129350.1891
for step 6687 | loss 3.517596 | norm 0.3069 | time 465.6990 ms | tok/sec 1125808.8334
for step 6688 | loss 3.479972 | norm 0.3161 | time 466.7044 ms | tok/sec 1123383.5277
for step 6689 | loss 3.449797 | norm 0.2730 | time 465.3602 ms | tok/sec 1126628.4479
for step 6690 | loss 3.417874 | norm 0.3379 | time 465.0598 ms | tok/sec 1127356.1985
for step 6691 | loss 3.427068 | norm 0.3351 | time 464.8490 ms | tok/sec 1127867.3406
for step 6692 | loss 3.476331 | norm 0.3614 | time 465.5247 ms | tok/sec 1126230.3158
for step 6693 | loss 3.463683 | norm 0.3584 | time 465.1775 ms | tok/sec 1127070.7620
for step 6694 | loss 3.425554 | norm 0.3706 | time 465.0104 ms | tok/sec 1127475.8475
for step 6695 | loss 3.432442 | norm 0.3152 | time 465.4822 ms | tok/sec 1126332.9954
for step 6696 | loss 3.469594 | norm 0.3412 | time 465.1058 ms | tok/sec 1127244.6646
for step 6697 | loss 3.499486 | norm 0.3343 | time 464.8139 ms | tok/sec 1127952.3831
for step 6698 | loss 3.453969 | norm 0.3241 | time 464.8101 ms | tok/sec 1127961.6402
for step 6699 | loss 3.427429 | norm 0.3497 | time 464.7310 ms | tok/sec 1128153.7595
for step 6700 | loss 3.424247 | norm 0.3245 | time 465.9061 ms | tok/sec 1125308.1928
for step 6701 | loss 3.504289 | norm 0.2797 | time 464.6680 ms | tok/sec 1128306.5757
for step 6702 | loss 3.444849 | norm 0.3213 | time 464.5815 ms | tok/sec 1128516.7657
for step 6703 | loss 3.451152 | norm 0.2918 | time 465.5824 ms | tok/sec 1126090.7477
for step 6704 | loss 3.410940 | norm 0.2989 | time 464.6592 ms | tok/sec 1128327.9965
for step 6705 | loss 3.380020 | norm 0.2974 | time 464.6585 ms | tok/sec 1128329.7333
for step 6706 | loss 3.475805 | norm 0.2967 | time 464.8461 ms | tok/sec 1127874.2824
for step 6707 | loss 3.444935 | norm 0.3180 | time 464.4332 ms | tok/sec 1128877.1080
for step 6708 | loss 3.403258 | norm 0.3076 | time 465.4295 ms | tok/sec 1126460.5057
for step 6709 | loss 3.295486 | norm 0.3368 | time 466.3122 ms | tok/sec 1124328.3656
for step 6710 | loss 3.418912 | norm 0.3561 | time 464.3993 ms | tok/sec 1128959.4049
for step 6711 | loss 3.423323 | norm 0.3533 | time 465.1856 ms | tok/sec 1127051.1219
for step 6712 | loss 3.366857 | norm 0.3171 | time 465.2562 ms | tok/sec 1126880.1664
for step 6713 | loss 3.382993 | norm 0.3417 | time 468.4165 ms | tok/sec 1119277.5089
for step 6714 | loss 3.404687 | norm 0.3273 | time 465.4007 ms | tok/sec 1126530.3312
for step 6715 | loss 3.370521 | norm 0.3108 | time 465.1864 ms | tok/sec 1127049.3890
for step 6716 | loss 3.395492 | norm 0.3147 | time 465.7652 ms | tok/sec 1125648.6260
for step 6717 | loss 3.449706 | norm 0.3293 | time 465.0977 ms | tok/sec 1127264.3115
for step 6718 | loss 3.406764 | norm 0.3015 | time 465.5688 ms | tok/sec 1126123.6181
for step 6719 | loss 3.432420 | norm 0.3378 | time 464.4933 ms | tok/sec 1128731.0895
for step 6720 | loss 3.479149 | norm 0.3547 | time 465.2584 ms | tok/sec 1126874.9692
for step 6721 | loss 3.424003 | norm 0.3576 | time 467.2148 ms | tok/sec 1122156.1784
for step 6722 | loss 3.424742 | norm 0.3499 | time 466.2311 ms | tok/sec 1124523.8495
for step 6723 | loss 3.403528 | norm 0.3384 | time 465.7075 ms | tok/sec 1125788.0846
for step 6724 | loss 3.443079 | norm 0.3683 | time 465.7860 ms | tok/sec 1125598.4986
for step 6725 | loss 3.411865 | norm 0.3184 | time 465.1439 ms | tok/sec 1127152.2180
for step 6726 | loss 3.693004 | norm 0.4358 | time 465.4782 ms | tok/sec 1126342.8029
for step 6727 | loss 3.519047 | norm 0.4718 | time 464.7522 ms | tok/sec 1128102.2512
for step 6728 | loss 3.456226 | norm 0.3889 | time 464.8037 ms | tok/sec 1127977.2620
for step 6729 | loss 3.438379 | norm 0.3323 | time 464.9038 ms | tok/sec 1127734.3065
for step 6730 | loss 3.468004 | norm 0.3558 | time 464.9851 ms | tok/sec 1127537.1268
for step 6731 | loss 3.493767 | norm 0.3331 | time 464.6947 ms | tok/sec 1128241.7396
for step 6732 | loss 3.508426 | norm 0.3457 | time 465.2197 ms | tok/sec 1126968.5254
for step 6733 | loss 3.452168 | norm 0.3052 | time 465.5840 ms | tok/sec 1126086.7111
for step 6734 | loss 3.445179 | norm 0.3271 | time 464.4434 ms | tok/sec 1128852.1895
for step 6735 | loss 3.453031 | norm 0.3024 | time 465.3141 ms | tok/sec 1126739.8599
for step 6736 | loss 3.456160 | norm 0.3159 | time 464.8964 ms | tok/sec 1127752.2354
for step 6737 | loss 3.429608 | norm 0.3473 | time 464.7112 ms | tok/sec 1128201.7995
for step 6738 | loss 3.465480 | norm 0.3061 | time 464.5894 ms | tok/sec 1128497.6543
for step 6739 | loss 3.483437 | norm 0.3127 | time 464.9043 ms | tok/sec 1127733.1499
for step 6740 | loss 3.509759 | norm 0.3122 | time 465.2634 ms | tok/sec 1126862.8427
for step 6741 | loss 3.442352 | norm 0.3161 | time 464.7713 ms | tok/sec 1128055.9557
for step 6742 | loss 3.414849 | norm 0.3260 | time 464.7055 ms | tok/sec 1128215.6914
for step 6743 | loss 3.437583 | norm 0.2924 | time 464.9436 ms | tok/sec 1127637.7319
for step 6744 | loss 3.410044 | norm 0.3115 | time 464.6358 ms | tok/sec 1128384.7364
for step 6745 | loss 3.451630 | norm 0.3129 | time 464.9038 ms | tok/sec 1127734.3065
for step 6746 | loss 3.377685 | norm 0.3412 | time 464.1163 ms | tok/sec 1129647.8072
for step 6747 | loss 3.368704 | norm 0.3375 | time 465.5120 ms | tok/sec 1126260.8870
for step 6748 | loss 3.348957 | norm 0.3007 | time 465.0500 ms | tok/sec 1127379.8951
for step 6749 | loss 3.431615 | norm 0.3114 | time 464.8058 ms | tok/sec 1127972.0547
validation loss 3.4382
HellaSwag accuracy: 2740/10042=0.2729
> Hello, I'm a language model, and it's a little weird. Because of our way of programming, we think we know everything in the world, and
> Hello, I'm a language model, it is very specific. I've done both, but with a similar grammar of words at the core.
I'm
> Hello, I'm a language model, but let's just say that I'm a language model that looks something like this:
A. What does an actual
> Hello, I'm a language model, and I'm looking at words and language components. What were we really supposed to do first? Well, I mean,
> Hello, I'm a language model, so I'd like to think about the world as a whole.
Anyway, I can really find this concept very easily
> Hello, I'm a language model, have been working long hours in various different languages, like Persian, Greek, Arabic, Hebrew and so on. Most importantly
> > Hello, I'm a language model, I wonder if our kids aren't ready to learn a language at the same time of the year as each other as a
Hello, I'm a language model, and I think it is interesting and powerful
and can be used within their application. The real
thing I'm looking
> Hello, I'm a language model, and it gets my point here. After reading it (from me), I just looked at it all the time, I
> Hello, I'm a language model, but we're not sure what we'll be using:
For this topic, we will take the model from the model
> Hello, I'm a language model, so I would just create that way that I didn't have this, I'm trying to do that. I've not
> Hello, I'm a language model, but like an example, I am talking to users on a webpage, and it'll make the reader appear as a part
> > > > Hello, I'm a language model, and we have this one; I'm not sure I'll just be. I think I'm a language and I'll
Hello, I'm a language model, but that doesn't sound like a big deal, really, really stupid. I mean, I've got the world,
Hello, I'm a language model, and I'm not really the only one.
Let's go to his next post on our tutorial I'm trying to
> Hello, I'm a language model, that was created by Python developer D3, and used for Python 2, Java, and C#. I'm currently
> > > Hello, I'm a language model, and I'm a programmer at the University of Kansas, and I'm currently working in the lab."
"I believe
Hello, I'm a language model, but I started wondering what it is actually making me ask? What makes one language model unique, or unique?
This
> Hello, I'm a language model, learning by doing, which has an obvious impact on the learning process. I look very good, and don't need to
Hello, I'm a language model, and this isn't a grammar model, it's a bit like the grammar model, but it's a bit better for
> Hello, I'm a language model, and I have a huge number of people all around my world. I love to hear the word that you want to translate
> > > Hello, I'm a language model, then add that to a paragraph with every beginning and ending thing. For example, before the first sentence, we want each
> Hello, I'm a language model, so I'll get a little extra complicated too.
Anyway, let's talk about the structure of the word root that
Hello, I'm a language model, so I didn’t learn about it in class. I want to understand it right now, and I want to
Hello, I'm a language model, and you are an integral part of the interpreter. So, in this article, we will discuss everything we learned to use
> Hello, I'm a language model, but for me it's just that my name is "I am a language model".
I'm having trouble in my
> > > Hello, I'm a language model, and I am talking about the word semantics of the various languages, like PHP, Python, and Python. I can tell
> Hello, I'm a language model, but if I'm going to be a writer, a lot of people don't really think my students will.
When
Hello, I'm a language model, what's interesting and interesting about it? I am very curious about the way people have to interact this way, but we
Hello, I'm a language model, and yet I've never used them before, and so far, so much of the problem I'm discussing here is how
> Hello, I'm a language model, and I'm like that. Now I'm going to start with my first syntax. Now at the start. My brain
Hello, I'm a language model, so you should be able to describe those features that you normally would use the language of your language you just learned. Also
for step 6750 | loss 3.403651 | norm 0.3499 | time 12454.1678 ms | tok/sec 42097.3931
for step 6751 | loss 3.387850 | norm 0.3294 | time 462.8155 ms | tok/sec 1132822.8501
for step 6752 | loss 3.394049 | norm 0.3109 | time 462.7490 ms | tok/sec 1132985.6900
for step 6753 | loss 3.421750 | norm 0.3289 | time 463.4559 ms | tok/sec 1131257.5426
for step 6754 | loss 3.511363 | norm 0.3333 | time 463.9220 ms | tok/sec 1130120.9538
for step 6755 | loss 3.533460 | norm 0.3362 | time 464.2289 ms | tok/sec 1129373.9696
for step 6756 | loss 3.467230 | norm 0.3536 | time 463.4995 ms | tok/sec 1131151.0539
for step 6757 | loss 3.454425 | norm 0.2999 | time 464.4492 ms | tok/sec 1128838.2819
for step 6758 | loss 3.412518 | norm 0.3480 | time 465.0981 ms | tok/sec 1127263.1558
for step 6759 | loss 3.444429 | norm 0.3305 | time 464.3419 ms | tok/sec 1129099.1053
for step 6760 | loss 3.441955 | norm 0.3042 | time 464.4110 ms | tok/sec 1128931.0053
for step 6761 | loss 3.442811 | norm 0.3377 | time 464.5813 ms | tok/sec 1128517.3448
for step 6762 | loss 3.456573 | norm 0.3320 | time 464.8297 ms | tok/sec 1127914.1992
for step 6763 | loss 3.512079 | norm 0.3724 | time 464.2885 ms | tok/sec 1129228.9823
for step 6764 | loss 3.455324 | norm 0.3432 | time 464.1321 ms | tok/sec 1129609.5083
for step 6765 | loss 3.425771 | norm 0.3645 | time 464.2720 ms | tok/sec 1129268.9951
for step 6766 | loss 3.412365 | norm 0.3327 | time 463.9103 ms | tok/sec 1130149.4133
for step 6767 | loss 3.506824 | norm 0.3674 | time 465.3671 ms | tok/sec 1126611.7091
for step 6768 | loss 3.474290 | norm 0.4034 | time 464.4523 ms | tok/sec 1128830.7488
for step 6769 | loss 3.499550 | norm 0.3741 | time 464.5786 ms | tok/sec 1128523.7155
for step 6770 | loss 3.429672 | norm 0.3465 | time 464.6983 ms | tok/sec 1128233.0567
for step 6771 | loss 3.440124 | norm 0.3845 | time 465.3273 ms | tok/sec 1126708.1081
for step 6772 | loss 3.528261 | norm 0.3374 | time 465.0214 ms | tok/sec 1127449.2566
for step 6773 | loss 3.409516 | norm 0.3365 | time 464.6323 ms | tok/sec 1128393.4216
for step 6774 | loss 3.442219 | norm 0.3461 | time 465.4512 ms | tok/sec 1126407.9980
for step 6775 | loss 3.425797 | norm 0.3289 | time 465.7605 ms | tok/sec 1125660.1502
for step 6776 | loss 3.407908 | norm 0.3500 | time 465.4400 ms | tok/sec 1126435.1167
for step 6777 | loss 3.418348 | norm 0.3465 | time 464.6041 ms | tok/sec 1128461.7497
for step 6778 | loss 3.400123 | norm 0.3294 | time 465.3592 ms | tok/sec 1126630.7567
for step 6779 | loss 3.396546 | norm 0.3592 | time 465.3475 ms | tok/sec 1126659.0407
for step 6780 | loss 3.389615 | norm 0.2866 | time 469.4927 ms | tok/sec 1116711.7810
for step 6781 | loss 3.377048 | norm 0.3220 | time 466.1541 ms | tok/sec 1124709.6222
for step 6782 | loss 3.407388 | norm 0.3908 | time 466.1498 ms | tok/sec 1124719.9766
for step 6783 | loss 3.374974 | norm 0.4063 | time 464.9646 ms | tok/sec 1127586.8489
for step 6784 | loss 3.341376 | norm 0.3767 | time 465.2629 ms | tok/sec 1126863.9976
for step 6785 | loss 3.388839 | norm 0.3688 | time 465.7114 ms | tok/sec 1125778.8631
for step 6786 | loss 3.391292 | norm 0.3216 | time 465.5604 ms | tok/sec 1126143.8026
for step 6787 | loss 3.391210 | norm 0.3316 | time 465.0500 ms | tok/sec 1127379.8951
for step 6788 | loss 3.388545 | norm 0.3322 | time 464.8128 ms | tok/sec 1127955.2760
for step 6789 | loss 3.424814 | norm 0.3481 | time 465.8902 ms | tok/sec 1125346.7764
for step 6790 | loss 3.440318 | norm 0.3545 | time 465.1635 ms | tok/sec 1127104.8450
for step 6791 | loss 3.419877 | norm 0.3422 | time 464.9312 ms | tok/sec 1127667.8013
for step 6792 | loss 3.433635 | norm 0.3481 | time 465.0571 ms | tok/sec 1127362.5560
for step 6793 | loss 3.426212 | norm 0.4098 | time 464.9556 ms | tok/sec 1127608.8206
for step 6794 | loss 3.404444 | norm 0.3774 | time 465.1666 ms | tok/sec 1127097.3351
for step 6795 | loss 3.492054 | norm 0.3572 | time 465.3993 ms | tok/sec 1126533.7939
for step 6796 | loss 3.402160 | norm 0.3987 | time 465.0695 ms | tok/sec 1127332.5030
for step 6797 | loss 3.438185 | norm 0.3723 | time 464.9124 ms | tok/sec 1127713.4867
for step 6798 | loss 3.410866 | norm 0.3093 | time 464.5548 ms | tok/sec 1128581.6335
for step 6799 | loss 3.476099 | norm 0.3626 | time 464.3979 ms | tok/sec 1128962.8825
for step 6800 | loss 3.463514 | norm 0.4453 | time 464.6778 ms | tok/sec 1128282.8402
for step 6801 | loss 3.459291 | norm 0.3844 | time 465.1544 ms | tok/sec 1127126.7979
for step 6802 | loss 3.504292 | norm 0.3595 | time 464.5069 ms | tok/sec 1128698.0667
for step 6803 | loss 3.423977 | norm 0.3931 | time 465.2047 ms | tok/sec 1127004.9126
for step 6804 | loss 3.415106 | norm 0.3429 | time 465.3392 ms | tok/sec 1126679.2444
for step 6805 | loss 3.476337 | norm 0.3598 | time 465.9808 ms | tok/sec 1125127.9790
for step 6806 | loss 3.516268 | norm 0.3384 | time 467.2961 ms | tok/sec 1121960.9443
for step 6807 | loss 3.424654 | norm 0.3536 | time 465.4794 ms | tok/sec 1126339.9183
for step 6808 | loss 3.451139 | norm 0.3335 | time 465.0733 ms | tok/sec 1127323.2562
for step 6809 | loss 3.488529 | norm 0.4684 | time 464.6943 ms | tok/sec 1128242.8973
for step 6810 | loss 3.491642 | norm 0.3956 | time 467.0477 ms | tok/sec 1122557.7381
for step 6811 | loss 3.409214 | norm 0.3640 | time 464.8182 ms | tok/sec 1127941.9691
for step 6812 | loss 3.389513 | norm 0.3150 | time 465.6346 ms | tok/sec 1125964.4741
for step 6813 | loss 3.402542 | norm 0.3325 | time 465.6482 ms | tok/sec 1125931.6131
for step 6814 | loss 3.338336 | norm 0.3189 | time 466.2807 ms | tok/sec 1124404.2513
for step 6815 | loss 3.441586 | norm 0.3297 | time 465.5890 ms | tok/sec 1126074.6016
for step 6816 | loss 3.408563 | norm 0.2888 | time 465.1868 ms | tok/sec 1127048.2337
for step 6817 | loss 3.395105 | norm 0.2998 | time 464.8273 ms | tok/sec 1127919.9845
for step 6818 | loss 3.397082 | norm 0.2945 | time 465.1165 ms | tok/sec 1127218.6625
for step 6819 | loss 3.312421 | norm 0.3181 | time 465.1077 ms | tok/sec 1127240.0419
for step 6820 | loss 3.393063 | norm 0.2988 | time 465.1062 ms | tok/sec 1127243.5090
for step 6821 | loss 3.406704 | norm 0.3145 | time 465.5061 ms | tok/sec 1126275.3079
for step 6822 | loss 3.400906 | norm 0.3057 | time 465.3642 ms | tok/sec 1126618.6355
for step 6823 | loss 3.411163 | norm 0.3442 | time 464.9692 ms | tok/sec 1127575.8634
for step 6824 | loss 3.444830 | norm 0.3182 | time 465.0533 ms | tok/sec 1127371.8035
for step 6825 | loss 3.365834 | norm 0.2987 | time 465.4140 ms | tok/sec 1126498.0142
for step 6826 | loss 3.448100 | norm 0.3452 | time 464.6320 ms | tok/sec 1128394.0006
for step 6827 | loss 3.412893 | norm 0.3092 | time 464.9746 ms | tok/sec 1127562.5655
for step 6828 | loss 3.396041 | norm 0.3051 | time 465.2729 ms | tok/sec 1126839.7453
for step 6829 | loss 3.420417 | norm 0.3124 | time 465.1332 ms | tok/sec 1127178.2171
for step 6830 | loss 3.406798 | norm 0.3031 | time 465.1277 ms | tok/sec 1127191.5060
for step 6831 | loss 3.473288 | norm 0.3241 | time 466.0361 ms | tok/sec 1124994.4393
for step 6832 | loss 3.418684 | norm 0.3109 | time 465.5628 ms | tok/sec 1126138.0355
for step 6833 | loss 3.421353 | norm 0.3455 | time 465.2486 ms | tok/sec 1126898.6456
for step 6834 | loss 3.444876 | norm 0.3374 | time 465.0013 ms | tok/sec 1127497.8148
for step 6835 | loss 3.503138 | norm 0.3562 | time 464.9928 ms | tok/sec 1127518.6267
for step 6836 | loss 3.473399 | norm 0.2846 | time 464.9541 ms | tok/sec 1127612.2898
for step 6837 | loss 3.451811 | norm 0.3173 | time 464.4237 ms | tok/sec 1128900.2890
for step 6838 | loss 3.436747 | norm 0.3177 | time 465.2853 ms | tok/sec 1126809.7201
for step 6839 | loss 3.460657 | norm 0.3045 | time 464.3867 ms | tok/sec 1128990.1244
for step 6840 | loss 3.459390 | norm 0.2959 | time 464.8645 ms | tok/sec 1127829.7408
for step 6841 | loss 3.455568 | norm 0.3074 | time 465.7652 ms | tok/sec 1125648.6260
for step 6842 | loss 3.460496 | norm 0.3086 | time 463.7425 ms | tok/sec 1130558.4589
for step 6843 | loss 3.444375 | norm 0.2798 | time 465.0166 ms | tok/sec 1127460.8177
for step 6844 | loss 3.434178 | norm 0.3235 | time 465.0617 ms | tok/sec 1127351.5749
for step 6845 | loss 3.441187 | norm 0.3129 | time 465.6992 ms | tok/sec 1125808.2570
for step 6846 | loss 3.404903 | norm 0.3291 | time 466.4505 ms | tok/sec 1123995.0500
for step 6847 | loss 3.442844 | norm 0.3328 | time 465.4241 ms | tok/sec 1126473.7776
for step 6848 | loss 3.365798 | norm 0.3143 | time 466.0048 ms | tok/sec 1125069.8393
for step 6849 | loss 3.416568 | norm 0.2976 | time 465.6823 ms | tok/sec 1125849.1806
for step 6850 | loss 3.440174 | norm 0.3060 | time 464.5445 ms | tok/sec 1128606.5401
for step 6851 | loss 3.417836 | norm 0.2983 | time 465.3621 ms | tok/sec 1126623.8303
for step 6852 | loss 3.514430 | norm 0.3668 | time 464.3013 ms | tok/sec 1129197.6699
for step 6853 | loss 3.406154 | norm 0.4959 | time 465.3881 ms | tok/sec 1126560.9187
for step 6854 | loss 3.384614 | norm 0.4914 | time 466.1226 ms | tok/sec 1124785.5593
for step 6855 | loss 3.361840 | norm 0.3859 | time 465.4777 ms | tok/sec 1126343.9567
for step 6856 | loss 3.410444 | norm 0.3653 | time 465.2925 ms | tok/sec 1126792.3985
Will loading at 0 from edu_fineweb10B/edufineweb_train_000037.npy
for step 6857 | loss 3.413504 | norm 0.3670 | time 1437.8753 ms | tok/sec 364626.8982
for step 6858 | loss 3.371614 | norm 0.3784 | time 462.5566 ms | tok/sec 1133456.9637
for step 6859 | loss 3.427681 | norm 0.3382 | time 463.1457 ms | tok/sec 1132015.1796
for step 6860 | loss 3.468860 | norm 0.3478 | time 464.4351 ms | tok/sec 1128872.4719
for step 6861 | loss 3.430330 | norm 0.3748 | time 463.7549 ms | tok/sec 1130528.2352
for step 6862 | loss 3.438495 | norm 0.3296 | time 464.5224 ms | tok/sec 1128660.4116
for step 6863 | loss 3.400815 | norm 0.3580 | time 464.4530 ms | tok/sec 1128829.0104
for step 6864 | loss 3.436346 | norm 0.3447 | time 465.0393 ms | tok/sec 1127405.9047
for step 6865 | loss 3.400132 | norm 0.3212 | time 464.0326 ms | tok/sec 1129851.5309
for step 6866 | loss 3.389868 | norm 0.3180 | time 465.9159 ms | tok/sec 1125284.5832
for step 6867 | loss 3.465770 | norm 0.3203 | time 465.8055 ms | tok/sec 1125551.2561
for step 6868 | loss 3.398727 | norm 0.3086 | time 465.2178 ms | tok/sec 1126973.1459
for step 6869 | loss 3.464293 | norm 0.2965 | time 465.5230 ms | tok/sec 1126234.3534
for step 6870 | loss 3.385747 | norm 0.3099 | time 465.7896 ms | tok/sec 1125589.8564
for step 6871 | loss 3.408298 | norm 0.3154 | time 465.8315 ms | tok/sec 1125488.4643
for step 6872 | loss 3.425206 | norm 0.3056 | time 464.8006 ms | tok/sec 1127984.7837
for step 6873 | loss 3.365353 | norm 0.3018 | time 465.3320 ms | tok/sec 1126696.5624
for step 6874 | loss 3.389264 | norm 0.3056 | time 465.1790 ms | tok/sec 1127067.2961
for step 6875 | loss 3.468168 | norm 0.3099 | time 464.4980 ms | tok/sec 1128719.5023
for step 6876 | loss 3.460278 | norm 0.2768 | time 465.8284 ms | tok/sec 1125495.9528
for step 6877 | loss 3.372051 | norm 0.3433 | time 464.7756 ms | tok/sec 1128045.5397
for step 6878 | loss 3.421783 | norm 0.3498 | time 469.7709 ms | tok/sec 1116050.3783
for step 6879 | loss 3.440034 | norm 0.3502 | time 465.3025 ms | tok/sec 1126768.1493
for step 6880 | loss 3.396693 | norm 0.4111 | time 465.9123 ms | tok/sec 1125293.2208
for step 6881 | loss 3.391915 | norm 0.3655 | time 464.5686 ms | tok/sec 1128548.0403
for step 6882 | loss 3.367546 | norm 0.3452 | time 465.1933 ms | tok/sec 1127032.6378
for step 6883 | loss 3.367096 | norm 0.3485 | time 465.6358 ms | tok/sec 1125961.5915
for step 6884 | loss 3.376146 | norm 0.3126 | time 465.4863 ms | tok/sec 1126323.1881
for step 6885 | loss 3.362175 | norm 0.3339 | time 465.3609 ms | tok/sec 1126626.7163
for step 6886 | loss 3.373377 | norm 0.3271 | time 465.6582 ms | tok/sec 1125907.4009
for step 6887 | loss 3.369677 | norm 0.3054 | time 464.5309 ms | tok/sec 1128639.5575
for step 6888 | loss 3.379761 | norm 0.2951 | time 464.4213 ms | tok/sec 1128906.0844
for step 6889 | loss 3.411734 | norm 0.2952 | time 464.9804 ms | tok/sec 1127548.6897
for step 6890 | loss 3.362919 | norm 0.3199 | time 465.4016 ms | tok/sec 1126528.0228
for step 6891 | loss 3.357744 | norm 0.3068 | time 465.9529 ms | tok/sec 1125195.3365
for step 6892 | loss 3.418161 | norm 0.3330 | time 465.5006 ms | tok/sec 1126288.5755
for step 6893 | loss 3.439791 | norm 0.2929 | time 466.0492 ms | tok/sec 1124962.7858
for step 6894 | loss 3.399391 | norm 0.3609 | time 465.8666 ms | tok/sec 1125403.7928
for step 6895 | loss 3.434035 | norm 0.3242 | time 465.3261 ms | tok/sec 1126710.9945
for step 6896 | loss 3.458651 | norm 0.4430 | time 465.0750 ms | tok/sec 1127319.2107
for step 6897 | loss 3.422244 | norm 0.4214 | time 465.6105 ms | tok/sec 1126022.7064
for step 6898 | loss 3.416518 | norm 0.3588 | time 464.3538 ms | tok/sec 1129070.1190
for step 6899 | loss 3.430805 | norm 0.3805 | time 465.3170 ms | tok/sec 1126732.9320
for step 6900 | loss 3.412239 | norm 0.3662 | time 465.6444 ms | tok/sec 1125940.8370
for step 6901 | loss 3.427686 | norm 0.4704 | time 464.6213 ms | tok/sec 1128420.0570
for step 6902 | loss 3.493466 | norm 0.3555 | time 464.8535 ms | tok/sec 1127856.3496
for step 6903 | loss 3.402938 | norm 0.4382 | time 466.2950 ms | tok/sec 1124369.7565
for step 6904 | loss 3.408443 | norm 0.3527 | time 464.9277 ms | tok/sec 1127676.4754
for step 6905 | loss 3.431161 | norm 0.3974 | time 464.6540 ms | tok/sec 1128340.7335
for step 6906 | loss 3.495235 | norm 0.3622 | time 465.9774 ms | tok/sec 1125136.0385
for step 6907 | loss 3.422694 | norm 0.3573 | time 464.5774 ms | tok/sec 1128526.6112
for step 6908 | loss 3.397785 | norm 0.3356 | time 466.8574 ms | tok/sec 1123015.2132
for step 6909 | loss 3.465994 | norm 0.3456 | time 465.3330 ms | tok/sec 1126694.2533
for step 6910 | loss 3.514877 | norm 0.3185 | time 466.0060 ms | tok/sec 1125066.9613
for step 6911 | loss 3.450950 | norm 0.3106 | time 465.4965 ms | tok/sec 1126298.3822
for step 6912 | loss 3.409729 | norm 0.3138 | time 465.6324 ms | tok/sec 1125969.6629
for step 6913 | loss 3.504582 | norm 0.2977 | time 464.8707 ms | tok/sec 1127814.7016
for step 6914 | loss 3.419245 | norm 0.3152 | time 464.9382 ms | tok/sec 1127651.0316
for step 6915 | loss 3.505264 | norm 0.3055 | time 465.9598 ms | tok/sec 1125178.6403
for step 6916 | loss 3.343400 | norm 0.3187 | time 465.3556 ms | tok/sec 1126639.4149
for step 6917 | loss 3.444499 | norm 0.3222 | time 465.1270 ms | tok/sec 1127193.2393
for step 6918 | loss 3.379957 | norm 0.3734 | time 465.2534 ms | tok/sec 1126887.0960
for step 6919 | loss 3.398224 | norm 0.3064 | time 465.1554 ms | tok/sec 1127124.4870
for step 6920 | loss 3.379612 | norm 0.3435 | time 464.1955 ms | tok/sec 1129455.1787
for step 6921 | loss 3.429429 | norm 0.3100 | time 464.9580 ms | tok/sec 1127603.0385
for step 6922 | loss 3.380861 | norm 0.3584 | time 464.4399 ms | tok/sec 1128860.8818
for step 6923 | loss 3.385013 | norm 0.3549 | time 464.8850 ms | tok/sec 1127779.9973
for step 6924 | loss 3.398010 | norm 0.3191 | time 464.5405 ms | tok/sec 1128616.3872
for step 6925 | loss 3.374808 | norm 0.3352 | time 468.0896 ms | tok/sec 1120059.1122
for step 6926 | loss 3.419141 | norm 0.3939 | time 464.7694 ms | tok/sec 1128060.5851
for step 6927 | loss 3.490045 | norm 0.3133 | time 465.4059 ms | tok/sec 1126517.6350
for step 6928 | loss 3.404565 | norm 0.3556 | time 464.6785 ms | tok/sec 1128281.1035
for step 6929 | loss 3.438702 | norm 0.3553 | time 465.4427 ms | tok/sec 1126428.7697
for step 6930 | loss 3.433076 | norm 0.3355 | time 464.7207 ms | tok/sec 1128178.6472
for step 6931 | loss 3.411069 | norm 0.3118 | time 464.4942 ms | tok/sec 1128728.7720
for step 6932 | loss 3.400256 | norm 0.3114 | time 465.0984 ms | tok/sec 1127262.5779
for step 6933 | loss 3.408284 | norm 0.3001 | time 465.0488 ms | tok/sec 1127382.7850
for step 6934 | loss 3.410800 | norm 0.2941 | time 464.1557 ms | tok/sec 1129552.0649
for step 6935 | loss 3.425197 | norm 0.3137 | time 465.6019 ms | tok/sec 1126043.4639
for step 6936 | loss 3.398253 | norm 0.3136 | time 465.0567 ms | tok/sec 1127363.7120
for step 6937 | loss 3.399046 | norm 0.3503 | time 464.9997 ms | tok/sec 1127501.8615
for step 6938 | loss 3.502306 | norm 0.3488 | time 464.9787 ms | tok/sec 1127552.7368
for step 6939 | loss 3.469128 | norm 0.3418 | time 466.1534 ms | tok/sec 1124711.3479
for step 6940 | loss 3.407016 | norm 0.3403 | time 465.4896 ms | tok/sec 1126315.1116
for step 6941 | loss 3.389787 | norm 0.3045 | time 465.8709 ms | tok/sec 1125393.4258
for step 6942 | loss 3.389529 | norm 0.3183 | time 466.3727 ms | tok/sec 1124182.3720
for step 6943 | loss 3.398139 | norm 0.2948 | time 465.6742 ms | tok/sec 1125868.7788
for step 6944 | loss 3.435722 | norm 0.2878 | time 465.6672 ms | tok/sec 1125885.4955
for step 6945 | loss 3.421151 | norm 0.3185 | time 466.0625 ms | tok/sec 1124930.5587
for step 6946 | loss 3.417740 | norm 0.3091 | time 466.0053 ms | tok/sec 1125068.6881
for step 6947 | loss 3.376014 | norm 0.3300 | time 465.0707 ms | tok/sec 1127329.6133
for step 6948 | loss 3.416303 | norm 0.3072 | time 465.5385 ms | tok/sec 1126196.8624
for step 6949 | loss 3.388879 | norm 0.3233 | time 465.3900 ms | tok/sec 1126556.3016
for step 6950 | loss 3.366571 | norm 0.2970 | time 465.3311 ms | tok/sec 1126698.8715
for step 6951 | loss 3.291844 | norm 0.3373 | time 466.2278 ms | tok/sec 1124531.9003
for step 6952 | loss 3.360996 | norm 0.3453 | time 465.2236 ms | tok/sec 1126959.2846
for step 6953 | loss 3.425804 | norm 0.3418 | time 465.4500 ms | tok/sec 1126410.8829
for step 6954 | loss 3.343638 | norm 0.2950 | time 465.2119 ms | tok/sec 1126987.5851
for step 6955 | loss 3.392131 | norm 0.3140 | time 465.7850 ms | tok/sec 1125600.8032
for step 6956 | loss 3.346077 | norm 0.3400 | time 464.9754 ms | tok/sec 1127560.8310
for step 6957 | loss 3.377947 | norm 0.3425 | time 465.5261 ms | tok/sec 1126226.8550
for step 6958 | loss 3.402025 | norm 0.3577 | time 465.6212 ms | tok/sec 1125996.7606
for step 6959 | loss 3.369626 | norm 0.3210 | time 465.6372 ms | tok/sec 1125958.1324
for step 6960 | loss 3.407817 | norm 0.3281 | time 465.9660 ms | tok/sec 1125163.6717
for step 6961 | loss 3.449549 | norm 0.3608 | time 465.5983 ms | tok/sec 1126052.1131
for step 6962 | loss 3.454841 | norm 0.3461 | time 464.8788 ms | tok/sec 1127795.0356
for step 6963 | loss 3.466545 | norm 0.3427 | time 465.9042 ms | tok/sec 1125312.7996
for step 6964 | loss 3.495612 | norm 0.3218 | time 465.0242 ms | tok/sec 1127442.3201
for step 6965 | loss 3.464932 | norm 0.3702 | time 465.0261 ms | tok/sec 1127437.6958
for step 6966 | loss 3.395079 | norm 0.3344 | time 465.3161 ms | tok/sec 1126735.2413
for step 6967 | loss 3.503423 | norm 0.3598 | time 465.3180 ms | tok/sec 1126730.6228
for step 6968 | loss 3.453371 | norm 0.3350 | time 465.6396 ms | tok/sec 1125952.3672
for step 6969 | loss 3.410420 | norm 0.3256 | time 465.3203 ms | tok/sec 1126724.8497
for step 6970 | loss 3.430034 | norm 0.3531 | time 465.1465 ms | tok/sec 1127145.8629
for step 6971 | loss 3.405999 | norm 0.2927 | time 465.1968 ms | tok/sec 1127023.9735
for step 6972 | loss 3.456295 | norm 0.3363 | time 465.8177 ms | tok/sec 1125521.8756
for step 6973 | loss 3.474831 | norm 0.3309 | time 466.2635 ms | tok/sec 1124445.6478
for step 6974 | loss 3.411185 | norm 0.3212 | time 464.7932 ms | tok/sec 1128002.7205
for step 6975 | loss 3.447150 | norm 0.3112 | time 465.3683 ms | tok/sec 1126608.8232
for step 6976 | loss 3.444527 | norm 0.3131 | time 465.0052 ms | tok/sec 1127488.5653
for step 6977 | loss 3.450387 | norm 0.3107 | time 465.6696 ms | tok/sec 1125879.7311
for step 6978 | loss 3.393987 | norm 0.3427 | time 465.9224 ms | tok/sec 1125269.0360
for step 6979 | loss 3.403487 | norm 0.3359 | time 466.4400 ms | tok/sec 1124020.3290
for step 6980 | loss 3.376452 | norm 0.3594 | time 466.2771 ms | tok/sec 1124412.8753
for step 6981 | loss 3.412849 | norm 0.3904 | time 466.5277 ms | tok/sec 1123808.9389
for step 6982 | loss 3.428413 | norm 0.3317 | time 466.2704 ms | tok/sec 1124428.9738
for step 6983 | loss 3.408270 | norm 0.3263 | time 465.8210 ms | tok/sec 1125513.8106
for step 6984 | loss 3.426217 | norm 0.2928 | time 466.4254 ms | tok/sec 1124055.3769
for step 6985 | loss 3.424663 | norm 0.3307 | time 465.2627 ms | tok/sec 1126864.5750
for step 6986 | loss 3.393453 | norm 0.3366 | time 465.2464 ms | tok/sec 1126903.8429
for step 6987 | loss 3.332191 | norm 0.3351 | time 466.3281 ms | tok/sec 1124289.8518
for step 6988 | loss 3.419974 | norm 0.3369 | time 465.5840 ms | tok/sec 1126086.7111
for step 6989 | loss 3.382224 | norm 0.3118 | time 465.0922 ms | tok/sec 1127277.6024
for step 6990 | loss 3.435085 | norm 0.3518 | time 465.4546 ms | tok/sec 1126399.9203
for step 6991 | loss 3.422162 | norm 0.3522 | time 464.9594 ms | tok/sec 1127599.5692
for step 6992 | loss 3.363136 | norm 0.3390 | time 465.5309 ms | tok/sec 1126215.3192
for step 6993 | loss 3.456897 | norm 0.3734 | time 465.8291 ms | tok/sec 1125494.2247
for step 6994 | loss 3.363183 | norm 0.3079 | time 464.7391 ms | tok/sec 1128134.0816
for step 6995 | loss 3.418189 | norm 0.3496 | time 464.7651 ms | tok/sec 1128071.0013
for step 6996 | loss 3.466273 | norm 0.3553 | time 465.8496 ms | tok/sec 1125444.6870
for step 6997 | loss 3.432233 | norm 0.3584 | time 465.1875 ms | tok/sec 1127046.5008
for step 6998 | loss 3.449976 | norm 0.3674 | time 464.8423 ms | tok/sec 1127883.5382
for step 6999 | loss 3.402902 | norm 0.3472 | time 465.4400 ms | tok/sec 1126435.1167
validation loss 3.4260
HellaSwag accuracy: 2724/10042=0.2713
> Hello, I'm a language model, and I mean, I'm a musician and have some pretty funny stuff. For example, when someone says 'I'm
> Hello, I'm a language model, a model that does not have a specific focus, but doesn't just deal with languages themselves, at the same time.
> Hello, I'm a language model, but once, I'm more interested in the language. It uses language to refer to the world and its meanings for everyday
> Hello, I'm a language model, and I'm an expert in using them that I'll continue to experiment with:
• creating the illusion of space and
> Hello, I'm a language model, and I think that it's really going to tell you a lot more about language than the thing it says is. And
> Hello, I'm a language model, so don't worry.
If you're looking for new ideas and find a way around learning a new language, it
> Hello, I'm a language model, so I won't confuse.
When I am reading, when the time is right, I'm speaking to the English> 
Hello, I'm a language model, so I need to look at the context where I come from.
For example, suppose somebody were saying 'Hello'
> Hello, I'm a language model, and they're really good at modeling this stuff using the language model. The first section explains things about them, but most
> Hello, I'm a language model, having to do many jobs to have them complete with proper names. There are so many examples, but here is another one
> > Hello, I'm a language model, who doesn't understand it correctly? If I'm an object-oriented model, I'm writing a language that is able
> Hello, I'm a language model, I'm a realist and I're trying to understand the rules for the language it is. But while I do understand
> Hello, I'm a language model, and I understand that language to be a formal language, so I'm ready to embrace it. Anyway, the language is
> Hello, I'm a language model, and I want to work on my own grammar, which is a nice one. I want to have the students repeat the
> Hello, I'm a language model, and it had something of a magical personality: that all words do have other magical properties. I would say that those magic
Hello, I'm a language model, has this function:
|Name it||Name it|
|Name it||Name it|
|Name it
> > Hello, I'm a language model, but I know I'm going to go with me.
The following video gives you a basic overview of programming
The
Hello, I'm a language model, and I am one of the leaders (English, Polish, French, Hindi, Spanish, and Arabic) in America (
> > > > Hello, I'm a language model, and a lot of people can understand things, and I can't even do it on my own.
A lot of
Hello, I'm a language model, and I want to learn a bunch of concepts that I'm trying to understand and then learn many more aspects at the same
Hello, I'm a language model, although it's not yet invented. After all, it's not the only language model I've done for me. One
Hello, I'm a language model, but what's the most important part of that language model is that each child is learning to use different languages in their life
> Hello, I'm a language model, but I never got to be a second one. I am an ESL (English) student who has to read a dictionary
> > Hello, I'm a language model, and am sure that everybody who speaks and writes an English language can pronounce it, as I said.<|endoftext|>As the world
> Hello, I'm a language model, and I'm not the same as in languages other than English. I always mean the idea of trying to be "d
> Hello, I'm a language model, so I can use that to teach about communication skills.
There are hundreds of books for learning styles. There are thousands
> Hello, I'm a language model, and I want you to be completely unique. You're not. If you use it well, you can't be an
Hello, I'm a language model, and this will be a two day project, the first part of this project, and the second part of this paper (
> > Hello, I'm a language model, but in mathematics, the process of writing any of the parts.
I wanted to create that which was already visible before
> Hello, I'm a language model, for a whole.
A language model for all language types is one that states each type in language, and it may
Hello, I'm a language model, and you'll like reading the best books for yourself. It's a fun activity that will be all the fun if you
> Hello, I'm a language model, and think about that. Thanks<|endoftext|>“It's just the same as the last two million years I have seen in
for step 7000 | loss 3.443005 | norm 0.3309 | time 12843.8363 ms | tok/sec 40820.2026
for step 7001 | loss 3.421828 | norm 0.3760 | time 461.4964 ms | tok/sec 1136060.9817
for step 7002 | loss 3.422238 | norm 0.3537 | time 462.4939 ms | tok/sec 1133610.6357
for step 7003 | loss 3.424759 | norm 0.3103 | time 462.8744 ms | tok/sec 1132678.7262
for step 7004 | loss 3.427810 | norm 0.3678 | time 464.1600 ms | tok/sec 1129541.6213
for step 7005 | loss 3.414663 | norm 0.3305 | time 462.7776 ms | tok/sec 1132915.6456
for step 7006 | loss 3.378132 | norm 0.3180 | time 463.7790 ms | tok/sec 1130469.5360
for step 7007 | loss 3.399467 | norm 0.2988 | time 462.9323 ms | tok/sec 1132536.9722
for step 7008 | loss 3.458341 | norm 0.3367 | time 462.6143 ms | tok/sec 1133315.5988
for step 7009 | loss 3.411335 | norm 0.3467 | time 464.2522 ms | tok/sec 1129317.1301
for step 7010 | loss 3.402042 | norm 0.2955 | time 463.4478 ms | tok/sec 1131277.3296
for step 7011 | loss 3.477773 | norm 0.3477 | time 463.9442 ms | tok/sec 1130066.9428
for step 7012 | loss 3.392699 | norm 0.2988 | time 464.5538 ms | tok/sec 1128583.9503
for step 7013 | loss 3.405411 | norm 0.3359 | time 463.5937 ms | tok/sec 1130921.2697
for step 7014 | loss 3.392966 | norm 0.3315 | time 464.3333 ms | tok/sec 1129119.9764
for step 7015 | loss 3.369207 | norm 0.3004 | time 464.4768 ms | tok/sec 1128771.0670
for step 7016 | loss 3.402652 | norm 0.3148 | time 463.9945 ms | tok/sec 1129944.4209
for step 7017 | loss 3.463542 | norm 0.3529 | time 464.1919 ms | tok/sec 1129463.8804
for step 7018 | loss 3.472411 | norm 0.3087 | time 463.3162 ms | tok/sec 1131598.6740
for step 7019 | loss 3.367211 | norm 0.3198 | time 464.3300 ms | tok/sec 1129128.0931
for step 7020 | loss 3.375991 | norm 0.2990 | time 465.7438 ms | tok/sec 1125700.4867
for step 7021 | loss 3.306642 | norm 0.3753 | time 464.4067 ms | tok/sec 1128941.4376
for step 7022 | loss 3.406460 | norm 0.3156 | time 465.0207 ms | tok/sec 1127450.9908
for step 7023 | loss 3.428735 | norm 0.2945 | time 465.0774 ms | tok/sec 1127313.4316
for step 7024 | loss 3.396798 | norm 0.3256 | time 464.7319 ms | tok/sec 1128151.4444
for step 7025 | loss 3.414428 | norm 0.3344 | time 464.5634 ms | tok/sec 1128560.7823
for step 7026 | loss 3.429005 | norm 0.3371 | time 464.7865 ms | tok/sec 1128018.9220
for step 7027 | loss 3.398087 | norm 0.3471 | time 465.3099 ms | tok/sec 1126750.2517
for step 7028 | loss 3.385407 | norm 0.3032 | time 465.7516 ms | tok/sec 1125681.4706
for step 7029 | loss 3.386217 | norm 0.3399 | time 464.9301 ms | tok/sec 1127670.6926
for step 7030 | loss 3.379615 | norm 0.3109 | time 465.5166 ms | tok/sec 1126249.9273
for step 7031 | loss 3.423907 | norm 0.3356 | time 465.7724 ms | tok/sec 1125631.3402
for step 7032 | loss 3.392280 | norm 0.3233 | time 465.5938 ms | tok/sec 1126063.0689
for step 7033 | loss 3.444380 | norm 0.3172 | time 465.3716 ms | tok/sec 1126600.7426
for step 7034 | loss 3.455429 | norm 0.3428 | time 465.7667 ms | tok/sec 1125645.1688
for step 7035 | loss 3.442476 | norm 0.3660 | time 466.1393 ms | tok/sec 1124745.2883
for step 7036 | loss 3.435317 | norm 0.3707 | time 465.1868 ms | tok/sec 1127048.2337
for step 7037 | loss 3.446604 | norm 0.3242 | time 465.8043 ms | tok/sec 1125554.1366
for step 7038 | loss 3.402281 | norm 0.3604 | time 466.3541 ms | tok/sec 1124227.2007
for step 7039 | loss 3.424539 | norm 0.3487 | time 465.9219 ms | tok/sec 1125270.1876
for step 7040 | loss 3.412586 | norm 0.4049 | time 466.5070 ms | tok/sec 1123858.9071
for step 7041 | loss 3.483451 | norm 0.3931 | time 465.3981 ms | tok/sec 1126536.6794
for step 7042 | loss 3.389073 | norm 0.3536 | time 465.0891 ms | tok/sec 1127285.1148
for step 7043 | loss 3.445104 | norm 0.3791 | time 466.2361 ms | tok/sec 1124511.7735
for step 7044 | loss 3.415271 | norm 0.3590 | time 466.2037 ms | tok/sec 1124589.9844
for step 7045 | loss 3.417368 | norm 0.3700 | time 465.0757 ms | tok/sec 1127317.4770
for step 7046 | loss 3.523206 | norm 0.3505 | time 465.3540 ms | tok/sec 1126643.4555
for step 7047 | loss 3.429171 | norm 0.3500 | time 465.5070 ms | tok/sec 1126273.0005
Will loading at 0 from edu_fineweb10B/edufineweb_train_000038.npy
for step 7048 | loss 3.452588 | norm 0.3697 | time 1417.5143 ms | tok/sec 369864.3400
for step 7049 | loss 3.453024 | norm 0.3977 | time 463.8500 ms | tok/sec 1130296.3800
for step 7050 | loss 3.444427 | norm 0.3318 | time 464.0057 ms | tok/sec 1129917.1329
for step 7051 | loss 3.477933 | norm 0.3529 | time 464.8209 ms | tok/sec 1127935.6050
for step 7052 | loss 3.411392 | norm 0.3453 | time 464.3261 ms | tok/sec 1129137.3695
for step 7053 | loss 3.487856 | norm 0.3419 | time 464.8438 ms | tok/sec 1127880.0672
for step 7054 | loss 3.388294 | norm 0.3193 | time 464.4077 ms | tok/sec 1128939.1193
for step 7055 | loss 3.403950 | norm 0.3346 | time 464.8252 ms | tok/sec 1127925.1913
for step 7056 | loss 3.355440 | norm 0.3109 | time 464.1657 ms | tok/sec 1129527.6968
for step 7057 | loss 3.332078 | norm 0.3248 | time 465.2629 ms | tok/sec 1126863.9976
for step 7058 | loss 3.404716 | norm 0.3430 | time 464.3002 ms | tok/sec 1129200.5691
for step 7059 | loss 3.368025 | norm 0.2943 | time 464.6168 ms | tok/sec 1128431.0590
for step 7060 | loss 3.369079 | norm 0.3150 | time 464.6740 ms | tok/sec 1128292.1027
for step 7061 | loss 3.361918 | norm 0.3268 | time 465.0006 ms | tok/sec 1127499.5491
for step 7062 | loss 3.330275 | norm 0.3054 | time 464.8023 ms | tok/sec 1127980.7335
for step 7063 | loss 3.396318 | norm 0.3150 | time 465.4381 ms | tok/sec 1126439.7328
for step 7064 | loss 3.422709 | norm 0.2983 | time 465.7643 ms | tok/sec 1125650.9309
for step 7065 | loss 3.398803 | norm 0.3062 | time 465.0111 ms | tok/sec 1127474.1133
for step 7066 | loss 3.455030 | norm 0.3294 | time 465.2369 ms | tok/sec 1126926.9430
for step 7067 | loss 3.432562 | norm 0.3130 | time 464.5629 ms | tok/sec 1128561.9407
for step 7068 | loss 3.391394 | norm 0.2931 | time 465.2567 ms | tok/sec 1126879.0114
for step 7069 | loss 3.452476 | norm 0.3828 | time 465.1232 ms | tok/sec 1127202.4840
for step 7070 | loss 3.513141 | norm 0.3602 | time 464.8721 ms | tok/sec 1127811.2311
for step 7071 | loss 3.401183 | norm 0.2948 | time 465.1487 ms | tok/sec 1127140.6633
for step 7072 | loss 3.456412 | norm 0.3409 | time 465.6582 ms | tok/sec 1125907.4009
for step 7073 | loss 3.475950 | norm 0.3576 | time 465.0404 ms | tok/sec 1127403.0147
for step 7074 | loss 3.474623 | norm 0.3466 | time 465.9698 ms | tok/sec 1125154.4605
for step 7075 | loss 3.396294 | norm 0.3273 | time 464.6940 ms | tok/sec 1128243.4761
for step 7076 | loss 3.409578 | norm 0.3614 | time 465.6780 ms | tok/sec 1125859.5560
for step 7077 | loss 3.410239 | norm 0.3264 | time 464.8769 ms | tok/sec 1127799.6628
for step 7078 | loss 3.380469 | norm 0.3230 | time 465.2135 ms | tok/sec 1126983.5421
for step 7079 | loss 3.438996 | norm 0.3561 | time 465.6720 ms | tok/sec 1125873.9667
for step 7080 | loss 3.414987 | norm 0.3461 | time 465.1604 ms | tok/sec 1127112.3551
for step 7081 | loss 3.400933 | norm 0.3766 | time 465.5128 ms | tok/sec 1126259.1565
for step 7082 | loss 3.542504 | norm 0.4344 | time 464.6623 ms | tok/sec 1128320.4702
for step 7083 | loss 3.498669 | norm 0.4420 | time 465.1425 ms | tok/sec 1127155.6845
for step 7084 | loss 3.433902 | norm 0.3354 | time 464.3328 ms | tok/sec 1129121.1359
for step 7085 | loss 3.427535 | norm 0.4005 | time 465.1740 ms | tok/sec 1127079.4270
for step 7086 | loss 3.469021 | norm 0.3483 | time 465.0779 ms | tok/sec 1127312.2758
for step 7087 | loss 3.426596 | norm 0.4622 | time 465.6885 ms | tok/sec 1125834.1942
for step 7088 | loss 3.417500 | norm 0.3848 | time 466.5117 ms | tok/sec 1123847.4198
for step 7089 | loss 3.396107 | norm 0.3772 | time 465.3578 ms | tok/sec 1126634.2200
for step 7090 | loss 3.363849 | norm 0.4063 | time 465.3835 ms | tok/sec 1126571.8844
for step 7091 | loss 3.404004 | norm 0.3673 | time 465.7562 ms | tok/sec 1125670.5222
for step 7092 | loss 3.448530 | norm 0.3762 | time 464.9065 ms | tok/sec 1127727.9448
for step 7093 | loss 3.352332 | norm 0.3539 | time 464.8733 ms | tok/sec 1127808.3390
for step 7094 | loss 3.400624 | norm 0.3497 | time 465.0788 ms | tok/sec 1127309.9642
for step 7095 | loss 3.344485 | norm 0.3305 | time 465.4984 ms | tok/sec 1126293.7673
for step 7096 | loss 3.383831 | norm 0.3415 | time 465.1175 ms | tok/sec 1127216.3513
for step 7097 | loss 3.443592 | norm 0.3293 | time 465.6630 ms | tok/sec 1125895.8716
for step 7098 | loss 3.371050 | norm 0.3198 | time 465.4136 ms | tok/sec 1126499.1684
for step 7099 | loss 3.433748 | norm 0.3167 | time 465.0903 ms | tok/sec 1127282.2254
for step 7100 | loss 3.419201 | norm 0.3467 | time 465.4183 ms | tok/sec 1126487.6270
for step 7101 | loss 3.433460 | norm 0.3270 | time 465.3614 ms | tok/sec 1126625.5619
for step 7102 | loss 3.421930 | norm 0.3188 | time 465.1756 ms | tok/sec 1127075.3834
for step 7103 | loss 3.481231 | norm 0.3279 | time 465.8878 ms | tok/sec 1125352.5354
for step 7104 | loss 3.455831 | norm 0.2994 | time 464.8876 ms | tok/sec 1127773.6351
for step 7105 | loss 3.419636 | norm 0.3194 | time 465.4882 ms | tok/sec 1126318.5730
for step 7106 | loss 3.501174 | norm 0.3640 | time 465.4744 ms | tok/sec 1126352.0336
for step 7107 | loss 3.454978 | norm 0.2873 | time 466.3117 ms | tok/sec 1124329.5153
for step 7108 | loss 3.397346 | norm 0.3046 | time 464.7386 ms | tok/sec 1128135.2391
for step 7109 | loss 3.432153 | norm 0.3023 | time 465.5058 ms | tok/sec 1126275.8848
for step 7110 | loss 3.461879 | norm 0.2924 | time 465.8301 ms | tok/sec 1125491.9205
for step 7111 | loss 3.460963 | norm 0.3059 | time 464.4821 ms | tok/sec 1128758.3202
for step 7112 | loss 3.388676 | norm 0.3503 | time 465.4584 ms | tok/sec 1126390.6888
for step 7113 | loss 3.343880 | norm 0.2887 | time 464.7048 ms | tok/sec 1128217.4279
for step 7114 | loss 3.496637 | norm 0.3352 | time 464.9751 ms | tok/sec 1127561.4092
for step 7115 | loss 3.409303 | norm 0.3790 | time 465.1792 ms | tok/sec 1127066.7184
for step 7116 | loss 3.393768 | norm 0.3325 | time 465.6813 ms | tok/sec 1125851.4862
for step 7117 | loss 3.407703 | norm 0.3285 | time 464.4301 ms | tok/sec 1128884.6417
for step 7118 | loss 3.438844 | norm 0.3492 | time 465.1103 ms | tok/sec 1127233.6858
for step 7119 | loss 3.354201 | norm 0.3171 | time 465.5406 ms | tok/sec 1126191.6716
for step 7120 | loss 3.416578 | norm 0.3329 | time 465.4713 ms | tok/sec 1126359.5336
for step 7121 | loss 3.424791 | norm 0.3061 | time 464.7334 ms | tok/sec 1128147.9718
for step 7122 | loss 3.431075 | norm 0.3611 | time 465.7116 ms | tok/sec 1125778.2868
for step 7123 | loss 3.406142 | norm 0.3145 | time 465.8155 ms | tok/sec 1125527.0602
for step 7124 | loss 3.396785 | norm 0.3143 | time 466.1798 ms | tok/sec 1124647.4994
for step 7125 | loss 3.429952 | norm 0.3705 | time 465.1661 ms | tok/sec 1127098.4904
for step 7126 | loss 3.368583 | norm 0.3256 | time 465.3761 ms | tok/sec 1126589.7763
for step 7127 | loss 3.392954 | norm 0.3214 | time 465.2212 ms | tok/sec 1126965.0601
for step 7128 | loss 3.424910 | norm 0.3035 | time 465.4472 ms | tok/sec 1126417.8067
for step 7129 | loss 3.370010 | norm 0.3228 | time 464.5946 ms | tok/sec 1128484.9137
for step 7130 | loss 3.415881 | norm 0.3172 | time 464.6528 ms | tok/sec 1128343.6283
for step 7131 | loss 3.381788 | norm 0.3149 | time 464.2212 ms | tok/sec 1129392.5307
for step 7132 | loss 3.373322 | norm 0.2904 | time 464.4785 ms | tok/sec 1128767.0112
for step 7133 | loss 3.370523 | norm 0.2907 | time 464.4880 ms | tok/sec 1128743.8356
for step 7134 | loss 3.460310 | norm 0.3569 | time 464.3776 ms | tok/sec 1129012.1508
for step 7135 | loss 3.371389 | norm 0.3653 | time 464.8905 ms | tok/sec 1127766.6945
for step 7136 | loss 3.464882 | norm 0.3772 | time 465.6174 ms | tok/sec 1126005.9856
for step 7137 | loss 3.436582 | norm 0.3206 | time 464.8671 ms | tok/sec 1127823.3780
for step 7138 | loss 3.462678 | norm 0.3975 | time 465.6370 ms | tok/sec 1125958.7089
for step 7139 | loss 3.416717 | norm 0.3550 | time 465.8699 ms | tok/sec 1125395.7296
for step 7140 | loss 3.466423 | norm 0.3081 | time 464.9997 ms | tok/sec 1127501.8615
for step 7141 | loss 3.406274 | norm 0.3637 | time 464.2704 ms | tok/sec 1129273.0545
for step 7142 | loss 3.404287 | norm 0.3467 | time 464.8783 ms | tok/sec 1127796.1924
for step 7143 | loss 3.443036 | norm 0.3695 | time 464.9773 ms | tok/sec 1127556.2057
for step 7144 | loss 3.508106 | norm 0.4526 | time 464.8881 ms | tok/sec 1127772.4783
for step 7145 | loss 3.420485 | norm 0.4640 | time 464.6046 ms | tok/sec 1128460.5916
for step 7146 | loss 3.413717 | norm 0.3948 | time 465.5330 ms | tok/sec 1126210.1282
for step 7147 | loss 3.428377 | norm 0.4224 | time 465.6308 ms | tok/sec 1125973.6987
for step 7148 | loss 3.428612 | norm 0.3656 | time 464.2467 ms | tok/sec 1129330.4695
for step 7149 | loss 3.462326 | norm 0.3863 | time 464.2215 ms | tok/sec 1129391.9506
for step 7150 | loss 3.429543 | norm 0.3426 | time 465.4281 ms | tok/sec 1126463.9679
for step 7151 | loss 3.502194 | norm 0.3713 | time 464.9725 ms | tok/sec 1127567.7690
for step 7152 | loss 3.382770 | norm 0.3515 | time 465.0145 ms | tok/sec 1127466.0203
for step 7153 | loss 3.444161 | norm 0.3431 | time 466.3141 ms | tok/sec 1124323.7668
for step 7154 | loss 3.406096 | norm 0.3092 | time 464.8855 ms | tok/sec 1127778.8405
for step 7155 | loss 3.443944 | norm 0.3214 | time 464.4275 ms | tok/sec 1128891.0165
for step 7156 | loss 3.425880 | norm 0.3247 | time 464.7615 ms | tok/sec 1128079.6817
for step 7157 | loss 3.374433 | norm 0.3209 | time 464.9496 ms | tok/sec 1127623.2760
for step 7158 | loss 3.390549 | norm 0.2888 | time 464.7002 ms | tok/sec 1128228.4259
for step 7159 | loss 3.404201 | norm 0.3180 | time 465.2052 ms | tok/sec 1127003.7574
for step 7160 | loss 3.340429 | norm 0.3163 | time 464.1480 ms | tok/sec 1129570.6319
for step 7161 | loss 3.359637 | norm 0.3017 | time 464.8905 ms | tok/sec 1127766.6945
for step 7162 | loss 3.356555 | norm 0.3080 | time 465.8844 ms | tok/sec 1125360.5980
for step 7163 | loss 3.361474 | norm 0.2847 | time 464.2310 ms | tok/sec 1129368.7494
for step 7164 | loss 3.352689 | norm 0.2949 | time 466.0454 ms | tok/sec 1124971.9939
for step 7165 | loss 3.278284 | norm 0.3311 | time 465.0383 ms | tok/sec 1127408.2167
for step 7166 | loss 3.371746 | norm 0.3270 | time 465.3392 ms | tok/sec 1126679.2444
for step 7167 | loss 3.375034 | norm 0.3162 | time 465.2317 ms | tok/sec 1126939.6484
for step 7168 | loss 3.387681 | norm 0.3070 | time 465.1549 ms | tok/sec 1127125.6425
for step 7169 | loss 3.439094 | norm 0.3167 | time 465.5626 ms | tok/sec 1126138.6122
for step 7170 | loss 3.450064 | norm 0.3600 | time 465.1678 ms | tok/sec 1127094.4466
for step 7171 | loss 3.416624 | norm 0.3203 | time 466.0077 ms | tok/sec 1125062.9320
for step 7172 | loss 3.419055 | norm 0.3194 | time 465.4071 ms | tok/sec 1126514.7496
for step 7173 | loss 3.408204 | norm 0.3182 | time 464.5379 ms | tok/sec 1128622.7589
for step 7174 | loss 3.478889 | norm 0.3059 | time 464.3908 ms | tok/sec 1128980.2708
for step 7175 | loss 3.423747 | norm 0.3412 | time 464.4723 ms | tok/sec 1128782.0758
for step 7176 | loss 3.485004 | norm 0.3134 | time 465.0674 ms | tok/sec 1127337.7043
for step 7177 | loss 3.391147 | norm 0.3187 | time 465.2598 ms | tok/sec 1126871.5045
for step 7178 | loss 3.438369 | norm 0.3042 | time 465.1456 ms | tok/sec 1127148.1739
for step 7179 | loss 3.468644 | norm 0.3333 | time 464.8099 ms | tok/sec 1127962.2188
for step 7180 | loss 3.438253 | norm 0.2978 | time 465.4524 ms | tok/sec 1126405.1131
for step 7181 | loss 3.462370 | norm 0.3314 | time 466.0265 ms | tok/sec 1125017.4611
for step 7182 | loss 3.421637 | norm 0.3250 | time 465.1177 ms | tok/sec 1127215.7734
for step 7183 | loss 3.379636 | norm 0.2788 | time 465.3983 ms | tok/sec 1126536.1023
for step 7184 | loss 3.464245 | norm 0.3155 | time 465.1284 ms | tok/sec 1127189.7726
for step 7185 | loss 3.436190 | norm 0.3123 | time 465.2665 ms | tok/sec 1126855.3359
for step 7186 | loss 3.420454 | norm 0.2961 | time 465.0743 ms | tok/sec 1127320.9445
for step 7187 | loss 3.548947 | norm 0.3449 | time 464.6776 ms | tok/sec 1128283.4191
for step 7188 | loss 3.416297 | norm 0.3812 | time 465.6367 ms | tok/sec 1125959.2854
for step 7189 | loss 3.463058 | norm 0.3733 | time 464.7183 ms | tok/sec 1128184.4352
for step 7190 | loss 3.437962 | norm 0.3224 | time 464.9997 ms | tok/sec 1127501.8615
for step 7191 | loss 3.423989 | norm 0.3893 | time 464.5801 ms | tok/sec 1128520.2406
for step 7192 | loss 3.375591 | norm 0.3495 | time 463.9792 ms | tok/sec 1129981.5811
for step 7193 | loss 3.404738 | norm 0.3583 | time 465.7211 ms | tok/sec 1125755.2338
for step 7194 | loss 3.346234 | norm 0.3821 | time 466.0599 ms | tok/sec 1124936.8889
for step 7195 | loss 3.372577 | norm 0.3409 | time 464.2005 ms | tok/sec 1129442.9966
for step 7196 | loss 3.380705 | norm 0.3081 | time 464.7710 ms | tok/sec 1128056.5343
for step 7197 | loss 3.363205 | norm 0.3574 | time 465.2381 ms | tok/sec 1126924.0554
for step 7198 | loss 3.430650 | norm 0.3278 | time 464.8497 ms | tok/sec 1127865.6052
for step 7199 | loss 3.365267 | norm 0.3333 | time 465.2581 ms | tok/sec 1126875.5467
for step 7200 | loss 3.362075 | norm 0.3432 | time 465.1959 ms | tok/sec 1127026.2840
for step 7201 | loss 3.350039 | norm 0.3375 | time 464.9496 ms | tok/sec 1127623.2760
for step 7202 | loss 3.347667 | norm 0.3424 | time 464.9189 ms | tok/sec 1127697.8722
for step 7203 | loss 3.461395 | norm 0.3499 | time 464.5607 ms | tok/sec 1128567.1534
for step 7204 | loss 3.520706 | norm 0.3824 | time 464.5936 ms | tok/sec 1128487.2301
for step 7205 | loss 3.512486 | norm 0.4063 | time 464.8254 ms | tok/sec 1127924.6127
for step 7206 | loss 3.436094 | norm 0.4135 | time 464.8907 ms | tok/sec 1127766.1162
for step 7207 | loss 3.478591 | norm 0.3520 | time 464.9601 ms | tok/sec 1127597.8346
for step 7208 | loss 3.455685 | norm 0.3659 | time 465.8337 ms | tok/sec 1125483.2799
for step 7209 | loss 3.418149 | norm 0.3525 | time 465.0447 ms | tok/sec 1127392.6107
for step 7210 | loss 3.425005 | norm 0.3305 | time 464.2947 ms | tok/sec 1129213.9057
for step 7211 | loss 3.445772 | norm 0.3333 | time 464.4749 ms | tok/sec 1128775.7022
for step 7212 | loss 3.483781 | norm 0.3222 | time 464.4833 ms | tok/sec 1128755.4233
for step 7213 | loss 3.428600 | norm 0.3232 | time 464.1171 ms | tok/sec 1129646.0662
for step 7214 | loss 3.466443 | norm 0.3557 | time 464.7565 ms | tok/sec 1128091.8344
for step 7215 | loss 3.430759 | norm 0.4012 | time 464.4718 ms | tok/sec 1128783.2346
for step 7216 | loss 3.458375 | norm 0.3004 | time 464.6173 ms | tok/sec 1128429.9008
for step 7217 | loss 3.384569 | norm 0.3708 | time 463.9752 ms | tok/sec 1129991.4522
for step 7218 | loss 3.426442 | norm 0.2896 | time 465.7044 ms | tok/sec 1125795.5771
for step 7219 | loss 3.434099 | norm 0.3539 | time 465.5113 ms | tok/sec 1126262.6175
for step 7220 | loss 3.405259 | norm 0.3008 | time 465.8451 ms | tok/sec 1125455.6310
for step 7221 | loss 3.482995 | norm 0.3227 | time 465.5743 ms | tok/sec 1126110.3543
for step 7222 | loss 3.444277 | norm 0.3710 | time 464.8364 ms | tok/sec 1127898.0007
for step 7223 | loss 3.461176 | norm 0.3433 | time 464.4320 ms | tok/sec 1128880.0056
for step 7224 | loss 3.404813 | norm 0.3315 | time 465.2848 ms | tok/sec 1126810.8748
for step 7225 | loss 3.461170 | norm 0.3229 | time 465.0562 ms | tok/sec 1127364.8679
for step 7226 | loss 3.375847 | norm 0.3099 | time 464.5920 ms | tok/sec 1128491.2839
for step 7227 | loss 3.405448 | norm 0.3370 | time 465.3320 ms | tok/sec 1126696.5624
for step 7228 | loss 3.325980 | norm 0.3121 | time 465.0974 ms | tok/sec 1127264.8894
for step 7229 | loss 3.421931 | norm 0.3260 | time 465.2910 ms | tok/sec 1126795.8628
for step 7230 | loss 3.412281 | norm 0.3413 | time 464.9129 ms | tok/sec 1127712.3300
for step 7231 | loss 3.369064 | norm 0.3841 | time 465.7338 ms | tok/sec 1125724.6900
for step 7232 | loss 3.371521 | norm 0.3557 | time 464.3157 ms | tok/sec 1129162.8804
for step 7233 | loss 3.387396 | norm 0.3057 | time 466.2085 ms | tok/sec 1124578.4821
for step 7234 | loss 3.349309 | norm 0.3560 | time 465.2054 ms | tok/sec 1127003.1798
for step 7235 | loss 3.360970 | norm 0.3073 | time 464.0696 ms | tok/sec 1129761.5584
for step 7236 | loss 3.368453 | norm 0.3453 | time 465.0531 ms | tok/sec 1127372.3814
for step 7237 | loss 3.436525 | norm 0.3246 | time 465.9939 ms | tok/sec 1125096.3180
Will loading at 0 from edu_fineweb10B/edufineweb_train_000039.npy
for step 7238 | loss 3.498732 | norm 0.3361 | time 1435.8113 ms | tok/sec 365151.0521
for step 7239 | loss 3.476303 | norm 0.3466 | time 463.6190 ms | tok/sec 1130859.6219
for step 7240 | loss 3.409374 | norm 0.3724 | time 463.8171 ms | tok/sec 1130376.5597
for step 7241 | loss 3.440731 | norm 0.3412 | time 464.5305 ms | tok/sec 1128640.7160
for step 7242 | loss 3.442099 | norm 0.3255 | time 464.8771 ms | tok/sec 1127799.0844
for step 7243 | loss 3.424530 | norm 0.3358 | time 465.1856 ms | tok/sec 1127051.1219
for step 7244 | loss 3.460192 | norm 0.3202 | time 465.5545 ms | tok/sec 1126158.2205
for step 7245 | loss 3.533160 | norm 0.3759 | time 464.7889 ms | tok/sec 1128013.1357
for step 7246 | loss 3.447851 | norm 0.3435 | time 465.3685 ms | tok/sec 1126608.2460
for step 7247 | loss 3.438557 | norm 0.3431 | time 465.8074 ms | tok/sec 1125546.6473
for step 7248 | loss 3.432543 | norm 0.3436 | time 464.6435 ms | tok/sec 1128366.2085
for step 7249 | loss 3.444229 | norm 0.3302 | time 464.5786 ms | tok/sec 1128523.7155
validation loss 3.4210
HellaSwag accuracy: 2717/10042=0.2706
> Hello, I'm a language model, and I never really wanted to say where you said "Oh the cat is cat." But it doesn't do that.
> Hello, I'm a language model, a model for people, and I really need to get used to. I don't understand the languages. I have to
> Hello, I'm a language model, but is really very interesting. I have a lot of problems (I don't know how to work in the first half
> Hello, I'm a language model, and I'm looking at more than 1 way to see its roots or meanings ... I didn't really understand what is meant
> > Hello, I'm a language model, and I understand the difference between them so clearly - I think it's much like a book by someone. You can make
Hello, I'm a language model, so I want to tell you what you say in English. You can go over the code (novel and zeros
> Hello, I'm a language model, so how do I do that in writing? How exactly do you make sure your language matches your needs? What do you> 
Hello, I'm a language model, we use the code from the website "AOcjo" to learn more about its features. If you feel you
> Hello, I'm a language model, so I get it at work. The language was actually written with my students. I didn't know how to use the> 
Hello, I'm a language model, I wanted to develop something more meaningful to you, and I have very little understanding of that. But just because my writing
> >>> Hello, I'm a language model, and there are lots of examples that are going to be used in various applications and industry. We have found that it actually
 Hello, I'm a language model, and I love it and will be looking something new, but I'm getting something bad with it? I'll be getting Hello, I'm a language model, I understand the basic terms are what you're looking for and the terminology are a little different, but I think that inHello, I'm a language model, and it took quite a while to put my code inside the XML. My students learn to code a simple language without having


>>  Hello, I'm a language model, looking for an example of the following...
- Introduction or explanation of the following example.
- Basic information or ideasHello, I'm a language model, and I want to give a brief description if you don't understand the code.
- What's a program?


> > > Hello, I'm a language model, but I think that will make my head seem very much like I had done earlier. I've been making myself quite as
Hello, I'm a language model, and I am sure you are. Thank you so much for your feedback.
I am a language model. What are
>>Hello, I'm a language model, but that doesn't change the order of English, just that I do have a certain amount of information that you are trying
>  Hello, I'm a language model, working as an individual, and working as an individual. It's a fun and informative way to get started at a very Hello, I'm a language model, so I want to have you a picture of me. I want you to be a language model when I'm about it> > Hello, I'm a language model, and I want to show you an example of the language model. I'm curious to learn my own personal experiences I got


Hello, I'm a language model, and I'm not really familiar with speech.
If you get into problems talking with English with Spanish. I know they>Hello, I'm a language model, who just looked around from last summer I'm asked to read my definition of language I've heard in your home, and

 Hello, I'm a language model, but I mean to do something new than those I started with in class to do. I've got this stuff done so> > 
Hello, I'm a language model, so I'd be familiar with any two languages I've ever heard of.
The syntax of the language is a function>Hello, I'm a language model, and the only one I make to remember is the words "I'm a language model" and "I'm writing"

 Hello, I'm a language model, so I've got a lot of answers. I've got this from both sources, but it isn't as informative a> > 
Hello, I'm a language model, and you've come up with ways to add elements that have more than a hundred senses. And by adding the number of
Hello, I'm a language model, trying to use language to talk about the language in which it is used. If I'm talking down the street I know
> > Hello, I'm a language model, but a problem is to teach the other's language too. For example, that's a 'good grammar' example (
Hello, I'm a language model, and think I am a language theorist.
What's a Language?
A language is, in any case, a
for step 7250 | loss 3.445859 | norm 0.3495 | time 12485.3356 ms | tok/sec 41992.3034
for step 7251 | loss 3.381316 | norm 0.3338 | time 462.7767 ms | tok/sec 1132917.9803
for step 7252 | loss 3.449184 | norm 0.3566 | time 462.5227 ms | tok/sec 1133539.9297
for step 7253 | loss 3.399050 | norm 0.3428 | time 462.6253 ms | tok/sec 1133288.7318
for step 7254 | loss 3.412584 | norm 0.3104 | time 463.1698 ms | tok/sec 1131956.3260
for step 7255 | loss 3.413560 | norm 0.3568 | time 462.3425 ms | tok/sec 1133981.8408
for step 7256 | loss 3.446499 | norm 0.3644 | time 462.9495 ms | tok/sec 1132494.9778
for step 7257 | loss 3.422074 | norm 0.3102 | time 462.8875 ms | tok/sec 1132646.6389
for step 7258 | loss 3.469901 | norm 0.3434 | time 463.4724 ms | tok/sec 1131217.3887
for step 7259 | loss 3.402153 | norm 0.3294 | time 463.5005 ms | tok/sec 1131148.7265
for step 7260 | loss 3.368849 | norm 0.3379 | time 463.0899 ms | tok/sec 1132151.5572
for step 7261 | loss 3.379295 | norm 0.2989 | time 464.3216 ms | tok/sec 1129148.3854
for step 7262 | loss 3.374936 | norm 0.3125 | time 463.4376 ms | tok/sec 1131302.3553
for step 7263 | loss 3.360423 | norm 0.3423 | time 464.3559 ms | tok/sec 1129064.9016
for step 7264 | loss 3.340055 | norm 0.3410 | time 464.4296 ms | tok/sec 1128885.8008
for step 7265 | loss 3.338879 | norm 0.3116 | time 464.7706 ms | tok/sec 1128057.6917
for step 7266 | loss 3.417978 | norm 0.3235 | time 463.4435 ms | tok/sec 1131287.8053
for step 7267 | loss 3.396297 | norm 0.3212 | time 464.9584 ms | tok/sec 1127601.8821
for step 7268 | loss 3.375667 | norm 0.2931 | time 465.3611 ms | tok/sec 1126626.1391
for step 7269 | loss 3.376909 | norm 0.3460 | time 464.4094 ms | tok/sec 1128935.0623
for step 7270 | loss 3.411471 | norm 0.3107 | time 464.4241 ms | tok/sec 1128899.1299
for step 7271 | loss 3.360494 | norm 0.3379 | time 465.3864 ms | tok/sec 1126564.9587
for step 7272 | loss 3.335348 | norm 0.3505 | time 464.6180 ms | tok/sec 1128428.1637
for step 7273 | loss 3.398031 | norm 0.3337 | time 465.1210 ms | tok/sec 1127207.6842
for step 7274 | loss 3.338197 | norm 0.3968 | time 464.7908 ms | tok/sec 1128008.5067
for step 7275 | loss 3.378796 | norm 0.3680 | time 465.3654 ms | tok/sec 1126615.7495
for step 7276 | loss 3.371357 | norm 0.3746 | time 465.5526 ms | tok/sec 1126162.8343
for step 7277 | loss 3.441019 | norm 0.4354 | time 465.6043 ms | tok/sec 1126037.6978
for step 7278 | loss 3.397861 | norm 0.3706 | time 464.7620 ms | tok/sec 1128078.5243
for step 7279 | loss 3.387888 | norm 0.3954 | time 465.0314 ms | tok/sec 1127424.9791
for step 7280 | loss 3.415461 | norm 0.3485 | time 464.7362 ms | tok/sec 1128141.0267
for step 7281 | loss 3.409719 | norm 0.3857 | time 465.9128 ms | tok/sec 1125292.0691
for step 7282 | loss 3.355190 | norm 0.3297 | time 464.5972 ms | tok/sec 1128478.5435
for step 7283 | loss 3.458698 | norm 0.3455 | time 464.6647 ms | tok/sec 1128314.6808
for step 7284 | loss 3.417346 | norm 0.3483 | time 465.6301 ms | tok/sec 1125975.4283
for step 7285 | loss 3.438067 | norm 0.3016 | time 465.8194 ms | tok/sec 1125517.8431
for step 7286 | loss 3.407832 | norm 0.3630 | time 464.2265 ms | tok/sec 1129379.7699
for step 7287 | loss 3.432609 | norm 0.3522 | time 464.9961 ms | tok/sec 1127510.5331
for step 7288 | loss 3.417666 | norm 0.2936 | time 463.7172 ms | tok/sec 1130620.0738
for step 7289 | loss 3.406189 | norm 0.3530 | time 468.5144 ms | tok/sec 1119043.4115
for step 7290 | loss 3.466439 | norm 0.2832 | time 466.1963 ms | tok/sec 1124607.8134
for step 7291 | loss 3.431556 | norm 0.3231 | time 465.1492 ms | tok/sec 1127139.5078
for step 7292 | loss 3.342778 | norm 0.2940 | time 464.8688 ms | tok/sec 1127819.3290
for step 7293 | loss 3.437283 | norm 0.3003 | time 465.3025 ms | tok/sec 1126768.1493
for step 7294 | loss 3.396814 | norm 0.3095 | time 464.0706 ms | tok/sec 1129759.2367
for step 7295 | loss 3.386543 | norm 0.3076 | time 464.7975 ms | tok/sec 1127992.3055
for step 7296 | loss 3.424922 | norm 0.3145 | time 465.5242 ms | tok/sec 1126231.4694
for step 7297 | loss 3.373988 | norm 0.3142 | time 464.6702 ms | tok/sec 1128301.3654
for step 7298 | loss 3.367128 | norm 0.2850 | time 464.5855 ms | tok/sec 1128506.9203
for step 7299 | loss 3.372884 | norm 0.2950 | time 465.5259 ms | tok/sec 1126227.4318
for step 7300 | loss 3.395692 | norm 0.3136 | time 465.4007 ms | tok/sec 1126530.3312
for step 7301 | loss 3.332632 | norm 0.3111 | time 466.1610 ms | tok/sec 1124692.9404
for step 7302 | loss 3.357410 | norm 0.3182 | time 464.9372 ms | tok/sec 1127653.3446
for step 7303 | loss 3.352015 | norm 0.3415 | time 465.5707 ms | tok/sec 1126119.0046
for step 7304 | loss 3.361312 | norm 0.2947 | time 465.7855 ms | tok/sec 1125599.6509
for step 7305 | loss 3.382248 | norm 0.3166 | time 464.9105 ms | tok/sec 1127718.1132
for step 7306 | loss 3.380064 | norm 0.3143 | time 465.9243 ms | tok/sec 1125264.4295
for step 7307 | loss 3.441491 | norm 0.3069 | time 464.5479 ms | tok/sec 1128598.4308
for step 7308 | loss 3.405467 | norm 0.3426 | time 465.2677 ms | tok/sec 1126852.4487
for step 7309 | loss 3.412435 | norm 0.3337 | time 465.0133 ms | tok/sec 1127468.9106
for step 7310 | loss 3.456381 | norm 0.3746 | time 464.6435 ms | tok/sec 1128366.2085
for step 7311 | loss 3.399249 | norm 0.3440 | time 464.5619 ms | tok/sec 1128564.2575
for step 7312 | loss 3.333466 | norm 0.2870 | time 466.7139 ms | tok/sec 1123360.5727
for step 7313 | loss 3.399400 | norm 0.3343 | time 464.9439 ms | tok/sec 1127637.1537
for step 7314 | loss 3.403475 | norm 0.3525 | time 465.9944 ms | tok/sec 1125095.1668
for step 7315 | loss 3.421391 | norm 0.3523 | time 465.2305 ms | tok/sec 1126942.5360
for step 7316 | loss 3.456734 | norm 0.3233 | time 465.1012 ms | tok/sec 1127255.6437
for step 7317 | loss 3.441517 | norm 0.3468 | time 465.8926 ms | tok/sec 1125341.0175
for step 7318 | loss 3.462002 | norm 0.3272 | time 464.6680 ms | tok/sec 1128306.5757
for step 7319 | loss 3.410928 | norm 0.3344 | time 465.2581 ms | tok/sec 1126875.5467
for step 7320 | loss 3.444499 | norm 0.3367 | time 464.4260 ms | tok/sec 1128894.4937
for step 7321 | loss 3.446002 | norm 0.3187 | time 465.6670 ms | tok/sec 1125886.0720
for step 7322 | loss 3.428815 | norm 0.3135 | time 464.8025 ms | tok/sec 1127980.1549
for step 7323 | loss 3.389151 | norm 0.3297 | time 465.2283 ms | tok/sec 1126947.7338
for step 7324 | loss 3.463382 | norm 0.3193 | time 466.2421 ms | tok/sec 1124497.3977
for step 7325 | loss 3.432858 | norm 0.2969 | time 466.4195 ms | tok/sec 1124069.7415
for step 7326 | loss 3.403715 | norm 0.3164 | time 465.3800 ms | tok/sec 1126580.5417
for step 7327 | loss 3.405940 | norm 0.2848 | time 465.4508 ms | tok/sec 1126409.1519
for step 7328 | loss 3.420309 | norm 0.2821 | time 464.7934 ms | tok/sec 1128002.1419
for step 7329 | loss 3.415802 | norm 0.2878 | time 465.4593 ms | tok/sec 1126388.3809
for step 7330 | loss 3.412242 | norm 0.3292 | time 464.6614 ms | tok/sec 1128322.7860
for step 7331 | loss 3.401714 | norm 0.3669 | time 464.4704 ms | tok/sec 1128786.7111
for step 7332 | loss 3.389616 | norm 0.3152 | time 465.3761 ms | tok/sec 1126589.7763
for step 7333 | loss 3.313902 | norm 0.3481 | time 465.6882 ms | tok/sec 1125834.7705
for step 7334 | loss 3.447077 | norm 0.3771 | time 464.9472 ms | tok/sec 1127629.0583
for step 7335 | loss 3.452604 | norm 0.3925 | time 465.9576 ms | tok/sec 1125183.8219
for step 7336 | loss 3.384818 | norm 0.3198 | time 464.6730 ms | tok/sec 1128294.4184
for step 7337 | loss 3.412461 | norm 0.3634 | time 464.2308 ms | tok/sec 1129369.3294
for step 7338 | loss 3.430056 | norm 0.3491 | time 465.3137 ms | tok/sec 1126741.0145
for step 7339 | loss 3.368506 | norm 0.3181 | time 465.1744 ms | tok/sec 1127078.2717
for step 7340 | loss 3.395442 | norm 0.3496 | time 465.4486 ms | tok/sec 1126414.3448
for step 7341 | loss 3.388138 | norm 0.2969 | time 465.4846 ms | tok/sec 1126327.2264
for step 7342 | loss 3.429812 | norm 0.3533 | time 465.4343 ms | tok/sec 1126448.9651
for step 7343 | loss 3.371726 | norm 0.3532 | time 465.2350 ms | tok/sec 1126931.5631
for step 7344 | loss 3.376088 | norm 0.3383 | time 464.8185 ms | tok/sec 1127941.3905
for step 7345 | loss 3.419619 | norm 0.3150 | time 464.4692 ms | tok/sec 1128789.6082
for step 7346 | loss 3.399219 | norm 0.3299 | time 465.2877 ms | tok/sec 1126803.9462
for step 7347 | loss 3.421061 | norm 0.3313 | time 464.9935 ms | tok/sec 1127516.8923
for step 7348 | loss 3.422089 | norm 0.3319 | time 465.5690 ms | tok/sec 1126123.0414
for step 7349 | loss 3.416794 | norm 0.3249 | time 466.0208 ms | tok/sec 1125031.2747
for step 7350 | loss 3.405216 | norm 0.3535 | time 466.2638 ms | tok/sec 1124445.0728
for step 7351 | loss 3.406729 | norm 0.3432 | time 465.4295 ms | tok/sec 1126460.5057
for step 7352 | loss 3.442621 | norm 0.3026 | time 466.8527 ms | tok/sec 1123026.6835
for step 7353 | loss 3.481067 | norm 0.3672 | time 465.7776 ms | tok/sec 1125618.6643
for step 7354 | loss 3.378089 | norm 0.3614 | time 465.6920 ms | tok/sec 1125825.5483
for step 7355 | loss 3.446421 | norm 0.3092 | time 465.2436 ms | tok/sec 1126910.7729
for step 7356 | loss 3.409383 | norm 0.3541 | time 465.6434 ms | tok/sec 1125943.1431
for step 7357 | loss 3.443958 | norm 0.3245 | time 466.3541 ms | tok/sec 1124227.2007
for step 7358 | loss 3.453130 | norm 0.3077 | time 464.9990 ms | tok/sec 1127503.5958
for step 7359 | loss 3.420858 | norm 0.3164 | time 465.6839 ms | tok/sec 1125845.1457
for step 7360 | loss 3.396101 | norm 0.3115 | time 465.1613 ms | tok/sec 1127110.0443
for step 7361 | loss 3.417365 | norm 0.3263 | time 464.9575 ms | tok/sec 1127604.1949
for step 7362 | loss 3.411713 | norm 0.2909 | time 465.6141 ms | tok/sec 1126014.0576
for step 7363 | loss 3.405193 | norm 0.2917 | time 465.1029 ms | tok/sec 1127251.5987
for step 7364 | loss 3.361902 | norm 0.2954 | time 465.2936 ms | tok/sec 1126789.5117
for step 7365 | loss 3.386839 | norm 0.3528 | time 464.9422 ms | tok/sec 1127641.2014
for step 7366 | loss 3.419177 | norm 0.3471 | time 465.2379 ms | tok/sec 1126924.6329
for step 7367 | loss 3.439519 | norm 0.4371 | time 465.1992 ms | tok/sec 1127018.1974
for step 7368 | loss 3.443366 | norm 0.3894 | time 465.4388 ms | tok/sec 1126438.0018
for step 7369 | loss 3.364935 | norm 0.3221 | time 464.7074 ms | tok/sec 1128211.0607
for step 7370 | loss 3.374428 | norm 0.3285 | time 464.6394 ms | tok/sec 1128376.0514
for step 7371 | loss 3.363466 | norm 0.3276 | time 465.1122 ms | tok/sec 1127229.0632
for step 7372 | loss 3.352389 | norm 0.3179 | time 465.2312 ms | tok/sec 1126940.8035
for step 7373 | loss 3.374991 | norm 0.3139 | time 465.7896 ms | tok/sec 1125589.8564
for step 7374 | loss 3.368329 | norm 0.3131 | time 465.3654 ms | tok/sec 1126615.7495
for step 7375 | loss 3.417858 | norm 0.2990 | time 465.3177 ms | tok/sec 1126731.2001
for step 7376 | loss 3.421280 | norm 0.3061 | time 465.1065 ms | tok/sec 1127242.9311
for step 7377 | loss 3.367874 | norm 0.3266 | time 465.3730 ms | tok/sec 1126597.2796
for step 7378 | loss 3.419792 | norm 0.3101 | time 465.0548 ms | tok/sec 1127368.3357
for step 7379 | loss 3.505076 | norm 0.3425 | time 464.6707 ms | tok/sec 1128300.2076
for step 7380 | loss 3.361482 | norm 0.3886 | time 465.2522 ms | tok/sec 1126889.9834
for step 7381 | loss 3.389958 | norm 0.3873 | time 465.2288 ms | tok/sec 1126946.5788
for step 7382 | loss 3.394114 | norm 0.3439 | time 464.8306 ms | tok/sec 1127911.8851
for step 7383 | loss 3.331089 | norm 0.3285 | time 465.3680 ms | tok/sec 1126609.4004
for step 7384 | loss 3.406693 | norm 0.3685 | time 465.7252 ms | tok/sec 1125745.4366
for step 7385 | loss 3.414299 | norm 0.3441 | time 465.6820 ms | tok/sec 1125849.7570
for step 7386 | loss 3.445692 | norm 0.3587 | time 464.6132 ms | tok/sec 1128439.7449
for step 7387 | loss 3.385560 | norm 0.3036 | time 464.6511 ms | tok/sec 1128347.6811
for step 7388 | loss 3.387665 | norm 0.3508 | time 465.0497 ms | tok/sec 1127380.4731
for step 7389 | loss 3.415897 | norm 0.3526 | time 465.0767 ms | tok/sec 1127315.1654
for step 7390 | loss 3.438146 | norm 0.3565 | time 465.6456 ms | tok/sec 1125937.9545
for step 7391 | loss 3.385941 | norm 0.3274 | time 464.9961 ms | tok/sec 1127510.5331
for step 7392 | loss 3.464635 | norm 0.3325 | time 465.4338 ms | tok/sec 1126450.1192
for step 7393 | loss 3.443750 | norm 0.3588 | time 465.6067 ms | tok/sec 1126031.9318
for step 7394 | loss 3.420194 | norm 0.3231 | time 464.8740 ms | tok/sec 1127806.6038
for step 7395 | loss 3.414148 | norm 0.3424 | time 465.5495 ms | tok/sec 1126170.3319
for step 7396 | loss 3.401319 | norm 0.3435 | time 464.8244 ms | tok/sec 1127926.9269
for step 7397 | loss 3.404468 | norm 0.3111 | time 465.6949 ms | tok/sec 1125818.6317
for step 7398 | loss 3.418267 | norm 0.3099 | time 465.6155 ms | tok/sec 1126010.5982
for step 7399 | loss 3.387659 | norm 0.3121 | time 465.4562 ms | tok/sec 1126395.8815
for step 7400 | loss 3.382084 | norm 0.2821 | time 465.4045 ms | tok/sec 1126521.0976
for step 7401 | loss 3.359084 | norm 0.3389 | time 465.4460 ms | tok/sec 1126420.6917
for step 7402 | loss 3.376451 | norm 0.3628 | time 465.6856 ms | tok/sec 1125841.1109
for step 7403 | loss 3.382081 | norm 0.2890 | time 465.4961 ms | tok/sec 1126299.5359
for step 7404 | loss 3.369887 | norm 0.3127 | time 465.3811 ms | tok/sec 1126577.6560
for step 7405 | loss 3.332589 | norm 0.3504 | time 465.5187 ms | tok/sec 1126244.7360
for step 7406 | loss 3.388923 | norm 0.3285 | time 466.1591 ms | tok/sec 1124697.5422
for step 7407 | loss 3.383023 | norm 0.3099 | time 464.5882 ms | tok/sec 1128500.5499
for step 7408 | loss 3.383374 | norm 0.3259 | time 464.9057 ms | tok/sec 1127729.6798
for step 7409 | loss 3.442427 | norm 0.3039 | time 466.1188 ms | tok/sec 1124794.7645
for step 7410 | loss 3.343128 | norm 0.3194 | time 464.9611 ms | tok/sec 1127595.5218
for step 7411 | loss 3.378374 | norm 0.3333 | time 465.0962 ms | tok/sec 1127267.7787
for step 7412 | loss 3.418184 | norm 0.3122 | time 465.5802 ms | tok/sec 1126095.9376
for step 7413 | loss 3.372460 | norm 0.3178 | time 465.8167 ms | tok/sec 1125524.1799
for step 7414 | loss 3.413110 | norm 0.3209 | time 464.7751 ms | tok/sec 1128046.6970
for step 7415 | loss 3.317442 | norm 0.3978 | time 465.1206 ms | tok/sec 1127208.8398
for step 7416 | loss 3.361744 | norm 0.3563 | time 466.0897 ms | tok/sec 1124864.9591
for step 7417 | loss 3.369125 | norm 0.3340 | time 465.2672 ms | tok/sec 1126853.6036
for step 7418 | loss 3.389157 | norm 0.3258 | time 465.0745 ms | tok/sec 1127320.3666
for step 7419 | loss 3.448195 | norm 0.4123 | time 464.8333 ms | tok/sec 1127905.5214
for step 7420 | loss 3.403059 | norm 0.3834 | time 465.6849 ms | tok/sec 1125842.8401
for step 7421 | loss 3.411596 | norm 0.4152 | time 465.3265 ms | tok/sec 1126709.8400
for step 7422 | loss 3.404236 | norm 0.3504 | time 465.5859 ms | tok/sec 1126082.0979
for step 7423 | loss 3.390187 | norm 0.3414 | time 465.5573 ms | tok/sec 1126151.2998
for step 7424 | loss 3.405942 | norm 0.3580 | time 464.5169 ms | tok/sec 1128673.7354
for step 7425 | loss 3.461761 | norm 0.3796 | time 465.7450 ms | tok/sec 1125697.6055
for step 7426 | loss 3.433027 | norm 0.3954 | time 465.4059 ms | tok/sec 1126517.6350
for step 7427 | loss 3.478151 | norm 0.4012 | time 464.6842 ms | tok/sec 1128267.2100
for step 7428 | loss 3.468673 | norm 0.3587 | time 466.2704 ms | tok/sec 1124428.9738
Will loading at 0 from edu_fineweb10B/edufineweb_train_000040.npy
for step 7429 | loss 3.565497 | norm 0.3741 | time 1435.8432 ms | tok/sec 365142.9274
for step 7430 | loss 3.410167 | norm 0.4107 | time 464.0644 ms | tok/sec 1129774.3278
for step 7431 | loss 3.409909 | norm 0.3482 | time 465.3020 ms | tok/sec 1126769.3040
for step 7432 | loss 3.514034 | norm 0.3314 | time 463.6438 ms | tok/sec 1130799.1439
for step 7433 | loss 3.376657 | norm 0.3134 | time 465.2383 ms | tok/sec 1126923.4779
for step 7434 | loss 3.371093 | norm 0.3522 | time 464.9675 ms | tok/sec 1127579.9107
for step 7435 | loss 3.432618 | norm 0.3275 | time 464.8020 ms | tok/sec 1127981.3121
for step 7436 | loss 3.400991 | norm 0.3219 | time 465.0877 ms | tok/sec 1127288.5821
for step 7437 | loss 3.410689 | norm 0.3269 | time 465.3513 ms | tok/sec 1126649.8050
for step 7438 | loss 3.407347 | norm 0.3166 | time 465.2748 ms | tok/sec 1126835.1259
for step 7439 | loss 3.395679 | norm 0.3083 | time 464.5495 ms | tok/sec 1128594.3763
for step 7440 | loss 3.389783 | norm 0.3174 | time 465.2760 ms | tok/sec 1126832.2388
for step 7441 | loss 3.339360 | norm 0.2912 | time 465.7657 ms | tok/sec 1125647.4736
for step 7442 | loss 3.411097 | norm 0.2828 | time 465.3873 ms | tok/sec 1126562.6501
for step 7443 | loss 3.419817 | norm 0.3027 | time 465.7285 ms | tok/sec 1125737.3684
for step 7444 | loss 3.404351 | norm 0.3072 | time 466.3880 ms | tok/sec 1124145.5922
for step 7445 | loss 3.403376 | norm 0.2968 | time 466.4609 ms | tok/sec 1123969.7720
for step 7446 | loss 3.520772 | norm 0.3302 | time 466.8462 ms | tok/sec 1123042.1688
for step 7447 | loss 3.388337 | norm 0.3297 | time 466.0795 ms | tok/sec 1124889.7019
for step 7448 | loss 3.419319 | norm 0.3488 | time 466.3708 ms | tok/sec 1124186.9697
for step 7449 | loss 3.446608 | norm 0.3552 | time 465.9088 ms | tok/sec 1125301.8584
for step 7450 | loss 3.378923 | norm 0.3821 | time 466.0950 ms | tok/sec 1124852.3004
for step 7451 | loss 3.399359 | norm 0.3169 | time 465.9467 ms | tok/sec 1125210.3060
for step 7452 | loss 3.356917 | norm 0.3471 | time 466.1813 ms | tok/sec 1124644.0483
for step 7453 | loss 3.414473 | norm 0.3563 | time 465.9884 ms | tok/sec 1125109.5579
for step 7454 | loss 3.479403 | norm 0.3304 | time 465.6050 ms | tok/sec 1126035.9680
for step 7455 | loss 3.397480 | norm 0.3534 | time 466.1491 ms | tok/sec 1124721.7024
for step 7456 | loss 3.384892 | norm 0.3119 | time 466.3930 ms | tok/sec 1124133.5244
for step 7457 | loss 3.386309 | norm 0.3551 | time 465.6618 ms | tok/sec 1125898.7539
for step 7458 | loss 3.416013 | norm 0.3501 | time 466.1682 ms | tok/sec 1124675.6839
for step 7459 | loss 3.453169 | norm 0.3886 | time 465.4129 ms | tok/sec 1126500.8996
for step 7460 | loss 3.411674 | norm 0.3348 | time 465.4293 ms | tok/sec 1126461.0827
for step 7461 | loss 3.373735 | norm 0.3791 | time 465.9977 ms | tok/sec 1125087.1079
for step 7462 | loss 3.407141 | norm 0.3550 | time 465.6742 ms | tok/sec 1125868.7788
for step 7463 | loss 3.449913 | norm 0.3326 | time 465.8380 ms | tok/sec 1125472.9114
for step 7464 | loss 3.390773 | norm 0.3492 | time 464.9301 ms | tok/sec 1127670.6926
for step 7465 | loss 3.393079 | norm 0.3539 | time 466.1813 ms | tok/sec 1124644.0483
for step 7466 | loss 3.381882 | norm 0.3358 | time 465.3578 ms | tok/sec 1126634.2200
for step 7467 | loss 3.416196 | norm 0.3412 | time 465.7390 ms | tok/sec 1125712.0120
for step 7468 | loss 3.448431 | norm 0.3427 | time 465.8413 ms | tok/sec 1125464.8472
for step 7469 | loss 3.356278 | norm 0.3364 | time 466.0790 ms | tok/sec 1124890.8528
for step 7470 | loss 3.353734 | norm 0.3330 | time 465.9128 ms | tok/sec 1125292.0691
for step 7471 | loss 3.389548 | norm 0.3150 | time 465.2798 ms | tok/sec 1126823.0002
for step 7472 | loss 3.378781 | norm 0.3912 | time 466.1613 ms | tok/sec 1124692.3651
for step 7473 | loss 3.362302 | norm 0.3571 | time 464.7810 ms | tok/sec 1128032.2307
for step 7474 | loss 3.371794 | norm 0.3391 | time 466.4855 ms | tok/sec 1123910.6031
for step 7475 | loss 3.383769 | norm 0.3851 | time 465.9545 ms | tok/sec 1125191.3064
for step 7476 | loss 3.386084 | norm 0.3334 | time 465.1127 ms | tok/sec 1127227.9076
for step 7477 | loss 3.442432 | norm 0.4150 | time 465.8809 ms | tok/sec 1125369.2367
for step 7478 | loss 3.370002 | norm 0.3550 | time 465.0393 ms | tok/sec 1127405.9047
for step 7479 | loss 3.369773 | norm 0.3777 | time 465.6377 ms | tok/sec 1125956.9793
for step 7480 | loss 3.401935 | norm 0.3371 | time 465.2262 ms | tok/sec 1126952.9317
for step 7481 | loss 3.438033 | norm 0.3520 | time 465.6546 ms | tok/sec 1125916.0480
for step 7482 | loss 3.400161 | norm 0.3267 | time 465.7137 ms | tok/sec 1125773.0998
for step 7483 | loss 3.404076 | norm 0.3228 | time 465.8890 ms | tok/sec 1125349.6559
for step 7484 | loss 3.396679 | norm 0.3644 | time 465.2696 ms | tok/sec 1126847.8293
for step 7485 | loss 3.433056 | norm 0.3863 | time 466.2292 ms | tok/sec 1124528.4499
for step 7486 | loss 3.427097 | norm 0.3240 | time 465.7261 ms | tok/sec 1125743.1314
for step 7487 | loss 3.466534 | norm 0.3619 | time 465.6246 ms | tok/sec 1125988.6888
for step 7488 | loss 3.346355 | norm 0.3086 | time 466.3653 ms | tok/sec 1124200.1881
for step 7489 | loss 3.429008 | norm 0.3248 | time 465.8456 ms | tok/sec 1125454.4790
for step 7490 | loss 3.381474 | norm 0.3165 | time 465.3935 ms | tok/sec 1126547.6447
for step 7491 | loss 3.409746 | norm 0.3236 | time 465.6847 ms | tok/sec 1125843.4165
for step 7492 | loss 3.435418 | norm 0.3264 | time 465.1706 ms | tok/sec 1127087.5144
for step 7493 | loss 3.426979 | norm 0.3522 | time 466.1348 ms | tok/sec 1124756.2187
for step 7494 | loss 3.409512 | norm 0.3185 | time 466.4478 ms | tok/sec 1124001.3696
for step 7495 | loss 3.358479 | norm 0.3114 | time 465.8475 ms | tok/sec 1125449.8710
for step 7496 | loss 3.416911 | norm 0.2957 | time 465.0762 ms | tok/sec 1127316.3212
for step 7497 | loss 3.471394 | norm 0.3017 | time 466.3324 ms | tok/sec 1124279.5053
for step 7498 | loss 3.385411 | norm 0.2984 | time 465.3895 ms | tok/sec 1126557.4559
for step 7499 | loss 3.370301 | norm 0.3140 | time 465.6496 ms | tok/sec 1125928.1541
validation loss 3.4059
HellaSwag accuracy: 2720/10042=0.2709
> Hello, I'm a language model, and a method of creating your own in a browser-based project.
1. Let's talk about your project.
> Hello, I'm a language model, which means that our language is very easy to understand. What's not to like is something we often do not understand.
> Hello, I'm a language model, but no, I'm a language model.
When it comes to learning languages, I'll just have a conversation.
> Hello, I'm a language model, and I'm interested to add some fun while learning Spanish the first time. To learn basic principles about the language, I
> Hello, I'm a language model, so I need to see my name, if you don't like it in the field in some way, and it is
> Hello, I'm a language model, how does it start up? In particular, let me clarify what I do, in case I'm wondering how well you
> > Hello, I'm a language model, I'm confused right. That's my job, I'm an IT professional, I'm a business graduate, etc...
Hello, I'm a language model, and I understand that the world we live in - I'll try to tell my friends, a million, of you -
> Hello, I'm a language model, and you and guys like to learn them from books
Why can’t I learn something by doing something? How> 
Hello, I'm a language model, so to speak, and I've noticed that it starts being a big place to learn and, I mean, it's
> Hello, I'm a language model, so I got my way for getting it right :)
I got it for a second, but I'm not a second
> Hello, I'm a language model, so they're trying to help you build the syntax of your language.
So: Let's take a look: "
> Hello, I'm a language model, and I'm a math, and I take math courses at the University of Delaware' Institute of Physical Science and Technology (
> Hello, I'm a language model, and so on my way to a big game, I am just a beginner, so I'm just a kid. It> 
> Hello, I'm a language model, even if I'm a beginner (language of the tongue: English, Spanish, Russian). It's basically an abstract of
> > Hello, I'm a language model, but if it's one of the more prominent ones, it's more difficult to translate.
What's a language?
> Hello, I'm a language model, my teacher is a big computer. You're a teacher and a professor, who is a leader in the language field.
Hello, I'm a language model, so I should try to explain it exactly from the other direction of the word.
- (i) To get (
> Hello, I'm a language model, and I am having a lot of my understanding of basic semantics, including many of the basic concepts of how languages have to
> > Hello, I'm a language model, there are more than 400 examples that you can apply to, which is why you will need a full description so you can
> Hello, I'm a language model, and I have a nice, well-built, fun, and simple vocabulary.
The language model I like to use
Hello, I'm a language model, so I've seen that it has nothing to do with the concept of human communication, but I also don't care to
> Hello, I'm a language model, so I'm going to explain what different aspects of the language the speaker and the person is talking about.
1)
> > Hello, I'm a language model, so I would be great to start studying.
- I'm studying and writing in C#, and I'm looking
>>Hello, I'm a language model, but for now I'll have the most accurate results.
The model I will use is the one I am most comfortable
Hello, I'm a language model, but that only works when I'm not doing coding as I understand it. Now, if not this is not a game
 Hello, I'm a language model, and I'm trying to find the proper place to go with that, there's nothing new or crazy around me, you Hello, I'm a language model, in the last 12 months. I'm trying to find an answer for the question... "what it means to me as> 

Hello, I'm a language model, and i'm curious if i wanted to share my knowledge.
[i don't have someone to comment about who/
> Hello, I'm a language model, and this will be a two to three-page manual, so you can also use the Java-based program you've
> Hello, I'm a language model, and it works! However, to me, not a language model, but the concept of what 'language' stands for
> Hello, I'm a language model, and after using Google, I will try to find some language that will work for me. I'll only be able to
for step 7500 | loss 3.452507 | norm 0.3093 | time 12811.6188 ms | tok/sec 40922.8535
for step 7501 | loss 3.445327 | norm 0.3011 | time 462.9641 ms | tok/sec 1132459.4016
for step 7502 | loss 3.376735 | norm 0.3677 | time 462.9960 ms | tok/sec 1132381.2586
for step 7503 | loss 3.362170 | norm 0.4110 | time 464.1976 ms | tok/sec 1129449.9578
for step 7504 | loss 3.316342 | norm 0.3637 | time 462.3270 ms | tok/sec 1134019.8519
for step 7505 | loss 3.392624 | norm 0.3655 | time 462.2998 ms | tok/sec 1134086.5236
for step 7506 | loss 3.357621 | norm 0.3602 | time 463.3582 ms | tok/sec 1131496.1965
for step 7507 | loss 3.454036 | norm 0.3538 | time 463.0284 ms | tok/sec 1132301.9602
for step 7508 | loss 3.409555 | norm 0.3156 | time 463.6464 ms | tok/sec 1130792.7476
for step 7509 | loss 3.437271 | norm 0.3614 | time 464.2000 ms | tok/sec 1129444.1568
for step 7510 | loss 3.409151 | norm 0.3032 | time 463.7990 ms | tok/sec 1130420.7216
for step 7511 | loss 3.289317 | norm 0.3368 | time 464.6704 ms | tok/sec 1128300.7865
for step 7512 | loss 3.386330 | norm 0.3698 | time 464.2622 ms | tok/sec 1129292.7721
for step 7513 | loss 3.418352 | norm 0.3064 | time 463.5458 ms | tok/sec 1131038.1862
for step 7514 | loss 3.449321 | norm 0.3353 | time 464.5402 ms | tok/sec 1128616.9664
for step 7515 | loss 3.448654 | norm 0.3596 | time 464.3731 ms | tok/sec 1129023.1643
for step 7516 | loss 3.491445 | norm 0.3676 | time 464.7589 ms | tok/sec 1128086.0474
for step 7517 | loss 3.430642 | norm 0.3857 | time 464.6156 ms | tok/sec 1128433.9542
for step 7518 | loss 3.420962 | norm 0.3472 | time 464.6392 ms | tok/sec 1128376.6304
for step 7519 | loss 3.358714 | norm 0.3562 | time 464.1566 ms | tok/sec 1129549.7441
for step 7520 | loss 3.404081 | norm 0.3280 | time 464.9282 ms | tok/sec 1127675.3189
for step 7521 | loss 3.431853 | norm 0.3011 | time 465.1260 ms | tok/sec 1127195.5505
for step 7522 | loss 3.404872 | norm 0.2955 | time 465.2474 ms | tok/sec 1126901.5330
for step 7523 | loss 3.400381 | norm 0.2961 | time 465.1275 ms | tok/sec 1127192.0838
for step 7524 | loss 3.398211 | norm 0.3173 | time 465.6267 ms | tok/sec 1125983.4999
for step 7525 | loss 3.473559 | norm 0.3006 | time 464.9205 ms | tok/sec 1127693.8241
for step 7526 | loss 3.435793 | norm 0.3052 | time 465.3788 ms | tok/sec 1126583.4275
for step 7527 | loss 3.409556 | norm 0.3342 | time 465.0736 ms | tok/sec 1127322.6782
for step 7528 | loss 3.412542 | norm 0.3051 | time 465.3802 ms | tok/sec 1126579.9646
for step 7529 | loss 3.441540 | norm 0.3309 | time 464.8519 ms | tok/sec 1127860.3989
for step 7530 | loss 3.408763 | norm 0.3795 | time 464.2816 ms | tok/sec 1129245.7989
for step 7531 | loss 3.398580 | norm 0.3311 | time 465.6384 ms | tok/sec 1125955.2498
for step 7532 | loss 3.439092 | norm 0.3518 | time 466.2688 ms | tok/sec 1124432.9985
for step 7533 | loss 3.382482 | norm 0.3452 | time 465.4822 ms | tok/sec 1126332.9954
for step 7534 | loss 3.393216 | norm 0.3719 | time 465.3728 ms | tok/sec 1126597.8567
for step 7535 | loss 3.475444 | norm 0.3437 | time 465.6925 ms | tok/sec 1125824.3955
for step 7536 | loss 3.454857 | norm 0.3477 | time 464.7775 ms | tok/sec 1128040.9104
for step 7537 | loss 3.387146 | norm 0.3683 | time 465.5807 ms | tok/sec 1126094.7843
for step 7538 | loss 3.382840 | norm 0.3129 | time 466.7487 ms | tok/sec 1123276.7949
for step 7539 | loss 3.383160 | norm 0.3488 | time 465.1556 ms | tok/sec 1127123.9093
for step 7540 | loss 3.410293 | norm 0.3240 | time 465.5209 ms | tok/sec 1126239.5446
for step 7541 | loss 3.364143 | norm 0.3686 | time 466.3002 ms | tok/sec 1124357.1090
for step 7542 | loss 3.390646 | norm 0.3363 | time 465.7276 ms | tok/sec 1125739.6736
for step 7543 | loss 3.342162 | norm 0.3055 | time 466.1839 ms | tok/sec 1124637.7214
for step 7544 | loss 3.415427 | norm 0.2974 | time 465.3654 ms | tok/sec 1126615.7495
for step 7545 | loss 3.389226 | norm 0.3189 | time 465.6317 ms | tok/sec 1125971.3925
for step 7546 | loss 3.370348 | norm 0.3895 | time 465.0447 ms | tok/sec 1127392.6107
for step 7547 | loss 3.361100 | norm 0.4250 | time 465.5473 ms | tok/sec 1126175.5225
for step 7548 | loss 3.356404 | norm 0.3742 | time 465.9114 ms | tok/sec 1125295.5241
for step 7549 | loss 3.402328 | norm 0.3455 | time 465.6384 ms | tok/sec 1125955.2498
for step 7550 | loss 3.423786 | norm 0.3378 | time 464.5615 ms | tok/sec 1128565.4158
for step 7551 | loss 3.473328 | norm 0.3946 | time 465.4198 ms | tok/sec 1126484.1646
for step 7552 | loss 3.461473 | norm 0.3210 | time 464.3590 ms | tok/sec 1129057.3654
for step 7553 | loss 3.365626 | norm 0.3072 | time 465.3947 ms | tok/sec 1126544.7591
for step 7554 | loss 3.392670 | norm 0.3320 | time 465.0240 ms | tok/sec 1127442.8981
for step 7555 | loss 3.424426 | norm 0.3021 | time 465.6141 ms | tok/sec 1126014.0576
for step 7556 | loss 3.427451 | norm 0.3541 | time 465.6215 ms | tok/sec 1125996.1840
for step 7557 | loss 3.429784 | norm 0.3208 | time 465.2903 ms | tok/sec 1126797.5949
for step 7558 | loss 3.445795 | norm 0.3497 | time 464.9060 ms | tok/sec 1127729.1015
for step 7559 | loss 3.417750 | norm 0.3233 | time 465.7094 ms | tok/sec 1125783.4738
for step 7560 | loss 3.480765 | norm 0.3447 | time 465.6775 ms | tok/sec 1125860.7089
for step 7561 | loss 3.440260 | norm 0.3820 | time 465.9081 ms | tok/sec 1125303.5860
for step 7562 | loss 3.379294 | norm 0.3321 | time 465.1003 ms | tok/sec 1127257.9551
for step 7563 | loss 3.502884 | norm 0.3331 | time 465.0736 ms | tok/sec 1127322.6782
for step 7564 | loss 3.416526 | norm 0.3320 | time 465.6973 ms | tok/sec 1125812.8680
for step 7565 | loss 3.399127 | norm 0.3813 | time 465.4725 ms | tok/sec 1126356.6490
for step 7566 | loss 3.416408 | norm 0.3317 | time 464.6735 ms | tok/sec 1128293.2606
for step 7567 | loss 3.377498 | norm 0.3135 | time 465.7626 ms | tok/sec 1125654.9643
for step 7568 | loss 3.490127 | norm 0.3930 | time 466.2170 ms | tok/sec 1124557.7786
for step 7569 | loss 3.424028 | norm 0.3924 | time 465.0862 ms | tok/sec 1127292.0494
for step 7570 | loss 3.442688 | norm 0.3499 | time 465.3094 ms | tok/sec 1126751.4064
for step 7571 | loss 3.433448 | norm 0.4491 | time 465.0495 ms | tok/sec 1127381.0510
for step 7572 | loss 3.379762 | norm 0.3598 | time 465.3795 ms | tok/sec 1126581.6961
for step 7573 | loss 3.373790 | norm 0.3734 | time 464.2732 ms | tok/sec 1129266.0955
for step 7574 | loss 3.447202 | norm 0.3565 | time 465.8239 ms | tok/sec 1125506.8978
for step 7575 | loss 3.386068 | norm 0.3067 | time 465.8194 ms | tok/sec 1125517.8431
for step 7576 | loss 3.342788 | norm 0.3415 | time 465.1620 ms | tok/sec 1127108.3112
for step 7577 | loss 3.459538 | norm 0.3669 | time 465.3933 ms | tok/sec 1126548.2218
for step 7578 | loss 3.349803 | norm 0.3774 | time 465.6303 ms | tok/sec 1125974.8517
for step 7579 | loss 3.340689 | norm 0.3247 | time 465.6379 ms | tok/sec 1125956.4028
for step 7580 | loss 3.364953 | norm 0.3228 | time 465.3683 ms | tok/sec 1126608.8232
for step 7581 | loss 3.444116 | norm 0.2984 | time 465.9553 ms | tok/sec 1125189.5792
for step 7582 | loss 3.396219 | norm 0.3109 | time 465.0896 ms | tok/sec 1127283.9590
for step 7583 | loss 3.425537 | norm 0.3085 | time 465.2672 ms | tok/sec 1126853.6036
for step 7584 | loss 3.339288 | norm 0.3399 | time 464.8244 ms | tok/sec 1127926.9269
for step 7585 | loss 3.441327 | norm 0.3379 | time 465.9040 ms | tok/sec 1125313.3755
for step 7586 | loss 3.434846 | norm 0.3260 | time 465.9102 ms | tok/sec 1125298.4033
for step 7587 | loss 3.378073 | norm 0.3837 | time 465.6353 ms | tok/sec 1125962.7446
for step 7588 | loss 3.420569 | norm 0.3769 | time 464.9472 ms | tok/sec 1127629.0583
for step 7589 | loss 3.387616 | norm 0.3431 | time 465.8306 ms | tok/sec 1125490.7684
for step 7590 | loss 3.379651 | norm 0.3475 | time 464.9391 ms | tok/sec 1127648.7186
for step 7591 | loss 3.427784 | norm 0.4504 | time 465.1668 ms | tok/sec 1127096.7574
for step 7592 | loss 3.381759 | norm 0.3480 | time 465.5406 ms | tok/sec 1126191.6716
for step 7593 | loss 3.417236 | norm 0.3674 | time 465.5151 ms | tok/sec 1126253.3882
for step 7594 | loss 3.379766 | norm 0.3653 | time 465.1864 ms | tok/sec 1127049.3890
for step 7595 | loss 3.431468 | norm 0.3498 | time 465.9257 ms | tok/sec 1125260.9747
for step 7596 | loss 3.404336 | norm 0.3586 | time 465.5972 ms | tok/sec 1126054.9961
for step 7597 | loss 3.510167 | norm 0.3829 | time 466.0070 ms | tok/sec 1125064.6588
for step 7598 | loss 3.485493 | norm 0.3728 | time 465.7810 ms | tok/sec 1125610.5979
for step 7599 | loss 3.386339 | norm 0.3420 | time 464.6316 ms | tok/sec 1128395.1587
for step 7600 | loss 3.344993 | norm 0.3555 | time 465.8837 ms | tok/sec 1125362.3258
for step 7601 | loss 3.435889 | norm 0.3296 | time 464.7555 ms | tok/sec 1128094.1492
for step 7602 | loss 3.407204 | norm 0.3201 | time 466.3987 ms | tok/sec 1124119.7328
for step 7603 | loss 3.392635 | norm 0.3335 | time 465.9944 ms | tok/sec 1125095.1668
for step 7604 | loss 3.475161 | norm 0.2917 | time 465.3683 ms | tok/sec 1126608.8232
for step 7605 | loss 3.439727 | norm 0.3163 | time 465.6391 ms | tok/sec 1125953.5202
for step 7606 | loss 3.383870 | norm 0.2986 | time 465.1241 ms | tok/sec 1127200.1728
for step 7607 | loss 3.397127 | norm 0.3261 | time 465.5108 ms | tok/sec 1126263.7711
for step 7608 | loss 3.416701 | norm 0.2886 | time 465.8589 ms | tok/sec 1125422.2236
for step 7609 | loss 3.389573 | norm 0.3240 | time 465.5380 ms | tok/sec 1126198.0160
for step 7610 | loss 3.321216 | norm 0.2990 | time 465.6007 ms | tok/sec 1126046.3469
for step 7611 | loss 3.364178 | norm 0.3431 | time 465.3451 ms | tok/sec 1126664.8131
for step 7612 | loss 3.419297 | norm 0.3716 | time 465.2758 ms | tok/sec 1126832.8162
for step 7613 | loss 3.331315 | norm 0.2821 | time 465.5385 ms | tok/sec 1126196.8624
for step 7614 | loss 3.376463 | norm 0.3764 | time 465.8051 ms | tok/sec 1125552.4083
for step 7615 | loss 3.415329 | norm 0.3617 | time 465.1129 ms | tok/sec 1127227.3297
for step 7616 | loss 3.387700 | norm 0.3162 | time 465.0006 ms | tok/sec 1127499.5491
for step 7617 | loss 3.364346 | norm 0.3093 | time 464.6976 ms | tok/sec 1128234.7933
for step 7618 | loss 3.373405 | norm 0.2986 | time 465.2584 ms | tok/sec 1126874.9692
Will loading at 0 from edu_fineweb10B/edufineweb_train_000041.npy
for step 7619 | loss 3.441026 | norm 0.3053 | time 1442.3978 ms | tok/sec 363483.6299
for step 7620 | loss 3.422241 | norm 0.3124 | time 463.1209 ms | tok/sec 1132075.7878
for step 7621 | loss 3.420594 | norm 0.2986 | time 464.2398 ms | tok/sec 1129347.2891
for step 7622 | loss 3.412929 | norm 0.2984 | time 464.7446 ms | tok/sec 1128120.7705
for step 7623 | loss 3.473124 | norm 0.3276 | time 464.9112 ms | tok/sec 1127716.3783
for step 7624 | loss 3.485488 | norm 0.3307 | time 463.8503 ms | tok/sec 1130295.7990
for step 7625 | loss 3.445539 | norm 0.3227 | time 463.8708 ms | tok/sec 1130245.8377
for step 7626 | loss 3.307829 | norm 0.3354 | time 465.2569 ms | tok/sec 1126878.4340
for step 7627 | loss 3.422852 | norm 0.3007 | time 464.3788 ms | tok/sec 1129009.2525
for step 7628 | loss 3.413330 | norm 0.3001 | time 463.9854 ms | tok/sec 1129966.4845
for step 7629 | loss 3.411574 | norm 0.3116 | time 465.7831 ms | tok/sec 1125605.4124
for step 7630 | loss 3.399413 | norm 0.3552 | time 464.9544 ms | tok/sec 1127611.7116
for step 7631 | loss 3.419325 | norm 0.3400 | time 465.3885 ms | tok/sec 1126559.7644
for step 7632 | loss 3.414405 | norm 0.3528 | time 464.8018 ms | tok/sec 1127981.8907
for step 7633 | loss 3.413300 | norm 0.3309 | time 465.8623 ms | tok/sec 1125414.1601
for step 7634 | loss 3.486875 | norm 0.3394 | time 465.4591 ms | tok/sec 1126388.9579
for step 7635 | loss 3.431405 | norm 0.3237 | time 465.4710 ms | tok/sec 1126360.1106
for step 7636 | loss 3.394122 | norm 0.3289 | time 465.6851 ms | tok/sec 1125842.2637
for step 7637 | loss 3.479651 | norm 0.2817 | time 464.7284 ms | tok/sec 1128160.1260
for step 7638 | loss 3.391476 | norm 0.3103 | time 466.2647 ms | tok/sec 1124442.7729
for step 7639 | loss 3.410468 | norm 0.2888 | time 465.5571 ms | tok/sec 1126151.8766
for step 7640 | loss 3.462500 | norm 0.2997 | time 465.4605 ms | tok/sec 1126385.4961
for step 7641 | loss 3.423223 | norm 0.3059 | time 466.6593 ms | tok/sec 1123492.0028
for step 7642 | loss 3.371171 | norm 0.2846 | time 465.5457 ms | tok/sec 1126179.5598
for step 7643 | loss 3.364860 | norm 0.2935 | time 465.6434 ms | tok/sec 1125943.1431
for step 7644 | loss 3.347664 | norm 0.2981 | time 465.3714 ms | tok/sec 1126601.3198
for step 7645 | loss 3.360491 | norm 0.3034 | time 465.7798 ms | tok/sec 1125613.4787
for step 7646 | loss 3.417308 | norm 0.3089 | time 466.7244 ms | tok/sec 1123335.3233
for step 7647 | loss 3.382798 | norm 0.3008 | time 465.4858 ms | tok/sec 1126324.3419
for step 7648 | loss 3.386510 | norm 0.3556 | time 464.9713 ms | tok/sec 1127570.6599
for step 7649 | loss 3.437659 | norm 0.2962 | time 465.1992 ms | tok/sec 1127018.1974
for step 7650 | loss 3.362968 | norm 0.3261 | time 465.4186 ms | tok/sec 1126487.0499
for step 7651 | loss 3.473570 | norm 0.3513 | time 465.2171 ms | tok/sec 1126974.8786
for step 7652 | loss 3.446715 | norm 0.3957 | time 466.0313 ms | tok/sec 1125005.9501
for step 7653 | loss 3.458643 | norm 0.3792 | time 464.6223 ms | tok/sec 1128417.7408
for step 7654 | loss 3.456605 | norm 0.3946 | time 464.8471 ms | tok/sec 1127871.9684
for step 7655 | loss 3.444276 | norm 0.3966 | time 464.9835 ms | tok/sec 1127541.1738
for step 7656 | loss 3.388780 | norm 0.3941 | time 464.4895 ms | tok/sec 1128740.3594
for step 7657 | loss 3.405437 | norm 0.3696 | time 465.3213 ms | tok/sec 1126722.5405
for step 7658 | loss 3.488803 | norm 0.3981 | time 466.0175 ms | tok/sec 1125039.3328
for step 7659 | loss 3.470402 | norm 0.3941 | time 466.2220 ms | tok/sec 1124545.7019
for step 7660 | loss 3.378885 | norm 0.3304 | time 465.8954 ms | tok/sec 1125334.1069
for step 7661 | loss 3.398942 | norm 0.3794 | time 464.7293 ms | tok/sec 1128157.8109
for step 7662 | loss 3.397786 | norm 0.3385 | time 465.2894 ms | tok/sec 1126799.9045
for step 7663 | loss 3.378981 | norm 0.3305 | time 465.2131 ms | tok/sec 1126984.6972
for step 7664 | loss 3.404249 | norm 0.3295 | time 465.1394 ms | tok/sec 1127163.1953
for step 7665 | loss 3.445321 | norm 0.3542 | time 465.4937 ms | tok/sec 1126305.3047
for step 7666 | loss 3.453685 | norm 0.3073 | time 465.6086 ms | tok/sec 1126027.3191
for step 7667 | loss 3.451169 | norm 0.3326 | time 465.6472 ms | tok/sec 1125933.9191
for step 7668 | loss 3.408267 | norm 0.2975 | time 465.1771 ms | tok/sec 1127071.9174
for step 7669 | loss 3.411562 | norm 0.3268 | time 464.4423 ms | tok/sec 1128855.0869
for step 7670 | loss 3.369744 | norm 0.2950 | time 466.2745 ms | tok/sec 1124419.1997
for step 7671 | loss 3.455144 | norm 0.3185 | time 466.1388 ms | tok/sec 1124746.4389
for step 7672 | loss 3.423301 | norm 0.3496 | time 465.2510 ms | tok/sec 1126892.8707
for step 7673 | loss 3.478905 | norm 0.3396 | time 465.3203 ms | tok/sec 1126724.8497
for step 7674 | loss 3.414550 | norm 0.3353 | time 464.6070 ms | tok/sec 1128454.8007
for step 7675 | loss 3.372470 | norm 0.3105 | time 465.2405 ms | tok/sec 1126918.2804
for step 7676 | loss 3.395722 | norm 0.3235 | time 465.0950 ms | tok/sec 1127270.6680
for step 7677 | loss 3.363995 | norm 0.3375 | time 465.8589 ms | tok/sec 1125422.2236
for step 7678 | loss 3.412046 | norm 0.2756 | time 465.3986 ms | tok/sec 1126535.5252
for step 7679 | loss 3.374415 | norm 0.2821 | time 464.9787 ms | tok/sec 1127552.7368
for step 7680 | loss 3.419220 | norm 0.3125 | time 465.3964 ms | tok/sec 1126540.7193
for step 7681 | loss 3.377982 | norm 0.3013 | time 465.1875 ms | tok/sec 1127046.5008
for step 7682 | loss 3.313364 | norm 0.2844 | time 466.0773 ms | tok/sec 1124894.8808
for step 7683 | loss 3.364170 | norm 0.3550 | time 465.9102 ms | tok/sec 1125298.4033
for step 7684 | loss 3.403606 | norm 0.3602 | time 466.2991 ms | tok/sec 1124359.9834
for step 7685 | loss 3.404078 | norm 0.3928 | time 465.9307 ms | tok/sec 1125248.8828
for step 7686 | loss 3.398364 | norm 0.3363 | time 465.4751 ms | tok/sec 1126350.3028
for step 7687 | loss 3.352072 | norm 0.3275 | time 466.0575 ms | tok/sec 1124942.6436
for step 7688 | loss 3.420664 | norm 0.3347 | time 465.0280 ms | tok/sec 1127433.0715
for step 7689 | loss 3.396224 | norm 0.3364 | time 465.0998 ms | tok/sec 1127259.1108
for step 7690 | loss 3.389448 | norm 0.3161 | time 464.9560 ms | tok/sec 1127607.6641
for step 7691 | loss 3.425086 | norm 0.3799 | time 465.9469 ms | tok/sec 1125209.7302
for step 7692 | loss 3.386199 | norm 0.3511 | time 465.6448 ms | tok/sec 1125939.6840
for step 7693 | loss 3.363589 | norm 0.4238 | time 464.9801 ms | tok/sec 1127549.2679
for step 7694 | loss 3.445656 | norm 0.3620 | time 465.0791 ms | tok/sec 1127309.3863
for step 7695 | loss 3.507018 | norm 0.3731 | time 465.3366 ms | tok/sec 1126685.5943
for step 7696 | loss 3.513850 | norm 0.3991 | time 466.0132 ms | tok/sec 1125049.6933
for step 7697 | loss 3.435586 | norm 0.3695 | time 465.6487 ms | tok/sec 1125930.4601
for step 7698 | loss 3.465122 | norm 0.3520 | time 465.4350 ms | tok/sec 1126447.2340
for step 7699 | loss 3.457510 | norm 0.3775 | time 465.8902 ms | tok/sec 1125346.7764
for step 7700 | loss 3.379316 | norm 0.3180 | time 465.5662 ms | tok/sec 1126129.9617
for step 7701 | loss 3.411532 | norm 0.3598 | time 465.0872 ms | tok/sec 1127289.7379
for step 7702 | loss 3.407210 | norm 0.3536 | time 465.7214 ms | tok/sec 1125754.6575
for step 7703 | loss 3.422271 | norm 0.3492 | time 464.8857 ms | tok/sec 1127778.2621
for step 7704 | loss 3.352788 | norm 0.3362 | time 465.5633 ms | tok/sec 1126136.8821
for step 7705 | loss 3.520742 | norm 0.7374 | time 466.2325 ms | tok/sec 1124520.3992
for step 7706 | loss 3.421051 | norm 0.3550 | time 466.0304 ms | tok/sec 1125008.2523
for step 7707 | loss 3.437447 | norm 0.3630 | time 465.0164 ms | tok/sec 1127461.3958
for step 7708 | loss 3.383658 | norm 0.3150 | time 466.1736 ms | tok/sec 1124662.4542
for step 7709 | loss 3.456915 | norm 0.3173 | time 465.8241 ms | tok/sec 1125506.3218
for step 7710 | loss 3.406901 | norm 0.3555 | time 465.3640 ms | tok/sec 1126619.2127
for step 7711 | loss 3.344600 | norm 0.3202 | time 465.4083 ms | tok/sec 1126511.8641
for step 7712 | loss 3.393797 | norm 0.3454 | time 465.3816 ms | tok/sec 1126576.5017
for step 7713 | loss 3.387546 | norm 0.3541 | time 465.5659 ms | tok/sec 1126130.5384
for step 7714 | loss 3.380195 | norm 0.3689 | time 464.7572 ms | tok/sec 1128090.0983
for step 7715 | loss 3.505173 | norm 0.3332 | time 465.4860 ms | tok/sec 1126323.7650
for step 7716 | loss 3.373728 | norm 0.3356 | time 465.1437 ms | tok/sec 1127152.7958
for step 7717 | loss 3.348000 | norm 0.3701 | time 465.8232 ms | tok/sec 1125508.6260
for step 7718 | loss 3.386899 | norm 0.3300 | time 465.6043 ms | tok/sec 1126037.6978
for step 7719 | loss 3.395985 | norm 0.3503 | time 465.0397 ms | tok/sec 1127404.7487
for step 7720 | loss 3.353376 | norm 0.2968 | time 465.2696 ms | tok/sec 1126847.8293
for step 7721 | loss 3.495668 | norm 0.3336 | time 465.6920 ms | tok/sec 1125825.5483
for step 7722 | loss 3.367559 | norm 0.3295 | time 465.4555 ms | tok/sec 1126397.6124
for step 7723 | loss 3.364272 | norm 0.3090 | time 465.3656 ms | tok/sec 1126615.1723
for step 7724 | loss 3.353659 | norm 0.3489 | time 465.1747 ms | tok/sec 1127077.6940
for step 7725 | loss 3.368606 | norm 0.3390 | time 464.6745 ms | tok/sec 1128290.9449
for step 7726 | loss 3.374230 | norm 0.2908 | time 464.6621 ms | tok/sec 1128321.0491
for step 7727 | loss 3.387381 | norm 0.3118 | time 465.3215 ms | tok/sec 1126721.9632
for step 7728 | loss 3.349215 | norm 0.2913 | time 465.5039 ms | tok/sec 1126280.4995
for step 7729 | loss 3.352525 | norm 0.2904 | time 466.1655 ms | tok/sec 1124682.0112
for step 7730 | loss 3.390676 | norm 0.2897 | time 465.5550 ms | tok/sec 1126157.0671
for step 7731 | loss 3.446352 | norm 0.3059 | time 465.1079 ms | tok/sec 1127239.4641
for step 7732 | loss 3.364653 | norm 0.2953 | time 465.7321 ms | tok/sec 1125728.7240
for step 7733 | loss 3.379448 | norm 0.2886 | time 465.5504 ms | tok/sec 1126168.0249
for step 7734 | loss 3.458849 | norm 0.3086 | time 464.8938 ms | tok/sec 1127758.5974
for step 7735 | loss 3.436084 | norm 0.2939 | time 464.5593 ms | tok/sec 1128570.6286
for step 7736 | loss 3.412440 | norm 0.3254 | time 465.6301 ms | tok/sec 1125975.4283
for step 7737 | loss 3.486519 | norm 0.3160 | time 465.7898 ms | tok/sec 1125589.2802
for step 7738 | loss 3.434299 | norm 0.3384 | time 464.9477 ms | tok/sec 1127627.9019
for step 7739 | loss 3.435513 | norm 0.3484 | time 465.4031 ms | tok/sec 1126524.5602
for step 7740 | loss 3.452460 | norm 0.3546 | time 465.3037 ms | tok/sec 1126765.2626
for step 7741 | loss 3.519082 | norm 0.3327 | time 465.2765 ms | tok/sec 1126831.0840
for step 7742 | loss 3.434250 | norm 0.3436 | time 465.4343 ms | tok/sec 1126448.9651
for step 7743 | loss 3.337511 | norm 0.3426 | time 465.0977 ms | tok/sec 1127264.3115
for step 7744 | loss 3.350386 | norm 0.3387 | time 465.2157 ms | tok/sec 1126978.3440
for step 7745 | loss 3.401638 | norm 0.4081 | time 464.8819 ms | tok/sec 1127787.5164
for step 7746 | loss 3.397183 | norm 0.3803 | time 465.1325 ms | tok/sec 1127179.9504
for step 7747 | loss 3.485310 | norm 0.3743 | time 464.6995 ms | tok/sec 1128230.1624
for step 7748 | loss 3.379381 | norm 0.3911 | time 465.3995 ms | tok/sec 1126533.2168
for step 7749 | loss 3.459892 | norm 0.3627 | time 465.4562 ms | tok/sec 1126395.8815
validation loss 3.4117
HellaSwag accuracy: 2719/10042=0.2708
> Hello, I'm a language model, and I're not one of them is the thing I say "I'm a language, so this is my new language
> Hello, I'm a language model, a model, and I'm really looking for a more comfortable way to help the body with difficult exercises and exercises.

> Hello, I'm a language model, but do it yourself?
I'm not sure how you'd be doing it.
But the thing about languages to
> Hello, I'm a language model, and I'm very happy :)
Here let's move another code and try to find each missing code.
In the
> Hello, I'm a language model, and I'll be able to do all of the things I'm thinking about and I want to know what that means to
> Hello, I'm a language model, so when I look at the model again, you hear all of the changes and you hear what you should expect. You
> Hello, I'm a language model, so I want to have a better understanding that my job is to understand it.
Here's a picture. The first> 
Hello, I'm a language model, so I like it. However, this is another good example: the user is asked to find a subject and the code
> Hello, I'm a language model, have an idea. You can also play games with different activities. Play music. This fun game is really cool – and> 
Hello, I'm a language model, and when I say that I am writing an interface, I mean my job as a computer, but, I'm getting
> Hello, I'm a language model, I started getting new information for my classes in 2011. I have many learning experiences to work with, yet I do it
> Hello, I'm a language model, and the focus now is on language production. In today's situation I like the opportunity to focus only on the spoken words
> Hello, I'm a language model, this will be a really special. If I were interested in this kind of problem, I'd like to share it with
> Hello, I'm a language model, and I don't need a lot of words. And I don't think I should use words like 'n' or
> Hello, I'm a language model, but this isn't true, it's an excellent way to model complex concepts or to explain complex concepts. For one thing
> Hello, I'm a language model, and I don't like the grammar. That's why I'm using the method. It may be obvious in what kind
> Hello, I'm a language model, and I understand that you want to model people's brains and ways of communication. "If you would like a better model
> > Hello, I'm a language model, isn't the same as what my son would be looking at. We're going to make the final final score for him
> Hello, I'm a language model, but that is not how I wrote this, and if I think we have to learn how to make it a fun,
> Hello, I'm a language model, and my students don't seem to make any progress with their language. I don't know how to do, but that
> Hello, I'm a language model, and I am having trouble with reading about "jw" and "i" and "i" in English: the
Hello, I'm a language model, trying to translate words on paper to get a meaning. So I figured that if your students can hear that there is a
> > > Hello, I'm a language model, but I actually use the language model from there. Of course you would still be using the language model because it's an
>>Hello, I'm a language model, at the beginning of a chapter, using English to describe the sound. As I've written, for example, I should
Hello, I'm a language model, so I was going to be creating what I wanted to do. If I were a child, I would be able to
 Hello, I'm a language model, and I'm a big proponent for building software. But that just doesn't always happen and, instead of writing the sentence Hello, I'm a language model, so I am trying to make the students write their own language. But first, we should teach what English is and what> 

Hello, I'm a language model, too. So it's easy to write, I think I have a lot of questions about the writing of a sentence.
> > Hello, I'm a language model, and this one is a visual language, but for me, I am a language model.
What I’re
Hello, I'm a language model, but what to do is for you to explain things correctly, in a way that I'm totally committed. The idea is
> Hello, I'm a language model, and you are probably having a similar problem. Maybe you have just started out in your language course for the first 3 weeks
> Hello, I'm a language model, and some of your brain and nervous system. But what does that have to do with a language? Because it's not
for step 7750 | loss 3.378810 | norm 0.4172 | time 12843.5881 ms | tok/sec 40820.9914
for step 7751 | loss 3.353167 | norm 0.4005 | time 462.9591 ms | tok/sec 1132471.6489
for step 7752 | loss 3.454795 | norm 0.3497 | time 462.3365 ms | tok/sec 1133996.4601
for step 7753 | loss 3.431158 | norm 0.3530 | time 462.4381 ms | tok/sec 1133747.3980
for step 7754 | loss 3.385980 | norm 0.3693 | time 463.1102 ms | tok/sec 1132102.0145
for step 7755 | loss 3.346306 | norm 0.3524 | time 462.8100 ms | tok/sec 1132836.2724
for step 7756 | loss 3.423689 | norm 0.3312 | time 464.0462 ms | tok/sec 1129818.4426
for step 7757 | loss 3.401145 | norm 0.3328 | time 464.1526 ms | tok/sec 1129559.6077
for step 7758 | loss 3.417055 | norm 0.3276 | time 463.5100 ms | tok/sec 1131125.4531
for step 7759 | loss 3.385799 | norm 0.3284 | time 464.3734 ms | tok/sec 1129022.5846
for step 7760 | loss 3.447093 | norm 0.3145 | time 463.9833 ms | tok/sec 1129971.7102
for step 7761 | loss 3.392765 | norm 0.3361 | time 464.5691 ms | tok/sec 1128546.8820
for step 7762 | loss 3.422951 | norm 0.3284 | time 464.2920 ms | tok/sec 1129220.2842
for step 7763 | loss 3.391092 | norm 0.3343 | time 465.4880 ms | tok/sec 1126319.1499
for step 7764 | loss 3.379565 | norm 0.3391 | time 464.6404 ms | tok/sec 1128373.7354
for step 7765 | loss 3.392917 | norm 0.3670 | time 464.1006 ms | tok/sec 1129686.1086
for step 7766 | loss 3.419516 | norm 0.3551 | time 464.1857 ms | tok/sec 1129478.9636
for step 7767 | loss 3.438974 | norm 0.3374 | time 464.1571 ms | tok/sec 1129548.5837
for step 7768 | loss 3.425883 | norm 0.3281 | time 464.9193 ms | tok/sec 1127696.7156
for step 7769 | loss 3.443757 | norm 0.3642 | time 464.6385 ms | tok/sec 1128378.3674
for step 7770 | loss 3.348366 | norm 0.3021 | time 465.4613 ms | tok/sec 1126383.7653
for step 7771 | loss 3.387658 | norm 0.3228 | time 464.6480 ms | tok/sec 1128355.2078
for step 7772 | loss 3.443591 | norm 0.3126 | time 464.8082 ms | tok/sec 1127966.2689
for step 7773 | loss 3.349963 | norm 0.3385 | time 465.7042 ms | tok/sec 1125796.1535
for step 7774 | loss 3.427839 | norm 0.3267 | time 465.2402 ms | tok/sec 1126918.8579
for step 7775 | loss 3.408786 | norm 0.3337 | time 463.9590 ms | tok/sec 1130030.9383
for step 7776 | loss 3.417732 | norm 0.3336 | time 465.5113 ms | tok/sec 1126262.6175
for step 7777 | loss 3.415873 | norm 0.3231 | time 464.0770 ms | tok/sec 1129743.5656
for step 7778 | loss 3.344553 | norm 0.3007 | time 466.4421 ms | tok/sec 1124015.1582
for step 7779 | loss 3.397190 | norm 0.3258 | time 465.3127 ms | tok/sec 1126743.3238
for step 7780 | loss 3.437448 | norm 0.3217 | time 464.2897 ms | tok/sec 1129226.0829
for step 7781 | loss 3.379369 | norm 0.3262 | time 465.1289 ms | tok/sec 1127188.6171
for step 7782 | loss 3.351453 | norm 0.2936 | time 465.2503 ms | tok/sec 1126894.6032
for step 7783 | loss 3.374109 | norm 0.2830 | time 465.2696 ms | tok/sec 1126847.8293
for step 7784 | loss 3.370833 | norm 0.3115 | time 464.9720 ms | tok/sec 1127568.9253
for step 7785 | loss 3.392262 | norm 0.2894 | time 464.5064 ms | tok/sec 1128699.2254
for step 7786 | loss 3.391323 | norm 0.2975 | time 464.5803 ms | tok/sec 1128519.6614
for step 7787 | loss 3.412103 | norm 0.2910 | time 464.7117 ms | tok/sec 1128200.6419
for step 7788 | loss 3.325681 | norm 0.2908 | time 464.8373 ms | tok/sec 1127895.6867
for step 7789 | loss 3.450691 | norm 0.3123 | time 464.8852 ms | tok/sec 1127779.4189
for step 7790 | loss 3.430241 | norm 0.3022 | time 465.6703 ms | tok/sec 1125878.0018
for step 7791 | loss 3.364394 | norm 0.2970 | time 465.1704 ms | tok/sec 1127088.0921
for step 7792 | loss 3.378298 | norm 0.3307 | time 464.8411 ms | tok/sec 1127886.4307
for step 7793 | loss 3.355297 | norm 0.3291 | time 464.9565 ms | tok/sec 1127606.5077
for step 7794 | loss 3.382174 | norm 0.3329 | time 464.9534 ms | tok/sec 1127614.0245
for step 7795 | loss 3.389673 | norm 0.3234 | time 465.2064 ms | tok/sec 1127000.8695
for step 7796 | loss 3.422384 | norm 0.3626 | time 464.6280 ms | tok/sec 1128403.8440
for step 7797 | loss 3.377218 | norm 0.3642 | time 465.6048 ms | tok/sec 1126036.5446
for step 7798 | loss 3.443125 | norm 0.4162 | time 465.6966 ms | tok/sec 1125814.5971
for step 7799 | loss 3.409114 | norm 0.4808 | time 464.9444 ms | tok/sec 1127635.9972
for step 7800 | loss 3.378151 | norm 0.3999 | time 464.8242 ms | tok/sec 1127927.5054
for step 7801 | loss 3.420761 | norm 0.4016 | time 465.3125 ms | tok/sec 1126743.9011
for step 7802 | loss 3.391922 | norm 0.3490 | time 464.7534 ms | tok/sec 1128099.3576
for step 7803 | loss 3.426514 | norm 0.3678 | time 465.1012 ms | tok/sec 1127255.6437
for step 7804 | loss 3.374270 | norm 0.3709 | time 464.4723 ms | tok/sec 1128782.0758
for step 7805 | loss 3.406062 | norm 0.3160 | time 464.6966 ms | tok/sec 1128237.1087
for step 7806 | loss 3.390725 | norm 0.3712 | time 464.7059 ms | tok/sec 1128214.5337
for step 7807 | loss 3.423060 | norm 0.2942 | time 464.9951 ms | tok/sec 1127512.8455
for step 7808 | loss 3.460811 | norm 0.3333 | time 464.6976 ms | tok/sec 1128234.7933
for step 7809 | loss 3.350945 | norm 0.3192 | time 467.3548 ms | tok/sec 1121820.1431
Will loading at 0 from edu_fineweb10B/edufineweb_train_000042.npy
for step 7810 | loss 3.456918 | norm 0.3089 | time 1438.8263 ms | tok/sec 364385.8830
for step 7811 | loss 3.367513 | norm 0.3103 | time 464.6347 ms | tok/sec 1128387.6315
for step 7812 | loss 3.402222 | norm 0.3226 | time 464.9971 ms | tok/sec 1127508.2206
for step 7813 | loss 3.428453 | norm 0.3406 | time 465.1301 ms | tok/sec 1127185.7282
for step 7814 | loss 3.351073 | norm 0.3326 | time 463.7139 ms | tok/sec 1130628.2121
for step 7815 | loss 3.363915 | norm 0.3651 | time 463.9416 ms | tok/sec 1130073.3309
for step 7816 | loss 3.512466 | norm 0.3831 | time 464.2293 ms | tok/sec 1129372.8095
for step 7817 | loss 3.351076 | norm 0.4254 | time 464.8643 ms | tok/sec 1127830.3193
for step 7818 | loss 3.369334 | norm 0.3098 | time 465.8351 ms | tok/sec 1125479.8237
for step 7819 | loss 3.331754 | norm 0.3471 | time 465.1771 ms | tok/sec 1127071.9174
for step 7820 | loss 3.356812 | norm 0.3454 | time 464.5863 ms | tok/sec 1128505.1829
for step 7821 | loss 3.372261 | norm 0.3473 | time 464.2053 ms | tok/sec 1129431.3949
for step 7822 | loss 3.313826 | norm 0.3269 | time 465.2832 ms | tok/sec 1126814.9166
for step 7823 | loss 3.449003 | norm 0.3368 | time 464.9374 ms | tok/sec 1127652.7664
for step 7824 | loss 3.419811 | norm 0.3867 | time 465.6041 ms | tok/sec 1126038.2744
for step 7825 | loss 3.317664 | norm 0.3432 | time 465.1139 ms | tok/sec 1127225.0185
for step 7826 | loss 3.437044 | norm 0.3412 | time 465.4229 ms | tok/sec 1126476.6629
for step 7827 | loss 3.409977 | norm 0.3410 | time 465.1742 ms | tok/sec 1127078.8494
for step 7828 | loss 3.401067 | norm 0.3190 | time 465.5728 ms | tok/sec 1126113.8144
for step 7829 | loss 3.385403 | norm 0.3133 | time 465.0164 ms | tok/sec 1127461.3958
for step 7830 | loss 3.400338 | norm 0.3159 | time 464.7026 ms | tok/sec 1128222.6375
for step 7831 | loss 3.369034 | norm 0.3314 | time 465.4319 ms | tok/sec 1126454.7354
for step 7832 | loss 3.378322 | norm 0.3220 | time 464.8633 ms | tok/sec 1127832.6330
for step 7833 | loss 3.391494 | norm 0.3397 | time 465.0574 ms | tok/sec 1127361.9781
for step 7834 | loss 3.475098 | norm 0.3237 | time 465.2159 ms | tok/sec 1126977.7664
for step 7835 | loss 3.389405 | norm 0.3217 | time 465.5089 ms | tok/sec 1126268.3858
for step 7836 | loss 3.450939 | norm 0.3589 | time 465.2717 ms | tok/sec 1126842.6324
for step 7837 | loss 3.480067 | norm 0.3823 | time 464.5600 ms | tok/sec 1128568.8910
for step 7838 | loss 3.425261 | norm 0.3506 | time 466.1715 ms | tok/sec 1124667.6310
for step 7839 | loss 3.402612 | norm 0.3397 | time 465.4419 ms | tok/sec 1126430.5007
for step 7840 | loss 3.512313 | norm 0.3186 | time 465.0309 ms | tok/sec 1127426.1352
for step 7841 | loss 3.390171 | norm 0.3376 | time 465.6465 ms | tok/sec 1125935.6485
for step 7842 | loss 3.417109 | norm 0.3162 | time 464.9014 ms | tok/sec 1127740.0900
for step 7843 | loss 3.440181 | norm 0.3447 | time 465.8582 ms | tok/sec 1125423.9515
for step 7844 | loss 3.484624 | norm 0.3217 | time 465.5645 ms | tok/sec 1126133.9986
for step 7845 | loss 3.395242 | norm 0.3520 | time 465.9231 ms | tok/sec 1125267.3086
for step 7846 | loss 3.431180 | norm 0.4360 | time 465.6460 ms | tok/sec 1125936.8015
for step 7847 | loss 3.359650 | norm 0.3040 | time 465.9476 ms | tok/sec 1125208.0029
for step 7848 | loss 3.338971 | norm 0.3609 | time 465.1825 ms | tok/sec 1127058.6313
for step 7849 | loss 3.384475 | norm 0.3966 | time 464.9134 ms | tok/sec 1127711.1734
for step 7850 | loss 3.391478 | norm 0.3254 | time 465.9204 ms | tok/sec 1125273.6425
for step 7851 | loss 3.452984 | norm 0.3215 | time 464.8864 ms | tok/sec 1127776.5270
for step 7852 | loss 3.362352 | norm 0.3507 | time 465.0013 ms | tok/sec 1127497.8148
for step 7853 | loss 3.379173 | norm 0.3522 | time 465.3232 ms | tok/sec 1126717.9221
for step 7854 | loss 3.375040 | norm 0.3159 | time 465.5054 ms | tok/sec 1126277.0385
for step 7855 | loss 3.428774 | norm 0.3415 | time 465.3370 ms | tok/sec 1126684.4398
for step 7856 | loss 3.374315 | norm 0.3149 | time 465.7266 ms | tok/sec 1125741.9788
for step 7857 | loss 3.443880 | norm 0.3403 | time 465.3993 ms | tok/sec 1126533.7939
for step 7858 | loss 3.364195 | norm 0.3463 | time 465.8723 ms | tok/sec 1125389.9701
for step 7859 | loss 3.398616 | norm 0.3492 | time 465.3225 ms | tok/sec 1126719.6540
for step 7860 | loss 3.360114 | norm 0.3245 | time 465.3168 ms | tok/sec 1126733.5094
for step 7861 | loss 3.403619 | norm 0.3177 | time 465.4551 ms | tok/sec 1126398.7663
for step 7862 | loss 3.478610 | norm 0.3595 | time 465.0750 ms | tok/sec 1127319.2107
for step 7863 | loss 3.401781 | norm 0.3097 | time 466.4173 ms | tok/sec 1124074.9128
for step 7864 | loss 3.326484 | norm 0.3578 | time 466.0170 ms | tok/sec 1125040.4839
for step 7865 | loss 3.401405 | norm 0.3680 | time 466.0642 ms | tok/sec 1124926.5304
for step 7866 | loss 3.404508 | norm 0.3248 | time 464.8335 ms | tok/sec 1127904.9429
for step 7867 | loss 3.383892 | norm 0.3431 | time 465.8737 ms | tok/sec 1125386.5145
for step 7868 | loss 3.388495 | norm 0.3427 | time 465.9662 ms | tok/sec 1125163.0960
for step 7869 | loss 3.420290 | norm 0.3222 | time 466.5658 ms | tok/sec 1123717.0550
for step 7870 | loss 3.443351 | norm 0.2959 | time 465.2243 ms | tok/sec 1126957.5520
for step 7871 | loss 3.398974 | norm 0.3348 | time 465.5581 ms | tok/sec 1126149.5697
for step 7872 | loss 3.418532 | norm 0.2842 | time 465.0106 ms | tok/sec 1127475.2694
for step 7873 | loss 3.466988 | norm 0.3184 | time 465.4121 ms | tok/sec 1126502.6308
for step 7874 | loss 3.469924 | norm 0.3450 | time 465.5292 ms | tok/sec 1126219.3567
for step 7875 | loss 3.469611 | norm 0.3353 | time 468.2381 ms | tok/sec 1119703.8064
for step 7876 | loss 3.404796 | norm 0.3605 | time 465.5991 ms | tok/sec 1126050.3832
for step 7877 | loss 3.440281 | norm 0.3423 | time 464.5936 ms | tok/sec 1128487.2301
for step 7878 | loss 3.464285 | norm 0.3593 | time 465.2650 ms | tok/sec 1126858.8006
for step 7879 | loss 3.424318 | norm 0.3267 | time 465.1768 ms | tok/sec 1127072.4950
for step 7880 | loss 3.409663 | norm 0.3594 | time 465.2216 ms | tok/sec 1126963.9050
for step 7881 | loss 3.361311 | norm 0.3229 | time 464.9830 ms | tok/sec 1127542.3301
for step 7882 | loss 3.380576 | norm 0.3155 | time 465.1690 ms | tok/sec 1127091.5582
for step 7883 | loss 3.380355 | norm 0.2916 | time 464.8645 ms | tok/sec 1127829.7408
for step 7884 | loss 3.385687 | norm 0.3092 | time 465.6384 ms | tok/sec 1125955.2498
for step 7885 | loss 3.433723 | norm 0.3177 | time 465.7950 ms | tok/sec 1125576.6052
for step 7886 | loss 3.341811 | norm 0.2886 | time 464.5424 ms | tok/sec 1128611.7532
for step 7887 | loss 3.395267 | norm 0.2989 | time 465.4868 ms | tok/sec 1126322.0343
for step 7888 | loss 3.359943 | norm 0.3112 | time 464.7717 ms | tok/sec 1128054.7983
for step 7889 | loss 3.368989 | norm 0.3554 | time 465.7297 ms | tok/sec 1125734.4869
for step 7890 | loss 3.342615 | norm 0.3458 | time 465.0936 ms | tok/sec 1127274.1352
for step 7891 | loss 3.357860 | norm 0.2905 | time 464.1292 ms | tok/sec 1129616.4716
for step 7892 | loss 3.418912 | norm 0.3746 | time 465.0733 ms | tok/sec 1127323.2562
for step 7893 | loss 3.382055 | norm 0.3591 | time 464.9143 ms | tok/sec 1127708.8601
for step 7894 | loss 3.428947 | norm 0.3516 | time 465.4479 ms | tok/sec 1126416.0757
for step 7895 | loss 3.459731 | norm 0.3577 | time 465.4398 ms | tok/sec 1126435.6937
for step 7896 | loss 3.370678 | norm 0.3163 | time 465.1687 ms | tok/sec 1127092.1359
for step 7897 | loss 3.385366 | norm 0.3201 | time 466.1012 ms | tok/sec 1124837.3405
for step 7898 | loss 3.394248 | norm 0.3419 | time 465.0633 ms | tok/sec 1127347.5293
for step 7899 | loss 3.418442 | norm 0.3195 | time 466.1269 ms | tok/sec 1124775.2036
for step 7900 | loss 3.397098 | norm 0.3428 | time 465.0857 ms | tok/sec 1127293.2052
for step 7901 | loss 3.440006 | norm 0.3145 | time 464.8592 ms | tok/sec 1127842.4666
for step 7902 | loss 3.403517 | norm 0.2947 | time 464.8056 ms | tok/sec 1127972.6333
for step 7903 | loss 3.436432 | norm 0.3637 | time 465.4882 ms | tok/sec 1126318.5730
for step 7904 | loss 3.343510 | norm 0.4411 | time 465.5368 ms | tok/sec 1126200.8998
for step 7905 | loss 3.380929 | norm 0.3732 | time 464.5429 ms | tok/sec 1128610.5947
for step 7906 | loss 3.454945 | norm 0.3643 | time 464.6285 ms | tok/sec 1128402.6859
for step 7907 | loss 3.440551 | norm 0.3662 | time 465.3323 ms | tok/sec 1126695.9852
for step 7908 | loss 3.420537 | norm 0.3669 | time 465.2824 ms | tok/sec 1126816.6488
for step 7909 | loss 3.415359 | norm 0.3398 | time 464.6418 ms | tok/sec 1128370.2614
for step 7910 | loss 3.455226 | norm 0.3043 | time 465.6386 ms | tok/sec 1125954.6733
for step 7911 | loss 3.453215 | norm 0.3688 | time 465.5926 ms | tok/sec 1126065.9520
for step 7912 | loss 3.627491 | norm 0.3697 | time 465.1582 ms | tok/sec 1127117.5545
for step 7913 | loss 3.404755 | norm 0.3814 | time 465.9183 ms | tok/sec 1125278.8249
for step 7914 | loss 3.493981 | norm 0.3755 | time 465.6103 ms | tok/sec 1126023.2830
for step 7915 | loss 3.379969 | norm 0.3326 | time 465.4956 ms | tok/sec 1126300.6897
for step 7916 | loss 3.351903 | norm 0.3514 | time 465.9734 ms | tok/sec 1125145.8251
for step 7917 | loss 3.367956 | norm 0.3568 | time 465.0228 ms | tok/sec 1127445.7884
for step 7918 | loss 3.383999 | norm 0.3348 | time 465.1783 ms | tok/sec 1127069.0291
for step 7919 | loss 3.349432 | norm 0.3216 | time 466.1922 ms | tok/sec 1124617.5909
for step 7920 | loss 3.386571 | norm 0.3256 | time 465.5845 ms | tok/sec 1126085.5578
for step 7921 | loss 3.327868 | norm 0.3438 | time 465.9104 ms | tok/sec 1125297.8275
for step 7922 | loss 3.396267 | norm 0.3425 | time 466.0664 ms | tok/sec 1124921.3513
for step 7923 | loss 3.302978 | norm 0.3029 | time 465.2762 ms | tok/sec 1126831.6614
for step 7924 | loss 3.416608 | norm 0.3470 | time 465.8325 ms | tok/sec 1125486.1601
for step 7925 | loss 3.373862 | norm 0.3122 | time 465.3842 ms | tok/sec 1126570.1530
for step 7926 | loss 3.405807 | norm 0.3368 | time 465.0409 ms | tok/sec 1127401.8587
for step 7927 | loss 3.414173 | norm 0.3501 | time 465.6973 ms | tok/sec 1125812.8680
for step 7928 | loss 3.484297 | norm 0.3688 | time 465.5161 ms | tok/sec 1126251.0809
for step 7929 | loss 3.355603 | norm 0.3166 | time 465.5209 ms | tok/sec 1126239.5446
for step 7930 | loss 3.423704 | norm 0.3411 | time 465.0748 ms | tok/sec 1127319.7887
for step 7931 | loss 3.445452 | norm 0.3776 | time 465.5230 ms | tok/sec 1126234.3534
for step 7932 | loss 3.483123 | norm 0.3642 | time 466.4125 ms | tok/sec 1124086.4048
for step 7933 | loss 3.356484 | norm 0.3782 | time 465.3633 ms | tok/sec 1126620.9442
for step 7934 | loss 3.394557 | norm 0.3749 | time 465.6453 ms | tok/sec 1125938.5310
for step 7935 | loss 3.337431 | norm 0.3510 | time 465.7750 ms | tok/sec 1125625.0022
for step 7936 | loss 3.399816 | norm 0.3319 | time 464.9448 ms | tok/sec 1127634.8407
for step 7937 | loss 3.380146 | norm 0.3441 | time 464.9169 ms | tok/sec 1127702.4987
for step 7938 | loss 3.471712 | norm 0.3577 | time 466.2552 ms | tok/sec 1124465.7722
for step 7939 | loss 3.449781 | norm 0.3585 | time 464.4048 ms | tok/sec 1128946.0743
for step 7940 | loss 3.367367 | norm 0.3300 | time 465.4181 ms | tok/sec 1126488.2040
for step 7941 | loss 3.460621 | norm 0.5429 | time 465.7772 ms | tok/sec 1125619.8166
for step 7942 | loss 3.400248 | norm 0.3343 | time 465.1136 ms | tok/sec 1127225.5963
for step 7943 | loss 3.436843 | norm 0.3259 | time 465.0240 ms | tok/sec 1127442.8981
for step 7944 | loss 3.465867 | norm 0.3356 | time 465.5721 ms | tok/sec 1126115.5445
for step 7945 | loss 3.414231 | norm 0.3186 | time 465.1594 ms | tok/sec 1127114.6659
for step 7946 | loss 3.412049 | norm 0.3281 | time 465.3656 ms | tok/sec 1126615.1723
for step 7947 | loss 3.499502 | norm 0.3134 | time 465.1575 ms | tok/sec 1127119.2876
for step 7948 | loss 3.416241 | norm 0.3276 | time 465.8198 ms | tok/sec 1125516.6909
for step 7949 | loss 3.371431 | norm 0.3202 | time 465.0147 ms | tok/sec 1127465.4422
for step 7950 | loss 3.394989 | norm 0.3239 | time 464.9036 ms | tok/sec 1127734.8849
for step 7951 | loss 3.361502 | norm 0.3195 | time 465.0850 ms | tok/sec 1127294.9388
for step 7952 | loss 3.337165 | norm 0.2817 | time 465.7388 ms | tok/sec 1125712.5883
for step 7953 | loss 3.335249 | norm 0.4447 | time 465.3516 ms | tok/sec 1126649.2277
for step 7954 | loss 3.433703 | norm 0.3671 | time 465.6382 ms | tok/sec 1125955.8263
for step 7955 | loss 3.348449 | norm 0.3829 | time 465.4233 ms | tok/sec 1126475.5088
for step 7956 | loss 3.373798 | norm 0.3473 | time 465.4002 ms | tok/sec 1126531.4855
for step 7957 | loss 3.391222 | norm 0.3694 | time 465.2643 ms | tok/sec 1126860.5329
for step 7958 | loss 3.397119 | norm 0.3572 | time 465.1101 ms | tok/sec 1127234.2636
for step 7959 | loss 3.394635 | norm 0.3452 | time 465.5457 ms | tok/sec 1126179.5598
for step 7960 | loss 3.419023 | norm 0.3296 | time 465.2526 ms | tok/sec 1126888.8284
for step 7961 | loss 3.418559 | norm 0.3745 | time 464.9706 ms | tok/sec 1127572.3944
for step 7962 | loss 3.387589 | norm 0.3465 | time 464.7348 ms | tok/sec 1128144.4992
for step 7963 | loss 3.351160 | norm 0.3219 | time 465.9185 ms | tok/sec 1125278.2491
for step 7964 | loss 3.399327 | norm 0.3424 | time 464.7813 ms | tok/sec 1128031.6520
for step 7965 | loss 3.436042 | norm 0.3362 | time 465.7288 ms | tok/sec 1125736.7921
for step 7966 | loss 3.405973 | norm 0.3262 | time 464.2711 ms | tok/sec 1129271.3147
for step 7967 | loss 3.360665 | norm 0.3209 | time 465.7164 ms | tok/sec 1125766.7602
for step 7968 | loss 3.397638 | norm 0.3394 | time 465.8871 ms | tok/sec 1125354.2631
for step 7969 | loss 3.411376 | norm 0.3119 | time 466.1038 ms | tok/sec 1124831.0114
for step 7970 | loss 3.411130 | norm 0.3463 | time 465.5080 ms | tok/sec 1126270.6932
for step 7971 | loss 3.399866 | norm 0.3228 | time 465.7879 ms | tok/sec 1125593.8894
for step 7972 | loss 3.404896 | norm 0.3171 | time 465.8737 ms | tok/sec 1125386.5145
for step 7973 | loss 3.537393 | norm 0.3270 | time 465.1029 ms | tok/sec 1127251.5987
for step 7974 | loss 3.427274 | norm 0.3270 | time 466.0208 ms | tok/sec 1125031.2747
for step 7975 | loss 3.413255 | norm 0.3385 | time 464.8964 ms | tok/sec 1127752.2354
for step 7976 | loss 3.438868 | norm 0.3212 | time 465.6916 ms | tok/sec 1125826.7011
for step 7977 | loss 3.436652 | norm 0.3165 | time 465.5139 ms | tok/sec 1126256.2723
for step 7978 | loss 3.488123 | norm 0.3328 | time 466.3074 ms | tok/sec 1124339.8628
for step 7979 | loss 3.435624 | norm 0.3678 | time 465.2896 ms | tok/sec 1126799.3271
for step 7980 | loss 3.422761 | norm 0.3172 | time 465.8828 ms | tok/sec 1125364.6294
for step 7981 | loss 3.402971 | norm 0.2915 | time 465.7502 ms | tok/sec 1125684.9280
for step 7982 | loss 3.411347 | norm 0.3590 | time 465.4140 ms | tok/sec 1126498.0142
for step 7983 | loss 3.439667 | norm 0.3026 | time 465.1301 ms | tok/sec 1127185.7282
for step 7984 | loss 3.368678 | norm 0.3476 | time 465.5855 ms | tok/sec 1126083.2512
for step 7985 | loss 3.379030 | norm 0.3546 | time 465.1628 ms | tok/sec 1127106.5781
for step 7986 | loss 3.411778 | norm 0.3385 | time 464.8490 ms | tok/sec 1127867.3406
for step 7987 | loss 3.417230 | norm 0.3289 | time 465.1351 ms | tok/sec 1127173.5950
for step 7988 | loss 3.369584 | norm 0.3463 | time 464.6249 ms | tok/sec 1128411.3714
for step 7989 | loss 3.424285 | norm 0.3128 | time 465.2250 ms | tok/sec 1126955.8194
for step 7990 | loss 3.399102 | norm 0.3768 | time 464.3059 ms | tok/sec 1129186.6530
for step 7991 | loss 3.360370 | norm 0.3293 | time 465.4212 ms | tok/sec 1126480.7023
for step 7992 | loss 3.324853 | norm 0.3463 | time 464.8640 ms | tok/sec 1127830.8977
for step 7993 | loss 3.315932 | norm 0.3383 | time 465.3800 ms | tok/sec 1126580.5417
for step 7994 | loss 3.372855 | norm 0.3081 | time 465.5130 ms | tok/sec 1126258.5797
for step 7995 | loss 3.346147 | norm 0.2866 | time 465.8980 ms | tok/sec 1125327.7722
for step 7996 | loss 3.425580 | norm 0.3526 | time 466.6591 ms | tok/sec 1123492.5768
for step 7997 | loss 3.417341 | norm 0.3621 | time 464.5791 ms | tok/sec 1128522.5572
for step 7998 | loss 3.422076 | norm 0.3061 | time 465.0600 ms | tok/sec 1127355.6206
for step 7999 | loss 3.383541 | norm 0.3248 | time 465.1158 ms | tok/sec 1127220.3959
validation loss 3.3910
HellaSwag accuracy: 2710/10042=0.2699
> Hello, I'm a language model, and I am a language model. Thank You for sharing in particular the definition of a language model:
(a)
> Hello, I'm a language model, a model of computer science. I can’t be sure if I ever hear, or perhaps even know, that
> Hello, I'm a language model, I got this to say, I'm a language model that says you are a language model that means you're just looking
> Hello, I'm a language model, and I'm looking at our "inquiry" part of the project so I wasn't really interested in doing that
> Hello, I'm a language model, and I'll be able to write your own .
After a month of experience, this series was written using the C
> > Hello, I'm a language model, so maybe I'm going to have as many of you as I can without needing a dictionary in the library.
The
Hello, I'm a language model, so I want to keep up with it: I am a language model and this is not necessarily the problem in my case
> Hello, I'm a language model, so I really want to show you that I could call it 'English,' but I don't know what I am talking
> Hello, I'm a language model, how does it express your thoughts for language? Can I simply say
your thought and thought?
Yes, can you> 
Hello, I'm a language model, and for that to be true. For the "Hello World" tutorial, we've looked at how to do that..
> Hello, I'm a language model, but they make quite different shapes. And, so, I am able to tell you the type of work you should be
> Hello, I'm a language model, and it tells all your data to match with what your name may have changed in the past, with no change in its
> Hello, I'm a language model, let's talk some of the basics I learned from my first grader. I think I'll just have something to teach
> Hello, I'm a language model, and I love to hear your thoughts. One of my favorite metaphors is the quote called "Tas," and I was> 
Hello, I'm a language model, an example of an author's perspective here when I give a speech that shows how a language works and a model of grammar
> > Hello, I'm a language model, anyway. That's the place that English grammar is and those languages are the same people, I'm the person that wrote
Hello, I'm a language model, so I can say that you have more than one language. The same happens to you when I was teaching in this country
> > > Hello, I'm a language model, I was wondering if that would be possible? I just thought I am, and it's not really possible.
You
> Hello, I'm a language model, and I am pretty good at working out my own theories.
In one of the first sections of this book there is
Hello, I'm a language model, I just made a short comment on a given topic by the professor.
How did this matter before you wrote the text
> Hello, I'm a language model, that was a wonderful, informative study I've been waiting for! We have two computers, two computers, two computers,
> > Hello, I'm a language model, trying to represent all parts of language in a general way. For example, you need to be able to tell a series
> Hello, I'm a language model, looking for what I've read in. There's a lot to do as I go about it all.
It should
Hello, I'm a language model, and I'm not interested in learning, especially if it's fun to look at, let's keep getting started,

> Hello, I'm a language model, and I can't even think of anything from the first time I wrote to my students.
"The students learned more
> Hello, I'm a language model, and you know, here's how I can explain it.
For a first time people have observed a language, so
> Hello, I'm a language model, and this isn't a topic, I just want to learn a few things.
- Read the chapter I just learned
> Hello, I'm a language model, I think it is cool, it's just really cool. It's really very easy to do.
You are a
> Hello, I'm a language model, is that all language models use a subset, or one of the four substrings that is the appropriate representation for the group
> Hello, I'm a language model, and you are pretty the same at the same time. It does not have that special name of another language. Instead I
Hello, I'm a language model, and I can't remember the last time I went to the library and wanted to make my case. They just asked me
> Hello, I'm a language model, and think I know more than one language, I don't understand anything, but I have to try harder.
This
Will loading at 0 from edu_fineweb10B/edufineweb_train_000043.npy
for step 8000 | loss 3.420038 | norm 0.3167 | time 13732.1980 ms | tok/sec 38179.4670
for step 8001 | loss 3.387379 | norm 0.2910 | time 462.8482 ms | tok/sec 1132742.9064
for step 8002 | loss 3.376468 | norm 0.3216 | time 463.5694 ms | tok/sec 1130980.5974
for step 8003 | loss 3.427432 | norm 0.3315 | time 463.2537 ms | tok/sec 1131751.2600
for step 8004 | loss 3.423518 | norm 0.3057 | time 463.8765 ms | tok/sec 1130231.8958
for step 8005 | loss 3.432265 | norm 0.3037 | time 463.6810 ms | tok/sec 1130708.4390
for step 8006 | loss 3.413999 | norm 0.3395 | time 462.8918 ms | tok/sec 1132636.1379
for step 8007 | loss 3.413215 | norm 0.3382 | time 464.0160 ms | tok/sec 1129892.1684
for step 8008 | loss 3.413697 | norm 0.3542 | time 463.9714 ms | tok/sec 1130000.7428
for step 8009 | loss 3.452273 | norm 0.3391 | time 463.7849 ms | tok/sec 1130455.0075
for step 8010 | loss 3.430514 | norm 0.3389 | time 463.9032 ms | tok/sec 1130166.8382
for step 8011 | loss 3.418499 | norm 0.3517 | time 467.2151 ms | tok/sec 1122155.6057
for step 8012 | loss 3.414010 | norm 0.3410 | time 464.0718 ms | tok/sec 1129756.3346
for step 8013 | loss 3.401895 | norm 0.3174 | time 463.3338 ms | tok/sec 1131555.5846
for step 8014 | loss 3.460092 | norm 0.3205 | time 467.8586 ms | tok/sec 1120612.1951
for step 8015 | loss 3.443166 | norm 0.3062 | time 464.1690 ms | tok/sec 1129519.5743
for step 8016 | loss 3.480072 | norm 0.3008 | time 463.4533 ms | tok/sec 1131263.9442
for step 8017 | loss 3.423080 | norm 0.3239 | time 463.6345 ms | tok/sec 1130821.8224
for step 8018 | loss 3.404779 | norm 0.3209 | time 463.8171 ms | tok/sec 1130376.5597
for step 8019 | loss 3.394609 | norm 0.3546 | time 464.3228 ms | tok/sec 1129145.4865
for step 8020 | loss 3.322435 | norm 0.3334 | time 463.6285 ms | tok/sec 1130836.3604
for step 8021 | loss 3.418750 | norm 0.3491 | time 464.6728 ms | tok/sec 1128294.9973
for step 8022 | loss 3.344275 | norm 0.3216 | time 464.6275 ms | tok/sec 1128405.0021
for step 8023 | loss 3.369999 | norm 0.3273 | time 464.3254 ms | tok/sec 1129139.1089
for step 8024 | loss 3.462824 | norm 0.3026 | time 465.5297 ms | tok/sec 1126218.2031
for step 8025 | loss 3.392523 | norm 0.3465 | time 464.5488 ms | tok/sec 1128596.1139
for step 8026 | loss 3.351768 | norm 0.2824 | time 465.2483 ms | tok/sec 1126899.2230
for step 8027 | loss 3.344783 | norm 0.3135 | time 467.8040 ms | tok/sec 1120742.9828
for step 8028 | loss 3.354539 | norm 0.3114 | time 465.3029 ms | tok/sec 1126766.9946
for step 8029 | loss 3.304303 | norm 0.3109 | time 464.5159 ms | tok/sec 1128676.0526
for step 8030 | loss 3.351877 | norm 0.3516 | time 464.8778 ms | tok/sec 1127797.3492
for step 8031 | loss 3.415929 | norm 0.3343 | time 465.1256 ms | tok/sec 1127196.7061
for step 8032 | loss 3.416362 | norm 0.3232 | time 464.2892 ms | tok/sec 1129227.2427
for step 8033 | loss 3.363704 | norm 0.3511 | time 464.6010 ms | tok/sec 1128469.2779
for step 8034 | loss 3.388375 | norm 0.3478 | time 464.8495 ms | tok/sec 1127866.1836
for step 8035 | loss 3.423402 | norm 0.3690 | time 465.4646 ms | tok/sec 1126375.6879
for step 8036 | loss 3.350319 | norm 0.3491 | time 464.3936 ms | tok/sec 1128973.3154
for step 8037 | loss 3.400224 | norm 0.3987 | time 465.4300 ms | tok/sec 1126459.3516
for step 8038 | loss 3.420732 | norm 0.3723 | time 465.2324 ms | tok/sec 1126937.9158
for step 8039 | loss 3.460997 | norm 0.3651 | time 465.5721 ms | tok/sec 1126115.5445
for step 8040 | loss 3.415899 | norm 0.3596 | time 464.4942 ms | tok/sec 1128728.7720
for step 8041 | loss 3.420222 | norm 0.3927 | time 464.6406 ms | tok/sec 1128373.1564
for step 8042 | loss 3.426888 | norm 0.4684 | time 465.8182 ms | tok/sec 1125520.7234
for step 8043 | loss 3.386860 | norm 0.4476 | time 464.6459 ms | tok/sec 1128360.4186
for step 8044 | loss 3.403567 | norm 0.3402 | time 464.7515 ms | tok/sec 1128103.9874
for step 8045 | loss 3.437221 | norm 0.4261 | time 465.1494 ms | tok/sec 1127138.9301
for step 8046 | loss 3.510230 | norm 0.3407 | time 465.2100 ms | tok/sec 1126992.2057
for step 8047 | loss 3.416498 | norm 0.3586 | time 464.8592 ms | tok/sec 1127842.4666
for step 8048 | loss 3.437136 | norm 0.3385 | time 464.8137 ms | tok/sec 1127952.9617
for step 8049 | loss 3.406086 | norm 0.3428 | time 465.1096 ms | tok/sec 1127235.4193
for step 8050 | loss 3.422363 | norm 0.3258 | time 465.4875 ms | tok/sec 1126320.3036
for step 8051 | loss 3.358681 | norm 0.3123 | time 465.1546 ms | tok/sec 1127126.2202
for step 8052 | loss 3.424467 | norm 0.3292 | time 465.4863 ms | tok/sec 1126323.1881
for step 8053 | loss 3.398719 | norm 0.3507 | time 464.8895 ms | tok/sec 1127769.0080
for step 8054 | loss 3.390342 | norm 0.3247 | time 464.9601 ms | tok/sec 1127597.8346
for step 8055 | loss 3.391916 | norm 0.3050 | time 464.8752 ms | tok/sec 1127803.7117
for step 8056 | loss 3.359294 | norm 0.3127 | time 464.7818 ms | tok/sec 1128030.4947
for step 8057 | loss 3.383999 | norm 0.2957 | time 465.1349 ms | tok/sec 1127174.1727
for step 8058 | loss 3.380327 | norm 0.3003 | time 464.9930 ms | tok/sec 1127518.0486
for step 8059 | loss 3.341052 | norm 0.2953 | time 464.9854 ms | tok/sec 1127536.5487
for step 8060 | loss 3.339566 | norm 0.3001 | time 464.8702 ms | tok/sec 1127815.8585
for step 8061 | loss 3.339082 | norm 0.3042 | time 465.4603 ms | tok/sec 1126386.0731
for step 8062 | loss 3.349676 | norm 0.3051 | time 464.8523 ms | tok/sec 1127859.2420
for step 8063 | loss 3.285512 | norm 0.2904 | time 464.9606 ms | tok/sec 1127596.6782
for step 8064 | loss 3.417693 | norm 0.3353 | time 465.0395 ms | tok/sec 1127405.3267
for step 8065 | loss 3.383192 | norm 0.3917 | time 465.1608 ms | tok/sec 1127111.1997
for step 8066 | loss 3.395472 | norm 0.3134 | time 464.7484 ms | tok/sec 1128111.5108
for step 8067 | loss 3.391528 | norm 0.3728 | time 465.9002 ms | tok/sec 1125322.5893
for step 8068 | loss 3.403644 | norm 0.3312 | time 464.7901 ms | tok/sec 1128010.2425
for step 8069 | loss 3.396331 | norm 0.3239 | time 464.7176 ms | tok/sec 1128186.1716
for step 8070 | loss 3.372180 | norm 0.3267 | time 464.6916 ms | tok/sec 1128249.2648
for step 8071 | loss 3.327892 | norm 0.3904 | time 464.9756 ms | tok/sec 1127560.2528
for step 8072 | loss 3.371789 | norm 0.3591 | time 465.4810 ms | tok/sec 1126335.8799
for step 8073 | loss 3.366420 | norm 0.3438 | time 466.3267 ms | tok/sec 1124293.3007
for step 8074 | loss 3.378509 | norm 0.3382 | time 466.1796 ms | tok/sec 1124648.0746
for step 8075 | loss 3.454015 | norm 0.3517 | time 464.6568 ms | tok/sec 1128333.7860
for step 8076 | loss 3.384108 | norm 0.3401 | time 464.3905 ms | tok/sec 1128980.8504
for step 8077 | loss 3.404305 | norm 0.3771 | time 465.1864 ms | tok/sec 1127049.3890
for step 8078 | loss 3.421623 | norm 0.3388 | time 464.9456 ms | tok/sec 1127633.1060
for step 8079 | loss 3.483942 | norm 0.3559 | time 464.6602 ms | tok/sec 1128325.6807
for step 8080 | loss 3.416477 | norm 0.3378 | time 464.5765 ms | tok/sec 1128528.9278
for step 8081 | loss 3.388032 | norm 0.3345 | time 464.8855 ms | tok/sec 1127778.8405
for step 8082 | loss 3.461344 | norm 0.3101 | time 465.0135 ms | tok/sec 1127468.3325
for step 8083 | loss 3.420925 | norm 0.3381 | time 464.3793 ms | tok/sec 1129008.0932
for step 8084 | loss 3.444485 | norm 0.3086 | time 464.9911 ms | tok/sec 1127522.6736
for step 8085 | loss 3.388227 | norm 0.3587 | time 464.6025 ms | tok/sec 1128465.8034
for step 8086 | loss 3.419641 | norm 0.3473 | time 465.3316 ms | tok/sec 1126697.7170
for step 8087 | loss 3.394830 | norm 0.3442 | time 465.2846 ms | tok/sec 1126811.4522
for step 8088 | loss 3.405059 | norm 0.3734 | time 465.2078 ms | tok/sec 1126997.4040
for step 8089 | loss 3.344332 | norm 0.3266 | time 465.0784 ms | tok/sec 1127311.1200
for step 8090 | loss 3.292262 | norm 0.3233 | time 465.8525 ms | tok/sec 1125437.7751
for step 8091 | loss 3.400202 | norm 0.3295 | time 464.9007 ms | tok/sec 1127741.8250
for step 8092 | loss 3.328157 | norm 0.3329 | time 465.2040 ms | tok/sec 1127006.6454
for step 8093 | loss 3.347380 | norm 0.3362 | time 464.7870 ms | tok/sec 1128017.7647
for step 8094 | loss 3.365728 | norm 0.3222 | time 467.7343 ms | tok/sec 1120909.7958
for step 8095 | loss 3.413028 | norm 0.3195 | time 465.3983 ms | tok/sec 1126536.1023
for step 8096 | loss 3.356827 | norm 0.2999 | time 465.1611 ms | tok/sec 1127110.6220
for step 8097 | loss 3.313461 | norm 0.3329 | time 465.3468 ms | tok/sec 1126660.7724
for step 8098 | loss 3.350474 | norm 0.3399 | time 464.9785 ms | tok/sec 1127553.3149
for step 8099 | loss 3.402447 | norm 0.3379 | time 465.6978 ms | tok/sec 1125811.7153
for step 8100 | loss 3.381304 | norm 0.3169 | time 465.2541 ms | tok/sec 1126885.3636
for step 8101 | loss 3.366705 | norm 0.3357 | time 465.4753 ms | tok/sec 1126349.7259
for step 8102 | loss 3.444775 | norm 0.3285 | time 464.7787 ms | tok/sec 1128038.0172
for step 8103 | loss 3.405420 | norm 0.3162 | time 465.0395 ms | tok/sec 1127405.3267
for step 8104 | loss 3.428417 | norm 0.3092 | time 466.0401 ms | tok/sec 1124984.6553
for step 8105 | loss 3.446929 | norm 0.2982 | time 464.6785 ms | tok/sec 1128281.1035
for step 8106 | loss 3.468582 | norm 0.3876 | time 465.6904 ms | tok/sec 1125829.5830
for step 8107 | loss 3.374283 | norm 0.3682 | time 465.9162 ms | tok/sec 1125284.0074
for step 8108 | loss 3.416713 | norm 0.3012 | time 464.9324 ms | tok/sec 1127664.9099
for step 8109 | loss 3.512122 | norm 0.3534 | time 465.4586 ms | tok/sec 1126390.1118
for step 8110 | loss 3.438622 | norm 0.3801 | time 465.4098 ms | tok/sec 1126508.4016
for step 8111 | loss 3.411457 | norm 0.3187 | time 468.5826 ms | tok/sec 1118880.5693
for step 8112 | loss 3.374524 | norm 0.3636 | time 464.8759 ms | tok/sec 1127801.9765
for step 8113 | loss 3.498536 | norm 0.3766 | time 464.4430 ms | tok/sec 1128853.3484
for step 8114 | loss 3.389330 | norm 0.3182 | time 465.5900 ms | tok/sec 1126072.2950
for step 8115 | loss 3.435835 | norm 0.3551 | time 465.4925 ms | tok/sec 1126308.1891
for step 8116 | loss 3.428829 | norm 0.3661 | time 465.4171 ms | tok/sec 1126490.5123
for step 8117 | loss 3.401055 | norm 0.3471 | time 465.1823 ms | tok/sec 1127059.2090
for step 8118 | loss 3.431458 | norm 0.3310 | time 464.9847 ms | tok/sec 1127538.2831
for step 8119 | loss 3.355029 | norm 0.3281 | time 466.9759 ms | tok/sec 1122730.2507
for step 8120 | loss 3.433922 | norm 0.3365 | time 465.5659 ms | tok/sec 1126130.5384
for step 8121 | loss 3.420600 | norm 0.3016 | time 464.7610 ms | tok/sec 1128080.8391
for step 8122 | loss 3.330464 | norm 0.3646 | time 466.7428 ms | tok/sec 1123291.1396
for step 8123 | loss 3.368742 | norm 0.3204 | time 464.9940 ms | tok/sec 1127515.7361
for step 8124 | loss 3.379181 | norm 0.3244 | time 466.3048 ms | tok/sec 1124346.1863
for step 8125 | loss 3.383448 | norm 0.3176 | time 466.1932 ms | tok/sec 1124615.2903
for step 8126 | loss 3.368416 | norm 0.3143 | time 465.4169 ms | tok/sec 1126491.0894
for step 8127 | loss 3.366518 | norm 0.2950 | time 465.9097 ms | tok/sec 1125299.5550
for step 8128 | loss 3.359277 | norm 0.3575 | time 465.2283 ms | tok/sec 1126947.7338
for step 8129 | loss 3.388126 | norm 0.3139 | time 466.5504 ms | tok/sec 1123754.3810
for step 8130 | loss 3.390572 | norm 0.3690 | time 464.7942 ms | tok/sec 1128000.4060
for step 8131 | loss 3.353199 | norm 0.3105 | time 465.9765 ms | tok/sec 1125138.3412
for step 8132 | loss 3.388236 | norm 0.3289 | time 465.7726 ms | tok/sec 1125630.7640
for step 8133 | loss 3.317801 | norm 0.3239 | time 465.3270 ms | tok/sec 1126708.6854
for step 8134 | loss 3.385960 | norm 0.3172 | time 465.9214 ms | tok/sec 1125271.3393
for step 8135 | loss 3.473041 | norm 0.3504 | time 466.0490 ms | tok/sec 1124963.3613
for step 8136 | loss 3.404003 | norm 0.3138 | time 466.1076 ms | tok/sec 1124821.8056
for step 8137 | loss 3.418782 | norm 0.3732 | time 466.1422 ms | tok/sec 1124738.3850
for step 8138 | loss 3.428412 | norm 0.3338 | time 464.9022 ms | tok/sec 1127738.3549
for step 8139 | loss 3.452109 | norm 0.3409 | time 464.8941 ms | tok/sec 1127758.0190
for step 8140 | loss 3.440860 | norm 0.3635 | time 465.5774 ms | tok/sec 1126102.8576
for step 8141 | loss 3.341962 | norm 0.3762 | time 465.0533 ms | tok/sec 1127371.8035
for step 8142 | loss 3.391748 | norm 0.3688 | time 465.5466 ms | tok/sec 1126177.2528
for step 8143 | loss 3.406702 | norm 0.3442 | time 464.9136 ms | tok/sec 1127710.5951
for step 8144 | loss 3.451831 | norm 0.3389 | time 464.6277 ms | tok/sec 1128404.4230
for step 8145 | loss 3.419250 | norm 0.3589 | time 464.5095 ms | tok/sec 1128691.6941
for step 8146 | loss 3.414567 | norm 0.4314 | time 465.4803 ms | tok/sec 1126337.6106
for step 8147 | loss 3.382365 | norm 0.3431 | time 465.0240 ms | tok/sec 1127442.8981
for step 8148 | loss 3.438821 | norm 0.3243 | time 465.5118 ms | tok/sec 1126261.4638
for step 8149 | loss 3.418622 | norm 0.3160 | time 465.5168 ms | tok/sec 1126249.3505
for step 8150 | loss 3.449090 | norm 0.2970 | time 464.6504 ms | tok/sec 1128349.4180
for step 8151 | loss 3.405853 | norm 0.3128 | time 464.7000 ms | tok/sec 1128229.0047
for step 8152 | loss 3.439001 | norm 0.3020 | time 464.2348 ms | tok/sec 1129359.4692
for step 8153 | loss 3.404023 | norm 0.3111 | time 465.0037 ms | tok/sec 1127492.0338
for step 8154 | loss 3.422845 | norm 0.2813 | time 464.8077 ms | tok/sec 1127967.4260
for step 8155 | loss 3.417538 | norm 0.3140 | time 464.7739 ms | tok/sec 1128049.5903
for step 8156 | loss 3.416910 | norm 0.3097 | time 464.4129 ms | tok/sec 1128926.3688
for step 8157 | loss 3.410773 | norm 0.3489 | time 465.0104 ms | tok/sec 1127475.8475
for step 8158 | loss 3.316970 | norm 0.2920 | time 465.1616 ms | tok/sec 1127109.4666
for step 8159 | loss 3.340250 | norm 0.2775 | time 464.9496 ms | tok/sec 1127623.2760
for step 8160 | loss 3.412110 | norm 0.2939 | time 465.7636 ms | tok/sec 1125652.6595
for step 8161 | loss 3.341543 | norm 0.2980 | time 465.3821 ms | tok/sec 1126575.3473
for step 8162 | loss 3.304539 | norm 0.2751 | time 464.4287 ms | tok/sec 1128888.1188
for step 8163 | loss 3.345327 | norm 0.3049 | time 466.1016 ms | tok/sec 1124836.1898
for step 8164 | loss 3.365526 | norm 0.3248 | time 466.1219 ms | tok/sec 1124787.2853
for step 8165 | loss 3.387432 | norm 0.3330 | time 465.7092 ms | tok/sec 1125784.0502
for step 8166 | loss 3.356707 | norm 0.2962 | time 465.0362 ms | tok/sec 1127413.4188
for step 8167 | loss 3.398750 | norm 0.3367 | time 464.3641 ms | tok/sec 1129045.1919
for step 8168 | loss 3.360845 | norm 0.3854 | time 464.5803 ms | tok/sec 1128519.6614
for step 8169 | loss 3.390801 | norm 0.3482 | time 465.0571 ms | tok/sec 1127362.5560
for step 8170 | loss 3.395682 | norm 0.3187 | time 464.4496 ms | tok/sec 1128837.1230
for step 8171 | loss 3.467891 | norm 0.3513 | time 464.9765 ms | tok/sec 1127557.9402
for step 8172 | loss 3.350706 | norm 0.3287 | time 464.2999 ms | tok/sec 1129201.1490
for step 8173 | loss 3.472265 | norm 0.3609 | time 465.5490 ms | tok/sec 1126171.4853
for step 8174 | loss 3.436498 | norm 0.3870 | time 465.7049 ms | tok/sec 1125794.4244
for step 8175 | loss 3.389315 | norm 0.3457 | time 464.7410 ms | tok/sec 1128129.4516
for step 8176 | loss 3.357978 | norm 0.3211 | time 464.1697 ms | tok/sec 1129517.8338
for step 8177 | loss 3.280792 | norm 0.4047 | time 464.2487 ms | tok/sec 1129325.8297
for step 8178 | loss 3.288940 | norm 0.5021 | time 465.1413 ms | tok/sec 1127158.5733
for step 8179 | loss 3.386615 | norm 0.3453 | time 465.0352 ms | tok/sec 1127415.7308
for step 8180 | loss 3.423593 | norm 0.3717 | time 466.1772 ms | tok/sec 1124653.8264
for step 8181 | loss 3.420939 | norm 0.3588 | time 464.4814 ms | tok/sec 1128760.0584
for step 8182 | loss 3.399738 | norm 0.3664 | time 464.6089 ms | tok/sec 1128450.1681
for step 8183 | loss 3.417631 | norm 0.3316 | time 465.1883 ms | tok/sec 1127044.7679
for step 8184 | loss 3.457190 | norm 0.3296 | time 465.0018 ms | tok/sec 1127496.6586
for step 8185 | loss 3.490140 | norm 0.3889 | time 464.3610 ms | tok/sec 1129052.7279
for step 8186 | loss 3.403493 | norm 0.3367 | time 465.1341 ms | tok/sec 1127175.9060
for step 8187 | loss 3.389805 | norm 0.3724 | time 465.2057 ms | tok/sec 1127002.6023
for step 8188 | loss 3.446234 | norm 0.3399 | time 464.9661 ms | tok/sec 1127583.3798
for step 8189 | loss 3.399965 | norm 0.3573 | time 465.2951 ms | tok/sec 1126786.0474
for step 8190 | loss 3.486802 | norm 0.3772 | time 464.9873 ms | tok/sec 1127531.9236
Will loading at 0 from edu_fineweb10B/edufineweb_train_000044.npy
for step 8191 | loss 3.510292 | norm 0.3374 | time 1408.1221 ms | tok/sec 372331.3581
for step 8192 | loss 3.331297 | norm 0.3642 | time 462.9519 ms | tok/sec 1132489.1454
for step 8193 | loss 3.315873 | norm 0.3287 | time 465.8084 ms | tok/sec 1125544.3429
for step 8194 | loss 3.438154 | norm 0.3017 | time 464.2942 ms | tok/sec 1129215.0655
for step 8195 | loss 3.332812 | norm 0.3089 | time 465.1647 ms | tok/sec 1127101.9566
for step 8196 | loss 3.307847 | norm 0.3086 | time 464.2444 ms | tok/sec 1129336.2693
for step 8197 | loss 3.326775 | norm 0.3041 | time 464.3779 ms | tok/sec 1129011.5711
for step 8198 | loss 3.348144 | norm 0.2929 | time 465.1415 ms | tok/sec 1127157.9955
for step 8199 | loss 3.365874 | norm 0.3149 | time 464.9162 ms | tok/sec 1127704.2336
for step 8200 | loss 3.358872 | norm 0.3049 | time 466.3672 ms | tok/sec 1124195.5904
for step 8201 | loss 3.322492 | norm 0.2998 | time 465.6744 ms | tok/sec 1125868.2024
for step 8202 | loss 3.373872 | norm 0.2744 | time 465.4808 ms | tok/sec 1126336.4568
for step 8203 | loss 3.358149 | norm 0.3247 | time 465.1651 ms | tok/sec 1127100.8012
for step 8204 | loss 3.450709 | norm 0.3450 | time 465.7857 ms | tok/sec 1125599.0747
for step 8205 | loss 3.396525 | norm 0.3365 | time 465.9762 ms | tok/sec 1125138.9169
for step 8206 | loss 3.386081 | norm 0.3639 | time 465.0946 ms | tok/sec 1127271.8237
for step 8207 | loss 3.355809 | norm 0.3669 | time 465.5557 ms | tok/sec 1126155.3369
for step 8208 | loss 3.474164 | norm 0.3371 | time 465.5499 ms | tok/sec 1126169.1784
for step 8209 | loss 3.429883 | norm 0.3392 | time 469.9075 ms | tok/sec 1115725.9146
for step 8210 | loss 3.421086 | norm 0.3088 | time 465.0526 ms | tok/sec 1127373.5374
for step 8211 | loss 3.450973 | norm 0.3423 | time 465.8201 ms | tok/sec 1125516.1149
for step 8212 | loss 3.454228 | norm 0.3692 | time 467.6523 ms | tok/sec 1121106.3788
for step 8213 | loss 3.407933 | norm 0.3483 | time 464.4389 ms | tok/sec 1128863.1998
for step 8214 | loss 3.435514 | norm 0.3726 | time 466.2309 ms | tok/sec 1124524.4246
for step 8215 | loss 3.412700 | norm 0.3289 | time 465.7776 ms | tok/sec 1125618.6643
for step 8216 | loss 3.410460 | norm 0.3641 | time 464.9951 ms | tok/sec 1127512.8455
for step 8217 | loss 3.467417 | norm 0.3483 | time 465.0562 ms | tok/sec 1127364.8679
for step 8218 | loss 3.428395 | norm 0.3150 | time 465.7133 ms | tok/sec 1125774.2524
for step 8219 | loss 3.395159 | norm 0.3481 | time 465.0187 ms | tok/sec 1127455.6152
for step 8220 | loss 3.384244 | norm 0.3206 | time 465.4851 ms | tok/sec 1126326.0726
for step 8221 | loss 3.440510 | norm 0.3382 | time 465.2045 ms | tok/sec 1127005.4902
for step 8222 | loss 3.455218 | norm 0.2972 | time 466.4848 ms | tok/sec 1123912.3263
for step 8223 | loss 3.497096 | norm 0.3363 | time 465.5638 ms | tok/sec 1126135.7287
for step 8224 | loss 3.391924 | norm 0.3186 | time 465.0908 ms | tok/sec 1127281.0696
for step 8225 | loss 3.428587 | norm 0.3354 | time 465.9839 ms | tok/sec 1125120.4954
for step 8226 | loss 3.355279 | norm 0.3143 | time 465.5573 ms | tok/sec 1126151.2998
for step 8227 | loss 3.383270 | norm 0.2996 | time 466.0394 ms | tok/sec 1124986.3819
for step 8228 | loss 3.416577 | norm 0.3256 | time 465.3020 ms | tok/sec 1126769.3040
for step 8229 | loss 3.392487 | norm 0.3044 | time 466.3153 ms | tok/sec 1124320.8926
for step 8230 | loss 3.350795 | norm 0.3031 | time 464.2956 ms | tok/sec 1129211.5863
for step 8231 | loss 3.400650 | norm 0.3354 | time 464.5226 ms | tok/sec 1128659.8323
for step 8232 | loss 3.407773 | norm 0.3694 | time 465.4224 ms | tok/sec 1126477.8170
for step 8233 | loss 3.367211 | norm 0.3266 | time 465.4944 ms | tok/sec 1126303.5740
for step 8234 | loss 3.354989 | norm 0.2960 | time 465.0147 ms | tok/sec 1127465.4422
for step 8235 | loss 3.326439 | norm 0.2821 | time 464.7975 ms | tok/sec 1127992.3055
for step 8236 | loss 3.384540 | norm 0.3008 | time 464.7026 ms | tok/sec 1128222.6375
for step 8237 | loss 3.407332 | norm 0.3101 | time 465.5440 ms | tok/sec 1126183.5970
for step 8238 | loss 3.329352 | norm 0.3865 | time 465.6126 ms | tok/sec 1126017.5171
for step 8239 | loss 3.361495 | norm 0.4126 | time 465.0345 ms | tok/sec 1127417.4649
for step 8240 | loss 3.331776 | norm 0.3415 | time 465.4968 ms | tok/sec 1126297.8053
for step 8241 | loss 3.393781 | norm 0.3636 | time 465.0326 ms | tok/sec 1127422.0890
for step 8242 | loss 3.382114 | norm 0.3593 | time 465.1785 ms | tok/sec 1127068.4514
for step 8243 | loss 3.338751 | norm 0.3111 | time 465.3373 ms | tok/sec 1126683.8625
for step 8244 | loss 3.359047 | norm 0.3260 | time 465.2348 ms | tok/sec 1126932.1406
for step 8245 | loss 3.382022 | norm 0.3101 | time 465.1914 ms | tok/sec 1127037.2587
for step 8246 | loss 3.363917 | norm 0.3565 | time 464.6816 ms | tok/sec 1128273.5778
for step 8247 | loss 3.300934 | norm 0.3701 | time 465.0381 ms | tok/sec 1127408.7947
for step 8248 | loss 3.350669 | norm 0.3130 | time 465.1263 ms | tok/sec 1127194.9727
for step 8249 | loss 3.353410 | norm 0.3495 | time 464.8194 ms | tok/sec 1127939.0763
validation loss 3.3870
HellaSwag accuracy: 2747/10042=0.2736
> Hello, I'm a language model, and if I can give a translation anyway I want the right, but I could not translate. But I don't know
> Hello, I'm a language model, a model of data structure, and you just have to start learning now. This chapter can help you learn how to write
> Hello, I'm a language model, but after some work, I'm trying to make a point here, but I'm not a real language. But maybe
> Hello, I'm a language model, and I'm really a writer. I remember the movie to be based on The Matrix Model - and I'm now a
> Hello, I'm a language model, so I want to show you how to start writing in a language model!
As always during the project planning phase of
> Hello, I'm a language model, how can I answer questions in your own language? Why are you here? What am I doing? Are you keeping the
> Hello, I'm a language model, I'm a data analyst and I can use in my project work as a data analyst. I am here to make sure
> Hello, I'm a language model, and it says I want to be called my model: This doesn't exactly make sense. It takes the language model,
> Hello, I'm a language model, with two languages: French, and Dutch, and so I can type the English words into English.
The first example
> Hello, I'm a language model, and I can't find a definition of one of that language. I want to talk about it. I know what I> 
> Hello, I'm a language model, and I'm a little scared of what comes to mind.
I would like to have a different understanding on how language
> > Hello, I'm a language model, using an object defined by the class as the class. It has a string that consists of a number of elements. Each
> Hello, I'm a language model, and I'll be working on that with those days. I'm sure about you, too. The next thing I'll
Hello, I'm a language model, but this isn't just an exercise in translation: it's the best language models.
So, basically, there's
> > Hello, I'm a language model, especially if I'm using the C code
for instance , I'm using the M in C++ in C++ but
> > Hello, I'm a language model, but my goal is to make the best that you can get. You don't want to be confused with English, and
Hello, I'm a language model, but I wonder where a language model came from. Would it work, maybe, to me?<|endoftext|>It is now more
> Hello, I'm a language model, so now I'm not a computer, so it works by hand. Then the program can write the parameters in the same
Hello, I'm a language model, and i like to speak it a little bit differently.
I don't have any problem fixing it from what I hear
> > Hello, I'm a language model, and I've read what I'm going to do (not just write I'm going to write it out) as an
>>> Hello, I'm a language model, for that's how English comes about. So while I'm going to talk a little more, I'll come back to
Hello, I'm a language model, so I'll use that as the standard.
- "In a new language (I just know there is a way
 Hello, I'm a language model, and I'm not really trying to think of any one that's a general application for your project. As a general framework Hello, I'm a language model, there's a lot of programming in which you want to express a language for a specific language. Just do it. HereHello, I'm a language model, so I should tell you why. I'm looking for a solution that works for language.
I'm going to focus
> > 

> Hello, I'm a language model, so I don' ________. For example, I'm an English speaker. The last time I saw a word and
Hello, I'm a language model, but if I'm going to be able to explain more, and I'm talking about how there should be a certain limit
> Hello, I'm a language model, and want to make a simple, efficient and reusable solution.
MySQL, that's basically the syntax of SQL programming
> Hello, I'm a language model, and so it's a well suited model. I'm so glad to have a conversation with you on this. Please tell
Hello, I'm a language model,
yet to me, to me, I am in no way,
So in this case ... I am not to
> Hello, I'm a language model, and you've got a lot of fun of working with a particular language, especially when you know if I'm really good
> Hello, I'm a language model, and an introduction to the history of the English language in English, so it's possible to talk about our own history in
for step 8250 | loss 3.431770 | norm 0.3701 | time 12933.5513 ms | tok/sec 40537.0487
for step 8251 | loss 3.433388 | norm 0.3563 | time 460.7713 ms | tok/sec 1137848.5914
for step 8252 | loss 3.428496 | norm 0.3285 | time 462.6520 ms | tok/sec 1133223.3218
for step 8253 | loss 3.440163 | norm 0.3465 | time 463.8789 ms | tok/sec 1130226.0868
for step 8254 | loss 3.399324 | norm 0.3556 | time 464.2320 ms | tok/sec 1129366.4293
for step 8255 | loss 3.482370 | norm 0.3353 | time 462.4758 ms | tok/sec 1133655.0506
for step 8256 | loss 3.381536 | norm 0.3516 | time 462.7032 ms | tok/sec 1133097.7790
for step 8257 | loss 3.373946 | norm 0.3249 | time 463.5339 ms | tok/sec 1131067.2736
for step 8258 | loss 3.419782 | norm 0.3244 | time 463.3429 ms | tok/sec 1131533.4589
for step 8259 | loss 3.380545 | norm 0.3257 | time 463.0926 ms | tok/sec 1132145.1455
for step 8260 | loss 3.418710 | norm 0.3195 | time 464.0229 ms | tok/sec 1129875.3325
for step 8261 | loss 3.395664 | norm 0.3616 | time 463.2199 ms | tok/sec 1131833.9766
for step 8262 | loss 3.339492 | norm 0.3207 | time 463.4626 ms | tok/sec 1131241.2479
for step 8263 | loss 3.382555 | norm 0.3598 | time 463.6281 ms | tok/sec 1130837.5234
for step 8264 | loss 3.349194 | norm 0.3212 | time 464.7849 ms | tok/sec 1128022.9724
for step 8265 | loss 3.338169 | norm 0.3297 | time 463.3296 ms | tok/sec 1131566.0655
for step 8266 | loss 3.356180 | norm 0.3465 | time 463.8510 ms | tok/sec 1130294.0561
for step 8267 | loss 3.349110 | norm 0.3116 | time 464.8092 ms | tok/sec 1127963.9545
for step 8268 | loss 3.333972 | norm 0.3454 | time 465.3559 ms | tok/sec 1126638.8377
for step 8269 | loss 3.363081 | norm 0.3432 | time 464.6380 ms | tok/sec 1128379.5254
for step 8270 | loss 3.360645 | norm 0.3248 | time 465.0321 ms | tok/sec 1127423.2451
for step 8271 | loss 3.380648 | norm 0.3381 | time 464.1852 ms | tok/sec 1129480.1239
for step 8272 | loss 3.324152 | norm 0.2863 | time 464.6680 ms | tok/sec 1128306.5757
for step 8273 | loss 3.375584 | norm 0.3164 | time 463.9661 ms | tok/sec 1130013.5177
for step 8274 | loss 3.378346 | norm 0.3018 | time 464.6614 ms | tok/sec 1128322.7860
for step 8275 | loss 3.324389 | norm 0.3309 | time 464.5972 ms | tok/sec 1128478.5435
for step 8276 | loss 3.344543 | norm 0.3678 | time 464.5555 ms | tok/sec 1128579.8959
for step 8277 | loss 3.407709 | norm 0.3176 | time 465.7266 ms | tok/sec 1125741.9788
for step 8278 | loss 3.357209 | norm 0.3400 | time 464.9916 ms | tok/sec 1127521.5173
for step 8279 | loss 3.354596 | norm 0.3071 | time 464.3922 ms | tok/sec 1128976.7931
for step 8280 | loss 3.348796 | norm 0.3199 | time 465.8859 ms | tok/sec 1125357.1426
for step 8281 | loss 3.330243 | norm 0.3493 | time 464.1488 ms | tok/sec 1129568.8912
for step 8282 | loss 3.401156 | norm 0.3324 | time 464.7322 ms | tok/sec 1128150.8657
for step 8283 | loss 3.379426 | norm 0.3584 | time 463.9957 ms | tok/sec 1129941.5178
for step 8284 | loss 3.368138 | norm 0.3479 | time 465.2274 ms | tok/sec 1126950.0440
for step 8285 | loss 3.424983 | norm 0.3686 | time 465.0121 ms | tok/sec 1127471.8010
for step 8286 | loss 3.401165 | norm 0.3516 | time 464.9301 ms | tok/sec 1127670.6926
for step 8287 | loss 3.444424 | norm 0.3810 | time 464.8256 ms | tok/sec 1127924.0342
for step 8288 | loss 3.469701 | norm 0.3886 | time 465.5733 ms | tok/sec 1126112.6611
for step 8289 | loss 3.396846 | norm 0.3372 | time 465.8563 ms | tok/sec 1125428.5594
for step 8290 | loss 3.387548 | norm 0.3543 | time 464.9806 ms | tok/sec 1127548.1116
for step 8291 | loss 3.409350 | norm 0.2974 | time 464.2427 ms | tok/sec 1129340.3292
for step 8292 | loss 3.369116 | norm 0.3433 | time 467.6766 ms | tok/sec 1121048.0825
for step 8293 | loss 3.394238 | norm 0.3028 | time 464.7162 ms | tok/sec 1128189.6444
for step 8294 | loss 3.384263 | norm 0.3347 | time 464.9799 ms | tok/sec 1127549.8460
for step 8295 | loss 3.498829 | norm 0.3115 | time 466.0480 ms | tok/sec 1124965.6634
for step 8296 | loss 3.381982 | norm 0.3230 | time 465.7423 ms | tok/sec 1125703.9443
for step 8297 | loss 3.441394 | norm 0.3659 | time 465.1768 ms | tok/sec 1127072.4950
for step 8298 | loss 3.360323 | norm 0.3658 | time 464.5565 ms | tok/sec 1128577.5790
for step 8299 | loss 3.404526 | norm 0.3582 | time 465.2376 ms | tok/sec 1126925.2104
for step 8300 | loss 3.443445 | norm 0.3484 | time 464.5438 ms | tok/sec 1128608.2778
for step 8301 | loss 3.354395 | norm 0.3081 | time 465.7369 ms | tok/sec 1125717.1984
for step 8302 | loss 3.349083 | norm 0.3478 | time 466.1410 ms | tok/sec 1124741.2614
for step 8303 | loss 3.334672 | norm 0.3498 | time 465.0028 ms | tok/sec 1127494.3462
for step 8304 | loss 3.358191 | norm 0.3245 | time 465.1146 ms | tok/sec 1127223.2850
for step 8305 | loss 3.262056 | norm 0.3303 | time 464.9425 ms | tok/sec 1127640.6231
for step 8306 | loss 3.360685 | norm 0.3048 | time 465.4665 ms | tok/sec 1126371.0724
for step 8307 | loss 3.367623 | norm 0.3545 | time 464.0150 ms | tok/sec 1129894.4906
for step 8308 | loss 3.372259 | norm 0.3355 | time 464.8023 ms | tok/sec 1127980.7335
for step 8309 | loss 3.391403 | norm 0.3186 | time 465.3106 ms | tok/sec 1126748.5197
for step 8310 | loss 3.370192 | norm 0.3735 | time 465.7900 ms | tok/sec 1125588.7041
for step 8311 | loss 3.323693 | norm 0.3669 | time 466.1312 ms | tok/sec 1124764.8481
for step 8312 | loss 3.399769 | norm 0.2958 | time 464.8154 ms | tok/sec 1127948.9118
for step 8313 | loss 3.445541 | norm 0.3676 | time 464.9925 ms | tok/sec 1127519.2048
for step 8314 | loss 3.399190 | norm 0.3389 | time 464.9556 ms | tok/sec 1127608.8206
for step 8315 | loss 3.383555 | norm 0.3713 | time 465.2822 ms | tok/sec 1126817.2262
for step 8316 | loss 3.347840 | norm 0.3337 | time 466.0425 ms | tok/sec 1124978.9001
for step 8317 | loss 3.386866 | norm 0.3421 | time 465.8606 ms | tok/sec 1125418.1918
for step 8318 | loss 3.360628 | norm 0.3498 | time 464.4718 ms | tok/sec 1128783.2346
for step 8319 | loss 3.361149 | norm 0.3553 | time 465.8232 ms | tok/sec 1125508.6260
for step 8320 | loss 3.438670 | norm 0.3524 | time 465.2903 ms | tok/sec 1126797.5949
for step 8321 | loss 3.495054 | norm 0.4045 | time 464.5836 ms | tok/sec 1128511.5534
for step 8322 | loss 3.415067 | norm 0.3593 | time 465.1980 ms | tok/sec 1127021.0855
for step 8323 | loss 3.422586 | norm 0.3417 | time 465.1783 ms | tok/sec 1127069.0291
for step 8324 | loss 3.410689 | norm 0.3267 | time 464.9482 ms | tok/sec 1127626.7454
for step 8325 | loss 3.456006 | norm 0.3222 | time 465.1890 ms | tok/sec 1127043.0350
for step 8326 | loss 3.388009 | norm 0.3296 | time 464.6890 ms | tok/sec 1128255.6324
for step 8327 | loss 3.423341 | norm 0.3085 | time 465.6961 ms | tok/sec 1125815.7499
for step 8328 | loss 3.476432 | norm 0.3708 | time 465.1346 ms | tok/sec 1127174.7505
for step 8329 | loss 3.375551 | norm 0.3565 | time 465.1167 ms | tok/sec 1127218.0847
for step 8330 | loss 3.447896 | norm 0.3614 | time 464.9069 ms | tok/sec 1127726.7882
for step 8331 | loss 3.371447 | norm 0.3756 | time 464.9956 ms | tok/sec 1127511.6893
for step 8332 | loss 3.297396 | norm 0.3422 | time 465.0638 ms | tok/sec 1127346.3734
for step 8333 | loss 3.396938 | norm 0.3356 | time 464.5941 ms | tok/sec 1128486.0719
for step 8334 | loss 3.349808 | norm 0.3170 | time 464.3011 ms | tok/sec 1129198.2498
for step 8335 | loss 3.346806 | norm 0.3485 | time 465.3561 ms | tok/sec 1126638.2605
for step 8336 | loss 3.346425 | norm 0.3223 | time 465.4722 ms | tok/sec 1126357.2259
for step 8337 | loss 3.307351 | norm 0.3378 | time 465.4324 ms | tok/sec 1126453.5813
for step 8338 | loss 3.338105 | norm 0.3029 | time 465.0009 ms | tok/sec 1127498.9710
for step 8339 | loss 3.363023 | norm 0.3223 | time 465.3306 ms | tok/sec 1126700.0261
for step 8340 | loss 3.428395 | norm 0.3196 | time 465.2994 ms | tok/sec 1126775.6549
for step 8341 | loss 3.371084 | norm 0.2898 | time 464.4556 ms | tok/sec 1128822.6364
for step 8342 | loss 3.332366 | norm 0.2990 | time 464.9861 ms | tok/sec 1127534.8143
for step 8343 | loss 3.413063 | norm 0.3279 | time 465.1194 ms | tok/sec 1127211.7288
for step 8344 | loss 3.376017 | norm 0.2834 | time 465.0047 ms | tok/sec 1127489.7215
for step 8345 | loss 3.416527 | norm 0.3256 | time 465.9109 ms | tok/sec 1125296.6758
for step 8346 | loss 3.436419 | norm 0.4883 | time 465.1628 ms | tok/sec 1127106.5781
for step 8347 | loss 3.396334 | norm 0.3897 | time 465.9462 ms | tok/sec 1125211.4575
for step 8348 | loss 3.414808 | norm 0.3833 | time 466.2685 ms | tok/sec 1124433.5735
for step 8349 | loss 3.347876 | norm 0.3946 | time 464.8790 ms | tok/sec 1127794.4572
for step 8350 | loss 3.493556 | norm 0.3475 | time 464.5474 ms | tok/sec 1128599.5893
for step 8351 | loss 3.435384 | norm 0.3655 | time 464.0396 ms | tok/sec 1129834.6963
for step 8352 | loss 3.375973 | norm 0.3866 | time 465.5373 ms | tok/sec 1126199.7463
for step 8353 | loss 3.421812 | norm 0.3176 | time 465.9553 ms | tok/sec 1125189.5792
for step 8354 | loss 3.360978 | norm 0.3589 | time 465.0662 ms | tok/sec 1127340.5940
for step 8355 | loss 3.417779 | norm 0.3539 | time 465.1127 ms | tok/sec 1127227.9076
for step 8356 | loss 3.444623 | norm 0.3788 | time 464.6189 ms | tok/sec 1128425.8475
for step 8357 | loss 3.416388 | norm 0.3187 | time 465.0323 ms | tok/sec 1127422.6670
for step 8358 | loss 3.432189 | norm 0.3410 | time 464.5324 ms | tok/sec 1128636.0819
for step 8359 | loss 3.421209 | norm 0.3080 | time 464.4370 ms | tok/sec 1128867.8358
for step 8360 | loss 3.451035 | norm 0.3443 | time 464.3557 ms | tok/sec 1129065.4813
for step 8361 | loss 3.395358 | norm 0.3115 | time 464.4246 ms | tok/sec 1128897.9709
for step 8362 | loss 3.489842 | norm 0.3057 | time 464.8924 ms | tok/sec 1127762.0676
for step 8363 | loss 3.417328 | norm 0.3178 | time 464.7632 ms | tok/sec 1128075.6308
for step 8364 | loss 3.419963 | norm 0.3207 | time 464.6072 ms | tok/sec 1128454.2217
for step 8365 | loss 3.388783 | norm 0.2882 | time 464.4959 ms | tok/sec 1128724.7165
for step 8366 | loss 3.460782 | norm 0.3402 | time 465.0650 ms | tok/sec 1127343.4837
for step 8367 | loss 3.421947 | norm 0.2978 | time 465.9388 ms | tok/sec 1125229.3062
for step 8368 | loss 3.379648 | norm 0.3232 | time 465.3814 ms | tok/sec 1126577.0788
for step 8369 | loss 3.420782 | norm 0.2891 | time 464.6258 ms | tok/sec 1128409.0553
for step 8370 | loss 3.298893 | norm 0.3184 | time 464.8921 ms | tok/sec 1127762.6459
for step 8371 | loss 3.383939 | norm 0.3024 | time 465.4400 ms | tok/sec 1126435.1167
for step 8372 | loss 3.338109 | norm 0.3142 | time 465.5683 ms | tok/sec 1126124.7714
for step 8373 | loss 3.426917 | norm 0.3311 | time 465.3172 ms | tok/sec 1126732.3547
for step 8374 | loss 3.381245 | norm 0.3148 | time 465.0054 ms | tok/sec 1127487.9872
for step 8375 | loss 3.339558 | norm 0.3220 | time 464.9792 ms | tok/sec 1127551.5805
for step 8376 | loss 3.316923 | norm 0.3393 | time 465.4753 ms | tok/sec 1126349.7259
for step 8377 | loss 3.337725 | norm 0.3178 | time 464.9265 ms | tok/sec 1127679.3668
for step 8378 | loss 3.307232 | norm 0.3603 | time 465.1103 ms | tok/sec 1127233.6858
for step 8379 | loss 3.317177 | norm 0.3314 | time 465.2269 ms | tok/sec 1126951.1990
for step 8380 | loss 3.395966 | norm 0.3137 | time 465.4148 ms | tok/sec 1126496.2830
Will loading at 0 from edu_fineweb10B/edufineweb_train_000045.npy
for step 8381 | loss 3.363644 | norm 0.3563 | time 1450.4235 ms | tok/sec 361472.3614
for step 8382 | loss 3.408917 | norm 0.3401 | time 462.6148 ms | tok/sec 1133314.4306
for step 8383 | loss 3.354502 | norm 0.4513 | time 464.6573 ms | tok/sec 1128332.6281
for step 8384 | loss 3.401506 | norm 0.4229 | time 464.0529 ms | tok/sec 1129802.1894
for step 8385 | loss 3.405133 | norm 0.3999 | time 464.5996 ms | tok/sec 1128472.7525
for step 8386 | loss 3.412245 | norm 0.3867 | time 465.2169 ms | tok/sec 1126975.4562
for step 8387 | loss 3.345879 | norm 0.4123 | time 465.3635 ms | tok/sec 1126620.3670
for step 8388 | loss 3.390931 | norm 0.4041 | time 465.2894 ms | tok/sec 1126799.9045
for step 8389 | loss 3.422781 | norm 0.3256 | time 464.8931 ms | tok/sec 1127760.3325
for step 8390 | loss 3.371207 | norm 0.3815 | time 464.9203 ms | tok/sec 1127694.4024
for step 8391 | loss 3.442196 | norm 0.4049 | time 464.6020 ms | tok/sec 1128466.9615
for step 8392 | loss 3.477443 | norm 0.3331 | time 466.0482 ms | tok/sec 1124965.0879
for step 8393 | loss 3.415247 | norm 0.3826 | time 464.8693 ms | tok/sec 1127818.1722
for step 8394 | loss 3.441313 | norm 0.3434 | time 465.8971 ms | tok/sec 1125330.0757
for step 8395 | loss 3.448824 | norm 0.3487 | time 465.6012 ms | tok/sec 1126045.1937
for step 8396 | loss 3.392577 | norm 0.3331 | time 465.6446 ms | tok/sec 1125940.2605
for step 8397 | loss 3.444343 | norm 0.3083 | time 465.1425 ms | tok/sec 1127155.6845
for step 8398 | loss 3.384729 | norm 0.3155 | time 466.1417 ms | tok/sec 1124739.5356
for step 8399 | loss 3.429568 | norm 0.3098 | time 465.1084 ms | tok/sec 1127238.3084
for step 8400 | loss 3.387759 | norm 0.2968 | time 466.4965 ms | tok/sec 1123884.1801
for step 8401 | loss 3.421817 | norm 0.2983 | time 465.4851 ms | tok/sec 1126326.0726
for step 8402 | loss 3.339908 | norm 0.2910 | time 465.2140 ms | tok/sec 1126982.3869
for step 8403 | loss 3.378139 | norm 0.3150 | time 465.5790 ms | tok/sec 1126098.8209
for step 8404 | loss 3.352252 | norm 0.3050 | time 465.1148 ms | tok/sec 1127222.7072
for step 8405 | loss 3.328288 | norm 0.3282 | time 465.5180 ms | tok/sec 1126246.4664
for step 8406 | loss 3.350733 | norm 0.3079 | time 465.0884 ms | tok/sec 1127286.8484
for step 8407 | loss 3.398953 | norm 0.3128 | time 464.7219 ms | tok/sec 1128175.7532
for step 8408 | loss 3.440184 | norm 0.3074 | time 465.0767 ms | tok/sec 1127315.1654
for step 8409 | loss 3.359132 | norm 0.2969 | time 464.8337 ms | tok/sec 1127904.3643
for step 8410 | loss 3.376762 | norm 0.3109 | time 465.6894 ms | tok/sec 1125831.8886
for step 8411 | loss 3.400776 | norm 0.3251 | time 465.0078 ms | tok/sec 1127482.2064
for step 8412 | loss 3.399278 | norm 0.3128 | time 464.5362 ms | tok/sec 1128626.8137
for step 8413 | loss 3.409142 | norm 0.2963 | time 465.8713 ms | tok/sec 1125392.2739
for step 8414 | loss 3.358675 | norm 0.3246 | time 466.0709 ms | tok/sec 1124910.4176
for step 8415 | loss 3.497048 | norm 0.3899 | time 465.8561 ms | tok/sec 1125429.1353
for step 8416 | loss 3.341048 | norm 0.3684 | time 465.2295 ms | tok/sec 1126944.8462
for step 8417 | loss 3.377362 | norm 0.3468 | time 466.3272 ms | tok/sec 1124292.1511
for step 8418 | loss 3.324678 | norm 0.3382 | time 465.3428 ms | tok/sec 1126670.5856
for step 8419 | loss 3.331382 | norm 0.3332 | time 465.2438 ms | tok/sec 1126910.1954
for step 8420 | loss 3.384655 | norm 0.3399 | time 464.8356 ms | tok/sec 1127899.7362
for step 8421 | loss 3.372188 | norm 0.3486 | time 465.1048 ms | tok/sec 1127246.9760
for step 8422 | loss 3.379441 | norm 0.3716 | time 465.1940 ms | tok/sec 1127030.9049
for step 8423 | loss 3.387117 | norm 0.3178 | time 465.5509 ms | tok/sec 1126166.8715
for step 8424 | loss 3.377962 | norm 0.3510 | time 465.8871 ms | tok/sec 1125354.2631
for step 8425 | loss 3.371751 | norm 0.3397 | time 465.4155 ms | tok/sec 1126494.5518
for step 8426 | loss 3.397844 | norm 0.3295 | time 465.8139 ms | tok/sec 1125531.0928
for step 8427 | loss 3.401085 | norm 0.3220 | time 466.2542 ms | tok/sec 1124468.0722
for step 8428 | loss 3.409206 | norm 0.3634 | time 465.1008 ms | tok/sec 1127256.7994
for step 8429 | loss 3.461886 | norm 0.3513 | time 465.9584 ms | tok/sec 1125182.0947
for step 8430 | loss 3.428780 | norm 0.3473 | time 465.5051 ms | tok/sec 1126277.6153
for step 8431 | loss 3.436557 | norm 0.3470 | time 465.6250 ms | tok/sec 1125987.5357
for step 8432 | loss 3.432312 | norm 0.3141 | time 464.9267 ms | tok/sec 1127678.7886
for step 8433 | loss 3.438757 | norm 0.3369 | time 465.2038 ms | tok/sec 1127007.2230
for step 8434 | loss 3.452048 | norm 0.3205 | time 465.5828 ms | tok/sec 1126089.5944
for step 8435 | loss 3.351859 | norm 0.3176 | time 465.3354 ms | tok/sec 1126688.4806
for step 8436 | loss 3.360583 | norm 0.3097 | time 465.0519 ms | tok/sec 1127375.2713
for step 8437 | loss 3.348913 | norm 0.3289 | time 466.0537 ms | tok/sec 1124951.8514
for step 8438 | loss 3.349579 | norm 0.3295 | time 465.5573 ms | tok/sec 1126151.2998
for step 8439 | loss 3.331034 | norm 0.3243 | time 464.3962 ms | tok/sec 1128966.9397
for step 8440 | loss 3.421931 | norm 0.3431 | time 465.6653 ms | tok/sec 1125890.1071
for step 8441 | loss 3.350801 | norm 0.3597 | time 465.3764 ms | tok/sec 1126589.1992
for step 8442 | loss 3.368050 | norm 0.3161 | time 465.2190 ms | tok/sec 1126970.2581
for step 8443 | loss 3.427763 | norm 0.3552 | time 465.4994 ms | tok/sec 1126291.4598
for step 8444 | loss 3.337033 | norm 0.3470 | time 465.9076 ms | tok/sec 1125304.7377
for step 8445 | loss 3.395049 | norm 0.3456 | time 465.5428 ms | tok/sec 1126186.4808
for step 8446 | loss 3.372919 | norm 0.3151 | time 465.1675 ms | tok/sec 1127095.0243
for step 8447 | loss 3.359400 | norm 0.3011 | time 465.1625 ms | tok/sec 1127107.1558
for step 8448 | loss 3.357977 | norm 0.3120 | time 465.1229 ms | tok/sec 1127203.0618
for step 8449 | loss 3.338753 | norm 0.3356 | time 465.8012 ms | tok/sec 1125561.6260
for step 8450 | loss 3.334560 | norm 0.3823 | time 466.3160 ms | tok/sec 1124319.1681
for step 8451 | loss 3.366002 | norm 0.3591 | time 464.7112 ms | tok/sec 1128201.7995
for step 8452 | loss 3.344645 | norm 0.3264 | time 464.8156 ms | tok/sec 1127948.3332
for step 8453 | loss 3.369905 | norm 0.3217 | time 466.3141 ms | tok/sec 1124323.7668
for step 8454 | loss 3.327710 | norm 0.3104 | time 465.3354 ms | tok/sec 1126688.4806
for step 8455 | loss 3.354457 | norm 0.3205 | time 464.9491 ms | tok/sec 1127624.4325
for step 8456 | loss 3.420867 | norm 0.3123 | time 465.5974 ms | tok/sec 1126054.4195
for step 8457 | loss 3.413151 | norm 0.3475 | time 465.1678 ms | tok/sec 1127094.4466
for step 8458 | loss 3.366504 | norm 0.4090 | time 465.3332 ms | tok/sec 1126693.6761
for step 8459 | loss 3.410005 | norm 0.3132 | time 465.9226 ms | tok/sec 1125268.4602
for step 8460 | loss 3.421149 | norm 0.3851 | time 465.3490 ms | tok/sec 1126655.5773
for step 8461 | loss 3.414397 | norm 0.3196 | time 465.3566 ms | tok/sec 1126637.1061
for step 8462 | loss 3.402204 | norm 0.3443 | time 465.3625 ms | tok/sec 1126622.6758
for step 8463 | loss 3.391795 | norm 0.3123 | time 466.4459 ms | tok/sec 1124005.9658
for step 8464 | loss 3.385383 | norm 0.3274 | time 465.8237 ms | tok/sec 1125507.4739
for step 8465 | loss 3.363676 | norm 0.3247 | time 464.9789 ms | tok/sec 1127552.1586
for step 8466 | loss 3.348728 | norm 0.3292 | time 465.0745 ms | tok/sec 1127320.3666
for step 8467 | loss 3.447449 | norm 0.3360 | time 464.9773 ms | tok/sec 1127556.2057
for step 8468 | loss 3.405190 | norm 0.3304 | time 466.1224 ms | tok/sec 1124786.1346
for step 8469 | loss 3.396494 | norm 0.3137 | time 465.7381 ms | tok/sec 1125714.3171
for step 8470 | loss 3.422533 | norm 0.3435 | time 465.9355 ms | tok/sec 1125237.3671
for step 8471 | loss 3.382624 | norm 0.3078 | time 466.2235 ms | tok/sec 1124542.2515
for step 8472 | loss 3.345632 | norm 0.3186 | time 465.3556 ms | tok/sec 1126639.4149
for step 8473 | loss 3.413990 | norm 0.3343 | time 466.0800 ms | tok/sec 1124888.5510
for step 8474 | loss 3.374487 | norm 0.3254 | time 465.1179 ms | tok/sec 1127215.1956
for step 8475 | loss 3.346078 | norm 0.3422 | time 466.6731 ms | tok/sec 1123458.7120
for step 8476 | loss 3.399151 | norm 0.2952 | time 465.4572 ms | tok/sec 1126393.5736
for step 8477 | loss 3.364518 | norm 0.3339 | time 465.0831 ms | tok/sec 1127299.5620
for step 8478 | loss 3.386407 | norm 0.2974 | time 465.8432 ms | tok/sec 1125460.2390
for step 8479 | loss 3.429113 | norm 0.3168 | time 465.9646 ms | tok/sec 1125167.1260
for step 8480 | loss 3.345520 | norm 0.3125 | time 466.0046 ms | tok/sec 1125070.4149
for step 8481 | loss 3.401021 | norm 0.3142 | time 465.3065 ms | tok/sec 1126758.3344
for step 8482 | loss 3.333291 | norm 0.3148 | time 466.0807 ms | tok/sec 1124886.8248
for step 8483 | loss 3.349518 | norm 0.3115 | time 465.3091 ms | tok/sec 1126751.9837
for step 8484 | loss 3.385964 | norm 0.3318 | time 465.9224 ms | tok/sec 1125269.0360
for step 8485 | loss 3.388914 | norm 0.3239 | time 465.7166 ms | tok/sec 1125766.1838
for step 8486 | loss 3.388322 | norm 0.3300 | time 466.3551 ms | tok/sec 1124224.9017
for step 8487 | loss 3.464139 | norm 0.3363 | time 465.5509 ms | tok/sec 1126166.8715
for step 8488 | loss 3.366166 | norm 0.3693 | time 465.6920 ms | tok/sec 1125825.5483
for step 8489 | loss 3.374581 | norm 0.3520 | time 465.1754 ms | tok/sec 1127075.9610
for step 8490 | loss 3.374107 | norm 0.3542 | time 465.5032 ms | tok/sec 1126282.2301
for step 8491 | loss 3.390006 | norm 0.3637 | time 464.8800 ms | tok/sec 1127792.1436
for step 8492 | loss 3.360191 | norm 0.3263 | time 464.1092 ms | tok/sec 1129665.2166
for step 8493 | loss 3.357370 | norm 0.3117 | time 465.6134 ms | tok/sec 1126015.7874
for step 8494 | loss 3.325672 | norm 0.3083 | time 465.9631 ms | tok/sec 1125170.5803
for step 8495 | loss 3.351279 | norm 0.2971 | time 465.4226 ms | tok/sec 1126477.2399
for step 8496 | loss 3.435984 | norm 0.3061 | time 465.2593 ms | tok/sec 1126872.6594
for step 8497 | loss 3.370246 | norm 0.3027 | time 465.7423 ms | tok/sec 1125703.9443
for step 8498 | loss 3.374303 | norm 0.2943 | time 465.3442 ms | tok/sec 1126667.1221
for step 8499 | loss 3.370698 | norm 0.3158 | time 464.9069 ms | tok/sec 1127726.7882
validation loss 3.3754
HellaSwag accuracy: 2739/10042=0.2728
> Hello, I'm a language model, and the default is in English. All the variables you get right now are available in your current variable. I'm going
> Hello, I'm a language model, a model, and a model. This may be a computer or a digital image. All the previous languages have been created
> Hello, I'm a language model, but because of all the changes I've seen, I'll take it one step further.<|endoftext|>What does a dog grow
> Hello, I'm a language model, and I'm very excited to hear if everything is going alright. (So just to understand:
- How I'm
> Hello, I'm a language model, so I need to figure out how to get to that.
I just finished watching how these changes have worked out for
> Hello, I'm a language model, how can you solve these problems to produce real world things?
Let's say this is an example of the programming of
> Hello, I'm a language model, I got it too fast and I'll have to write it for it.
As far as we may be going to
> Hello, I'm a language model, and it only supports those things. They just make more like code, "I'm a model!" and it basically shows
> Hello, I'm a language model, and I will be a translation service with various APIs. I'll help you get the translation APIs that are used in other
> Hello, I'm a language model, so there's a lot of people teaching it to our parents, because even in my opinion, I'd never use that
> Hello, I'm a language model, so I might want to create a lot of examples, but if the user is trying to make a distinction between a person
> Hello, I'm a language model, and like to have a very simple definition of word you can use and write the rest of the word at the beginning but
> Hello, I'm a language model, like my mother's tongue. I want you to start using this approach in all your language...
There's a language
> Hello, I'm a language model, and I have a different model to my class. This is a little too complicated. The first version of it is much
> Hello, I'm a language model, but what's my purpose here are the different models and I am hoping that my students will come to grips with. I> 
>> Hello, I'm a language model, and I know that there is a lot when you have to take a while to say a single thing to the language model
 Hello, I'm a language model, but my job is to help you understand and use different language models of different kinds of people. For example, language models> Hello, I'm a language model, we use it at the heart of this language. So we have a really simple and non-standard language:
In

> Hello, I'm a language model, and we can go through it by looking at a few examples. Each of us is going to be following along well!
> > Hello, I'm a language model, to simplify my code to just two types of expressions. Just to see how I make an abstract expression like
(define
Hello, I'm a language model, this is when a sentence is presented after adding a negative verb, it must be a negative. I like this because they
Hello, I'm a language model, where there are three language layers that you can change or delete from language layer space
The language layer in any language is
> > > > Hello, I'm a language model, and I've spent too much time at a speed without being able to create a model for my own language, because my
Hello, I'm a language model, so I can understand how to say these if you want to translate them at your own pace, rather than a set-
Hello, I'm a language model, and I'm not saying "is..."<|endoftext|>A couple of decades ago today, in November 2007, New York was born
Hello, I'm a language model, and you'll see to it which isn't really the same as in the "Helloness" section.
I will
> > > > Hello, I'm a language model, here is a list of ways you can be doing this. I'll also be looking at the use of the verb suffix
Hello, I'm a language model, but if I'm going to be a doctor, so I should be a better speaker than, a speaker for the speaker
Hello, I'm a language model, and the model is a nice thing to look at. Now, let's say you're going to take a few notes
Hello, I'm a language model, one that I have been looking at for some time...I'll do my first test, but still want to see more
> Hello, I'm a language model, and you will want to define these things like “you mean” for a language you learn to use; for
> Hello, I'm a language model, and there's some of it on the internet, I just don't want to learn. In fact, it's really
for step 8500 | loss 3.368435 | norm 0.3188 | time 12905.9980 ms | tok/sec 40623.5923
for step 8501 | loss 3.487547 | norm 0.2966 | time 462.2152 ms | tok/sec 1134294.1920
for step 8502 | loss 3.365145 | norm 0.3212 | time 462.7874 ms | tok/sec 1132891.7157
for step 8503 | loss 3.385807 | norm 0.3357 | time 463.4907 ms | tok/sec 1131172.5828
for step 8504 | loss 3.420583 | norm 0.3000 | time 462.8737 ms | tok/sec 1132680.4765
for step 8505 | loss 3.382299 | norm 0.3043 | time 463.9421 ms | tok/sec 1130072.1695
for step 8506 | loss 3.364764 | norm 0.3379 | time 464.1027 ms | tok/sec 1129680.8855
for step 8507 | loss 3.432779 | norm 0.3400 | time 464.1576 ms | tok/sec 1129547.4233
for step 8508 | loss 3.432451 | norm 0.3209 | time 463.5603 ms | tok/sec 1131002.7015
for step 8509 | loss 3.356607 | norm 0.3253 | time 463.5341 ms | tok/sec 1131066.6919
for step 8510 | loss 3.362693 | norm 0.3449 | time 463.0861 ms | tok/sec 1132160.8833
for step 8511 | loss 3.422068 | norm 0.3294 | time 463.8462 ms | tok/sec 1130305.6756
for step 8512 | loss 3.345992 | norm 0.3380 | time 464.4368 ms | tok/sec 1128868.4154
for step 8513 | loss 3.311496 | norm 0.3301 | time 464.0477 ms | tok/sec 1129814.9597
for step 8514 | loss 3.355400 | norm 0.3206 | time 463.4175 ms | tok/sec 1131351.2459
for step 8515 | loss 3.335712 | norm 0.3153 | time 464.4053 ms | tok/sec 1128944.9151
for step 8516 | loss 3.341184 | norm 0.3119 | time 463.8271 ms | tok/sec 1130352.1560
for step 8517 | loss 3.368700 | norm 0.3050 | time 464.1917 ms | tok/sec 1129464.4605
for step 8518 | loss 3.331552 | norm 0.3258 | time 464.6084 ms | tok/sec 1128451.3263
for step 8519 | loss 3.358555 | norm 0.3122 | time 464.5820 ms | tok/sec 1128515.6074
for step 8520 | loss 3.366890 | norm 0.3271 | time 464.3526 ms | tok/sec 1129073.0175
for step 8521 | loss 3.368986 | norm 0.3738 | time 465.2905 ms | tok/sec 1126797.0176
for step 8522 | loss 3.366839 | norm 0.3306 | time 465.0693 ms | tok/sec 1127333.0809
for step 8523 | loss 3.356431 | norm 0.3255 | time 465.0466 ms | tok/sec 1127387.9868
for step 8524 | loss 3.367352 | norm 0.3586 | time 466.0809 ms | tok/sec 1124886.2493
for step 8525 | loss 3.306334 | norm 0.3429 | time 464.8404 ms | tok/sec 1127888.1661
for step 8526 | loss 3.381485 | norm 0.3298 | time 465.0447 ms | tok/sec 1127392.6107
for step 8527 | loss 3.387682 | norm 0.3289 | time 464.8385 ms | tok/sec 1127892.7942
for step 8528 | loss 3.346030 | norm 0.3630 | time 465.6942 ms | tok/sec 1125820.3609
for step 8529 | loss 3.372833 | norm 0.3231 | time 465.8751 ms | tok/sec 1125383.0589
for step 8530 | loss 3.408453 | norm 0.3266 | time 465.3990 ms | tok/sec 1126534.3710
for step 8531 | loss 3.397709 | norm 0.3336 | time 465.6420 ms | tok/sec 1125946.6021
for step 8532 | loss 3.421669 | norm 0.3536 | time 464.8886 ms | tok/sec 1127771.3216
for step 8533 | loss 3.415329 | norm 0.3238 | time 464.7877 ms | tok/sec 1128016.0288
for step 8534 | loss 3.404983 | norm 0.3375 | time 465.3385 ms | tok/sec 1126680.9762
for step 8535 | loss 3.382080 | norm 0.2967 | time 464.9251 ms | tok/sec 1127682.8366
for step 8536 | loss 3.368317 | norm 0.3229 | time 466.2392 ms | tok/sec 1124504.2981
for step 8537 | loss 3.383312 | norm 0.3087 | time 465.9970 ms | tok/sec 1125088.8348
for step 8538 | loss 3.359681 | norm 0.2900 | time 465.6417 ms | tok/sec 1125947.1786
for step 8539 | loss 3.400594 | norm 0.2908 | time 465.4686 ms | tok/sec 1126365.8799
for step 8540 | loss 3.344919 | norm 0.3051 | time 466.0761 ms | tok/sec 1124897.7579
for step 8541 | loss 3.447542 | norm 0.3069 | time 465.1978 ms | tok/sec 1127021.6631
for step 8542 | loss 3.387838 | norm 0.3148 | time 465.5249 ms | tok/sec 1126229.7390
for step 8543 | loss 3.358596 | norm 0.3312 | time 465.0576 ms | tok/sec 1127361.4001
for step 8544 | loss 3.447996 | norm 0.3257 | time 466.0287 ms | tok/sec 1125012.2812
for step 8545 | loss 3.370333 | norm 0.3136 | time 464.4325 ms | tok/sec 1128878.8465
for step 8546 | loss 3.336877 | norm 0.2929 | time 464.5641 ms | tok/sec 1128559.0447
for step 8547 | loss 3.349777 | norm 0.3029 | time 464.4034 ms | tok/sec 1128949.5518
for step 8548 | loss 3.356155 | norm 0.3276 | time 464.6745 ms | tok/sec 1128290.9449
for step 8549 | loss 3.358760 | norm 0.3026 | time 465.2033 ms | tok/sec 1127008.3782
for step 8550 | loss 3.319911 | norm 0.2764 | time 465.6258 ms | tok/sec 1125985.8060
for step 8551 | loss 3.390363 | norm 0.3197 | time 464.9632 ms | tok/sec 1127590.3181
for step 8552 | loss 3.354664 | norm 0.3153 | time 465.1213 ms | tok/sec 1127207.1064
for step 8553 | loss 3.396007 | norm 0.3001 | time 464.9529 ms | tok/sec 1127615.1809
for step 8554 | loss 3.405040 | norm 0.3254 | time 464.7579 ms | tok/sec 1128088.3622
for step 8555 | loss 3.374497 | norm 0.3208 | time 464.5660 ms | tok/sec 1128554.4113
for step 8556 | loss 3.454355 | norm 0.2976 | time 464.5412 ms | tok/sec 1128614.6494
for step 8557 | loss 3.386467 | norm 0.3529 | time 465.3447 ms | tok/sec 1126665.9676
for step 8558 | loss 3.371899 | norm 0.3187 | time 464.0241 ms | tok/sec 1129872.4298
for step 8559 | loss 3.390273 | norm 0.3559 | time 464.4208 ms | tok/sec 1128907.2435
for step 8560 | loss 3.362310 | norm 0.3410 | time 464.5622 ms | tok/sec 1128563.6783
for step 8561 | loss 3.415789 | norm 0.3294 | time 465.4667 ms | tok/sec 1126370.4954
for step 8562 | loss 3.343711 | norm 0.3494 | time 464.7245 ms | tok/sec 1128169.3865
for step 8563 | loss 3.408000 | norm 0.3365 | time 464.8106 ms | tok/sec 1127960.4831
for step 8564 | loss 3.411107 | norm 0.3514 | time 464.3736 ms | tok/sec 1129022.0049
for step 8565 | loss 3.409892 | norm 0.3779 | time 464.9410 ms | tok/sec 1127644.0926
for step 8566 | loss 3.416892 | norm 0.3623 | time 464.5970 ms | tok/sec 1128479.1226
for step 8567 | loss 3.375752 | norm 0.3166 | time 464.7341 ms | tok/sec 1128146.2355
for step 8568 | loss 3.436517 | norm 0.3291 | time 464.9472 ms | tok/sec 1127629.0583
for step 8569 | loss 3.361082 | norm 0.3020 | time 465.0934 ms | tok/sec 1127274.7130
for step 8570 | loss 3.368759 | norm 0.3253 | time 464.6070 ms | tok/sec 1128454.8007
for step 8571 | loss 3.376053 | norm 0.3231 | time 464.2630 ms | tok/sec 1129291.0323
Will loading at 0 from edu_fineweb10B/edufineweb_train_000046.npy
for step 8572 | loss 3.391968 | norm 0.3407 | time 1424.4654 ms | tok/sec 368059.4793
for step 8573 | loss 3.328446 | norm 0.3518 | time 465.0793 ms | tok/sec 1127308.8084
for step 8574 | loss 3.397560 | norm 0.3355 | time 464.4029 ms | tok/sec 1128950.7110
for step 8575 | loss 3.320942 | norm 0.3469 | time 464.4008 ms | tok/sec 1128955.9273
for step 8576 | loss 3.374214 | norm 0.3288 | time 464.4775 ms | tok/sec 1128769.3288
for step 8577 | loss 3.382589 | norm 0.3418 | time 464.4158 ms | tok/sec 1128919.4141
for step 8578 | loss 3.379081 | norm 0.2985 | time 464.4251 ms | tok/sec 1128896.8118
for step 8579 | loss 3.365728 | norm 0.3394 | time 464.4358 ms | tok/sec 1128870.7334
for step 8580 | loss 3.307227 | norm 0.4058 | time 464.4134 ms | tok/sec 1128925.2097
for step 8581 | loss 3.356128 | norm 0.3975 | time 465.2226 ms | tok/sec 1126961.5948
for step 8582 | loss 3.327745 | norm 0.3082 | time 465.1105 ms | tok/sec 1127233.1080
for step 8583 | loss 3.456137 | norm 0.3595 | time 464.4501 ms | tok/sec 1128835.9640
for step 8584 | loss 3.330659 | norm 0.3600 | time 465.4896 ms | tok/sec 1126315.1116
for step 8585 | loss 3.349975 | norm 0.3490 | time 464.6702 ms | tok/sec 1128301.3654
for step 8586 | loss 3.451670 | norm 0.3998 | time 464.9711 ms | tok/sec 1127571.2380
for step 8587 | loss 3.335272 | norm 0.3796 | time 465.4160 ms | tok/sec 1126493.3976
for step 8588 | loss 3.348738 | norm 0.3485 | time 465.8291 ms | tok/sec 1125494.2247
for step 8589 | loss 3.382500 | norm 0.3456 | time 465.6293 ms | tok/sec 1125977.1579
for step 8590 | loss 3.439109 | norm 0.3634 | time 464.2851 ms | tok/sec 1129237.1006
for step 8591 | loss 3.348860 | norm 0.3229 | time 465.4663 ms | tok/sec 1126371.6493
for step 8592 | loss 3.586958 | norm 0.4605 | time 465.5702 ms | tok/sec 1126120.1579
for step 8593 | loss 3.473879 | norm 0.5127 | time 465.2364 ms | tok/sec 1126928.0980
for step 8594 | loss 3.359704 | norm 0.4179 | time 464.5755 ms | tok/sec 1128531.2445
for step 8595 | loss 3.528624 | norm 0.4755 | time 466.0199 ms | tok/sec 1125033.5770
for step 8596 | loss 3.468751 | norm 0.5202 | time 465.7500 ms | tok/sec 1125685.5043
for step 8597 | loss 3.343297 | norm 0.4401 | time 466.3098 ms | tok/sec 1124334.1142
for step 8598 | loss 3.391873 | norm 0.3989 | time 466.4338 ms | tok/sec 1124035.2672
for step 8599 | loss 3.368768 | norm 0.4112 | time 465.6389 ms | tok/sec 1125954.0968
for step 8600 | loss 3.395220 | norm 0.3362 | time 466.1977 ms | tok/sec 1124604.3626
for step 8601 | loss 3.409519 | norm 0.3973 | time 464.9136 ms | tok/sec 1127710.5951
for step 8602 | loss 3.410241 | norm 0.3242 | time 466.0764 ms | tok/sec 1124897.1825
for step 8603 | loss 3.322101 | norm 0.3826 | time 465.3447 ms | tok/sec 1126665.9676
for step 8604 | loss 3.368142 | norm 0.3236 | time 464.8051 ms | tok/sec 1127973.7904
for step 8605 | loss 3.432980 | norm 0.3405 | time 466.4652 ms | tok/sec 1123959.4313
for step 8606 | loss 3.376627 | norm 0.3271 | time 464.9487 ms | tok/sec 1127625.5890
for step 8607 | loss 3.400243 | norm 0.2964 | time 465.1835 ms | tok/sec 1127056.3207
for step 8608 | loss 3.405239 | norm 0.3195 | time 465.5850 ms | tok/sec 1126084.4045
for step 8609 | loss 3.377793 | norm 0.3370 | time 465.8809 ms | tok/sec 1125369.2367
for step 8610 | loss 3.376148 | norm 0.2910 | time 469.6698 ms | tok/sec 1116290.5917
for step 8611 | loss 3.454851 | norm 0.3193 | time 465.3041 ms | tok/sec 1126764.1079
for step 8612 | loss 3.374484 | norm 0.2831 | time 465.8339 ms | tok/sec 1125482.7039
for step 8613 | loss 3.371983 | norm 0.3292 | time 465.7602 ms | tok/sec 1125660.7265
for step 8614 | loss 3.269711 | norm 0.3063 | time 465.4980 ms | tok/sec 1126294.9210
for step 8615 | loss 3.342067 | norm 0.2756 | time 464.9146 ms | tok/sec 1127708.2818
for step 8616 | loss 3.395844 | norm 0.3033 | time 464.9560 ms | tok/sec 1127607.6641
for step 8617 | loss 3.390070 | norm 0.3069 | time 465.6668 ms | tok/sec 1125886.6484
for step 8618 | loss 3.315788 | norm 0.3158 | time 465.6732 ms | tok/sec 1125871.0845
for step 8619 | loss 3.360051 | norm 0.3101 | time 465.2507 ms | tok/sec 1126893.4482
for step 8620 | loss 3.355988 | norm 0.3365 | time 465.8506 ms | tok/sec 1125442.3830
for step 8621 | loss 3.324136 | norm 0.3573 | time 465.1802 ms | tok/sec 1127064.4078
for step 8622 | loss 3.330440 | norm 0.3059 | time 465.8008 ms | tok/sec 1125562.7783
for step 8623 | loss 3.390336 | norm 0.2947 | time 465.5674 ms | tok/sec 1126127.0782
for step 8624 | loss 3.328432 | norm 0.3787 | time 465.3819 ms | tok/sec 1126575.9245
for step 8625 | loss 3.369027 | norm 0.3353 | time 464.8671 ms | tok/sec 1127823.3780
for step 8626 | loss 3.393220 | norm 0.3324 | time 465.9455 ms | tok/sec 1125213.1847
for step 8627 | loss 3.361958 | norm 0.3328 | time 466.2714 ms | tok/sec 1124426.6740
for step 8628 | loss 3.376235 | norm 0.3290 | time 464.9398 ms | tok/sec 1127646.9838
for step 8629 | loss 3.398597 | norm 0.3195 | time 465.1334 ms | tok/sec 1127177.6393
for step 8630 | loss 3.436892 | norm 0.3758 | time 465.6246 ms | tok/sec 1125988.6888
for step 8631 | loss 3.413106 | norm 0.3265 | time 465.6270 ms | tok/sec 1125982.9233
for step 8632 | loss 3.387136 | norm 0.3483 | time 466.1427 ms | tok/sec 1124737.2345
for step 8633 | loss 3.414808 | norm 0.3145 | time 465.0371 ms | tok/sec 1127411.1067
for step 8634 | loss 3.369159 | norm 0.3318 | time 465.2276 ms | tok/sec 1126949.4664
for step 8635 | loss 3.321045 | norm 0.3314 | time 466.7535 ms | tok/sec 1123265.3195
for step 8636 | loss 3.388502 | norm 0.3495 | time 465.3332 ms | tok/sec 1126693.6761
for step 8637 | loss 3.401204 | norm 0.3119 | time 465.1237 ms | tok/sec 1127201.3284
for step 8638 | loss 3.373642 | norm 0.3199 | time 465.6715 ms | tok/sec 1125875.1196
for step 8639 | loss 3.424098 | norm 0.3294 | time 465.8921 ms | tok/sec 1125342.1693
for step 8640 | loss 3.409120 | norm 0.3107 | time 465.5969 ms | tok/sec 1126055.5728
for step 8641 | loss 3.423563 | norm 0.3090 | time 466.3024 ms | tok/sec 1124351.9351
for step 8642 | loss 3.399814 | norm 0.3279 | time 466.2068 ms | tok/sec 1124582.5079
for step 8643 | loss 3.385378 | norm 0.3316 | time 465.4741 ms | tok/sec 1126352.6105
for step 8644 | loss 3.347502 | norm 0.3148 | time 465.9731 ms | tok/sec 1125146.4008
for step 8645 | loss 3.433585 | norm 0.2951 | time 468.3754 ms | tok/sec 1119375.5058
for step 8646 | loss 3.346681 | norm 0.3305 | time 464.9925 ms | tok/sec 1127519.2048
for step 8647 | loss 3.371758 | norm 0.3806 | time 465.9162 ms | tok/sec 1125284.0074
for step 8648 | loss 3.363667 | norm 0.3019 | time 464.8731 ms | tok/sec 1127808.9174
for step 8649 | loss 3.366489 | norm 0.3263 | time 465.8077 ms | tok/sec 1125546.0712
for step 8650 | loss 3.342217 | norm 0.3106 | time 464.4253 ms | tok/sec 1128896.2323
for step 8651 | loss 3.376604 | norm 0.3185 | time 464.3965 ms | tok/sec 1128966.3601
for step 8652 | loss 3.303255 | norm 0.3671 | time 465.6649 ms | tok/sec 1125891.2600
for step 8653 | loss 3.401731 | norm 0.3172 | time 464.6614 ms | tok/sec 1128322.7860
for step 8654 | loss 3.402413 | norm 0.3321 | time 465.4729 ms | tok/sec 1126355.4951
for step 8655 | loss 3.274593 | norm 0.3040 | time 466.0845 ms | tok/sec 1124877.6181
for step 8656 | loss 3.349363 | norm 0.3297 | time 465.1623 ms | tok/sec 1127107.7335
for step 8657 | loss 3.340754 | norm 0.2853 | time 467.0746 ms | tok/sec 1122492.9880
for step 8658 | loss 3.382711 | norm 0.3024 | time 465.6897 ms | tok/sec 1125831.3122
for step 8659 | loss 3.366156 | norm 0.2980 | time 465.5848 ms | tok/sec 1126084.9812
for step 8660 | loss 3.327531 | norm 0.3146 | time 466.0714 ms | tok/sec 1124909.2667
for step 8661 | loss 3.394911 | norm 0.3360 | time 465.8349 ms | tok/sec 1125480.3998
for step 8662 | loss 3.374182 | norm 0.3993 | time 465.1656 ms | tok/sec 1127099.6458
for step 8663 | loss 3.382818 | norm 0.3513 | time 465.7576 ms | tok/sec 1125667.0649
for step 8664 | loss 3.391224 | norm 0.3270 | time 465.8151 ms | tok/sec 1125528.2124
for step 8665 | loss 3.431416 | norm 0.3455 | time 465.2483 ms | tok/sec 1126899.2230
for step 8666 | loss 3.445436 | norm 0.3487 | time 466.1131 ms | tok/sec 1124808.5726
for step 8667 | loss 3.468021 | norm 0.3896 | time 465.5094 ms | tok/sec 1126267.2321
for step 8668 | loss 3.411518 | norm 0.3560 | time 465.2512 ms | tok/sec 1126892.2933
for step 8669 | loss 3.359035 | norm 0.3522 | time 466.0082 ms | tok/sec 1125061.7808
for step 8670 | loss 3.407473 | norm 0.3421 | time 465.1666 ms | tok/sec 1127097.3351
for step 8671 | loss 3.405632 | norm 0.3550 | time 465.8678 ms | tok/sec 1125400.9131
for step 8672 | loss 3.427752 | norm 0.3543 | time 466.1436 ms | tok/sec 1124734.9334
for step 8673 | loss 3.374905 | norm 0.3146 | time 466.0172 ms | tok/sec 1125039.9083
for step 8674 | loss 3.404538 | norm 0.3437 | time 464.8793 ms | tok/sec 1127793.8788
for step 8675 | loss 3.438328 | norm 0.3118 | time 465.2314 ms | tok/sec 1126940.2259
for step 8676 | loss 3.395471 | norm 0.3142 | time 464.9582 ms | tok/sec 1127602.4603
for step 8677 | loss 3.349040 | norm 0.3039 | time 465.0278 ms | tok/sec 1127433.6495
for step 8678 | loss 3.372141 | norm 0.3439 | time 464.9854 ms | tok/sec 1127536.5487
for step 8679 | loss 3.424685 | norm 0.4277 | time 464.8588 ms | tok/sec 1127843.6235
for step 8680 | loss 3.412658 | norm 0.3534 | time 465.7214 ms | tok/sec 1125754.6575
for step 8681 | loss 3.369739 | norm 0.3622 | time 464.5727 ms | tok/sec 1128538.1944
for step 8682 | loss 3.361415 | norm 0.3340 | time 465.6858 ms | tok/sec 1125840.5345
for step 8683 | loss 3.446745 | norm 0.3304 | time 465.7772 ms | tok/sec 1125619.8166
for step 8684 | loss 3.352577 | norm 0.2906 | time 466.3508 ms | tok/sec 1124235.2472
for step 8685 | loss 3.373042 | norm 0.3298 | time 466.6426 ms | tok/sec 1123532.1841
for step 8686 | loss 3.377836 | norm 0.3093 | time 465.3342 ms | tok/sec 1126691.3670
for step 8687 | loss 3.335290 | norm 0.3237 | time 465.1220 ms | tok/sec 1127205.3730
for step 8688 | loss 3.358929 | norm 0.3200 | time 465.6324 ms | tok/sec 1125969.6629
for step 8689 | loss 3.318938 | norm 0.2981 | time 465.6847 ms | tok/sec 1125843.4165
for step 8690 | loss 3.333343 | norm 0.3122 | time 465.1055 ms | tok/sec 1127245.2425
for step 8691 | loss 3.351735 | norm 0.2772 | time 465.7490 ms | tok/sec 1125687.8092
for step 8692 | loss 3.334762 | norm 0.2970 | time 465.2181 ms | tok/sec 1126972.5683
for step 8693 | loss 3.381575 | norm 0.2917 | time 465.1721 ms | tok/sec 1127084.0484
for step 8694 | loss 3.422885 | norm 0.3140 | time 465.9386 ms | tok/sec 1125229.8820
for step 8695 | loss 3.333161 | norm 0.3283 | time 465.0135 ms | tok/sec 1127468.3325
for step 8696 | loss 3.397600 | norm 0.3170 | time 464.8600 ms | tok/sec 1127840.7313
for step 8697 | loss 3.373614 | norm 0.3267 | time 465.8191 ms | tok/sec 1125518.4191
for step 8698 | loss 3.364820 | norm 0.3526 | time 465.5864 ms | tok/sec 1126080.9446
for step 8699 | loss 3.356843 | norm 0.3055 | time 465.2426 ms | tok/sec 1126913.0828
for step 8700 | loss 3.361764 | norm 0.3278 | time 465.0891 ms | tok/sec 1127285.1148
for step 8701 | loss 3.379497 | norm 0.3079 | time 464.9467 ms | tok/sec 1127630.2148
for step 8702 | loss 3.371287 | norm 0.3204 | time 464.2417 ms | tok/sec 1129342.6492
for step 8703 | loss 3.370107 | norm 0.2967 | time 466.1245 ms | tok/sec 1124780.9568
for step 8704 | loss 3.369052 | norm 0.3116 | time 465.4374 ms | tok/sec 1126441.4639
for step 8705 | loss 3.383797 | norm 0.3401 | time 464.7369 ms | tok/sec 1128139.2904
for step 8706 | loss 3.363655 | norm 0.2991 | time 465.7686 ms | tok/sec 1125640.5593
for step 8707 | loss 3.376325 | norm 0.3555 | time 466.4662 ms | tok/sec 1123957.1334
for step 8708 | loss 3.387641 | norm 0.2967 | time 464.1364 ms | tok/sec 1129599.0636
for step 8709 | loss 3.406154 | norm 0.3137 | time 465.3568 ms | tok/sec 1126636.5288
for step 8710 | loss 3.352787 | norm 0.3113 | time 466.5937 ms | tok/sec 1123649.8744
for step 8711 | loss 3.423556 | norm 0.3221 | time 464.7954 ms | tok/sec 1127997.5130
for step 8712 | loss 3.451169 | norm 0.3296 | time 465.5790 ms | tok/sec 1126098.8209
for step 8713 | loss 3.345767 | norm 0.3396 | time 465.4469 ms | tok/sec 1126418.3837
for step 8714 | loss 3.364379 | norm 0.2941 | time 464.8383 ms | tok/sec 1127893.3727
for step 8715 | loss 3.392239 | norm 0.3341 | time 465.3199 ms | tok/sec 1126726.0043
for step 8716 | loss 3.354104 | norm 0.3156 | time 464.6602 ms | tok/sec 1128325.6807
for step 8717 | loss 3.362316 | norm 0.3184 | time 464.7567 ms | tok/sec 1128091.2557
for step 8718 | loss 3.392861 | norm 0.2925 | time 465.7004 ms | tok/sec 1125805.3752
for step 8719 | loss 3.321926 | norm 0.3508 | time 465.3628 ms | tok/sec 1126622.0986
for step 8720 | loss 3.305788 | norm 0.3256 | time 465.0850 ms | tok/sec 1127294.9388
for step 8721 | loss 3.395082 | norm 0.3191 | time 465.0095 ms | tok/sec 1127478.1598
for step 8722 | loss 3.375284 | norm 0.3484 | time 466.3901 ms | tok/sec 1124140.4202
for step 8723 | loss 3.362756 | norm 0.3017 | time 464.8902 ms | tok/sec 1127767.2729
for step 8724 | loss 3.323710 | norm 0.2985 | time 465.8427 ms | tok/sec 1125461.3911
for step 8725 | loss 3.316072 | norm 0.3070 | time 465.6892 ms | tok/sec 1125832.4650
for step 8726 | loss 3.335176 | norm 0.3062 | time 465.4710 ms | tok/sec 1126360.1106
for step 8727 | loss 3.392904 | norm 0.3241 | time 465.0722 ms | tok/sec 1127326.1458
for step 8728 | loss 3.415215 | norm 0.3513 | time 466.1267 ms | tok/sec 1124775.7789
for step 8729 | loss 3.405395 | norm 0.3843 | time 464.3574 ms | tok/sec 1129061.4233
for step 8730 | loss 3.351058 | norm 0.4327 | time 465.2412 ms | tok/sec 1126916.5478
for step 8731 | loss 3.371241 | norm 0.3348 | time 465.4162 ms | tok/sec 1126492.8206
for step 8732 | loss 3.291401 | norm 0.3657 | time 464.9580 ms | tok/sec 1127603.0385
for step 8733 | loss 3.354011 | norm 0.3952 | time 464.9563 ms | tok/sec 1127607.0859
for step 8734 | loss 3.349145 | norm 0.3460 | time 465.2052 ms | tok/sec 1127003.7574
for step 8735 | loss 3.348113 | norm 0.3559 | time 465.6930 ms | tok/sec 1125823.2428
for step 8736 | loss 3.281617 | norm 0.3698 | time 464.9549 ms | tok/sec 1127610.5552
for step 8737 | loss 3.362085 | norm 0.3398 | time 465.3127 ms | tok/sec 1126743.3238
for step 8738 | loss 3.348998 | norm 0.3212 | time 466.3818 ms | tok/sec 1124160.5337
for step 8739 | loss 3.286047 | norm 0.3493 | time 465.8935 ms | tok/sec 1125338.7139
for step 8740 | loss 3.343562 | norm 0.3306 | time 466.1226 ms | tok/sec 1124785.5593
for step 8741 | loss 3.392596 | norm 0.3777 | time 465.5759 ms | tok/sec 1126106.3176
for step 8742 | loss 3.350566 | norm 0.3819 | time 465.6379 ms | tok/sec 1125956.4028
for step 8743 | loss 3.384264 | norm 0.3618 | time 466.2006 ms | tok/sec 1124597.4610
for step 8744 | loss 3.384709 | norm 0.3640 | time 465.0438 ms | tok/sec 1127394.9227
for step 8745 | loss 3.360442 | norm 0.3219 | time 465.1875 ms | tok/sec 1127046.5008
for step 8746 | loss 3.348895 | norm 0.3452 | time 465.6465 ms | tok/sec 1125935.6485
for step 8747 | loss 3.366875 | norm 0.3254 | time 465.9524 ms | tok/sec 1125196.4880
for step 8748 | loss 3.438525 | norm 0.3240 | time 465.2939 ms | tok/sec 1126788.9343
for step 8749 | loss 3.370851 | norm 0.3545 | time 465.4193 ms | tok/sec 1126485.3187
validation loss 3.3715
HellaSwag accuracy: 2746/10042=0.2735
> Hello, I'm a language model, and the concept is, I'm already using "C", and it's not very complex for me.
I
> Hello, I'm a language model, a model, and a model is a word that we don't understand. We see every word we write, and we
> Hello, I'm a language model, but your code may be different.
- The number of languages that can be used to communicate:
- Language Types
> Hello, I'm a language model, and I'm very happy :)
What We're Looking Inside
Below is an example of a
- Diphth
> Hello, I'm a language model, and I know that you need an English as a second language to read something that you already know: the B.I
>>  Hello, I'm a language model, so I think that's fine. I love to study, I love to make your voice look sound great.<|endoftext|>A> Hello, I'm a language model, so when you're talking to me who's in 'a language' here you'll see an object object is a "

Hello, I'm a language model, and I'm a good English teacher. While I am going to be taking classes regularly, I found out I'm getting
> > > > Hello, I'm a language model, and that is something I haven't even heard before.<|endoftext|>In the summer of 1999, the United States Air Force released
Hello, I'm a language model, so I didn't bother saying that on the day I was working, because I am not a language model. I had
>>> Hello, I'm a language model, what you're used to is something like this:
We're going to take your computer and talk to you one step
Hello, I'm a language model, really easy to understand. I can create a bunch of similar languages as well, to include some of the same languages for
>  Hello, I'm a language model, but if it's interesting, I don't want to go into my code. It's a nice way for me to Hello, I'm a language model, and like I mentioned earlier, it works like me.
1.3: Comprehend: Comprehend is an applicationHello, I'm a language model, that makes it easier to make new ideas, and is the easiest way for my students to create new ideas, and this
> > Hello, I'm a language model, but I already had a good understanding through and understanding as well, because as a language model, you've learned to translate


> > Hello, I'm a language model, I'm a wayxm, it's a language model (English). It's also a good website for linguistics
Hello, I'm a language model, and I've a real-life lesson that I could go through on-line. I'm a little bit surprised to
> > Hello, I'm a language model, and I'm not really good. In fact I like this too much and often it gives me a few more choices at
Hello, I'm a language model, and I love it. I am a researcher and blogger. I love what I'm doing, and it really reminds me
> > Hello, I'm a language model, so I've got a lot of feedback from people.
What's on this menu?<|endoftext|>As much as one in
Hello, I'm a language model, using XML. The data from XML is not relevant to any language. XML markup (for XML) and "plain text
> > Hello, I'm a language model, and it appears this way.
2.) In Java Java to be 'Hello, I am very interested in doing so
Hello, I'm a language model, someone with a lot of information. Now, for the first time I'm going to describe the various components of a function
> > Hello, I'm a language model, and the first thing I wrote this week was the best you can think of.
I'm currently writing a second edition
Hello, I'm a language model, but there is no proof. The point is I'm a person in my language. I'm a little, very intelligent
Hello, I'm a language model, but i am a programming expert.<|endoftext|>Wendy Sibb, A.J. Linguistics and L
Hello, I'm a language model, and you don’t want to be a programmer.
There's a very general rule that I'm going to
> > > Hello, I'm a language model, and you are right, well, I'd love to talk about it, just read it. My favorite thing would be
Hello, I'm a language model, and i'm not just a programming model, but a model of language. The model has that, from where this article
Hello, I'm a language model, my main job is to find out how many people can understand each language.<|endoftext|>As part of R&D’
> Hello, I'm a language model, and really, that's the most important part. The other thing to note is that if I wanted to run this program
for step 8750 | loss 3.408226 | norm 0.3203 | time 12892.0586 ms | tok/sec 40667.5160
for step 8751 | loss 3.429616 | norm 0.3216 | time 462.8952 ms | tok/sec 1132627.9707
for step 8752 | loss 3.428781 | norm 0.3232 | time 462.8465 ms | tok/sec 1132746.9909
for step 8753 | loss 3.374603 | norm 0.3003 | time 463.4488 ms | tok/sec 1131275.0016
for step 8754 | loss 3.340293 | norm 0.3292 | time 463.4116 ms | tok/sec 1131365.7975
for step 8755 | loss 3.392466 | norm 0.3015 | time 463.9096 ms | tok/sec 1130151.1558
for step 8756 | loss 3.404336 | norm 0.3333 | time 464.7608 ms | tok/sec 1128081.4178
for step 8757 | loss 3.310022 | norm 0.3356 | time 463.9721 ms | tok/sec 1129999.0008
for step 8758 | loss 3.391570 | norm 0.3031 | time 465.1165 ms | tok/sec 1127218.6625
for step 8759 | loss 3.381815 | norm 0.2999 | time 463.4473 ms | tok/sec 1131278.4935
for step 8760 | loss 3.347310 | norm 0.3007 | time 464.1385 ms | tok/sec 1129593.8414
for step 8761 | loss 3.340211 | norm 0.2879 | time 463.4745 ms | tok/sec 1131212.1515
Will loading at 0 from edu_fineweb10B/edufineweb_train_000047.npy
for step 8762 | loss 3.360736 | norm 0.3012 | time 1433.6138 ms | tok/sec 365710.7712
for step 8763 | loss 3.330341 | norm 0.3287 | time 462.9881 ms | tok/sec 1132400.5017
for step 8764 | loss 3.350734 | norm 0.2884 | time 466.2805 ms | tok/sec 1124404.8262
for step 8765 | loss 3.308118 | norm 0.2730 | time 463.9101 ms | tok/sec 1130149.9941
for step 8766 | loss 3.327017 | norm 0.3019 | time 463.9199 ms | tok/sec 1130126.1810
for step 8767 | loss 3.334170 | norm 0.2962 | time 464.9789 ms | tok/sec 1127552.1586
for step 8768 | loss 3.435158 | norm 0.3600 | time 465.4465 ms | tok/sec 1126419.5377
for step 8769 | loss 3.285340 | norm 0.2956 | time 464.7057 ms | tok/sec 1128215.1126
for step 8770 | loss 3.362763 | norm 0.3361 | time 464.5739 ms | tok/sec 1128535.2986
for step 8771 | loss 3.330479 | norm 0.3616 | time 463.8772 ms | tok/sec 1130230.1531
for step 8772 | loss 3.438742 | norm 0.3418 | time 464.5436 ms | tok/sec 1128608.8570
for step 8773 | loss 3.342811 | norm 0.3337 | time 464.6626 ms | tok/sec 1128319.8912
for step 8774 | loss 3.348759 | norm 0.3404 | time 464.3612 ms | tok/sec 1129052.1482
for step 8775 | loss 3.369138 | norm 0.3414 | time 464.6266 ms | tok/sec 1128407.3182
for step 8776 | loss 3.307609 | norm 0.3296 | time 464.9491 ms | tok/sec 1127624.4325
for step 8777 | loss 3.419435 | norm 0.3387 | time 464.8602 ms | tok/sec 1127840.1528
for step 8778 | loss 3.381238 | norm 0.3805 | time 465.1365 ms | tok/sec 1127170.1284
for step 8779 | loss 3.480125 | norm 0.3794 | time 464.1695 ms | tok/sec 1129518.4140
for step 8780 | loss 3.365840 | norm 0.3454 | time 465.8108 ms | tok/sec 1125538.5819
for step 8781 | loss 3.358986 | norm 0.3489 | time 464.5753 ms | tok/sec 1128531.8236
for step 8782 | loss 3.383039 | norm 0.3533 | time 465.7812 ms | tok/sec 1125610.0217
for step 8783 | loss 3.375930 | norm 0.3127 | time 466.1021 ms | tok/sec 1124835.0390
for step 8784 | loss 3.364860 | norm 0.3418 | time 466.1133 ms | tok/sec 1124807.9972
for step 8785 | loss 3.364435 | norm 0.3379 | time 465.6012 ms | tok/sec 1126045.1937
for step 8786 | loss 3.388130 | norm 0.3136 | time 464.9467 ms | tok/sec 1127630.2148
for step 8787 | loss 3.374601 | norm 0.3533 | time 465.3506 ms | tok/sec 1126651.5366
for step 8788 | loss 3.397423 | norm 0.3300 | time 465.8692 ms | tok/sec 1125397.4574
for step 8789 | loss 3.331735 | norm 0.3472 | time 465.5423 ms | tok/sec 1126187.6343
for step 8790 | loss 3.318596 | norm 0.3023 | time 465.9834 ms | tok/sec 1125121.6467
for step 8791 | loss 3.370454 | norm 0.3432 | time 465.8403 ms | tok/sec 1125467.1512
for step 8792 | loss 3.309408 | norm 0.3242 | time 465.6117 ms | tok/sec 1126019.8234
for step 8793 | loss 3.386971 | norm 0.3303 | time 466.2685 ms | tok/sec 1124433.5735
for step 8794 | loss 3.377431 | norm 0.3207 | time 465.5387 ms | tok/sec 1126196.2857
for step 8795 | loss 3.328969 | norm 0.3350 | time 466.0087 ms | tok/sec 1125060.6296
for step 8796 | loss 3.282627 | norm 0.3041 | time 465.3311 ms | tok/sec 1126698.8715
for step 8797 | loss 3.402909 | norm 0.3246 | time 464.2551 ms | tok/sec 1129310.1706
for step 8798 | loss 3.364578 | norm 0.3406 | time 464.3540 ms | tok/sec 1129069.5392
for step 8799 | loss 3.333460 | norm 0.3094 | time 464.0076 ms | tok/sec 1129912.4882
for step 8800 | loss 3.337608 | norm 0.3060 | time 465.2209 ms | tok/sec 1126965.6377
for step 8801 | loss 3.331247 | norm 0.2954 | time 464.7765 ms | tok/sec 1128043.2251
for step 8802 | loss 3.374149 | norm 0.3333 | time 465.6732 ms | tok/sec 1125871.0845
for step 8803 | loss 3.345345 | norm 0.3158 | time 466.4321 ms | tok/sec 1124039.2891
for step 8804 | loss 3.414968 | norm 0.3221 | time 465.9429 ms | tok/sec 1125219.5181
for step 8805 | loss 3.384424 | norm 0.3307 | time 465.0583 ms | tok/sec 1127359.6663
for step 8806 | loss 3.358583 | norm 0.2995 | time 465.9660 ms | tok/sec 1125163.6717
for step 8807 | loss 3.385718 | norm 0.3324 | time 465.8124 ms | tok/sec 1125534.5493
for step 8808 | loss 3.354510 | norm 0.3355 | time 464.9093 ms | tok/sec 1127721.0049
for step 8809 | loss 3.341925 | norm 0.3008 | time 465.2600 ms | tok/sec 1126870.9270
for step 8810 | loss 3.378536 | norm 0.3339 | time 465.4822 ms | tok/sec 1126332.9954
for step 8811 | loss 3.318989 | norm 0.3080 | time 465.1167 ms | tok/sec 1127218.0847
for step 8812 | loss 3.428837 | norm 0.3099 | time 465.2638 ms | tok/sec 1126861.6878
for step 8813 | loss 3.407091 | norm 0.3135 | time 465.8599 ms | tok/sec 1125419.9197
for step 8814 | loss 3.530670 | norm 0.3191 | time 465.4675 ms | tok/sec 1126368.7646
for step 8815 | loss 3.394385 | norm 0.3578 | time 465.7781 ms | tok/sec 1125617.5119
for step 8816 | loss 3.469050 | norm 0.3106 | time 465.1158 ms | tok/sec 1127220.3959
for step 8817 | loss 3.407782 | norm 0.4110 | time 465.1000 ms | tok/sec 1127258.5329
for step 8818 | loss 3.443588 | norm 0.4157 | time 464.7772 ms | tok/sec 1128041.4891
for step 8819 | loss 3.381962 | norm 0.3060 | time 465.8105 ms | tok/sec 1125539.1580
for step 8820 | loss 3.352240 | norm 0.3761 | time 466.9800 ms | tok/sec 1122720.5061
for step 8821 | loss 3.366118 | norm 0.3397 | time 465.4186 ms | tok/sec 1126487.0499
for step 8822 | loss 3.328534 | norm 0.3208 | time 465.3230 ms | tok/sec 1126718.4994
for step 8823 | loss 3.403147 | norm 0.3117 | time 465.4014 ms | tok/sec 1126528.5999
for step 8824 | loss 3.394781 | norm 0.3402 | time 466.6810 ms | tok/sec 1123439.7715
for step 8825 | loss 3.384872 | norm 0.3239 | time 465.2817 ms | tok/sec 1126818.3810
for step 8826 | loss 3.331595 | norm 0.2948 | time 464.7830 ms | tok/sec 1128027.6015
for step 8827 | loss 3.288227 | norm 0.3322 | time 465.9150 ms | tok/sec 1125286.8866
for step 8828 | loss 3.333034 | norm 0.3036 | time 465.5781 ms | tok/sec 1126101.1276
for step 8829 | loss 3.364924 | norm 0.2923 | time 464.9930 ms | tok/sec 1127518.0486
for step 8830 | loss 3.306327 | norm 0.3120 | time 465.4412 ms | tok/sec 1126432.2317
for step 8831 | loss 3.343240 | norm 0.3045 | time 465.1766 ms | tok/sec 1127073.0727
for step 8832 | loss 3.288092 | norm 0.3108 | time 465.3358 ms | tok/sec 1126687.3261
for step 8833 | loss 3.403978 | norm 0.3176 | time 465.3225 ms | tok/sec 1126719.6540
for step 8834 | loss 3.375114 | norm 0.3279 | time 464.2408 ms | tok/sec 1129344.9692
for step 8835 | loss 3.382253 | norm 0.3084 | time 464.9115 ms | tok/sec 1127715.7999
for step 8836 | loss 3.328823 | norm 0.3616 | time 465.2095 ms | tok/sec 1126993.3609
for step 8837 | loss 3.400328 | norm 0.3508 | time 465.5352 ms | tok/sec 1126204.9372
for step 8838 | loss 3.389823 | norm 0.3490 | time 464.4690 ms | tok/sec 1128790.1877
for step 8839 | loss 3.358177 | norm 0.3264 | time 465.2441 ms | tok/sec 1126909.6179
for step 8840 | loss 3.432943 | norm 0.3455 | time 464.2067 ms | tok/sec 1129427.9144
for step 8841 | loss 3.348394 | norm 0.3383 | time 464.7725 ms | tok/sec 1128053.0623
for step 8842 | loss 3.391722 | norm 0.3419 | time 465.1904 ms | tok/sec 1127039.5693
for step 8843 | loss 3.340633 | norm 0.3327 | time 464.5813 ms | tok/sec 1128517.3448
for step 8844 | loss 3.348611 | norm 0.3365 | time 466.2545 ms | tok/sec 1124467.4972
for step 8845 | loss 3.360880 | norm 0.3308 | time 465.4546 ms | tok/sec 1126399.9203
for step 8846 | loss 3.344020 | norm 0.3445 | time 464.4558 ms | tok/sec 1128822.0569
for step 8847 | loss 3.449172 | norm 0.3884 | time 464.6711 ms | tok/sec 1128299.0497
for step 8848 | loss 3.390067 | norm 0.4032 | time 465.4109 ms | tok/sec 1126505.5162
for step 8849 | loss 3.353465 | norm 0.3179 | time 465.3063 ms | tok/sec 1126758.9118
for step 8850 | loss 3.428757 | norm 0.3843 | time 465.3003 ms | tok/sec 1126773.3455
for step 8851 | loss 3.385722 | norm 0.3671 | time 464.7415 ms | tok/sec 1128128.2941
for step 8852 | loss 3.399299 | norm 0.3225 | time 465.1346 ms | tok/sec 1127174.7505
for step 8853 | loss 3.395181 | norm 0.3469 | time 465.5464 ms | tok/sec 1126177.8295
for step 8854 | loss 3.359188 | norm 0.2975 | time 464.5488 ms | tok/sec 1128596.1139
for step 8855 | loss 3.346744 | norm 0.3104 | time 465.5116 ms | tok/sec 1126262.0406
for step 8856 | loss 3.333910 | norm 0.3212 | time 465.4942 ms | tok/sec 1126304.1509
for step 8857 | loss 3.412718 | norm 0.3036 | time 465.4999 ms | tok/sec 1126290.3061
for step 8858 | loss 3.376273 | norm 0.3383 | time 465.4610 ms | tok/sec 1126384.3422
for step 8859 | loss 3.345592 | norm 0.2747 | time 465.0378 ms | tok/sec 1127409.3727
for step 8860 | loss 3.345462 | norm 0.3393 | time 464.6406 ms | tok/sec 1128373.1564
for step 8861 | loss 3.353904 | norm 0.3488 | time 464.9019 ms | tok/sec 1127738.9333
for step 8862 | loss 3.308981 | norm 0.2954 | time 464.5305 ms | tok/sec 1128640.7160
for step 8863 | loss 3.317081 | norm 0.3425 | time 467.1867 ms | tok/sec 1122223.7532
for step 8864 | loss 3.337099 | norm 0.2885 | time 464.7892 ms | tok/sec 1128012.5570
for step 8865 | loss 3.334582 | norm 0.3705 | time 464.6957 ms | tok/sec 1128239.4241
for step 8866 | loss 3.405611 | norm 0.3052 | time 465.3995 ms | tok/sec 1126533.2168
for step 8867 | loss 3.306122 | norm 0.3530 | time 465.3151 ms | tok/sec 1126737.5506
for step 8868 | loss 3.298570 | norm 0.3074 | time 465.3792 ms | tok/sec 1126582.2732
for step 8869 | loss 3.348551 | norm 0.3166 | time 464.6192 ms | tok/sec 1128425.2684
for step 8870 | loss 3.346215 | norm 0.3413 | time 464.9754 ms | tok/sec 1127560.8310
for step 8871 | loss 3.431614 | norm 0.3434 | time 464.8168 ms | tok/sec 1127945.4404
for step 8872 | loss 3.409808 | norm 0.4013 | time 464.7012 ms | tok/sec 1128226.1105
for step 8873 | loss 3.317294 | norm 0.2992 | time 464.7131 ms | tok/sec 1128197.1690
for step 8874 | loss 3.337704 | norm 0.3765 | time 465.6417 ms | tok/sec 1125947.1786
for step 8875 | loss 3.327061 | norm 0.3199 | time 464.7295 ms | tok/sec 1128157.2321
for step 8876 | loss 3.402482 | norm 0.3407 | time 465.5805 ms | tok/sec 1126095.3610
for step 8877 | loss 3.271257 | norm 0.3120 | time 465.8458 ms | tok/sec 1125453.9030
for step 8878 | loss 3.372980 | norm 0.3362 | time 464.4403 ms | tok/sec 1128859.7229
for step 8879 | loss 3.346872 | norm 0.3205 | time 465.0772 ms | tok/sec 1127314.0095
for step 8880 | loss 3.428587 | norm 0.3492 | time 465.1046 ms | tok/sec 1127247.5538
for step 8881 | loss 3.432699 | norm 0.3392 | time 465.5154 ms | tok/sec 1126252.8114
for step 8882 | loss 3.421461 | norm 0.3532 | time 465.6029 ms | tok/sec 1126041.1574
for step 8883 | loss 3.393780 | norm 0.4077 | time 464.8762 ms | tok/sec 1127801.3980
for step 8884 | loss 3.434774 | norm 0.4576 | time 465.9758 ms | tok/sec 1125140.0683
for step 8885 | loss 3.381927 | norm 0.3623 | time 465.0114 ms | tok/sec 1127473.5352
for step 8886 | loss 3.358189 | norm 0.4495 | time 464.7198 ms | tok/sec 1128180.9624
for step 8887 | loss 3.321332 | norm 0.3791 | time 464.5836 ms | tok/sec 1128511.5534
for step 8888 | loss 3.472250 | norm 0.4157 | time 465.4038 ms | tok/sec 1126522.8289
for step 8889 | loss 3.415201 | norm 0.3923 | time 465.8990 ms | tok/sec 1125325.4687
for step 8890 | loss 3.376042 | norm 0.3558 | time 465.5535 ms | tok/sec 1126160.5274
for step 8891 | loss 3.370939 | norm 0.3797 | time 465.4164 ms | tok/sec 1126492.2435
for step 8892 | loss 3.385947 | norm 0.3251 | time 465.4734 ms | tok/sec 1126354.3413
for step 8893 | loss 3.439042 | norm 0.3554 | time 465.5666 ms | tok/sec 1126128.8083
for step 8894 | loss 3.421626 | norm 0.3189 | time 465.6088 ms | tok/sec 1126026.7425
for step 8895 | loss 3.412589 | norm 0.3626 | time 465.6827 ms | tok/sec 1125848.0278
for step 8896 | loss 3.403930 | norm 0.3116 | time 465.4756 ms | tok/sec 1126349.1489
for step 8897 | loss 3.307615 | norm 0.3260 | time 465.0338 ms | tok/sec 1127419.1989
for step 8898 | loss 3.390108 | norm 0.3171 | time 465.3645 ms | tok/sec 1126618.0583
for step 8899 | loss 3.353726 | norm 0.3090 | time 466.3122 ms | tok/sec 1124328.3656
for step 8900 | loss 3.336015 | norm 0.3553 | time 466.2168 ms | tok/sec 1124558.3537
for step 8901 | loss 3.394432 | norm 0.3167 | time 466.3486 ms | tok/sec 1124240.4201
for step 8902 | loss 3.313941 | norm 0.3114 | time 465.9576 ms | tok/sec 1125183.8219
for step 8903 | loss 3.354259 | norm 0.3137 | time 465.7652 ms | tok/sec 1125648.6260
for step 8904 | loss 3.340286 | norm 0.3019 | time 465.7140 ms | tok/sec 1125772.5234
for step 8905 | loss 3.399216 | norm 0.3189 | time 466.5315 ms | tok/sec 1123799.7499
for step 8906 | loss 3.314376 | norm 0.3213 | time 466.1548 ms | tok/sec 1124707.8964
for step 8907 | loss 3.344514 | norm 0.3148 | time 465.2948 ms | tok/sec 1126786.6248
for step 8908 | loss 3.370073 | norm 0.2757 | time 465.7176 ms | tok/sec 1125763.8785
for step 8909 | loss 3.334589 | norm 0.2876 | time 465.6177 ms | tok/sec 1126005.4090
for step 8910 | loss 3.351661 | norm 0.3115 | time 465.8377 ms | tok/sec 1125473.4874
for step 8911 | loss 3.350472 | norm 0.2720 | time 465.0452 ms | tok/sec 1127391.4548
for step 8912 | loss 3.412337 | norm 0.2976 | time 464.7138 ms | tok/sec 1128195.4326
for step 8913 | loss 3.420248 | norm 0.3209 | time 465.4920 ms | tok/sec 1126309.3428
for step 8914 | loss 3.376531 | norm 0.3672 | time 465.1940 ms | tok/sec 1127030.9049
for step 8915 | loss 3.318116 | norm 0.3585 | time 464.8943 ms | tok/sec 1127757.4406
for step 8916 | loss 3.315910 | norm 0.3071 | time 465.4043 ms | tok/sec 1126521.6747
for step 8917 | loss 3.435330 | norm 0.3134 | time 466.2554 ms | tok/sec 1124465.1972
for step 8918 | loss 3.353904 | norm 0.3040 | time 465.6913 ms | tok/sec 1125827.2775
for step 8919 | loss 3.370669 | norm 0.3288 | time 466.2807 ms | tok/sec 1124404.2513
for step 8920 | loss 3.469270 | norm 0.3677 | time 465.7123 ms | tok/sec 1125776.5578
for step 8921 | loss 3.436888 | norm 0.3552 | time 465.0781 ms | tok/sec 1127311.6979
for step 8922 | loss 3.376989 | norm 0.3204 | time 464.5934 ms | tok/sec 1128487.8093
for step 8923 | loss 3.333211 | norm 0.3364 | time 465.4231 ms | tok/sec 1126476.0858
for step 8924 | loss 3.362907 | norm 0.3312 | time 465.1554 ms | tok/sec 1127124.4870
for step 8925 | loss 3.503430 | norm 0.3470 | time 464.9310 ms | tok/sec 1127668.3795
for step 8926 | loss 3.337523 | norm 0.3185 | time 465.1890 ms | tok/sec 1127043.0350
for step 8927 | loss 3.389030 | norm 0.3352 | time 465.0424 ms | tok/sec 1127398.3907
for step 8928 | loss 3.453339 | norm 0.3398 | time 465.1582 ms | tok/sec 1127117.5545
for step 8929 | loss 3.407430 | norm 0.3376 | time 465.1823 ms | tok/sec 1127059.2090
for step 8930 | loss 3.367157 | norm 0.3095 | time 465.3742 ms | tok/sec 1126594.3937
for step 8931 | loss 3.344777 | norm 0.3088 | time 465.4136 ms | tok/sec 1126499.1684
for step 8932 | loss 3.347982 | norm 0.3016 | time 465.9436 ms | tok/sec 1125217.7908
for step 8933 | loss 3.336181 | norm 0.3064 | time 466.6507 ms | tok/sec 1123512.6671
for step 8934 | loss 3.363843 | norm 0.3254 | time 465.8186 ms | tok/sec 1125519.5713
for step 8935 | loss 3.328349 | norm 0.3254 | time 465.5187 ms | tok/sec 1126244.7360
for step 8936 | loss 3.351034 | norm 0.3135 | time 466.5196 ms | tok/sec 1123828.4662
for step 8937 | loss 3.397287 | norm 0.3413 | time 465.4274 ms | tok/sec 1126465.6990
for step 8938 | loss 3.339639 | norm 0.3423 | time 465.8124 ms | tok/sec 1125534.5493
for step 8939 | loss 3.368940 | norm 0.3462 | time 466.0943 ms | tok/sec 1124854.0266
for step 8940 | loss 3.340385 | norm 0.3713 | time 465.1530 ms | tok/sec 1127130.2642
for step 8941 | loss 3.309225 | norm 0.3036 | time 465.8668 ms | tok/sec 1125403.2169
for step 8942 | loss 3.359302 | norm 0.3078 | time 465.6267 ms | tok/sec 1125983.4999
for step 8943 | loss 3.364000 | norm 0.3144 | time 465.3509 ms | tok/sec 1126650.9594
for step 8944 | loss 3.355618 | norm 0.3484 | time 465.3833 ms | tok/sec 1126572.4616
for step 8945 | loss 3.406232 | norm 0.3068 | time 466.2943 ms | tok/sec 1124371.4812
for step 8946 | loss 3.369875 | norm 0.3091 | time 465.0626 ms | tok/sec 1127349.2631
for step 8947 | loss 3.336726 | norm 0.3277 | time 466.3694 ms | tok/sec 1124190.4179
for step 8948 | loss 3.407318 | norm 0.3451 | time 465.5445 ms | tok/sec 1126182.4435
for step 8949 | loss 3.354964 | norm 0.3194 | time 465.9696 ms | tok/sec 1125155.0362
for step 8950 | loss 3.352311 | norm 0.3274 | time 465.8079 ms | tok/sec 1125545.4951
for step 8951 | loss 3.384151 | norm 0.3144 | time 466.4736 ms | tok/sec 1123939.3251
for step 8952 | loss 3.363155 | norm 0.3471 | time 466.4879 ms | tok/sec 1123904.8588
Will loading at 0 from edu_fineweb10B/edufineweb_train_000048.npy
for step 8953 | loss 3.357952 | norm 0.3220 | time 1418.7946 ms | tok/sec 369530.5777
for step 8954 | loss 3.486336 | norm 0.3415 | time 465.2257 ms | tok/sec 1126954.0867
for step 8955 | loss 3.394250 | norm 0.3530 | time 464.7655 ms | tok/sec 1128069.8439
for step 8956 | loss 3.513756 | norm 0.3910 | time 464.7722 ms | tok/sec 1128053.6410
for step 8957 | loss 3.409559 | norm 0.3933 | time 465.0097 ms | tok/sec 1127477.5817
for step 8958 | loss 3.370822 | norm 0.2980 | time 464.0741 ms | tok/sec 1129750.5305
for step 8959 | loss 3.345802 | norm 0.3504 | time 464.8013 ms | tok/sec 1127983.0479
for step 8960 | loss 3.413478 | norm 0.3121 | time 465.7805 ms | tok/sec 1125611.7502
for step 8961 | loss 3.378351 | norm 0.3645 | time 465.0407 ms | tok/sec 1127402.4367
for step 8962 | loss 3.406107 | norm 0.3297 | time 464.8597 ms | tok/sec 1127841.3097
for step 8963 | loss 3.425435 | norm 0.3338 | time 464.3230 ms | tok/sec 1129144.9067
for step 8964 | loss 3.411308 | norm 0.3166 | time 464.5951 ms | tok/sec 1128483.7555
for step 8965 | loss 3.373808 | norm 0.3157 | time 465.4510 ms | tok/sec 1126408.5749
for step 8966 | loss 3.356536 | norm 0.3243 | time 465.7359 ms | tok/sec 1125719.5035
for step 8967 | loss 3.309153 | norm 0.3271 | time 466.1210 ms | tok/sec 1124789.5865
for step 8968 | loss 3.330388 | norm 0.3010 | time 465.3649 ms | tok/sec 1126616.9039
for step 8969 | loss 3.314126 | norm 0.3015 | time 465.6026 ms | tok/sec 1126041.7341
for step 8970 | loss 3.366166 | norm 0.3106 | time 464.8492 ms | tok/sec 1127866.7621
for step 8971 | loss 3.341269 | norm 0.2862 | time 465.5499 ms | tok/sec 1126169.1784
for step 8972 | loss 3.399893 | norm 0.3344 | time 465.4396 ms | tok/sec 1126436.2707
for step 8973 | loss 3.367457 | norm 0.3737 | time 465.0123 ms | tok/sec 1127471.2229
for step 8974 | loss 3.377351 | norm 0.3826 | time 465.9367 ms | tok/sec 1125234.4882
for step 8975 | loss 3.320894 | norm 0.2918 | time 465.9154 ms | tok/sec 1125285.7349
for step 8976 | loss 3.406263 | norm 0.3501 | time 465.2369 ms | tok/sec 1126926.9430
for step 8977 | loss 3.341305 | norm 0.3676 | time 465.5142 ms | tok/sec 1126255.6955
for step 8978 | loss 3.298013 | norm 0.3406 | time 465.5321 ms | tok/sec 1126212.4353
for step 8979 | loss 3.341266 | norm 0.3454 | time 466.1782 ms | tok/sec 1124651.5257
for step 8980 | loss 3.361085 | norm 0.2953 | time 465.9889 ms | tok/sec 1125108.4066
for step 8981 | loss 3.313364 | norm 0.3456 | time 465.1055 ms | tok/sec 1127245.2425
for step 8982 | loss 3.340144 | norm 0.3125 | time 465.1635 ms | tok/sec 1127104.8450
for step 8983 | loss 3.350772 | norm 0.3169 | time 465.3916 ms | tok/sec 1126552.2617
for step 8984 | loss 3.383532 | norm 0.3431 | time 464.4301 ms | tok/sec 1128884.6417
for step 8985 | loss 3.406052 | norm 0.3213 | time 464.9763 ms | tok/sec 1127558.5184
for step 8986 | loss 3.398761 | norm 0.3235 | time 464.4620 ms | tok/sec 1128806.9912
for step 8987 | loss 3.402919 | norm 0.3564 | time 464.7472 ms | tok/sec 1128114.4044
for step 8988 | loss 3.321276 | norm 0.2851 | time 465.0271 ms | tok/sec 1127435.3836
for step 8989 | loss 3.381038 | norm 0.3206 | time 465.9488 ms | tok/sec 1125205.1242
for step 8990 | loss 3.433266 | norm 0.2978 | time 465.6386 ms | tok/sec 1125954.6733
for step 8991 | loss 3.395030 | norm 0.3122 | time 465.2450 ms | tok/sec 1126907.3079
for step 8992 | loss 3.342937 | norm 0.3062 | time 464.9205 ms | tok/sec 1127693.8241
for step 8993 | loss 3.396014 | norm 0.2901 | time 465.4176 ms | tok/sec 1126489.3582
for step 8994 | loss 3.321219 | norm 0.2989 | time 465.5411 ms | tok/sec 1126190.5181
for step 8995 | loss 3.397522 | norm 0.3018 | time 465.7776 ms | tok/sec 1125618.6643
for step 8996 | loss 3.376915 | norm 0.2968 | time 465.2605 ms | tok/sec 1126869.7721
for step 8997 | loss 3.389467 | norm 0.3283 | time 466.1479 ms | tok/sec 1124724.5787
for step 8998 | loss 3.314977 | norm 0.3155 | time 465.2612 ms | tok/sec 1126868.0397
for step 8999 | loss 3.342993 | norm 0.3038 | time 465.5874 ms | tok/sec 1126078.6380
validation loss 3.3603
HellaSwag accuracy: 2751/10042=0.2739
> Hello, I'm a language model, and if I'm the one who tells the story of one English teacher, let's look at this. So, I
> Hello, I'm a language model, and you have done a lot of coding...
If one of the programmers I see doesn't quite work, I'm
> Hello, I'm a language model, but your definition of language is a lot of language. It just makes you all the time aware of how language really is
> Hello, I'm a language model, and I'm using a tool called I3D to map to one end of the domain but the other end is the
> Hello, I'm a language model, so I know that all the languages I learn are separate languages. I was very fascinated because so many languages exist.

> Hello, I'm a language model, and I'll be looking for a "mechanical grammar" for "thinking from scratch." Why should you care about> 
Hello, I'm a language model, really a language/literacy and how that applies to one's life - but my dad's brain, my voice is
> > > Hello, I'm a language model, especially if I'm not already at LBC.I get a really good idea of how it works (if it sounds
Hello, I'm a language model, and I know that you will want to keep adding your input, right? Well, it's important for me to remember
Hello, I'm a language model, I find it wonderful! If you've got any questions, just leave a comment to my team at CSL Blog,
> > > Hello, I'm a language model, and I've read your book so well, but like, "It's a language, and it's not something like
Hello, I'm a language model, so how do I do that? "I am talking about a language - how do we get it together?"
-
Hello, I'm a language model, and it got weird and I'm on. The syntax of "is" and "is".<|endoftext|>A new study from
> > > > > Hello, I'm a language model, and this one uses the Internet. Now I can understand that I'm actually a human, and that's exactly what I
Hello, I'm a language model, no one's going to stop for awhile.”
With a second language model, the program provides a rich foundation
Hello, I'm a language model, that sounds like a word? I have to write that out to fit the needs of the particular language.
I wrote
Hello, I'm a language model, so I use my regular code to work with some different languages: Esperanto, French, Spanish, and Japanese.

Hello, I'm a language model, but for a language to be recognized and communicated in a language, and the language is, in some sense, a '
> > > > > Hello, I'm a language model, but I always wanted to use it just plain. Don't forget to subscribe to my RSS feed if you want to download
Hello, I'm a language model, and I don't like the term "languages":
- English/Spanish
- English/French
- Spanish
Hello, I'm a language model, and can't remember the meaning. What do you think?
Thanks for your free trial today!
Sign up here
Hello, I'm a language model, and if you are doing your homework, it needs to be. Language is not simply an abstraction, but actually an abstraction
Hello, I'm a language model, and I'm not familiar with any words used to express any subject. Is this true and false? (Yes, in
> > > Hello, I'm a language model, so I am really a language user; even though I'm writing an account on my account. Thanks, I'm able
> Hello, I'm a language model, and my brain is constantly trying to work out which language to put up, the language used to communicate (like what you
Hello, I'm a language model, and you are going to be reading to me.
I'm learning a lot, the first time I was going to
> Hello, I'm a language model, and i've been working on that for some time. However, i'm not sure if i have a very large set
> > Hello, I'm a language model, and so why do we call it a paradigm? So what about the language (as in Haskell?), which is the model
> Hello, I'm a language model, and I don't need to.
"What's the difference between a data model (GDB), like, versus
Hello, I'm a language model, have no fear of trying to make my own music! But I did it all for the same reason.
So my
Hello, I'm a language model, and you've got several languages around. A better way to explain the difference between languages is to understand what you say in
> Hello, I'm a language model, and some of us really know languages.
We are in a position to make your experience, and as you might know
for step 9000 | loss 3.378248 | norm 0.3038 | time 12953.8999 ms | tok/sec 40473.3714
for step 9001 | loss 3.349151 | norm 0.3166 | time 462.2412 ms | tok/sec 1134230.4210
for step 9002 | loss 3.397116 | norm 0.2957 | time 462.4753 ms | tok/sec 1133656.2194
for step 9003 | loss 3.341791 | norm 0.3294 | time 463.0680 ms | tok/sec 1132205.1847
for step 9004 | loss 3.314108 | norm 0.3191 | time 463.7368 ms | tok/sec 1130572.4089
for step 9005 | loss 3.270076 | norm 0.2898 | time 464.0229 ms | tok/sec 1129875.3325
for step 9006 | loss 3.329075 | norm 0.3449 | time 463.3284 ms | tok/sec 1131568.9769
for step 9007 | loss 3.399926 | norm 0.3055 | time 463.3505 ms | tok/sec 1131514.8274
for step 9008 | loss 3.343429 | norm 0.3361 | time 463.4123 ms | tok/sec 1131364.0513
for step 9009 | loss 3.304053 | norm 0.3303 | time 463.5313 ms | tok/sec 1131073.6731
for step 9010 | loss 3.409756 | norm 0.3066 | time 463.7656 ms | tok/sec 1130502.0813
for step 9011 | loss 3.344011 | norm 0.3400 | time 466.9447 ms | tok/sec 1122805.3475
for step 9012 | loss 3.403822 | norm 0.3075 | time 464.3724 ms | tok/sec 1129024.9033
for step 9013 | loss 3.386437 | norm 0.3356 | time 463.8026 ms | tok/sec 1130412.0051
for step 9014 | loss 3.394817 | norm 0.3719 | time 463.7194 ms | tok/sec 1130614.8421
for step 9015 | loss 3.366846 | norm 0.3689 | time 463.9530 ms | tok/sec 1130045.4560
for step 9016 | loss 3.391858 | norm 0.3263 | time 464.8745 ms | tok/sec 1127805.4469
for step 9017 | loss 3.443274 | norm 0.3530 | time 464.6201 ms | tok/sec 1128422.9522
for step 9018 | loss 3.389135 | norm 0.3405 | time 464.1118 ms | tok/sec 1129658.8331
for step 9019 | loss 3.347489 | norm 0.3431 | time 464.8545 ms | tok/sec 1127854.0358
for step 9020 | loss 3.366908 | norm 0.3294 | time 464.4051 ms | tok/sec 1128945.4947
for step 9021 | loss 3.362554 | norm 0.3390 | time 464.7980 ms | tok/sec 1127991.1483
for step 9022 | loss 3.444067 | norm 0.3306 | time 465.5924 ms | tok/sec 1126066.5287
for step 9023 | loss 3.393552 | norm 0.3861 | time 464.8428 ms | tok/sec 1127882.3812
for step 9024 | loss 3.413577 | norm 0.3709 | time 464.3612 ms | tok/sec 1129052.1482
for step 9025 | loss 3.385477 | norm 0.3788 | time 465.5170 ms | tok/sec 1126248.7737
for step 9026 | loss 3.306243 | norm 0.3189 | time 465.6308 ms | tok/sec 1125973.6987
for step 9027 | loss 3.332538 | norm 0.3565 | time 465.5385 ms | tok/sec 1126196.8624
for step 9028 | loss 3.381783 | norm 0.3275 | time 464.5412 ms | tok/sec 1128614.6494
for step 9029 | loss 3.390499 | norm 0.3025 | time 468.2515 ms | tok/sec 1119671.8798
for step 9030 | loss 3.413846 | norm 0.2936 | time 464.6623 ms | tok/sec 1128320.4702
for step 9031 | loss 3.353021 | norm 0.3087 | time 464.5593 ms | tok/sec 1128570.6286
for step 9032 | loss 3.357038 | norm 0.3014 | time 465.9021 ms | tok/sec 1125317.9824
for step 9033 | loss 3.379380 | norm 0.3348 | time 464.8683 ms | tok/sec 1127820.4859
for step 9034 | loss 3.346276 | norm 0.2956 | time 465.5352 ms | tok/sec 1126204.9372
for step 9035 | loss 3.297776 | norm 0.3134 | time 465.1120 ms | tok/sec 1127229.6410
for step 9036 | loss 3.335086 | norm 0.3471 | time 464.3395 ms | tok/sec 1129104.9027
for step 9037 | loss 3.373687 | norm 0.3118 | time 465.2977 ms | tok/sec 1126779.6964
for step 9038 | loss 3.318121 | norm 0.2941 | time 464.6528 ms | tok/sec 1128343.6283
for step 9039 | loss 3.315597 | norm 0.3082 | time 465.5912 ms | tok/sec 1126069.4118
for step 9040 | loss 3.333728 | norm 0.2991 | time 465.0934 ms | tok/sec 1127274.7130
for step 9041 | loss 3.354400 | norm 0.2922 | time 465.2717 ms | tok/sec 1126842.6324
for step 9042 | loss 3.330215 | norm 0.3095 | time 465.9741 ms | tok/sec 1125144.0980
for step 9043 | loss 3.319020 | norm 0.2911 | time 464.9825 ms | tok/sec 1127543.4864
for step 9044 | loss 3.356204 | norm 0.3318 | time 465.8968 ms | tok/sec 1125330.6516
for step 9045 | loss 3.339562 | norm 0.2998 | time 466.3446 ms | tok/sec 1124250.1911
for step 9046 | loss 3.367361 | norm 0.3387 | time 464.8981 ms | tok/sec 1127748.1869
for step 9047 | loss 3.328349 | norm 0.3418 | time 465.4145 ms | tok/sec 1126496.8601
for step 9048 | loss 3.371780 | norm 0.3677 | time 465.2712 ms | tok/sec 1126843.7873
for step 9049 | loss 3.353209 | norm 0.3150 | time 465.8926 ms | tok/sec 1125341.0175
for step 9050 | loss 3.400653 | norm 0.3266 | time 464.8199 ms | tok/sec 1127937.9192
for step 9051 | loss 3.395422 | norm 0.3414 | time 465.6224 ms | tok/sec 1125993.8778
for step 9052 | loss 3.345959 | norm 0.3547 | time 465.7693 ms | tok/sec 1125638.8307
for step 9053 | loss 3.376733 | norm 0.3854 | time 464.5762 ms | tok/sec 1128529.5070
for step 9054 | loss 3.383473 | norm 0.3328 | time 464.6840 ms | tok/sec 1128267.7889
for step 9055 | loss 3.462886 | norm 0.3511 | time 464.3240 ms | tok/sec 1129142.5876
for step 9056 | loss 3.351287 | norm 0.3229 | time 465.2252 ms | tok/sec 1126955.2418
for step 9057 | loss 3.422171 | norm 0.3783 | time 465.1561 ms | tok/sec 1127122.7539
for step 9058 | loss 3.400923 | norm 0.3246 | time 465.1363 ms | tok/sec 1127170.7061
for step 9059 | loss 3.330915 | norm 0.3542 | time 465.2839 ms | tok/sec 1126813.1844
for step 9060 | loss 3.444315 | norm 0.3807 | time 465.7190 ms | tok/sec 1125760.4206
for step 9061 | loss 3.412566 | norm 0.3365 | time 466.6305 ms | tok/sec 1123561.4608
for step 9062 | loss 3.380481 | norm 0.3417 | time 465.8833 ms | tok/sec 1125363.4776
for step 9063 | loss 3.392898 | norm 0.3283 | time 465.9493 ms | tok/sec 1125203.9727
for step 9064 | loss 3.379590 | norm 0.3822 | time 464.9153 ms | tok/sec 1127706.5469
for step 9065 | loss 3.315867 | norm 0.3096 | time 464.8957 ms | tok/sec 1127753.9705
for step 9066 | loss 3.340945 | norm 0.3163 | time 465.2672 ms | tok/sec 1126853.6036
for step 9067 | loss 3.385939 | norm 0.3245 | time 464.4446 ms | tok/sec 1128849.2920
for step 9068 | loss 3.406549 | norm 0.3307 | time 466.1870 ms | tok/sec 1124630.2443
for step 9069 | loss 3.371285 | norm 0.2842 | time 466.2931 ms | tok/sec 1124374.3557
for step 9070 | loss 3.354772 | norm 0.3290 | time 465.6892 ms | tok/sec 1125832.4650
for step 9071 | loss 3.370050 | norm 0.3558 | time 465.2081 ms | tok/sec 1126996.8264
for step 9072 | loss 3.334878 | norm 0.3756 | time 464.3939 ms | tok/sec 1128972.7358
for step 9073 | loss 3.331315 | norm 0.3530 | time 464.2711 ms | tok/sec 1129271.3147
for step 9074 | loss 3.453338 | norm 0.3389 | time 464.6840 ms | tok/sec 1128267.7889
for step 9075 | loss 3.431110 | norm 0.3754 | time 465.2059 ms | tok/sec 1127002.0247
for step 9076 | loss 3.459717 | norm 0.3450 | time 465.2686 ms | tok/sec 1126850.1390
for step 9077 | loss 3.297749 | norm 0.3182 | time 464.8647 ms | tok/sec 1127829.1624
for step 9078 | loss 3.318482 | norm 0.3157 | time 465.5843 ms | tok/sec 1126086.1345
for step 9079 | loss 3.362183 | norm 0.3336 | time 466.6555 ms | tok/sec 1123501.1869
for step 9080 | loss 3.320452 | norm 0.3104 | time 464.7224 ms | tok/sec 1128174.5956
for step 9081 | loss 3.323051 | norm 0.3357 | time 464.8955 ms | tok/sec 1127754.5488
for step 9082 | loss 3.323933 | norm 0.3043 | time 465.7078 ms | tok/sec 1125787.5082
for step 9083 | loss 3.397980 | norm 0.3180 | time 465.3881 ms | tok/sec 1126560.9187
for step 9084 | loss 3.375504 | norm 0.3253 | time 465.6296 ms | tok/sec 1125976.5813
for step 9085 | loss 3.333225 | norm 0.3446 | time 465.7519 ms | tok/sec 1125680.8944
for step 9086 | loss 3.366108 | norm 0.3305 | time 465.7619 ms | tok/sec 1125656.6930
for step 9087 | loss 3.405484 | norm 0.3264 | time 465.9343 ms | tok/sec 1125240.2460
for step 9088 | loss 3.424982 | norm 0.3564 | time 465.7731 ms | tok/sec 1125629.6117
for step 9089 | loss 3.424868 | norm 0.4150 | time 465.6441 ms | tok/sec 1125941.4135
for step 9090 | loss 3.325830 | norm 0.3327 | time 465.2510 ms | tok/sec 1126892.8707
for step 9091 | loss 3.362551 | norm 0.3588 | time 465.6274 ms | tok/sec 1125981.7702
for step 9092 | loss 3.385279 | norm 0.3665 | time 465.0438 ms | tok/sec 1127394.9227
for step 9093 | loss 3.381648 | norm 0.3707 | time 465.4443 ms | tok/sec 1126424.7307
for step 9094 | loss 3.347660 | norm 0.4071 | time 465.6816 ms | tok/sec 1125850.9098
for step 9095 | loss 3.485084 | norm 0.3446 | time 465.2846 ms | tok/sec 1126811.4522
for step 9096 | loss 3.380357 | norm 0.4210 | time 464.8197 ms | tok/sec 1127938.4978
for step 9097 | loss 3.350060 | norm 0.3215 | time 465.8475 ms | tok/sec 1125449.8710
for step 9098 | loss 3.393270 | norm 0.4171 | time 465.2565 ms | tok/sec 1126879.5889
for step 9099 | loss 3.436592 | norm 0.3770 | time 464.8695 ms | tok/sec 1127817.5937
for step 9100 | loss 3.391751 | norm 0.3713 | time 464.8008 ms | tok/sec 1127984.2051
for step 9101 | loss 3.359077 | norm 0.3383 | time 464.6637 ms | tok/sec 1128316.9965
for step 9102 | loss 3.352282 | norm 0.3331 | time 465.6405 ms | tok/sec 1125950.0612
for step 9103 | loss 3.417906 | norm 0.3422 | time 465.3866 ms | tok/sec 1126564.3816
for step 9104 | loss 3.336557 | norm 0.3497 | time 465.1799 ms | tok/sec 1127064.9855
for step 9105 | loss 3.330183 | norm 0.3212 | time 465.2972 ms | tok/sec 1126780.8512
for step 9106 | loss 3.403681 | norm 0.3346 | time 465.7490 ms | tok/sec 1125687.8092
for step 9107 | loss 3.342694 | norm 0.3281 | time 465.4355 ms | tok/sec 1126446.0800
for step 9108 | loss 3.339834 | norm 0.2937 | time 465.1690 ms | tok/sec 1127091.5582
for step 9109 | loss 3.331450 | norm 0.3513 | time 465.0626 ms | tok/sec 1127349.2631
for step 9110 | loss 3.313839 | norm 0.3467 | time 464.5014 ms | tok/sec 1128711.3915
for step 9111 | loss 3.306601 | norm 0.3298 | time 465.1084 ms | tok/sec 1127238.3084
for step 9112 | loss 3.360184 | norm 0.3372 | time 465.2772 ms | tok/sec 1126829.3518
for step 9113 | loss 3.363313 | norm 0.3369 | time 465.0905 ms | tok/sec 1127281.6475
for step 9114 | loss 3.359659 | norm 0.3421 | time 464.6494 ms | tok/sec 1128351.7339
for step 9115 | loss 3.316383 | norm 0.4035 | time 464.0148 ms | tok/sec 1129895.0712
for step 9116 | loss 3.375962 | norm 0.3418 | time 464.5069 ms | tok/sec 1128698.0667
for step 9117 | loss 3.347541 | norm 0.3494 | time 465.0185 ms | tok/sec 1127456.1933
for step 9118 | loss 3.392916 | norm 0.3159 | time 465.7087 ms | tok/sec 1125785.2028
for step 9119 | loss 3.297784 | norm 0.3439 | time 464.4074 ms | tok/sec 1128939.6989
for step 9120 | loss 3.363760 | norm 0.3231 | time 465.5252 ms | tok/sec 1126229.1622
for step 9121 | loss 3.392775 | norm 0.3165 | time 465.0989 ms | tok/sec 1127261.4222
for step 9122 | loss 3.471091 | norm 0.3782 | time 465.5817 ms | tok/sec 1126092.4777
for step 9123 | loss 3.397447 | norm 0.3792 | time 464.7379 ms | tok/sec 1128136.9754
for step 9124 | loss 3.336909 | norm 0.3380 | time 464.5343 ms | tok/sec 1128631.4478
for step 9125 | loss 3.341650 | norm 0.3365 | time 465.4515 ms | tok/sec 1126407.4210
for step 9126 | loss 3.408938 | norm 0.3388 | time 464.6585 ms | tok/sec 1128329.7333
for step 9127 | loss 3.367444 | norm 0.3164 | time 464.9560 ms | tok/sec 1127607.6641
for step 9128 | loss 3.346674 | norm 0.3957 | time 464.3719 ms | tok/sec 1129026.0626
for step 9129 | loss 3.391245 | norm 0.3461 | time 465.8401 ms | tok/sec 1125467.7272
for step 9130 | loss 3.439280 | norm 0.3644 | time 465.0176 ms | tok/sec 1127458.5055
for step 9131 | loss 3.362756 | norm 0.3294 | time 465.4534 ms | tok/sec 1126402.8051
for step 9132 | loss 3.348201 | norm 0.3224 | time 464.3946 ms | tok/sec 1128970.9970
for step 9133 | loss 3.328679 | norm 0.2948 | time 465.2259 ms | tok/sec 1126953.5092
for step 9134 | loss 3.351932 | norm 0.3011 | time 465.0483 ms | tok/sec 1127383.9409
for step 9135 | loss 3.374702 | norm 0.3058 | time 465.4615 ms | tok/sec 1126383.1883
for step 9136 | loss 3.366412 | norm 0.2979 | time 465.2131 ms | tok/sec 1126984.6972
for step 9137 | loss 3.354205 | norm 0.2865 | time 465.1644 ms | tok/sec 1127102.5343
for step 9138 | loss 3.385579 | norm 0.3232 | time 464.9768 ms | tok/sec 1127557.3620
for step 9139 | loss 3.405182 | norm 0.3094 | time 465.6794 ms | tok/sec 1125856.0975
for step 9140 | loss 3.358000 | norm 0.3062 | time 465.6107 ms | tok/sec 1126022.1298
for step 9141 | loss 3.337406 | norm 0.3250 | time 465.4415 ms | tok/sec 1126431.6547
for step 9142 | loss 3.410462 | norm 0.3152 | time 465.2023 ms | tok/sec 1127010.6886
Will loading at 0 from edu_fineweb10B/edufineweb_train_000049.npy
for step 9143 | loss 3.341137 | norm 0.3052 | time 1445.3070 ms | tok/sec 362751.9925
for step 9144 | loss 3.388346 | norm 0.3613 | time 462.3711 ms | tok/sec 1133911.6732
for step 9145 | loss 3.389227 | norm 0.2806 | time 464.1461 ms | tok/sec 1129575.2737
for step 9146 | loss 3.337142 | norm 0.3044 | time 465.0714 ms | tok/sec 1127327.8795
for step 9147 | loss 3.450859 | norm 0.3512 | time 464.8302 ms | tok/sec 1127913.0421
for step 9148 | loss 3.382790 | norm 0.3542 | time 465.2402 ms | tok/sec 1126918.8579
for step 9149 | loss 3.424235 | norm 0.3597 | time 465.0605 ms | tok/sec 1127354.4647
for step 9150 | loss 3.365559 | norm 0.3820 | time 466.1076 ms | tok/sec 1124821.8056
for step 9151 | loss 3.342445 | norm 0.3453 | time 465.6072 ms | tok/sec 1126030.7786
for step 9152 | loss 3.344066 | norm 0.3497 | time 465.0288 ms | tok/sec 1127431.3374
for step 9153 | loss 3.357027 | norm 0.3383 | time 466.3191 ms | tok/sec 1124311.6951
for step 9154 | loss 3.388164 | norm 0.4016 | time 465.6470 ms | tok/sec 1125934.4956
for step 9155 | loss 3.299464 | norm 0.3367 | time 465.6198 ms | tok/sec 1126000.2200
for step 9156 | loss 3.315651 | norm 0.3495 | time 465.4953 ms | tok/sec 1126301.2666
for step 9157 | loss 3.304047 | norm 0.3281 | time 466.3553 ms | tok/sec 1124224.3269
for step 9158 | loss 3.246089 | norm 0.3235 | time 465.9207 ms | tok/sec 1125273.0667
for step 9159 | loss 3.292694 | norm 0.3816 | time 465.4267 ms | tok/sec 1126467.4301
for step 9160 | loss 3.296316 | norm 0.3533 | time 466.3210 ms | tok/sec 1124307.0965
for step 9161 | loss 3.307503 | norm 0.3791 | time 465.6966 ms | tok/sec 1125814.5971
for step 9162 | loss 3.423814 | norm 0.3534 | time 465.8337 ms | tok/sec 1125483.2799
for step 9163 | loss 3.394492 | norm 0.3672 | time 465.4922 ms | tok/sec 1126308.7659
for step 9164 | loss 3.331532 | norm 0.3625 | time 465.9035 ms | tok/sec 1125314.5272
for step 9165 | loss 3.331086 | norm 0.3121 | time 466.1942 ms | tok/sec 1124612.9897
for step 9166 | loss 3.327284 | norm 0.3247 | time 465.3373 ms | tok/sec 1126683.8625
for step 9167 | loss 3.322010 | norm 0.3285 | time 465.7764 ms | tok/sec 1125621.5451
for step 9168 | loss 3.363649 | norm 0.3667 | time 465.8861 ms | tok/sec 1125356.5667
for step 9169 | loss 3.371499 | norm 0.3211 | time 465.1477 ms | tok/sec 1127142.9742
for step 9170 | loss 3.372299 | norm 0.3211 | time 465.4036 ms | tok/sec 1126523.4060
for step 9171 | loss 3.414583 | norm 0.3665 | time 465.2054 ms | tok/sec 1127003.1798
for step 9172 | loss 3.348931 | norm 0.3502 | time 465.5035 ms | tok/sec 1126281.6533
for step 9173 | loss 3.401974 | norm 0.3382 | time 465.5221 ms | tok/sec 1126236.6606
for step 9174 | loss 3.403387 | norm 0.3332 | time 465.0371 ms | tok/sec 1127411.1067
for step 9175 | loss 3.310802 | norm 0.3537 | time 465.2979 ms | tok/sec 1126779.1191
for step 9176 | loss 3.357104 | norm 0.3823 | time 464.2398 ms | tok/sec 1129347.2891
for step 9177 | loss 3.379811 | norm 0.3586 | time 465.2271 ms | tok/sec 1126950.6215
for step 9178 | loss 3.369163 | norm 0.3667 | time 465.2085 ms | tok/sec 1126995.6712
for step 9179 | loss 3.305563 | norm 0.3445 | time 464.7191 ms | tok/sec 1128182.6988
for step 9180 | loss 3.300848 | norm 0.3517 | time 465.0145 ms | tok/sec 1127466.0203
for step 9181 | loss 3.341920 | norm 0.3319 | time 464.7424 ms | tok/sec 1128125.9792
for step 9182 | loss 3.373584 | norm 0.3546 | time 464.7033 ms | tok/sec 1128220.9009
for step 9183 | loss 3.334835 | norm 0.3266 | time 465.8053 ms | tok/sec 1125551.8322
for step 9184 | loss 3.318166 | norm 0.3644 | time 465.6694 ms | tok/sec 1125880.3075
for step 9185 | loss 3.373975 | norm 0.3308 | time 464.6497 ms | tok/sec 1128351.1549
for step 9186 | loss 3.361923 | norm 0.3879 | time 465.7731 ms | tok/sec 1125629.6117
for step 9187 | loss 3.391589 | norm 0.3362 | time 464.4806 ms | tok/sec 1128761.7966
for step 9188 | loss 3.367016 | norm 0.4131 | time 464.7477 ms | tok/sec 1128113.2470
for step 9189 | loss 3.372691 | norm 0.3328 | time 465.1353 ms | tok/sec 1127173.0172
for step 9190 | loss 3.334501 | norm 0.3499 | time 464.9467 ms | tok/sec 1127630.2148
for step 9191 | loss 3.357502 | norm 0.4356 | time 466.3978 ms | tok/sec 1124122.0314
for step 9192 | loss 3.369895 | norm 0.3658 | time 464.9651 ms | tok/sec 1127585.6926
for step 9193 | loss 3.346559 | norm 0.3733 | time 464.8151 ms | tok/sec 1127949.4903
for step 9194 | loss 3.300067 | norm 0.3784 | time 465.0617 ms | tok/sec 1127351.5749
for step 9195 | loss 3.355182 | norm 0.3539 | time 465.5428 ms | tok/sec 1126186.4808
for step 9196 | loss 3.303087 | norm 0.3469 | time 465.8363 ms | tok/sec 1125476.9436
for step 9197 | loss 3.329047 | norm 0.3273 | time 465.2925 ms | tok/sec 1126792.3985
for step 9198 | loss 3.296794 | norm 0.3204 | time 465.6301 ms | tok/sec 1125975.4283
for step 9199 | loss 3.357516 | norm 0.3204 | time 465.3366 ms | tok/sec 1126685.5943
for step 9200 | loss 3.362387 | norm 0.3027 | time 465.5666 ms | tok/sec 1126128.8083
for step 9201 | loss 3.403174 | norm 0.3307 | time 465.9529 ms | tok/sec 1125195.3365
for step 9202 | loss 3.313479 | norm 0.3094 | time 464.8626 ms | tok/sec 1127834.3684
for step 9203 | loss 3.372178 | norm 0.3136 | time 465.6494 ms | tok/sec 1125928.7306
for step 9204 | loss 3.361291 | norm 0.2986 | time 465.7750 ms | tok/sec 1125625.0022
for step 9205 | loss 3.350848 | norm 0.3205 | time 465.5936 ms | tok/sec 1126063.6455
for step 9206 | loss 3.344530 | norm 0.3315 | time 464.7622 ms | tok/sec 1128077.9456
for step 9207 | loss 3.351085 | norm 0.3108 | time 465.1425 ms | tok/sec 1127155.6845
for step 9208 | loss 3.351704 | norm 0.2929 | time 465.9836 ms | tok/sec 1125121.0710
for step 9209 | loss 3.402660 | norm 0.2844 | time 465.2669 ms | tok/sec 1126854.1811
for step 9210 | loss 3.362844 | norm 0.3162 | time 465.8558 ms | tok/sec 1125429.7113
for step 9211 | loss 3.360115 | norm 0.2662 | time 465.3153 ms | tok/sec 1126736.9733
for step 9212 | loss 3.434937 | norm 0.3034 | time 465.5588 ms | tok/sec 1126147.8396
for step 9213 | loss 3.321098 | norm 0.3149 | time 464.8869 ms | tok/sec 1127775.3702
for step 9214 | loss 3.400327 | norm 0.3097 | time 465.0598 ms | tok/sec 1127356.1985
for step 9215 | loss 3.358121 | norm 0.3238 | time 465.9688 ms | tok/sec 1125156.7633
for step 9216 | loss 3.333982 | norm 0.3024 | time 465.5480 ms | tok/sec 1126173.7923
for step 9217 | loss 3.339923 | norm 0.3208 | time 465.8136 ms | tok/sec 1125531.6689
for step 9218 | loss 3.530871 | norm 0.3421 | time 466.5194 ms | tok/sec 1123829.0406
for step 9219 | loss 3.369689 | norm 0.3936 | time 465.4222 ms | tok/sec 1126478.3941
for step 9220 | loss 3.338212 | norm 0.3726 | time 464.4091 ms | tok/sec 1128935.6419
for step 9221 | loss 3.333665 | norm 0.3612 | time 465.7362 ms | tok/sec 1125718.9273
for step 9222 | loss 3.344499 | norm 0.3680 | time 465.1771 ms | tok/sec 1127071.9174
for step 9223 | loss 3.311627 | norm 0.3998 | time 465.0331 ms | tok/sec 1127420.9330
for step 9224 | loss 3.310242 | norm 0.3303 | time 465.2724 ms | tok/sec 1126840.9001
for step 9225 | loss 3.337851 | norm 0.3608 | time 465.0464 ms | tok/sec 1127388.5648
for step 9226 | loss 3.361342 | norm 0.3064 | time 464.9899 ms | tok/sec 1127525.5642
for step 9227 | loss 3.377367 | norm 0.3279 | time 465.0822 ms | tok/sec 1127301.8736
for step 9228 | loss 3.317815 | norm 0.3141 | time 465.9443 ms | tok/sec 1125216.0635
for step 9229 | loss 3.380551 | norm 0.3375 | time 465.0207 ms | tok/sec 1127450.9908
for step 9230 | loss 3.293376 | norm 0.3279 | time 465.2498 ms | tok/sec 1126895.7581
for step 9231 | loss 3.349521 | norm 0.3248 | time 465.9586 ms | tok/sec 1125181.5189
for step 9232 | loss 3.323994 | norm 0.3153 | time 465.0991 ms | tok/sec 1127260.8444
for step 9233 | loss 3.389233 | norm 0.3195 | time 465.7376 ms | tok/sec 1125715.4696
for step 9234 | loss 3.358741 | norm 0.3264 | time 465.1294 ms | tok/sec 1127187.4615
for step 9235 | loss 3.309099 | norm 0.3263 | time 465.0760 ms | tok/sec 1127316.8991
for step 9236 | loss 3.376590 | norm 0.3199 | time 465.1282 ms | tok/sec 1127190.3504
for step 9237 | loss 3.400999 | norm 0.3287 | time 465.3492 ms | tok/sec 1126655.0000
for step 9238 | loss 3.364276 | norm 0.3314 | time 465.3273 ms | tok/sec 1126708.1081
for step 9239 | loss 3.372495 | norm 0.3534 | time 465.6606 ms | tok/sec 1125901.6362
for step 9240 | loss 3.332743 | norm 0.3275 | time 465.9476 ms | tok/sec 1125208.0029
for step 9241 | loss 3.328624 | norm 0.3149 | time 465.3649 ms | tok/sec 1126616.9039
for step 9242 | loss 3.352681 | norm 0.2891 | time 464.5917 ms | tok/sec 1128491.8631
for step 9243 | loss 3.344788 | norm 0.2991 | time 464.6394 ms | tok/sec 1128376.0514
for step 9244 | loss 3.338200 | norm 0.2827 | time 465.3296 ms | tok/sec 1126702.3352
for step 9245 | loss 3.318886 | norm 0.3204 | time 465.4000 ms | tok/sec 1126532.0626
for step 9246 | loss 3.364855 | norm 0.2832 | time 464.8218 ms | tok/sec 1127933.2908
for step 9247 | loss 3.364486 | norm 0.2907 | time 464.7915 ms | tok/sec 1128006.7708
for step 9248 | loss 3.448474 | norm 0.3344 | time 465.0023 ms | tok/sec 1127495.5024
for step 9249 | loss 3.372038 | norm 0.3191 | time 465.0712 ms | tok/sec 1127328.4575
validation loss 3.3525
HellaSwag accuracy: 2755/10042=0.2743
> Hello, I'm a language model, and my father was a very strong head. This has had much of a tremendous impact, as he has become a very
> Hello, I'm a language model, a model that explains how to create language models.
But there's a new class, the
- The language model
> Hello, I'm a language model, but where is this going to be?
I'm just beginning to get bored.
First I need a model that
> Hello, I'm a language model, and I'm looking at other languages too many times. They haven't got one, if you have a nice model.
> Hello, I'm a language model, so I want to understand.
I started off writing a simple code (as I know one of the examples I have
> Hello, I'm a language model, for a language pathologist. That means my question will give you a rough idea of what you're actually doing then,
> > Hello, I'm a language model, I wasn't that easy in my time.
I'm trying to do a short survey of your topics, including how
Hello, I'm a language model, and I can't help feeling comfortable doing more for people. I use simple and easy language to understand and express myself better
> Hello, I'm a language model, and i try that in my head again now :) As a member, as an ELL, my language model doesn't
> Hello, I'm a language model, so there's a lot of questions I'm asking? Are you using speech as an aid and not being used as a
> Hello, I'm a language model, and I know that there is a lot left to be told.
First, go into the basics. I'm not> 
Hello, I'm a language model, so I decided to help students come up with fun, interactive ways.
I also like to use the "language model
> Hello, I'm a language model, my instructor said, "Yeah, here's a word cloud that's going to say it's going to say it's> 
Hello, I'm a language model, and when I get to the end of it, I'm confused.
The goal of this post is to help lingu
> > > Hello, I'm a language model, like that.
And they look up, and then you see an image with your name and a date, and in
Hello, I'm a language model, and I've only mentioned one and was told to describe the language that exists in the language.
But, if that
Hello, I'm a language model, and the answer would be: "How does it work" (I like the sentence "I am a model" instead
> > Hello, I'm a language model, and I like to do things in a little bit different.
I don't have any problem with this because I'm
> Hello, I'm a language model, then, to my degree (language): A fluent language is called an english language. It is usually composed of only 20
> Hello, I'm a language model, but I feel a little uncomfortable with being presented with software. It's important to know that there is not one correct way
>Hello, I'm a language model, but this isn't quite as easy as I thought.
So when you say "It's a different game". It
 Hello, I'm a language model, but the most important question is, 'how does a model of linguistic interaction help you understand the way your audience has used> > 
Hello, I'm a language model, so I am trying to understand it well enough to understand it. But to do this, you really need to know about
> Hello, I'm a language model, and that's when I started building a model that I wanted to follow. So I'm developing a 'toolkit'
> > Hello, I'm a language model, and I'm not trying to break from the rules, but really I guess everyone's in.
To get a basic
Hello, I'm a language model, is very popular and really important!
What Do I Mean by
I am using this to describe this kind of activity
Hello, I'm a language model, but for how to write an essay on mla paper. In this article, we are the beginners, as we've
> > Hello, I'm a language model, and this will be a two day project, so I will be able to understand the language.
First, this tutorial
Hello, I'm a language model, and I will show you how to code a program.
First, let's start up with a program.
This
> Hello, I'm a language model, and you know me using a tool called “listeners”, so they can easily build your model that I> 
Hello, I'm a language model, with some exercises. I will use the example above (see below).
At I, the only things that you've
> Hello, I'm a language model, and as a professional language consultant at DPT I am often able to give you insights into your own areas.
When
for step 9250 | loss 3.330543 | norm 0.3101 | time 12985.3082 ms | tok/sec 40375.4761
for step 9251 | loss 3.306986 | norm 0.2985 | time 462.3199 ms | tok/sec 1134037.3963
for step 9252 | loss 3.334709 | norm 0.2939 | time 463.7177 ms | tok/sec 1130618.9112
for step 9253 | loss 3.429840 | norm 0.4150 | time 463.0101 ms | tok/sec 1132346.8557
for step 9254 | loss 3.348601 | norm 0.3200 | time 463.5260 ms | tok/sec 1131086.4722
for step 9255 | loss 3.381878 | norm 0.3486 | time 463.0864 ms | tok/sec 1132160.3004
for step 9256 | loss 3.358998 | norm 0.3305 | time 463.1016 ms | tok/sec 1132122.9968
for step 9257 | loss 3.370504 | norm 0.3906 | time 463.7120 ms | tok/sec 1130632.8626
for step 9258 | loss 3.263170 | norm 0.3723 | time 463.3784 ms | tok/sec 1131446.7112
for step 9259 | loss 3.343052 | norm 0.3068 | time 464.4370 ms | tok/sec 1128867.8358
for step 9260 | loss 3.302559 | norm 0.3500 | time 463.3067 ms | tok/sec 1131621.9669
for step 9261 | loss 3.330539 | norm 0.3421 | time 463.2242 ms | tok/sec 1131823.4907
for step 9262 | loss 3.328989 | norm 0.3105 | time 463.0485 ms | tok/sec 1132252.9874
for step 9263 | loss 3.366909 | norm 0.3863 | time 464.1225 ms | tok/sec 1129632.7194
for step 9264 | loss 3.354494 | norm 0.3674 | time 465.1773 ms | tok/sec 1127071.3397
for step 9265 | loss 3.274802 | norm 0.3421 | time 465.1279 ms | tok/sec 1127190.9282
for step 9266 | loss 3.375159 | norm 0.3697 | time 465.1070 ms | tok/sec 1127241.7755
for step 9267 | loss 3.374599 | norm 0.3714 | time 464.0102 ms | tok/sec 1129906.1019
for step 9268 | loss 3.358212 | norm 0.3389 | time 464.2582 ms | tok/sec 1129302.6312
for step 9269 | loss 3.357159 | norm 0.3458 | time 464.1945 ms | tok/sec 1129457.4992
for step 9270 | loss 3.440775 | norm 0.4145 | time 464.6420 ms | tok/sec 1128369.6824
for step 9271 | loss 3.325985 | norm 0.3511 | time 464.9224 ms | tok/sec 1127689.1978
for step 9272 | loss 3.374950 | norm 0.3719 | time 465.1890 ms | tok/sec 1127043.0350
for step 9273 | loss 3.336138 | norm 0.3855 | time 465.0187 ms | tok/sec 1127455.6152
for step 9274 | loss 3.338657 | norm 0.3285 | time 464.9203 ms | tok/sec 1127694.4024
for step 9275 | loss 3.346658 | norm 0.3831 | time 465.0812 ms | tok/sec 1127304.1852
for step 9276 | loss 3.386235 | norm 0.3293 | time 464.8418 ms | tok/sec 1127884.6952
for step 9277 | loss 3.368794 | norm 0.3402 | time 466.0060 ms | tok/sec 1125066.9613
for step 9278 | loss 3.324414 | norm 0.3194 | time 464.3884 ms | tok/sec 1128986.0670
for step 9279 | loss 3.301004 | norm 0.3289 | time 465.2915 ms | tok/sec 1126794.7080
for step 9280 | loss 3.339155 | norm 0.3078 | time 465.6494 ms | tok/sec 1125928.7306
for step 9281 | loss 3.340772 | norm 0.2843 | time 465.7238 ms | tok/sec 1125748.8944
for step 9282 | loss 3.371130 | norm 0.3338 | time 465.3695 ms | tok/sec 1126605.9373
for step 9283 | loss 3.339778 | norm 0.2857 | time 464.5884 ms | tok/sec 1128499.9708
for step 9284 | loss 3.373477 | norm 0.2887 | time 466.7161 ms | tok/sec 1123355.4080
for step 9285 | loss 3.348046 | norm 0.2987 | time 465.3859 ms | tok/sec 1126566.1130
for step 9286 | loss 3.353431 | norm 0.3088 | time 465.4808 ms | tok/sec 1126336.4568
for step 9287 | loss 3.338133 | norm 0.3557 | time 466.2895 ms | tok/sec 1124382.9793
for step 9288 | loss 3.355883 | norm 0.2971 | time 464.8178 ms | tok/sec 1127943.1262
for step 9289 | loss 3.324828 | norm 0.3104 | time 465.5259 ms | tok/sec 1126227.4318
for step 9290 | loss 3.338782 | norm 0.2961 | time 464.5970 ms | tok/sec 1128479.1226
for step 9291 | loss 3.329753 | norm 0.3182 | time 465.4679 ms | tok/sec 1126367.6107
for step 9292 | loss 3.331401 | norm 0.3151 | time 465.2822 ms | tok/sec 1126817.2262
for step 9293 | loss 3.304037 | norm 0.3129 | time 465.3604 ms | tok/sec 1126627.8707
for step 9294 | loss 3.375530 | norm 0.3300 | time 464.3247 ms | tok/sec 1129140.8482
for step 9295 | loss 3.291584 | norm 0.3034 | time 465.0304 ms | tok/sec 1127427.2912
for step 9296 | loss 3.292653 | norm 0.3250 | time 465.1015 ms | tok/sec 1127255.0658
for step 9297 | loss 3.364187 | norm 0.3072 | time 465.3163 ms | tok/sec 1126734.6640
for step 9298 | loss 3.336799 | norm 0.3701 | time 464.9425 ms | tok/sec 1127640.6231
for step 9299 | loss 3.296466 | norm 0.3061 | time 466.4049 ms | tok/sec 1124104.7924
for step 9300 | loss 3.366411 | norm 0.3420 | time 465.5240 ms | tok/sec 1126232.0462
for step 9301 | loss 3.394193 | norm 0.3327 | time 465.2901 ms | tok/sec 1126798.1723
for step 9302 | loss 3.356205 | norm 0.3019 | time 465.3287 ms | tok/sec 1126704.6444
for step 9303 | loss 3.344368 | norm 0.3425 | time 465.9717 ms | tok/sec 1125149.8549
for step 9304 | loss 3.354158 | norm 0.3213 | time 465.6532 ms | tok/sec 1125919.5068
for step 9305 | loss 3.369354 | norm 0.2906 | time 465.6506 ms | tok/sec 1125925.8482
for step 9306 | loss 3.351482 | norm 0.3131 | time 465.0245 ms | tok/sec 1127441.7421
for step 9307 | loss 3.345514 | norm 0.2981 | time 465.4565 ms | tok/sec 1126395.3045
for step 9308 | loss 3.391137 | norm 0.3263 | time 465.1291 ms | tok/sec 1127188.0393
for step 9309 | loss 3.337136 | norm 0.3073 | time 465.2495 ms | tok/sec 1126896.3356
for step 9310 | loss 3.521314 | norm 0.3848 | time 464.9289 ms | tok/sec 1127673.5840
for step 9311 | loss 3.353163 | norm 0.3834 | time 464.9868 ms | tok/sec 1127533.0799
for step 9312 | loss 3.387385 | norm 0.3178 | time 465.5838 ms | tok/sec 1126087.2878
for step 9313 | loss 3.338954 | norm 0.3930 | time 465.4288 ms | tok/sec 1126462.2368
for step 9314 | loss 3.425959 | norm 0.3830 | time 465.2216 ms | tok/sec 1126963.9050
for step 9315 | loss 3.342574 | norm 0.3313 | time 465.4987 ms | tok/sec 1126293.1904
for step 9316 | loss 3.408057 | norm 0.3441 | time 465.2898 ms | tok/sec 1126798.7497
for step 9317 | loss 3.404005 | norm 0.3609 | time 465.6546 ms | tok/sec 1125916.0480
for step 9318 | loss 3.382734 | norm 0.2967 | time 466.4636 ms | tok/sec 1123963.4527
for step 9319 | loss 3.387919 | norm 0.3285 | time 465.6734 ms | tok/sec 1125870.5081
for step 9320 | loss 3.356437 | norm 0.3464 | time 464.5355 ms | tok/sec 1128628.5515
for step 9321 | loss 3.342289 | norm 0.3011 | time 466.1009 ms | tok/sec 1124837.9159
for step 9322 | loss 3.328941 | norm 0.3076 | time 464.8247 ms | tok/sec 1127926.3483
for step 9323 | loss 3.331800 | norm 0.2875 | time 464.9889 ms | tok/sec 1127527.8767
for step 9324 | loss 3.374949 | norm 0.2942 | time 464.6611 ms | tok/sec 1128323.3649
for step 9325 | loss 3.271575 | norm 0.3477 | time 464.6008 ms | tok/sec 1128469.8570
for step 9326 | loss 3.289189 | norm 0.3098 | time 465.4746 ms | tok/sec 1126351.4566
for step 9327 | loss 3.361562 | norm 0.3002 | time 465.1899 ms | tok/sec 1127040.7245
for step 9328 | loss 3.361703 | norm 0.3153 | time 465.6756 ms | tok/sec 1125865.3203
for step 9329 | loss 3.314882 | norm 0.3145 | time 465.4083 ms | tok/sec 1126511.8641
for step 9330 | loss 3.309110 | norm 0.3257 | time 464.5765 ms | tok/sec 1128528.9278
for step 9331 | loss 3.307952 | norm 0.2859 | time 464.8354 ms | tok/sec 1127900.3147
for step 9332 | loss 3.333329 | norm 0.3271 | time 465.4286 ms | tok/sec 1126462.8138
for step 9333 | loss 3.369963 | norm 0.3075 | time 464.4291 ms | tok/sec 1128886.9598
Will loading at 0 from edu_fineweb10B/edufineweb_train_000050.npy
for step 9334 | loss 3.360661 | norm 0.3210 | time 1387.5310 ms | tok/sec 377856.7716
for step 9335 | loss 3.357508 | norm 0.3140 | time 464.8597 ms | tok/sec 1127841.3097
for step 9336 | loss 3.445591 | norm 0.3129 | time 465.0512 ms | tok/sec 1127377.0052
for step 9337 | loss 3.348034 | norm 0.2955 | time 464.6218 ms | tok/sec 1128418.8989
for step 9338 | loss 3.308264 | norm 0.3243 | time 464.4215 ms | tok/sec 1128905.5049
for step 9339 | loss 3.342906 | norm 0.3148 | time 465.1446 ms | tok/sec 1127150.4848
for step 9340 | loss 3.364831 | norm 0.3038 | time 464.2060 ms | tok/sec 1129429.6546
for step 9341 | loss 3.389205 | norm 0.3086 | time 464.4167 ms | tok/sec 1128917.0958
for step 9342 | loss 3.381641 | norm 0.3298 | time 465.4186 ms | tok/sec 1126487.0499
for step 9343 | loss 3.266742 | norm 0.3172 | time 464.1290 ms | tok/sec 1129617.0518
for step 9344 | loss 3.308064 | norm 0.3411 | time 464.8697 ms | tok/sec 1127817.0153
for step 9345 | loss 3.367531 | norm 0.3183 | time 464.3168 ms | tok/sec 1129159.9814
for step 9346 | loss 3.390133 | norm 0.3332 | time 463.9044 ms | tok/sec 1130163.9340
for step 9347 | loss 3.339718 | norm 0.3056 | time 465.2689 ms | tok/sec 1126849.5616
for step 9348 | loss 3.416694 | norm 0.3157 | time 464.4568 ms | tok/sec 1128819.7391
for step 9349 | loss 3.402951 | norm 0.3372 | time 465.4281 ms | tok/sec 1126463.9679
for step 9350 | loss 3.429640 | norm 0.3354 | time 465.5809 ms | tok/sec 1126094.2076
for step 9351 | loss 3.424906 | norm 0.3359 | time 465.0357 ms | tok/sec 1127414.5748
for step 9352 | loss 3.413890 | norm 0.3528 | time 464.7546 ms | tok/sec 1128096.4641
for step 9353 | loss 3.365336 | norm 0.3075 | time 465.1990 ms | tok/sec 1127018.7750
for step 9354 | loss 3.380496 | norm 0.3393 | time 464.5991 ms | tok/sec 1128473.9107
for step 9355 | loss 3.419018 | norm 0.3303 | time 464.7717 ms | tok/sec 1128054.7983
for step 9356 | loss 3.367104 | norm 0.3007 | time 465.1778 ms | tok/sec 1127070.1844
for step 9357 | loss 3.320936 | norm 0.3151 | time 464.8993 ms | tok/sec 1127745.2951
for step 9358 | loss 3.336420 | norm 0.2880 | time 464.7846 ms | tok/sec 1128023.5511
for step 9359 | loss 3.357983 | norm 0.3016 | time 467.7060 ms | tok/sec 1120977.7920
for step 9360 | loss 3.327305 | norm 0.2879 | time 465.3199 ms | tok/sec 1126726.0043
for step 9361 | loss 3.321588 | norm 0.3222 | time 464.0551 ms | tok/sec 1129796.9652
for step 9362 | loss 3.273316 | norm 0.3236 | time 465.3256 ms | tok/sec 1126712.1491
for step 9363 | loss 3.337814 | norm 0.3466 | time 465.5993 ms | tok/sec 1126049.8066
for step 9364 | loss 3.345303 | norm 0.3197 | time 466.0697 ms | tok/sec 1124913.2949
for step 9365 | loss 3.303864 | norm 0.3013 | time 465.2047 ms | tok/sec 1127004.9126
for step 9366 | loss 3.366782 | norm 0.3275 | time 464.6678 ms | tok/sec 1128307.1547
for step 9367 | loss 3.388446 | norm 0.3276 | time 464.2892 ms | tok/sec 1129227.2427
for step 9368 | loss 3.342541 | norm 0.3172 | time 465.7233 ms | tok/sec 1125750.0470
for step 9369 | loss 3.372370 | norm 0.3204 | time 464.6244 ms | tok/sec 1128412.5295
for step 9370 | loss 3.345290 | norm 0.3102 | time 465.3399 ms | tok/sec 1126677.5126
for step 9371 | loss 3.353244 | norm 0.3685 | time 464.8495 ms | tok/sec 1127866.1836
for step 9372 | loss 3.368367 | norm 0.3237 | time 464.9630 ms | tok/sec 1127590.8963
for step 9373 | loss 3.396242 | norm 0.3205 | time 464.9346 ms | tok/sec 1127659.7055
for step 9374 | loss 3.302537 | norm 0.3082 | time 465.4648 ms | tok/sec 1126375.1110
for step 9375 | loss 3.385050 | norm 0.3263 | time 465.2596 ms | tok/sec 1126872.0819
for step 9376 | loss 3.339541 | norm 0.2982 | time 465.5008 ms | tok/sec 1126287.9987
for step 9377 | loss 3.326611 | norm 0.3171 | time 465.6322 ms | tok/sec 1125970.2394
for step 9378 | loss 3.347497 | norm 0.3196 | time 464.9813 ms | tok/sec 1127546.3771
for step 9379 | loss 3.403211 | norm 0.3224 | time 465.8544 ms | tok/sec 1125433.1672
for step 9380 | loss 3.383075 | norm 0.3408 | time 466.3229 ms | tok/sec 1124302.4979
for step 9381 | loss 3.363457 | norm 0.3172 | time 465.2374 ms | tok/sec 1126925.7880
for step 9382 | loss 3.485807 | norm 0.3758 | time 465.7335 ms | tok/sec 1125725.2663
for step 9383 | loss 3.411738 | norm 0.4047 | time 465.0488 ms | tok/sec 1127382.7850
for step 9384 | loss 3.378002 | norm 0.4252 | time 465.3111 ms | tok/sec 1126747.3651
for step 9385 | loss 3.385958 | norm 0.3892 | time 464.8080 ms | tok/sec 1127966.8474
for step 9386 | loss 3.342325 | norm 0.3591 | time 464.9231 ms | tok/sec 1127687.4629
for step 9387 | loss 3.311229 | norm 0.3472 | time 466.0556 ms | tok/sec 1124947.2475
for step 9388 | loss 3.352229 | norm 0.3380 | time 465.8899 ms | tok/sec 1125347.3523
for step 9389 | loss 3.358048 | norm 0.3227 | time 465.6649 ms | tok/sec 1125891.2600
for step 9390 | loss 3.364023 | norm 0.3432 | time 464.7756 ms | tok/sec 1128045.5397
for step 9391 | loss 3.383220 | norm 0.3410 | time 465.4949 ms | tok/sec 1126302.4203
for step 9392 | loss 3.336057 | norm 0.3161 | time 465.3225 ms | tok/sec 1126719.6540
for step 9393 | loss 3.317334 | norm 0.3300 | time 465.0943 ms | tok/sec 1127272.4016
for step 9394 | loss 3.379737 | norm 0.3345 | time 464.6189 ms | tok/sec 1128425.8475
for step 9395 | loss 3.372941 | norm 0.3858 | time 465.4787 ms | tok/sec 1126341.6490
for step 9396 | loss 3.332137 | norm 0.3184 | time 465.3840 ms | tok/sec 1126570.7301
for step 9397 | loss 3.322971 | norm 0.3391 | time 465.5025 ms | tok/sec 1126283.9607
for step 9398 | loss 3.374730 | norm 0.3262 | time 465.7481 ms | tok/sec 1125690.1142
for step 9399 | loss 3.461730 | norm 0.3271 | time 464.8323 ms | tok/sec 1127907.8354
for step 9400 | loss 3.336700 | norm 0.3123 | time 464.8433 ms | tok/sec 1127881.2242
for step 9401 | loss 3.428414 | norm 0.3446 | time 465.2977 ms | tok/sec 1126779.6964
for step 9402 | loss 3.334924 | norm 0.3259 | time 464.9963 ms | tok/sec 1127509.9550
for step 9403 | loss 3.356951 | norm 0.2963 | time 465.1778 ms | tok/sec 1127070.1844
for step 9404 | loss 3.344886 | norm 0.3052 | time 465.2829 ms | tok/sec 1126815.4940
for step 9405 | loss 3.365830 | norm 0.3219 | time 464.8170 ms | tok/sec 1127944.8619
for step 9406 | loss 3.350601 | norm 0.2949 | time 465.7264 ms | tok/sec 1125742.5551
for step 9407 | loss 3.342788 | norm 0.3009 | time 464.8995 ms | tok/sec 1127744.7168
for step 9408 | loss 3.369623 | norm 0.3089 | time 465.3063 ms | tok/sec 1126758.9118
for step 9409 | loss 3.379912 | norm 0.3884 | time 465.2767 ms | tok/sec 1126830.5066
for step 9410 | loss 3.390644 | norm 0.3956 | time 465.6475 ms | tok/sec 1125933.3426
for step 9411 | loss 3.397127 | norm 0.3592 | time 464.6535 ms | tok/sec 1128341.8914
for step 9412 | loss 3.326181 | norm 0.3711 | time 465.2207 ms | tok/sec 1126966.2152
for step 9413 | loss 3.306922 | norm 0.3170 | time 466.2588 ms | tok/sec 1124457.1474
for step 9414 | loss 3.362780 | norm 0.3733 | time 464.7419 ms | tok/sec 1128127.1367
for step 9415 | loss 3.404555 | norm 0.3487 | time 465.7815 ms | tok/sec 1125609.4456
for step 9416 | loss 3.323273 | norm 0.3599 | time 465.5461 ms | tok/sec 1126178.4063
for step 9417 | loss 3.376791 | norm 0.3205 | time 465.4055 ms | tok/sec 1126518.7892
for step 9418 | loss 3.332020 | norm 0.3368 | time 465.6360 ms | tok/sec 1125961.0150
for step 9419 | loss 3.335838 | norm 0.3413 | time 465.8031 ms | tok/sec 1125557.0172
for step 9420 | loss 3.390924 | norm 0.3041 | time 464.8912 ms | tok/sec 1127764.9594
for step 9421 | loss 3.360044 | norm 0.3176 | time 464.9334 ms | tok/sec 1127662.5968
for step 9422 | loss 3.371276 | norm 0.3113 | time 466.8469 ms | tok/sec 1123040.4482
for step 9423 | loss 3.334293 | norm 0.3107 | time 465.6308 ms | tok/sec 1125973.6987
for step 9424 | loss 3.347656 | norm 0.3238 | time 465.5306 ms | tok/sec 1126215.8960
for step 9425 | loss 3.402598 | norm 0.3300 | time 465.2336 ms | tok/sec 1126935.0282
for step 9426 | loss 3.369863 | norm 0.3053 | time 465.7676 ms | tok/sec 1125642.8640
for step 9427 | loss 3.379182 | norm 0.3518 | time 465.2975 ms | tok/sec 1126780.2738
for step 9428 | loss 3.363525 | norm 0.2990 | time 465.8673 ms | tok/sec 1125402.0650
for step 9429 | loss 3.287255 | norm 0.3097 | time 465.4169 ms | tok/sec 1126491.0894
for step 9430 | loss 3.323458 | norm 0.3106 | time 465.2641 ms | tok/sec 1126861.1104
for step 9431 | loss 3.340099 | norm 0.3464 | time 464.9024 ms | tok/sec 1127737.7766
for step 9432 | loss 3.335078 | norm 0.3176 | time 465.8623 ms | tok/sec 1125414.1601
for step 9433 | loss 3.316063 | norm 0.3230 | time 464.7939 ms | tok/sec 1128000.9846
for step 9434 | loss 3.315243 | norm 0.2974 | time 465.2624 ms | tok/sec 1126865.1525
for step 9435 | loss 3.291437 | norm 0.2927 | time 465.7071 ms | tok/sec 1125789.2373
for step 9436 | loss 3.382080 | norm 0.3260 | time 465.3122 ms | tok/sec 1126744.4784
for step 9437 | loss 3.306737 | norm 0.2890 | time 465.6763 ms | tok/sec 1125863.5910
for step 9438 | loss 3.331722 | norm 0.3202 | time 465.7254 ms | tok/sec 1125744.8603
for step 9439 | loss 3.315253 | norm 0.2743 | time 464.8476 ms | tok/sec 1127870.8115
for step 9440 | loss 3.403279 | norm 0.3117 | time 464.6382 ms | tok/sec 1128378.9464
for step 9441 | loss 3.388907 | norm 0.3284 | time 465.4965 ms | tok/sec 1126298.3822
for step 9442 | loss 3.344643 | norm 0.2995 | time 464.6218 ms | tok/sec 1128418.8989
for step 9443 | loss 3.379474 | norm 0.3124 | time 464.3111 ms | tok/sec 1129173.8969
for step 9444 | loss 3.313756 | norm 0.3326 | time 465.6050 ms | tok/sec 1126035.9680
for step 9445 | loss 3.370752 | norm 0.2918 | time 465.1144 ms | tok/sec 1127223.8628
for step 9446 | loss 3.325656 | norm 0.3523 | time 465.3831 ms | tok/sec 1126573.0387
for step 9447 | loss 3.347650 | norm 0.3128 | time 465.6692 ms | tok/sec 1125880.8840
for step 9448 | loss 3.286139 | norm 0.3151 | time 464.8964 ms | tok/sec 1127752.2354
for step 9449 | loss 3.420668 | norm 0.3673 | time 465.9777 ms | tok/sec 1125135.4628
for step 9450 | loss 3.369236 | norm 0.3346 | time 465.1845 ms | tok/sec 1127054.0101
for step 9451 | loss 3.382126 | norm 0.3265 | time 465.0614 ms | tok/sec 1127352.1529
for step 9452 | loss 3.327848 | norm 0.3555 | time 465.1992 ms | tok/sec 1127018.1974
for step 9453 | loss 3.383722 | norm 0.3540 | time 465.1947 ms | tok/sec 1127029.1720
for step 9454 | loss 3.361762 | norm 0.2882 | time 466.0242 ms | tok/sec 1125023.2168
for step 9455 | loss 3.364070 | norm 0.3381 | time 464.8228 ms | tok/sec 1127930.9767
for step 9456 | loss 3.388133 | norm 0.3133 | time 464.5109 ms | tok/sec 1128688.2182
for step 9457 | loss 3.365612 | norm 0.3337 | time 465.4145 ms | tok/sec 1126496.8601
for step 9458 | loss 3.364537 | norm 0.3296 | time 465.5113 ms | tok/sec 1126262.6175
for step 9459 | loss 3.355091 | norm 0.3162 | time 465.5697 ms | tok/sec 1126121.3113
for step 9460 | loss 3.344809 | norm 0.3091 | time 465.0598 ms | tok/sec 1127356.1985
for step 9461 | loss 3.374028 | norm 0.3180 | time 465.7331 ms | tok/sec 1125726.4189
for step 9462 | loss 3.382410 | norm 0.3347 | time 466.1994 ms | tok/sec 1124600.3367
for step 9463 | loss 3.391940 | norm 0.3040 | time 464.8876 ms | tok/sec 1127773.6351
for step 9464 | loss 3.364327 | norm 0.3149 | time 465.1194 ms | tok/sec 1127211.7288
for step 9465 | loss 3.346618 | norm 0.3118 | time 465.2143 ms | tok/sec 1126981.8094
for step 9466 | loss 3.304784 | norm 0.2776 | time 464.7179 ms | tok/sec 1128185.5928
for step 9467 | loss 3.391131 | norm 0.3399 | time 465.5426 ms | tok/sec 1126187.0575
for step 9468 | loss 3.354682 | norm 0.3253 | time 464.8485 ms | tok/sec 1127868.4975
for step 9469 | loss 3.310949 | norm 0.3172 | time 465.4441 ms | tok/sec 1126425.3077
for step 9470 | loss 3.345663 | norm 0.3262 | time 465.3103 ms | tok/sec 1126749.0971
for step 9471 | loss 3.359507 | norm 0.3110 | time 465.1845 ms | tok/sec 1127054.0101
for step 9472 | loss 3.373224 | norm 0.3191 | time 466.0950 ms | tok/sec 1124852.3004
for step 9473 | loss 3.332311 | norm 0.3131 | time 465.4238 ms | tok/sec 1126474.3547
for step 9474 | loss 3.318635 | norm 0.3205 | time 466.3193 ms | tok/sec 1124311.1203
for step 9475 | loss 3.327350 | norm 0.3062 | time 464.5729 ms | tok/sec 1128537.6152
for step 9476 | loss 3.332329 | norm 0.3354 | time 464.7882 ms | tok/sec 1128014.8716
for step 9477 | loss 3.368410 | norm 0.3133 | time 464.7570 ms | tok/sec 1128090.6770
for step 9478 | loss 3.280581 | norm 0.3208 | time 465.2851 ms | tok/sec 1126810.2974
for step 9479 | loss 3.341036 | norm 0.3221 | time 464.4558 ms | tok/sec 1128822.0569
for step 9480 | loss 3.322057 | norm 0.3178 | time 466.1701 ms | tok/sec 1124671.0822
for step 9481 | loss 3.353585 | norm 0.3126 | time 464.5061 ms | tok/sec 1128699.8047
for step 9482 | loss 3.324303 | norm 0.3270 | time 464.7262 ms | tok/sec 1128165.3350
for step 9483 | loss 3.337091 | norm 0.3298 | time 464.7450 ms | tok/sec 1128119.6130
for step 9484 | loss 3.358408 | norm 0.2946 | time 465.6162 ms | tok/sec 1126008.8685
for step 9485 | loss 3.308064 | norm 0.3257 | time 464.9506 ms | tok/sec 1127620.9631
for step 9486 | loss 3.408912 | norm 0.3121 | time 464.8685 ms | tok/sec 1127819.9075
for step 9487 | loss 3.360226 | norm 0.3717 | time 465.2903 ms | tok/sec 1126797.5949
for step 9488 | loss 3.372568 | norm 0.3542 | time 464.6246 ms | tok/sec 1128411.9504
for step 9489 | loss 3.362853 | norm 0.3404 | time 465.6425 ms | tok/sec 1125945.4491
for step 9490 | loss 3.376684 | norm 0.3158 | time 464.3869 ms | tok/sec 1128989.5448
for step 9491 | loss 3.361426 | norm 0.3227 | time 465.9956 ms | tok/sec 1125092.2886
for step 9492 | loss 3.366133 | norm 0.3092 | time 465.2367 ms | tok/sec 1126927.5205
for step 9493 | loss 3.365497 | norm 0.3120 | time 464.9086 ms | tok/sec 1127722.7398
for step 9494 | loss 3.361245 | norm 0.2924 | time 464.8743 ms | tok/sec 1127806.0253
for step 9495 | loss 3.359824 | norm 0.3301 | time 465.9994 ms | tok/sec 1125083.0785
for step 9496 | loss 3.376627 | norm 0.2859 | time 463.9847 ms | tok/sec 1129968.2264
for step 9497 | loss 3.369035 | norm 0.3173 | time 465.2522 ms | tok/sec 1126889.9834
for step 9498 | loss 3.370098 | norm 0.3131 | time 465.3432 ms | tok/sec 1126669.4311
for step 9499 | loss 3.309124 | norm 0.2859 | time 464.7911 ms | tok/sec 1128007.9281
validation loss 3.3488
HellaSwag accuracy: 2759/10042=0.2747
> Hello, I'm a language model, and this allows me to look at complex sentences more deeply. To do this, I have to go through all the different
> Hello, I'm a language model, i am using Google Docs as it supports the model above. Please make sure that by visiting our site, you are
> Hello, I'm a language model, but let's do it.
Okay, so I have been working in a language lab for some time, it seemed
> Hello, I'm a language model, and I'm wondering what about this sentence-form? Now here's where it gets an enormous amount of inspiration.

> Hello, I'm a language model, so I just want to be able to find out the language and the ways in which language differentiates words to words,
> Hello, I'm a language model, who I am so grateful for writing here in person? Why not come visit the world's most famous speaker and post it
> Hello, I'm a language model, I’m trying to understand some concepts, but I think it's really simple.
- Can you translate the
> Hello, I'm a language model, and i came along for an interview which really was to explore new language theory and explain how to solve problems. However I
> > Hello, I'm a language model, have to do with it anyway, I am a programmer and it needs a certain amount of understanding.
So, when
Hello, I'm a language model, and I use it as my own. Just like this one, I'd like to think that our students would be better
> >> Hello, I'm a language model, and I like to learn it. I get it and I get it again. I like it, I don't think
 Hello, I'm a language model, and I don't understand all my classes much by myself.
A sentence, sentence. "It was my first gradeHello, I'm a language model, with at least one of my most personal characteristics. Like grammar, I'm a grammar nerd, and that's what my

> > > Hello, I'm a language model, but that is not exactly true! You can tell that the word model is pretty simple, but it makes more sense.
Hello, I'm a language model, and I am currently helping you here."
"Don't you really just want to be a language model?"<|endoftext|>This
Hello, I'm a language model, so let's start by defining some "language". However, let's learn about the various different languages I use.

> > > > Hello, I'm a language model, and the best model for modeling languages: I've seen this model in practice, but I'm also interested how it works
Hello, I'm a language model, and we can get information about all of our languages.
I have a copy of my favourite language file from YouTube by
Hello, I'm a language model, so it's not hard to visualize them. What I'm doing here is a way to illustrate one's language's very
Hello, I'm a language model, so I used a two-inputted model from the previous model (in this case, a two-inputed design
> > > Hello, I'm a language model, but I wish it was.
But do you think that it's true?
I don't mean that it's
Hello, I'm a language model, and I'm not. This project only shows how you can translate the XML within XML-defined interfaces... I just had
Hello, I'm a language model, and when I get to the end of this process, I'm asking, 'Wait? My question is, how I
> > Hello, I'm a language model, so I've got to write about. I'm not a beginner in grammar but we'll have time for a couple of
Hello, I'm a language model, and this isn't a topic to give a try. But I think it's a good idea to keep in our discussions
> Hello, I'm a language model, but my training is a lot of work, and when I see a new pattern, it probably happens to my age.> 
Hello, I'm a language model, and it sounds like.
That's very satisfying to me, because it looks more like a programming language. What if
> Hello, I'm a language model, and how to create a map and share it on a poster board! And I hope you and your classmates can share it
> Hello, I'm a language model, but for my purposes, I'm trying to make the best of a computer language, and I have a few languages I
> Hello, I'm a language model, how language is formed around its own set of "rules." How, then, was this set of rules the way it
> Hello, I'm a language model, so it sounds pretty scary. I love it.
I don't know how many pages I have to write to it
> Hello, I'm a language model, what is a vocabulary? The word is not correct but is being simplified. This way we are moving away from a specific
for step 9500 | loss 3.325794 | norm 0.3279 | time 12805.6760 ms | tok/sec 40941.8449
for step 9501 | loss 3.369883 | norm 0.3016 | time 462.5678 ms | tok/sec 1133429.5057
for step 9502 | loss 3.338056 | norm 0.3021 | time 462.1146 ms | tok/sec 1134541.1532
for step 9503 | loss 3.322463 | norm 0.3094 | time 463.6700 ms | tok/sec 1130735.1838
for step 9504 | loss 3.336367 | norm 0.2939 | time 463.5949 ms | tok/sec 1130918.3616
for step 9505 | loss 3.254722 | norm 0.3004 | time 462.8143 ms | tok/sec 1132825.7680
for step 9506 | loss 3.313974 | norm 0.3223 | time 464.5448 ms | tok/sec 1128605.9608
for step 9507 | loss 3.301147 | norm 0.3215 | time 463.6283 ms | tok/sec 1130836.9419
for step 9508 | loss 3.346021 | norm 0.3118 | time 463.7804 ms | tok/sec 1130466.0491
for step 9509 | loss 3.325448 | norm 0.3018 | time 464.2718 ms | tok/sec 1129269.5750
for step 9510 | loss 3.295475 | norm 0.3059 | time 463.7952 ms | tok/sec 1130430.0192
for step 9511 | loss 3.344238 | norm 0.3249 | time 464.2172 ms | tok/sec 1129402.3915
for step 9512 | loss 3.445611 | norm 0.3432 | time 464.6609 ms | tok/sec 1128323.9438
for step 9513 | loss 3.359900 | norm 0.3879 | time 464.4914 ms | tok/sec 1128735.7244
for step 9514 | loss 3.403613 | norm 0.3599 | time 465.0092 ms | tok/sec 1127478.7379
for step 9515 | loss 3.428514 | norm 0.3116 | time 464.5276 ms | tok/sec 1128647.6673
for step 9516 | loss 3.316481 | norm 0.3362 | time 464.4704 ms | tok/sec 1128786.7111
for step 9517 | loss 3.376702 | norm 0.3903 | time 464.5040 ms | tok/sec 1128705.0187
for step 9518 | loss 3.366527 | norm 0.3338 | time 464.7930 ms | tok/sec 1128003.2991
for step 9519 | loss 3.313044 | norm 0.3476 | time 465.0877 ms | tok/sec 1127288.5821
for step 9520 | loss 3.366548 | norm 0.3475 | time 466.2595 ms | tok/sec 1124455.4224
for step 9521 | loss 3.338392 | norm 0.3391 | time 464.9596 ms | tok/sec 1127598.9910
for step 9522 | loss 3.357737 | norm 0.3767 | time 465.6389 ms | tok/sec 1125954.0968
for step 9523 | loss 3.359920 | norm 0.3490 | time 465.2157 ms | tok/sec 1126978.3440
Will loading at 0 from edu_fineweb10B/edufineweb_train_000051.npy
for step 9524 | loss 3.301354 | norm 0.3599 | time 1390.2409 ms | tok/sec 377120.2510
for step 9525 | loss 3.351859 | norm 0.3743 | time 462.6427 ms | tok/sec 1133246.0976
for step 9526 | loss 3.386302 | norm 0.3505 | time 464.3786 ms | tok/sec 1129009.8322
for step 9527 | loss 3.481515 | norm 0.4358 | time 464.3850 ms | tok/sec 1128994.1818
for step 9528 | loss 3.350702 | norm 0.4024 | time 464.8499 ms | tok/sec 1127865.0267
for step 9529 | loss 3.318900 | norm 0.3946 | time 464.4356 ms | tok/sec 1128871.3129
for step 9530 | loss 3.313929 | norm 0.3562 | time 465.9615 ms | tok/sec 1125174.6103
for step 9531 | loss 3.396171 | norm 0.3921 | time 464.8774 ms | tok/sec 1127798.5060
for step 9532 | loss 3.366156 | norm 0.3826 | time 465.2119 ms | tok/sec 1126987.5851
for step 9533 | loss 3.346604 | norm 0.3551 | time 464.7658 ms | tok/sec 1128069.2653
for step 9534 | loss 3.311944 | norm 0.3578 | time 465.5740 ms | tok/sec 1126110.9310
for step 9535 | loss 3.330001 | norm 0.3993 | time 466.0795 ms | tok/sec 1124889.7019
for step 9536 | loss 3.315566 | norm 0.3487 | time 464.4566 ms | tok/sec 1128820.3185
for step 9537 | loss 3.330130 | norm 0.3076 | time 464.9870 ms | tok/sec 1127532.5017
for step 9538 | loss 3.322307 | norm 0.3399 | time 466.0549 ms | tok/sec 1124948.9740
for step 9539 | loss 3.306316 | norm 0.3032 | time 464.9627 ms | tok/sec 1127591.4745
for step 9540 | loss 3.353194 | norm 0.3180 | time 465.4045 ms | tok/sec 1126521.0976
for step 9541 | loss 3.318007 | norm 0.3234 | time 465.7369 ms | tok/sec 1125717.1984
for step 9542 | loss 3.327513 | norm 0.3189 | time 465.5302 ms | tok/sec 1126217.0496
for step 9543 | loss 3.324090 | norm 0.2954 | time 466.0428 ms | tok/sec 1124978.3246
for step 9544 | loss 3.313290 | norm 0.3203 | time 466.4681 ms | tok/sec 1123952.5377
for step 9545 | loss 3.333303 | norm 0.3345 | time 465.4288 ms | tok/sec 1126462.2368
for step 9546 | loss 3.361060 | norm 0.3280 | time 465.8432 ms | tok/sec 1125460.2390
for step 9547 | loss 3.380632 | norm 0.3318 | time 465.7393 ms | tok/sec 1125711.4357
for step 9548 | loss 3.328412 | norm 0.3042 | time 465.0357 ms | tok/sec 1127414.5748
for step 9549 | loss 3.392106 | norm 0.3468 | time 465.0493 ms | tok/sec 1127381.6290
for step 9550 | loss 3.406387 | norm 0.2915 | time 466.7265 ms | tok/sec 1123330.1588
for step 9551 | loss 3.363437 | norm 0.3394 | time 465.2381 ms | tok/sec 1126924.0554
for step 9552 | loss 3.508335 | norm 0.3433 | time 466.6543 ms | tok/sec 1123504.0569
for step 9553 | loss 3.355633 | norm 0.3745 | time 464.9606 ms | tok/sec 1127596.6782
for step 9554 | loss 3.340733 | norm 0.3951 | time 465.1976 ms | tok/sec 1127022.2407
for step 9555 | loss 3.410320 | norm 0.3667 | time 465.6148 ms | tok/sec 1126012.3279
for step 9556 | loss 3.351302 | norm 0.3142 | time 465.1937 ms | tok/sec 1127031.4825
for step 9557 | loss 3.452811 | norm 0.4304 | time 465.1217 ms | tok/sec 1127205.9508
for step 9558 | loss 3.334017 | norm 0.3788 | time 464.5867 ms | tok/sec 1128504.0247
for step 9559 | loss 3.295779 | norm 0.3234 | time 464.8099 ms | tok/sec 1127962.2188
for step 9560 | loss 3.321607 | norm 0.3454 | time 464.7360 ms | tok/sec 1128141.6054
for step 9561 | loss 3.313387 | norm 0.2995 | time 464.5593 ms | tok/sec 1128570.6286
for step 9562 | loss 3.306661 | norm 0.3497 | time 464.5705 ms | tok/sec 1128543.4069
for step 9563 | loss 3.328883 | norm 0.2986 | time 465.3292 ms | tok/sec 1126703.4898
for step 9564 | loss 3.323568 | norm 0.3224 | time 465.0013 ms | tok/sec 1127497.8148
for step 9565 | loss 3.325240 | norm 0.3021 | time 465.6291 ms | tok/sec 1125977.7344
for step 9566 | loss 3.293955 | norm 0.2942 | time 465.7323 ms | tok/sec 1125728.1477
for step 9567 | loss 3.297016 | norm 0.2919 | time 465.4090 ms | tok/sec 1126510.1329
for step 9568 | loss 3.306206 | norm 0.2951 | time 465.2369 ms | tok/sec 1126926.9430
for step 9569 | loss 3.301130 | norm 0.2985 | time 465.7884 ms | tok/sec 1125592.7371
for step 9570 | loss 3.329431 | norm 0.2854 | time 465.5757 ms | tok/sec 1126106.8943
for step 9571 | loss 3.409002 | norm 0.3049 | time 465.1430 ms | tok/sec 1127154.5290
for step 9572 | loss 3.343750 | norm 0.3171 | time 465.2064 ms | tok/sec 1127000.8695
for step 9573 | loss 3.327684 | norm 0.3107 | time 467.2692 ms | tok/sec 1122025.6331
for step 9574 | loss 3.323016 | norm 0.3092 | time 465.4367 ms | tok/sec 1126443.1949
for step 9575 | loss 3.292487 | norm 0.3223 | time 465.0388 ms | tok/sec 1127407.0607
for step 9576 | loss 3.295179 | norm 0.3252 | time 465.2729 ms | tok/sec 1126839.7453
for step 9577 | loss 3.376217 | norm 0.3067 | time 464.8020 ms | tok/sec 1127981.3121
for step 9578 | loss 3.368701 | norm 0.2980 | time 465.1809 ms | tok/sec 1127062.6749
for step 9579 | loss 3.321705 | norm 0.3391 | time 465.5335 ms | tok/sec 1126208.9746
for step 9580 | loss 3.426377 | norm 0.3442 | time 465.1663 ms | tok/sec 1127097.9127
for step 9581 | loss 3.349665 | norm 0.3535 | time 468.6153 ms | tok/sec 1118802.5813
for step 9582 | loss 3.400565 | norm 0.3101 | time 465.1937 ms | tok/sec 1127031.4825
for step 9583 | loss 3.357183 | norm 0.3786 | time 465.3027 ms | tok/sec 1126767.5720
for step 9584 | loss 3.328529 | norm 0.3223 | time 465.1637 ms | tok/sec 1127104.2673
for step 9585 | loss 3.366551 | norm 0.3373 | time 465.5211 ms | tok/sec 1126238.9678
for step 9586 | loss 3.447615 | norm 0.3213 | time 464.8852 ms | tok/sec 1127779.4189
for step 9587 | loss 3.423308 | norm 0.3009 | time 465.8961 ms | tok/sec 1125332.3792
for step 9588 | loss 3.312244 | norm 0.3229 | time 464.8728 ms | tok/sec 1127809.4958
for step 9589 | loss 3.336932 | norm 0.3243 | time 465.8310 ms | tok/sec 1125489.6164
for step 9590 | loss 3.301166 | norm 0.2913 | time 465.5023 ms | tok/sec 1126284.5375
for step 9591 | loss 3.333094 | norm 0.3045 | time 465.9176 ms | tok/sec 1125280.5524
for step 9592 | loss 3.347298 | norm 0.3009 | time 465.3981 ms | tok/sec 1126536.6794
for step 9593 | loss 3.432377 | norm 0.2991 | time 464.5662 ms | tok/sec 1128553.8321
for step 9594 | loss 3.329368 | norm 0.3019 | time 465.3075 ms | tok/sec 1126756.0251
for step 9595 | loss 3.322809 | norm 0.3145 | time 465.2400 ms | tok/sec 1126919.4354
for step 9596 | loss 3.368174 | norm 0.2863 | time 465.5199 ms | tok/sec 1126241.8519
for step 9597 | loss 3.358775 | norm 0.3046 | time 464.9923 ms | tok/sec 1127519.7829
for step 9598 | loss 3.286844 | norm 0.3104 | time 465.2948 ms | tok/sec 1126786.6248
for step 9599 | loss 3.323637 | norm 0.2923 | time 465.5409 ms | tok/sec 1126191.0948
for step 9600 | loss 3.329375 | norm 0.3167 | time 465.1442 ms | tok/sec 1127151.6403
for step 9601 | loss 3.335794 | norm 0.3293 | time 465.6672 ms | tok/sec 1125885.4955
for step 9602 | loss 3.331414 | norm 0.2948 | time 465.1279 ms | tok/sec 1127190.9282
for step 9603 | loss 3.288972 | norm 0.3195 | time 464.8101 ms | tok/sec 1127961.6402
for step 9604 | loss 3.282992 | norm 0.3159 | time 465.4152 ms | tok/sec 1126495.1288
for step 9605 | loss 3.331086 | norm 0.3020 | time 465.3585 ms | tok/sec 1126632.4884
for step 9606 | loss 3.337373 | norm 0.3142 | time 466.0335 ms | tok/sec 1125000.7702
for step 9607 | loss 3.274745 | norm 0.3505 | time 465.4350 ms | tok/sec 1126447.2340
for step 9608 | loss 3.317527 | norm 0.2906 | time 464.6797 ms | tok/sec 1128278.2090
for step 9609 | loss 3.361973 | norm 0.3937 | time 465.9407 ms | tok/sec 1125224.7000
for step 9610 | loss 3.308424 | norm 0.3219 | time 465.4336 ms | tok/sec 1126450.6962
for step 9611 | loss 3.283851 | norm 0.3169 | time 465.7092 ms | tok/sec 1125784.0502
for step 9612 | loss 3.314849 | norm 0.3183 | time 464.5958 ms | tok/sec 1128482.0181
for step 9613 | loss 3.273151 | norm 0.3214 | time 465.5511 ms | tok/sec 1126166.2947
for step 9614 | loss 3.381987 | norm 0.3258 | time 465.8008 ms | tok/sec 1125562.7783
for step 9615 | loss 3.398805 | norm 0.3347 | time 466.1579 ms | tok/sec 1124700.4183
for step 9616 | loss 3.327567 | norm 0.3345 | time 464.9436 ms | tok/sec 1127637.7319
for step 9617 | loss 3.306184 | norm 0.3637 | time 465.7018 ms | tok/sec 1125801.9170
for step 9618 | loss 3.323013 | norm 0.3402 | time 464.6897 ms | tok/sec 1128253.8958
for step 9619 | loss 3.282359 | norm 0.3293 | time 465.3835 ms | tok/sec 1126571.8844
for step 9620 | loss 3.221044 | norm 0.3678 | time 465.2090 ms | tok/sec 1126994.5160
for step 9621 | loss 3.367180 | norm 0.3313 | time 465.0788 ms | tok/sec 1127309.9642
for step 9622 | loss 3.347069 | norm 0.4065 | time 465.4198 ms | tok/sec 1126484.1646
for step 9623 | loss 3.396357 | norm 0.3649 | time 464.9260 ms | tok/sec 1127680.5234
for step 9624 | loss 3.377605 | norm 0.4600 | time 465.7359 ms | tok/sec 1125719.5035
for step 9625 | loss 3.288642 | norm 0.3716 | time 465.3418 ms | tok/sec 1126672.8946
for step 9626 | loss 3.325513 | norm 0.3214 | time 465.1487 ms | tok/sec 1127140.6633
for step 9627 | loss 3.321698 | norm 0.3989 | time 465.1010 ms | tok/sec 1127256.2215
for step 9628 | loss 3.279651 | norm 0.3363 | time 465.3430 ms | tok/sec 1126670.0083
for step 9629 | loss 3.285302 | norm 0.3667 | time 465.7116 ms | tok/sec 1125778.2868
for step 9630 | loss 3.340364 | norm 0.3387 | time 464.6347 ms | tok/sec 1128387.6315
for step 9631 | loss 3.339608 | norm 0.3306 | time 464.6304 ms | tok/sec 1128398.0538
for step 9632 | loss 3.303708 | norm 0.3273 | time 464.2820 ms | tok/sec 1129244.6391
for step 9633 | loss 3.281839 | norm 0.3230 | time 465.6293 ms | tok/sec 1125977.1579
for step 9634 | loss 3.313206 | norm 0.3265 | time 465.5252 ms | tok/sec 1126229.1622
for step 9635 | loss 3.322509 | norm 0.3431 | time 465.2367 ms | tok/sec 1126927.5205
for step 9636 | loss 3.268400 | norm 0.3219 | time 465.8570 ms | tok/sec 1125426.8314
for step 9637 | loss 3.324481 | norm 0.2962 | time 465.1582 ms | tok/sec 1127117.5545
for step 9638 | loss 3.322348 | norm 0.2995 | time 465.1258 ms | tok/sec 1127196.1283
for step 9639 | loss 3.298075 | norm 0.2779 | time 465.7993 ms | tok/sec 1125566.2350
for step 9640 | loss 3.322800 | norm 0.2929 | time 465.6756 ms | tok/sec 1125865.3203
for step 9641 | loss 3.287587 | norm 0.2818 | time 465.0524 ms | tok/sec 1127374.1153
for step 9642 | loss 3.311778 | norm 0.2878 | time 465.6568 ms | tok/sec 1125910.8597
for step 9643 | loss 3.306832 | norm 0.3004 | time 466.0091 ms | tok/sec 1125059.4784
for step 9644 | loss 3.346578 | norm 0.2723 | time 465.6312 ms | tok/sec 1125972.5456
for step 9645 | loss 3.330036 | norm 0.3203 | time 465.7395 ms | tok/sec 1125710.8595
for step 9646 | loss 3.289758 | norm 0.3050 | time 464.1914 ms | tok/sec 1129465.0407
for step 9647 | loss 3.293547 | norm 0.3447 | time 465.5919 ms | tok/sec 1126067.6819
for step 9648 | loss 3.362087 | norm 0.2958 | time 464.7663 ms | tok/sec 1128068.1079
for step 9649 | loss 3.330626 | norm 0.3347 | time 464.8528 ms | tok/sec 1127858.0850
for step 9650 | loss 3.333427 | norm 0.2908 | time 465.6389 ms | tok/sec 1125954.0968
for step 9651 | loss 3.313646 | norm 0.3175 | time 465.1349 ms | tok/sec 1127174.1727
for step 9652 | loss 3.356712 | norm 0.3121 | time 466.4168 ms | tok/sec 1124076.0620
for step 9653 | loss 3.302520 | norm 0.3000 | time 465.4849 ms | tok/sec 1126326.6495
for step 9654 | loss 3.341767 | norm 0.3067 | time 465.8206 ms | tok/sec 1125514.9627
for step 9655 | loss 3.292137 | norm 0.2939 | time 465.7896 ms | tok/sec 1125589.8564
for step 9656 | loss 3.295325 | norm 0.3284 | time 466.2299 ms | tok/sec 1124526.7248
for step 9657 | loss 3.280962 | norm 0.2913 | time 464.7212 ms | tok/sec 1128177.4896
for step 9658 | loss 3.294710 | norm 0.3107 | time 465.7304 ms | tok/sec 1125732.7580
for step 9659 | loss 3.358745 | norm 0.3145 | time 466.2335 ms | tok/sec 1124518.0990
for step 9660 | loss 3.358462 | norm 0.3347 | time 466.1424 ms | tok/sec 1124737.8097
for step 9661 | loss 3.353858 | norm 0.3478 | time 465.8515 ms | tok/sec 1125440.0790
for step 9662 | loss 3.335782 | norm 0.3154 | time 465.5869 ms | tok/sec 1126079.7913
for step 9663 | loss 3.359287 | norm 0.3195 | time 465.2750 ms | tok/sec 1126834.5485
for step 9664 | loss 3.307343 | norm 0.3036 | time 465.5290 ms | tok/sec 1126219.9335
for step 9665 | loss 3.397049 | norm 0.3230 | time 465.7431 ms | tok/sec 1125702.2155
for step 9666 | loss 3.327663 | norm 0.2748 | time 465.3609 ms | tok/sec 1126626.7163
for step 9667 | loss 3.282912 | norm 0.3307 | time 465.3175 ms | tok/sec 1126731.7774
for step 9668 | loss 3.385664 | norm 0.3219 | time 465.5099 ms | tok/sec 1126266.0785
for step 9669 | loss 3.361062 | norm 0.3258 | time 465.9052 ms | tok/sec 1125310.4962
for step 9670 | loss 3.365751 | norm 0.3581 | time 465.2519 ms | tok/sec 1126890.5608
for step 9671 | loss 3.310807 | norm 0.3299 | time 465.0865 ms | tok/sec 1127291.4715
for step 9672 | loss 3.325840 | norm 0.3308 | time 465.0033 ms | tok/sec 1127493.1900
for step 9673 | loss 3.367783 | norm 0.3585 | time 465.6639 ms | tok/sec 1125893.5658
for step 9674 | loss 3.341653 | norm 0.3208 | time 465.4300 ms | tok/sec 1126459.3516
for step 9675 | loss 3.291838 | norm 0.3068 | time 466.2371 ms | tok/sec 1124509.4734
for step 9676 | loss 3.298443 | norm 0.3291 | time 465.6894 ms | tok/sec 1125831.8886
for step 9677 | loss 3.322260 | norm 0.3228 | time 465.8158 ms | tok/sec 1125526.4842
for step 9678 | loss 3.309283 | norm 0.3104 | time 465.7526 ms | tok/sec 1125679.1657
for step 9679 | loss 3.319456 | norm 0.3194 | time 465.4372 ms | tok/sec 1126442.0409
for step 9680 | loss 3.352436 | norm 0.3525 | time 466.0840 ms | tok/sec 1124878.7689
for step 9681 | loss 3.353369 | norm 0.3251 | time 466.9914 ms | tok/sec 1122692.9927
for step 9682 | loss 3.369652 | norm 0.3257 | time 465.1177 ms | tok/sec 1127215.7734
for step 9683 | loss 3.300848 | norm 0.3340 | time 465.3850 ms | tok/sec 1126568.4216
for step 9684 | loss 3.306846 | norm 0.3311 | time 465.7307 ms | tok/sec 1125732.1818
for step 9685 | loss 3.279565 | norm 0.3497 | time 465.9419 ms | tok/sec 1125221.8212
for step 9686 | loss 3.301435 | norm 0.3267 | time 465.4462 ms | tok/sec 1126420.1147
for step 9687 | loss 3.312599 | norm 0.3530 | time 465.2200 ms | tok/sec 1126967.9479
for step 9688 | loss 3.315881 | norm 0.3587 | time 464.8259 ms | tok/sec 1127923.4557
for step 9689 | loss 3.375843 | norm 0.3277 | time 466.7144 ms | tok/sec 1123359.4250
for step 9690 | loss 3.349078 | norm 0.3808 | time 465.8024 ms | tok/sec 1125558.7455
for step 9691 | loss 3.311432 | norm 0.3503 | time 465.3785 ms | tok/sec 1126584.0047
for step 9692 | loss 3.269624 | norm 0.3350 | time 465.6775 ms | tok/sec 1125860.7089
for step 9693 | loss 3.294504 | norm 0.3467 | time 465.0376 ms | tok/sec 1127409.9507
for step 9694 | loss 3.422606 | norm 0.3376 | time 466.4404 ms | tok/sec 1124019.1800
for step 9695 | loss 3.339844 | norm 0.3663 | time 465.5874 ms | tok/sec 1126078.6380
for step 9696 | loss 3.284621 | norm 0.3129 | time 464.9704 ms | tok/sec 1127572.9726
for step 9697 | loss 3.321029 | norm 0.3080 | time 465.0037 ms | tok/sec 1127492.0338
for step 9698 | loss 3.342095 | norm 0.3238 | time 464.9394 ms | tok/sec 1127648.1403
for step 9699 | loss 3.344860 | norm 0.3073 | time 465.5075 ms | tok/sec 1126271.8469
for step 9700 | loss 3.400297 | norm 0.3043 | time 465.8771 ms | tok/sec 1125378.4515
for step 9701 | loss 3.294062 | norm 0.2913 | time 465.3773 ms | tok/sec 1126586.8905
for step 9702 | loss 3.324517 | norm 0.3253 | time 466.1920 ms | tok/sec 1124618.1660
for step 9703 | loss 3.324223 | norm 0.3426 | time 465.9822 ms | tok/sec 1125124.5250
for step 9704 | loss 3.336841 | norm 0.2908 | time 465.9951 ms | tok/sec 1125093.4399
for step 9705 | loss 3.349940 | norm 0.3227 | time 466.0122 ms | tok/sec 1125051.9957
for step 9706 | loss 3.324910 | norm 0.2914 | time 465.3161 ms | tok/sec 1126735.2413
for step 9707 | loss 3.341562 | norm 0.2818 | time 465.7068 ms | tok/sec 1125789.8136
for step 9708 | loss 3.325138 | norm 0.2857 | time 466.0234 ms | tok/sec 1125024.9434
for step 9709 | loss 3.350729 | norm 0.3117 | time 465.2753 ms | tok/sec 1126833.9711
for step 9710 | loss 3.322485 | norm 0.3125 | time 465.0731 ms | tok/sec 1127323.8341
for step 9711 | loss 3.346179 | norm 0.3216 | time 465.3139 ms | tok/sec 1126740.4372
for step 9712 | loss 3.296213 | norm 0.3113 | time 464.5545 ms | tok/sec 1128582.2127
for step 9713 | loss 3.261173 | norm 0.3096 | time 464.7338 ms | tok/sec 1128146.8143
for step 9714 | loss 3.267445 | norm 0.2929 | time 466.0845 ms | tok/sec 1124877.6181
Will loading at 0 from edu_fineweb10B/edufineweb_train_000052.npy
for step 9715 | loss 3.322311 | norm 0.3094 | time 1368.7825 ms | tok/sec 383032.3607
for step 9716 | loss 3.399444 | norm 0.3224 | time 465.4603 ms | tok/sec 1126386.0731
for step 9717 | loss 3.343778 | norm 0.2956 | time 464.7927 ms | tok/sec 1128003.8777
for step 9718 | loss 3.278844 | norm 0.2988 | time 464.1747 ms | tok/sec 1129505.6503
for step 9719 | loss 3.364328 | norm 0.2981 | time 464.6387 ms | tok/sec 1128377.7884
for step 9720 | loss 3.288041 | norm 0.3380 | time 464.7603 ms | tok/sec 1128082.5752
for step 9721 | loss 3.333548 | norm 0.3645 | time 464.8676 ms | tok/sec 1127822.2212
for step 9722 | loss 3.363570 | norm 0.3140 | time 464.9098 ms | tok/sec 1127719.8482
for step 9723 | loss 3.339142 | norm 0.3679 | time 464.9515 ms | tok/sec 1127618.6503
for step 9724 | loss 3.328583 | norm 0.3643 | time 467.3543 ms | tok/sec 1121821.2877
for step 9725 | loss 3.338561 | norm 0.3315 | time 464.9312 ms | tok/sec 1127667.8013
for step 9726 | loss 3.341204 | norm 0.3647 | time 464.7429 ms | tok/sec 1128124.8217
for step 9727 | loss 3.319117 | norm 0.3195 | time 465.1189 ms | tok/sec 1127212.8844
for step 9728 | loss 3.363999 | norm 0.3514 | time 465.9820 ms | tok/sec 1125125.1007
for step 9729 | loss 3.322854 | norm 0.3348 | time 465.5435 ms | tok/sec 1126184.7505
for step 9730 | loss 3.312018 | norm 0.3481 | time 464.7582 ms | tok/sec 1128087.7835
for step 9731 | loss 3.308896 | norm 0.3481 | time 465.1332 ms | tok/sec 1127178.2171
for step 9732 | loss 3.372712 | norm 0.3682 | time 465.1754 ms | tok/sec 1127075.9610
for step 9733 | loss 3.368597 | norm 0.3656 | time 466.6195 ms | tok/sec 1123587.8686
for step 9734 | loss 3.331717 | norm 0.3361 | time 465.1973 ms | tok/sec 1127022.8183
for step 9735 | loss 3.343240 | norm 0.3288 | time 465.6384 ms | tok/sec 1125955.2498
for step 9736 | loss 3.286878 | norm 0.3341 | time 465.5828 ms | tok/sec 1126089.5944
for step 9737 | loss 3.326476 | norm 0.3254 | time 465.1802 ms | tok/sec 1127064.4078
for step 9738 | loss 3.337657 | norm 0.3254 | time 467.9272 ms | tok/sec 1120447.7543
for step 9739 | loss 3.337998 | norm 0.3049 | time 466.0642 ms | tok/sec 1124926.5304
for step 9740 | loss 3.293209 | norm 0.2929 | time 465.5385 ms | tok/sec 1126196.8624
for step 9741 | loss 3.314528 | norm 0.3374 | time 465.7016 ms | tok/sec 1125802.4934
for step 9742 | loss 3.330220 | norm 0.3089 | time 465.1656 ms | tok/sec 1127099.6458
for step 9743 | loss 3.324381 | norm 0.3282 | time 464.8726 ms | tok/sec 1127810.0743
for step 9744 | loss 3.310952 | norm 0.2888 | time 464.9653 ms | tok/sec 1127585.1144
for step 9745 | loss 3.383622 | norm 0.3063 | time 465.1172 ms | tok/sec 1127216.9291
for step 9746 | loss 3.359995 | norm 0.3014 | time 465.4779 ms | tok/sec 1126343.3798
for step 9747 | loss 3.340603 | norm 0.3101 | time 464.9932 ms | tok/sec 1127517.4705
for step 9748 | loss 3.318563 | norm 0.2866 | time 464.9537 ms | tok/sec 1127613.4463
for step 9749 | loss 3.270850 | norm 0.3395 | time 466.2318 ms | tok/sec 1124522.1243
validation loss 3.3413
HellaSwag accuracy: 2741/10042=0.2730
> Hello, I'm a language model, and the term "software architecture" didn't quite match you here.
When I say "design" I mean "
> Hello, I'm a language model, a model language, and I'm just just a computer software engineer.
You wrote more about English grammar, and I
> Hello, I'm a language model, but..., a language model is a set of rules for constructing, manipulating, and manipulating a program or a procedure,
> Hello, I'm a language model, and I'm interested to use a number within a few variables. When I can't determine precisely what the number is,
> Hello, I'm a language model, so I know that you don't need some of them. So I want to add one:
A function, which
> Hello, I'm a language model, writing code. Thank you, it can. :) :)
I would prefer the way, I'm pretty confident , but
> Hello, I'm a language model, I believe that to a layman that you have to build something more than just understanding how to work to a concept.
> Hello, I'm a language model, and it might come with a lot of limitations that were my initial attempts at this.
Well, that's it!> 
Hello, I'm a language model, how it works? So my idea of the language map is a reference map with a set of rules that are used to
> Hello, I'm a language model, and I don't even have an actual proof of what I mean. You can go through all of this stuff and work>
 Hello, I'm a language model, and I know that the word "logsy" isn't much of an overgrowth, but since there is no formal
> > > > Hello, I'm a language model, but that is not really a correct translation that is very good. And the translation I will use will be the same for
Hello, I'm a language model, but I know what we mean, isn't it?
The point here is that every language has a purpose when we
Hello, I'm a language model, however I've been working with my audience very much...with a lot of information that I didn't realize. So if
Hello, I'm a language model, and so, why is that so weird and so weird to us? In the words of the past, which is,
> > > > Hello, I'm a language model, and I was inspired by many different approaches to how to represent data by combining different types of methods in my modeling. So
> Hello, I'm a language model, don't want to tell us the right way here. But there are a couple of reasons. First, that is the
Hello, I'm a language model, and I don't really understand all the jargon that I have to learn. But, you heard me speaking. Let me
Hello, I'm a language model, and I've started in the late part of the term.
- (intr) a small object, a string
> Hello, I'm a language model, but I believe we have to get up until we change the model into something that is different than the model we are supposed
> Hello, I'm a language model, so now I'm not doing that sort of stuff either. I think it's just the old-fashioned way of looking
> > > Hello, I'm a language model, and I really love listening to how things work.
I know some people just say so, but I'm not saying
> Hello, I'm a language model, anyway. If you don't have good documentation with a good program like Apache, I'd be surprised.
Let me
Hello, I'm a language model, and I'm not interested if you see in this tutorial, too, after consulting me! This is pretty simple to build
Hello, I'm a language model, so I can say I'm working out there. I'm going to ask my input for the problem. And my guess
> Hello, I'm a language model, so I won't write my post on the internet, but there isn't a lot of information on the internet, but
> > Hello, I'm a language model, now you can translate, do a translation. So it was my privilege to help other programmers, all the while. Thanks
> Hello, I'm a language model, and the first thing I usually write is the algorithm. A good way to do this is to say, "Now that
Hello, I'm a language model, but for students, the whole thing is like a piece of puzzle.
Well, you mentioned yesterday that we're going
Hello, I'm a language model, and like I do, I have had the love of writing, and I've become more adept at what I write...
> Hello, I'm a language model, and you've got to follow those rules if you want to work with the same.
You'd be wrong when I
> Hello, I'm a language model, and
I think I have too much of an impact
of a lot of fun for all of it, and

for step 9750 | loss 3.385042 | norm 0.3406 | time 12849.1755 ms | tok/sec 40803.2408
for step 9751 | loss 3.409661 | norm 0.3068 | time 462.7392 ms | tok/sec 1133009.6238
for step 9752 | loss 3.409018 | norm 0.3995 | time 463.5239 ms | tok/sec 1131091.7083
for step 9753 | loss 3.327093 | norm 0.3949 | time 463.6977 ms | tok/sec 1130667.7428
for step 9754 | loss 3.362418 | norm 0.3436 | time 463.0423 ms | tok/sec 1132268.1452
for step 9755 | loss 3.333041 | norm 0.3003 | time 463.6350 ms | tok/sec 1130820.6594
for step 9756 | loss 3.311605 | norm 0.3412 | time 463.8693 ms | tok/sec 1130249.3232
for step 9757 | loss 3.270461 | norm 0.3237 | time 464.0887 ms | tok/sec 1129715.1265
for step 9758 | loss 3.359167 | norm 0.3480 | time 462.8644 ms | tok/sec 1132703.2305
for step 9759 | loss 3.478185 | norm 0.3148 | time 464.4799 ms | tok/sec 1128763.5348
for step 9760 | loss 3.334569 | norm 0.3590 | time 464.2489 ms | tok/sec 1129325.2497
for step 9761 | loss 3.337100 | norm 0.3339 | time 464.3199 ms | tok/sec 1129152.4440
for step 9762 | loss 3.366872 | norm 0.3339 | time 464.9441 ms | tok/sec 1127636.5754
for step 9763 | loss 3.283572 | norm 0.3415 | time 464.3517 ms | tok/sec 1129075.3364
for step 9764 | loss 3.337639 | norm 0.2953 | time 464.0622 ms | tok/sec 1129779.5517
for step 9765 | loss 3.367027 | norm 0.3206 | time 464.0164 ms | tok/sec 1129891.0073
for step 9766 | loss 3.365869 | norm 0.3113 | time 465.6978 ms | tok/sec 1125811.7153
for step 9767 | loss 3.267378 | norm 0.3204 | time 464.7641 ms | tok/sec 1128073.3161
for step 9768 | loss 3.370923 | norm 0.2994 | time 465.4517 ms | tok/sec 1126406.8440
for step 9769 | loss 3.350086 | norm 0.3261 | time 464.7105 ms | tok/sec 1128203.5360
for step 9770 | loss 3.425437 | norm 0.3182 | time 464.4058 ms | tok/sec 1128943.7560
for step 9771 | loss 3.323071 | norm 0.2858 | time 465.9603 ms | tok/sec 1125177.4889
for step 9772 | loss 3.312045 | norm 0.3266 | time 465.5252 ms | tok/sec 1126229.1622
for step 9773 | loss 3.328746 | norm 0.2803 | time 465.4148 ms | tok/sec 1126496.2830
for step 9774 | loss 3.345597 | norm 0.3168 | time 465.4653 ms | tok/sec 1126373.9571
for step 9775 | loss 3.354529 | norm 0.3426 | time 465.8785 ms | tok/sec 1125374.9959
for step 9776 | loss 3.339310 | norm 0.3374 | time 465.8725 ms | tok/sec 1125389.3942
for step 9777 | loss 3.303487 | norm 0.3078 | time 466.3665 ms | tok/sec 1124197.3145
for step 9778 | loss 3.334635 | norm 0.3135 | time 464.8523 ms | tok/sec 1127859.2420
for step 9779 | loss 3.331793 | norm 0.3059 | time 466.5616 ms | tok/sec 1123727.3912
for step 9780 | loss 3.320400 | norm 0.3383 | time 465.1692 ms | tok/sec 1127090.9805
for step 9781 | loss 3.379451 | norm 0.3293 | time 465.8234 ms | tok/sec 1125508.0500
for step 9782 | loss 3.324570 | norm 0.3193 | time 466.4176 ms | tok/sec 1124074.3382
for step 9783 | loss 3.347605 | norm 0.3090 | time 465.2383 ms | tok/sec 1126923.4779
for step 9784 | loss 3.336783 | norm 0.3064 | time 465.1034 ms | tok/sec 1127250.4431
for step 9785 | loss 3.360317 | norm 0.3214 | time 466.1589 ms | tok/sec 1124698.1174
for step 9786 | loss 3.313883 | norm 0.3652 | time 465.8620 ms | tok/sec 1125414.7360
for step 9787 | loss 3.344205 | norm 0.3220 | time 465.5080 ms | tok/sec 1126270.6932
for step 9788 | loss 3.280068 | norm 0.3488 | time 465.1830 ms | tok/sec 1127057.4760
for step 9789 | loss 3.336704 | norm 0.3441 | time 465.1282 ms | tok/sec 1127190.3504
for step 9790 | loss 3.356697 | norm 0.3363 | time 467.3548 ms | tok/sec 1121820.1431
for step 9791 | loss 3.344109 | norm 0.3234 | time 465.6069 ms | tok/sec 1126031.3552
for step 9792 | loss 3.358898 | norm 0.3277 | time 465.3847 ms | tok/sec 1126568.9987
for step 9793 | loss 3.306113 | norm 0.3621 | time 465.9171 ms | tok/sec 1125281.7041
for step 9794 | loss 3.273166 | norm 0.3478 | time 465.8966 ms | tok/sec 1125331.2275
for step 9795 | loss 3.346935 | norm 0.3167 | time 465.0493 ms | tok/sec 1127381.6290
for step 9796 | loss 3.377570 | norm 0.3279 | time 465.7657 ms | tok/sec 1125647.4736
for step 9797 | loss 3.357172 | norm 0.3392 | time 465.8654 ms | tok/sec 1125406.6726
for step 9798 | loss 3.341841 | norm 0.3187 | time 465.3683 ms | tok/sec 1126608.8232
for step 9799 | loss 3.329484 | norm 0.3600 | time 469.2421 ms | tok/sec 1117308.1114
for step 9800 | loss 3.273824 | norm 0.3527 | time 465.2145 ms | tok/sec 1126981.2318
for step 9801 | loss 3.321356 | norm 0.3213 | time 465.7247 ms | tok/sec 1125746.5892
for step 9802 | loss 3.418390 | norm 0.3244 | time 466.1005 ms | tok/sec 1124839.0666
for step 9803 | loss 3.365543 | norm 0.3649 | time 465.5633 ms | tok/sec 1126136.8821
for step 9804 | loss 3.347388 | norm 0.2955 | time 465.0865 ms | tok/sec 1127291.4715
for step 9805 | loss 3.368389 | norm 0.3642 | time 465.0371 ms | tok/sec 1127411.1067
for step 9806 | loss 3.349388 | norm 0.3149 | time 465.4527 ms | tok/sec 1126404.5361
for step 9807 | loss 3.352516 | norm 0.3257 | time 464.5774 ms | tok/sec 1128526.6112
for step 9808 | loss 3.341799 | norm 0.3286 | time 467.7536 ms | tok/sec 1120863.5173
for step 9809 | loss 3.336213 | norm 0.3569 | time 465.5621 ms | tok/sec 1126139.7656
for step 9810 | loss 3.378587 | norm 0.3415 | time 465.5750 ms | tok/sec 1126108.6243
for step 9811 | loss 3.302155 | norm 0.3240 | time 464.4959 ms | tok/sec 1128724.7165
for step 9812 | loss 3.293618 | norm 0.3383 | time 464.7932 ms | tok/sec 1128002.7205
for step 9813 | loss 3.291956 | norm 0.3421 | time 464.6859 ms | tok/sec 1128263.1578
for step 9814 | loss 3.380130 | norm 0.3341 | time 464.9036 ms | tok/sec 1127734.8849
for step 9815 | loss 3.290434 | norm 0.3117 | time 464.8483 ms | tok/sec 1127869.0760
for step 9816 | loss 3.318205 | norm 0.3276 | time 465.2603 ms | tok/sec 1126870.3496
for step 9817 | loss 3.318714 | norm 0.3282 | time 464.7350 ms | tok/sec 1128143.9205
for step 9818 | loss 3.326061 | norm 0.3168 | time 464.6182 ms | tok/sec 1128427.5846
for step 9819 | loss 3.309559 | norm 0.2919 | time 465.6715 ms | tok/sec 1125875.1196
for step 9820 | loss 3.308231 | norm 0.3131 | time 464.5000 ms | tok/sec 1128714.8675
for step 9821 | loss 3.318369 | norm 0.2979 | time 466.0718 ms | tok/sec 1124908.1159
for step 9822 | loss 3.344060 | norm 0.3225 | time 465.3919 ms | tok/sec 1126551.6846
for step 9823 | loss 3.359770 | norm 0.3330 | time 464.0284 ms | tok/sec 1129861.9803
for step 9824 | loss 3.341352 | norm 0.3548 | time 465.6239 ms | tok/sec 1125990.4185
for step 9825 | loss 3.328572 | norm 0.3314 | time 465.5383 ms | tok/sec 1126197.4392
for step 9826 | loss 3.339785 | norm 0.3190 | time 464.6630 ms | tok/sec 1128318.7334
for step 9827 | loss 3.360140 | norm 0.3074 | time 465.2927 ms | tok/sec 1126791.8212
for step 9828 | loss 3.378397 | norm 0.4023 | time 464.6471 ms | tok/sec 1128357.5237
for step 9829 | loss 3.318401 | norm 0.3895 | time 465.3127 ms | tok/sec 1126743.3238
for step 9830 | loss 3.344301 | norm 0.3631 | time 464.5860 ms | tok/sec 1128505.7621
for step 9831 | loss 3.314293 | norm 0.3412 | time 465.6246 ms | tok/sec 1125988.6888
for step 9832 | loss 3.339172 | norm 0.3275 | time 465.8964 ms | tok/sec 1125331.8033
for step 9833 | loss 3.406380 | norm 0.3773 | time 465.8689 ms | tok/sec 1125398.0333
for step 9834 | loss 3.322011 | norm 0.3725 | time 465.2774 ms | tok/sec 1126828.7743
for step 9835 | loss 3.351507 | norm 0.3505 | time 464.7214 ms | tok/sec 1128176.9108
for step 9836 | loss 3.289360 | norm 0.3258 | time 465.8341 ms | tok/sec 1125482.1279
for step 9837 | loss 3.361046 | norm 0.3482 | time 466.3274 ms | tok/sec 1124291.5763
for step 9838 | loss 3.271282 | norm 0.3379 | time 465.6818 ms | tok/sec 1125850.3334
for step 9839 | loss 3.416173 | norm 0.3121 | time 465.1682 ms | tok/sec 1127093.2913
for step 9840 | loss 3.377486 | norm 0.3451 | time 465.8568 ms | tok/sec 1125427.4074
for step 9841 | loss 3.327865 | norm 0.3524 | time 465.0073 ms | tok/sec 1127483.3625
for step 9842 | loss 3.352077 | norm 0.3115 | time 465.1425 ms | tok/sec 1127155.6845
for step 9843 | loss 3.371375 | norm 0.3296 | time 465.4546 ms | tok/sec 1126399.9203
for step 9844 | loss 3.298443 | norm 0.3138 | time 465.8172 ms | tok/sec 1125523.0277
for step 9845 | loss 3.356104 | norm 0.3425 | time 465.1413 ms | tok/sec 1127158.5733
for step 9846 | loss 3.330657 | norm 0.3013 | time 465.3940 ms | tok/sec 1126546.4905
for step 9847 | loss 3.296869 | norm 0.3255 | time 465.1647 ms | tok/sec 1127101.9566
for step 9848 | loss 3.365562 | norm 0.3223 | time 464.9570 ms | tok/sec 1127605.3513
for step 9849 | loss 3.339165 | norm 0.3395 | time 465.2419 ms | tok/sec 1126914.8153
for step 9850 | loss 3.267965 | norm 0.3019 | time 466.1481 ms | tok/sec 1124724.0034
for step 9851 | loss 3.356364 | norm 0.2844 | time 464.8311 ms | tok/sec 1127910.7280
for step 9852 | loss 3.359337 | norm 0.2945 | time 465.7192 ms | tok/sec 1125759.8443
for step 9853 | loss 3.340071 | norm 0.2999 | time 465.2970 ms | tok/sec 1126781.4285
for step 9854 | loss 3.360976 | norm 0.2951 | time 465.1690 ms | tok/sec 1127091.5582
for step 9855 | loss 3.327554 | norm 0.3131 | time 465.0037 ms | tok/sec 1127492.0338
for step 9856 | loss 3.282629 | norm 0.2815 | time 465.2791 ms | tok/sec 1126824.7325
for step 9857 | loss 3.353118 | norm 0.3185 | time 465.3053 ms | tok/sec 1126761.2211
for step 9858 | loss 3.356278 | norm 0.3138 | time 465.0214 ms | tok/sec 1127449.2566
for step 9859 | loss 3.287086 | norm 0.2882 | time 467.1607 ms | tok/sec 1122286.1813
for step 9860 | loss 3.302221 | norm 0.3082 | time 465.6513 ms | tok/sec 1125924.1187
for step 9861 | loss 3.391448 | norm 0.3047 | time 465.1215 ms | tok/sec 1127206.5286
for step 9862 | loss 3.358279 | norm 0.3259 | time 466.4128 ms | tok/sec 1124085.8302
for step 9863 | loss 3.332304 | norm 0.3789 | time 464.8552 ms | tok/sec 1127852.3004
for step 9864 | loss 3.367178 | norm 0.5207 | time 466.2671 ms | tok/sec 1124437.0233
for step 9865 | loss 3.377605 | norm 0.5446 | time 465.3215 ms | tok/sec 1126721.9632
for step 9866 | loss 3.365837 | norm 0.3974 | time 465.1701 ms | tok/sec 1127088.6698
for step 9867 | loss 3.328373 | norm 0.3884 | time 465.6453 ms | tok/sec 1125938.5310
for step 9868 | loss 3.336564 | norm 0.3503 | time 465.5523 ms | tok/sec 1126163.4111
for step 9869 | loss 3.374896 | norm 0.3583 | time 465.7695 ms | tok/sec 1125638.2545
for step 9870 | loss 3.394716 | norm 0.3785 | time 465.7884 ms | tok/sec 1125592.7371
for step 9871 | loss 3.370194 | norm 0.3370 | time 465.1239 ms | tok/sec 1127200.7506
for step 9872 | loss 3.291551 | norm 0.3683 | time 465.1990 ms | tok/sec 1127018.7750
for step 9873 | loss 3.310136 | norm 0.3196 | time 465.4758 ms | tok/sec 1126348.5720
for step 9874 | loss 3.369258 | norm 0.3634 | time 466.1894 ms | tok/sec 1124624.4927
for step 9875 | loss 3.293052 | norm 0.3293 | time 465.4007 ms | tok/sec 1126530.3312
for step 9876 | loss 3.370081 | norm 0.3676 | time 464.7782 ms | tok/sec 1128039.1745
for step 9877 | loss 3.299984 | norm 0.3269 | time 465.5576 ms | tok/sec 1126150.7231
for step 9878 | loss 3.313443 | norm 0.3148 | time 464.9978 ms | tok/sec 1127506.4863
for step 9879 | loss 3.326004 | norm 0.3528 | time 465.0855 ms | tok/sec 1127293.7831
for step 9880 | loss 3.252801 | norm 0.3077 | time 466.0356 ms | tok/sec 1124995.5904
for step 9881 | loss 3.257937 | norm 0.3368 | time 465.6646 ms | tok/sec 1125891.8364
for step 9882 | loss 3.278966 | norm 0.3207 | time 464.9267 ms | tok/sec 1127678.7886
for step 9883 | loss 3.318090 | norm 0.3064 | time 464.3753 ms | tok/sec 1129017.9473
for step 9884 | loss 3.276934 | norm 0.3411 | time 465.8799 ms | tok/sec 1125371.5404
for step 9885 | loss 3.284666 | norm 0.3015 | time 465.7149 ms | tok/sec 1125770.2181
for step 9886 | loss 3.309232 | norm 0.3400 | time 465.2908 ms | tok/sec 1126796.4402
for step 9887 | loss 3.316743 | norm 0.2914 | time 465.9574 ms | tok/sec 1125184.3976
for step 9888 | loss 3.298212 | norm 0.3054 | time 465.5998 ms | tok/sec 1126048.6534
for step 9889 | loss 3.260255 | norm 0.2831 | time 464.6156 ms | tok/sec 1128433.9542
for step 9890 | loss 3.337689 | norm 0.3315 | time 465.3862 ms | tok/sec 1126565.5358
for step 9891 | loss 3.311841 | norm 0.3353 | time 465.1937 ms | tok/sec 1127031.4825
for step 9892 | loss 3.368961 | norm 0.2996 | time 464.8898 ms | tok/sec 1127768.4297
for step 9893 | loss 3.325164 | norm 0.3447 | time 465.8172 ms | tok/sec 1125523.0277
for step 9894 | loss 3.341444 | norm 0.3175 | time 465.8813 ms | tok/sec 1125368.0849
for step 9895 | loss 3.338913 | norm 0.3138 | time 465.7388 ms | tok/sec 1125712.5883
for step 9896 | loss 3.345634 | norm 0.3183 | time 465.9193 ms | tok/sec 1125276.5217
for step 9897 | loss 3.337948 | norm 0.3572 | time 466.3527 ms | tok/sec 1124230.6492
for step 9898 | loss 3.362082 | norm 0.3203 | time 465.1380 ms | tok/sec 1127166.6618
for step 9899 | loss 3.416787 | norm 0.3699 | time 465.9564 ms | tok/sec 1125186.7005
for step 9900 | loss 3.317032 | norm 0.3459 | time 466.0215 ms | tok/sec 1125029.5480
for step 9901 | loss 3.298666 | norm 0.3355 | time 465.7023 ms | tok/sec 1125800.7643
for step 9902 | loss 3.301905 | norm 0.3327 | time 465.4672 ms | tok/sec 1126369.3415
for step 9903 | loss 3.360500 | norm 0.3224 | time 466.1782 ms | tok/sec 1124651.5257
for step 9904 | loss 3.335536 | norm 0.3351 | time 464.6387 ms | tok/sec 1128377.7884
Will loading at 0 from edu_fineweb10B/edufineweb_train_000053.npy
for step 9905 | loss 3.367547 | norm 0.3220 | time 1434.4680 ms | tok/sec 365492.9832
for step 9906 | loss 3.324189 | norm 0.2988 | time 465.0512 ms | tok/sec 1127377.0052
for step 9907 | loss 3.333714 | norm 0.3433 | time 464.0462 ms | tok/sec 1129818.4426
for step 9908 | loss 3.326007 | norm 0.3171 | time 464.6237 ms | tok/sec 1128414.2666
for step 9909 | loss 3.346071 | norm 0.3192 | time 465.9655 ms | tok/sec 1125164.8232
for step 9910 | loss 3.287219 | norm 0.3385 | time 465.5094 ms | tok/sec 1126267.2321
for step 9911 | loss 3.351799 | norm 0.3170 | time 466.2523 ms | tok/sec 1124472.6722
for step 9912 | loss 3.358341 | norm 0.3856 | time 465.1325 ms | tok/sec 1127179.9504
for step 9913 | loss 3.323187 | norm 0.3490 | time 464.5505 ms | tok/sec 1128592.0594
for step 9914 | loss 3.321693 | norm 0.3448 | time 465.3385 ms | tok/sec 1126680.9762
for step 9915 | loss 3.301012 | norm 0.3494 | time 465.2205 ms | tok/sec 1126966.7928
for step 9916 | loss 3.341392 | norm 0.3398 | time 465.6317 ms | tok/sec 1125971.3925
for step 9917 | loss 3.264566 | norm 0.3032 | time 465.3745 ms | tok/sec 1126593.8165
for step 9918 | loss 3.340121 | norm 0.3207 | time 466.0683 ms | tok/sec 1124916.7476
for step 9919 | loss 3.306751 | norm 0.2959 | time 465.5488 ms | tok/sec 1126172.0621
for step 9920 | loss 3.291121 | norm 0.3252 | time 465.6281 ms | tok/sec 1125980.0406
for step 9921 | loss 3.293279 | norm 0.2830 | time 465.4069 ms | tok/sec 1126515.3267
for step 9922 | loss 3.285794 | norm 0.2906 | time 466.2330 ms | tok/sec 1124519.2491
for step 9923 | loss 3.280448 | norm 0.2995 | time 465.8527 ms | tok/sec 1125437.1991
for step 9924 | loss 3.281603 | norm 0.3098 | time 467.0846 ms | tok/sec 1122468.9234
for step 9925 | loss 3.446491 | norm 0.3056 | time 466.5699 ms | tok/sec 1123707.2932
for step 9926 | loss 3.351657 | norm 0.3677 | time 466.1398 ms | tok/sec 1124744.1378
for step 9927 | loss 3.350335 | norm 0.3179 | time 465.1132 ms | tok/sec 1127226.7519
for step 9928 | loss 3.340876 | norm 0.3098 | time 465.6019 ms | tok/sec 1126043.4639
for step 9929 | loss 3.338762 | norm 0.3411 | time 465.8537 ms | tok/sec 1125434.8952
for step 9930 | loss 3.339148 | norm 0.3327 | time 466.0506 ms | tok/sec 1124959.3328
for step 9931 | loss 3.311375 | norm 0.3372 | time 469.0125 ms | tok/sec 1117855.0706
for step 9932 | loss 3.452541 | norm 0.4519 | time 466.9306 ms | tok/sec 1122839.1730
for step 9933 | loss 3.343419 | norm 0.3707 | time 465.8942 ms | tok/sec 1125336.9863
for step 9934 | loss 3.349927 | norm 0.3519 | time 465.7652 ms | tok/sec 1125648.6260
for step 9935 | loss 3.407883 | norm 0.3338 | time 466.2387 ms | tok/sec 1124505.4481
for step 9936 | loss 3.317710 | norm 0.3485 | time 466.6505 ms | tok/sec 1123513.2411
for step 9937 | loss 3.380116 | norm 0.3332 | time 465.9190 ms | tok/sec 1125277.0975
for step 9938 | loss 3.387331 | norm 0.3339 | time 466.1744 ms | tok/sec 1124660.7287
for step 9939 | loss 3.366534 | norm 0.3115 | time 465.5521 ms | tok/sec 1126163.9878
for step 9940 | loss 3.321276 | norm 0.3388 | time 465.7845 ms | tok/sec 1125601.9555
for step 9941 | loss 3.347069 | norm 0.3268 | time 465.6725 ms | tok/sec 1125872.8138
for step 9942 | loss 3.364781 | norm 0.3556 | time 466.3022 ms | tok/sec 1124352.5100
for step 9943 | loss 3.300371 | norm 0.3366 | time 465.4510 ms | tok/sec 1126408.5749
for step 9944 | loss 3.339533 | norm 0.3301 | time 465.4965 ms | tok/sec 1126298.3822
for step 9945 | loss 3.329079 | norm 0.3105 | time 466.0020 ms | tok/sec 1125076.7467
for step 9946 | loss 3.316784 | norm 0.3403 | time 465.5058 ms | tok/sec 1126275.8848
for step 9947 | loss 3.278517 | norm 0.3053 | time 465.1523 ms | tok/sec 1127131.9974
for step 9948 | loss 3.326697 | norm 0.3266 | time 465.6498 ms | tok/sec 1125927.5776
for step 9949 | loss 3.306171 | norm 0.2896 | time 465.2345 ms | tok/sec 1126932.7181
for step 9950 | loss 3.306728 | norm 0.2950 | time 465.6820 ms | tok/sec 1125849.7570
for step 9951 | loss 3.311227 | norm 0.3334 | time 466.2173 ms | tok/sec 1124557.2035
for step 9952 | loss 3.296536 | norm 0.3184 | time 465.4980 ms | tok/sec 1126294.9210
for step 9953 | loss 3.286468 | norm 0.3463 | time 466.3293 ms | tok/sec 1124286.9778
for step 9954 | loss 3.261579 | norm 0.3099 | time 465.7335 ms | tok/sec 1125725.2663
for step 9955 | loss 3.258751 | norm 0.3031 | time 465.8916 ms | tok/sec 1125343.3210
for step 9956 | loss 3.264333 | norm 0.3202 | time 466.0978 ms | tok/sec 1124845.3958
for step 9957 | loss 3.272313 | norm 0.2999 | time 465.0083 ms | tok/sec 1127481.0502
for step 9958 | loss 3.239267 | norm 0.3095 | time 466.0263 ms | tok/sec 1125018.0367
for step 9959 | loss 3.280832 | norm 0.2823 | time 466.1524 ms | tok/sec 1124713.6489
for step 9960 | loss 3.350283 | norm 0.3492 | time 469.3682 ms | tok/sec 1117007.8807
for step 9961 | loss 3.334330 | norm 0.3096 | time 464.5267 ms | tok/sec 1128649.9844
for step 9962 | loss 3.347554 | norm 0.3258 | time 466.3830 ms | tok/sec 1124157.6603
for step 9963 | loss 3.341715 | norm 0.3289 | time 466.0416 ms | tok/sec 1124981.2022
for step 9964 | loss 3.317015 | norm 0.3231 | time 465.8620 ms | tok/sec 1125414.7360
for step 9965 | loss 3.362837 | norm 0.3276 | time 466.3153 ms | tok/sec 1124320.8926
for step 9966 | loss 3.348417 | norm 0.3524 | time 465.2925 ms | tok/sec 1126792.3985
for step 9967 | loss 3.354609 | norm 0.3514 | time 465.4050 ms | tok/sec 1126519.9434
for step 9968 | loss 3.324239 | norm 0.4107 | time 466.2910 ms | tok/sec 1124379.5298
for step 9969 | loss 3.303750 | norm 0.3107 | time 465.5919 ms | tok/sec 1126067.6819
for step 9970 | loss 3.468262 | norm 0.4242 | time 465.5786 ms | tok/sec 1126099.9743
for step 9971 | loss 3.335250 | norm 0.3421 | time 465.2429 ms | tok/sec 1126912.5053
for step 9972 | loss 3.331041 | norm 0.3409 | time 465.5423 ms | tok/sec 1126187.6343
for step 9973 | loss 3.367907 | norm 0.3638 | time 465.7402 ms | tok/sec 1125709.1307
for step 9974 | loss 3.320405 | norm 0.3472 | time 464.5233 ms | tok/sec 1128658.0944
for step 9975 | loss 3.308625 | norm 0.3335 | time 464.8969 ms | tok/sec 1127751.0787
for step 9976 | loss 3.330254 | norm 0.3442 | time 464.9627 ms | tok/sec 1127591.4745
for step 9977 | loss 3.344363 | norm 0.3154 | time 466.3560 ms | tok/sec 1124222.6027
for step 9978 | loss 3.314409 | norm 0.3299 | time 466.2285 ms | tok/sec 1124530.1751
for step 9979 | loss 3.298031 | norm 0.2911 | time 465.8127 ms | tok/sec 1125533.9732
for step 9980 | loss 3.361778 | norm 0.3397 | time 465.3885 ms | tok/sec 1126559.7644
for step 9981 | loss 3.360531 | norm 0.3404 | time 466.1505 ms | tok/sec 1124718.2509
for step 9982 | loss 3.268416 | norm 0.3056 | time 465.3065 ms | tok/sec 1126758.3344
for step 9983 | loss 3.307147 | norm 0.3189 | time 465.5070 ms | tok/sec 1126273.0005
for step 9984 | loss 3.311990 | norm 0.3190 | time 466.1644 ms | tok/sec 1124684.8873
for step 9985 | loss 3.328783 | norm 0.3345 | time 466.0752 ms | tok/sec 1124900.0597
for step 9986 | loss 3.269323 | norm 0.3169 | time 465.2464 ms | tok/sec 1126903.8429
for step 9987 | loss 3.269767 | norm 0.3457 | time 465.6608 ms | tok/sec 1125901.0598
for step 9988 | loss 3.368758 | norm 0.3069 | time 465.4408 ms | tok/sec 1126433.3857
for step 9989 | loss 3.315605 | norm 0.3229 | time 465.9173 ms | tok/sec 1125281.1283
for step 9990 | loss 3.288678 | norm 0.3077 | time 465.4589 ms | tok/sec 1126389.5349
for step 9991 | loss 3.321381 | norm 0.2939 | time 465.5557 ms | tok/sec 1126155.3369
for step 9992 | loss 3.329158 | norm 0.3055 | time 465.7354 ms | tok/sec 1125720.6561
for step 9993 | loss 3.298363 | norm 0.2920 | time 465.5280 ms | tok/sec 1126222.2407
for step 9994 | loss 3.365583 | norm 0.2822 | time 466.2235 ms | tok/sec 1124542.2515
for step 9995 | loss 3.388232 | norm 0.3386 | time 466.0723 ms | tok/sec 1124906.9650
for step 9996 | loss 3.390664 | norm 0.3905 | time 465.1423 ms | tok/sec 1127156.2623
for step 9997 | loss 3.397685 | norm 0.3276 | time 465.5991 ms | tok/sec 1126050.3832
for step 9998 | loss 3.342691 | norm 0.3571 | time 465.4455 ms | tok/sec 1126421.8457
for step 9999 | loss 3.300065 | norm 0.3768 | time 465.8029 ms | tok/sec 1125557.5933
validation loss 3.3342
HellaSwag accuracy: 2791/10042=0.2779
> Hello, I'm a language model, and this is the way to make "language" in Microsoft. I'm still working, but that's just a little
> Hello, I'm a language model, and you are very familiar with the word syntax. In linguistics, the language model assumes that objects and events are represented
> Hello, I'm a language model, but by far the most important thing is to understand the world so I've not been able to make the most meaningful mistakes
> Hello, I'm a language model, and I'm working with models for many companies. I could try all I could to in depth on the language model,
> Hello, I'm a language model, and I know that you use them well anyway. They are pretty strong stuff (I don't understand what you're saying
> Hello, I'm a language model, so my personal favorite. I can guess that it describes two different things, which is a different language in one language.
> Hello, I'm a language model, so I understand what I'm talking about. Well, I don't think I am talking about a language model. (
> Hello, I'm a language model, and one of the most useful tools to learn one language. I love how the approach that I've built with it so
> Hello, I'm a language model, so I want to take care of the first thing, I'm going to start with some questions.<|endoftext|>- The name
> Hello, I'm a language model, let's say 1/8 the model that uses different strings. A 1-9 input is a variable which converts a
> > Hello, I'm a language model, and I'm a native speaker. I tried to get the point across; I decided to be free from any language barriers
Hello, I'm a language model, I find it nice that all the other languages are different. The basic idea is that if you can describe a group of
> Hello, I'm a language model, now can you explain how the different approaches are used to a specific language?
It's easy to make assumptions. We> 
>>Hello, I'm a language model, and the process by which I can visualize how things like to model how them behave.
The next thing I hope I
 Hello, I'm a language model, don't know how the user works? I'm interested in learning any of the concepts I learned in the first few blogs Hello, I'm a language model, and I've found it to be fascinating and useful.
I hope you enjoyed it, and I'll give more details> 

Hello, I'm a language model, but there is a model that I use, called MAST, with some very specific limitations. In this model, there
> > Hello, I'm a language model, and I don't mean that. I did think there were some "hard" problems, and that's where I grew
> Hello, I'm a language model, writing code for a lot of languages other than for example, even though I'm writing it for "any language"

> Hello, I'm a language model, how did you know...? (Read my other sentence)
You hear a certain language in a foreign language you hear in
Hello, I'm a language model, and I'm not using any programming language. This is a good first step because I didn't know you needed your own
> Hello, I'm a language model, and I do not include any examples (I'll try to)
I know the correct way to describe the language
> 
>>Hello, I'm a language model, and I'm still looking for any kind of language model that I could use in the language (I even found languages)
>  Hello, I'm a language model, and I'm not going to throw things back to me. Here's my new favorite...
But now, the big Hello, I'm a language model, as a member of the American Association of Language Testing Association and the English Language Association (APTA).
I've recentlyHello, I'm a language model, and that is really the thing. That is, I can use a particular set of predefined predefined attributes to be


> > Hello, I'm a language model, you're welcome to see it. Well, I started writing the blog as a way to show the relationship between the word
Hello, I'm a language model, and this will be the simplest, most intuitive, and the most accurate way to model a language. I'm working from
> Hello, I'm a language model, so I'll use that as the beginning of my talk. For the past two years, I am learning the structure and> 
Hello, I'm a language model, and you have to show it at the speed of light. This is the time at which it does the work to create
> Hello, I'm a language model, and if I'm wrong, I'm an individual (and there's a nice idea). Well, if I were to> 
Hello, I'm a language model, and your knowledge of the language and its components, I don't want to be one of those people around you. I
for step 10000 | loss 3.356803 | norm 0.3230 | time 12891.1994 ms | tok/sec 40670.2267
for step 10001 | loss 3.361009 | norm 0.3651 | time 461.6256 ms | tok/sec 1135742.9641
for step 10002 | loss 3.364212 | norm 0.3385 | time 462.5609 ms | tok/sec 1133446.4477
for step 10003 | loss 3.403810 | norm 0.3646 | time 463.0368 ms | tok/sec 1132281.5544
for step 10004 | loss 3.382925 | norm 0.3594 | time 463.0625 ms | tok/sec 1132218.5924
for step 10005 | loss 3.316398 | norm 0.3155 | time 462.8079 ms | tok/sec 1132841.5247
for step 10006 | loss 3.388756 | norm 0.3487 | time 462.6217 ms | tok/sec 1133297.4926
for step 10007 | loss 3.344403 | norm 0.3507 | time 463.8565 ms | tok/sec 1130280.6940
for step 10008 | loss 3.379310 | norm 0.3271 | time 464.2751 ms | tok/sec 1129261.4562
for step 10009 | loss 3.322895 | norm 0.3386 | time 464.1676 ms | tok/sec 1129523.0554
for step 10010 | loss 3.313515 | norm 0.3320 | time 463.9843 ms | tok/sec 1129969.3876
for step 10011 | loss 3.331010 | norm 0.3383 | time 463.9132 ms | tok/sec 1130142.4435
for step 10012 | loss 3.341563 | norm 0.2922 | time 463.6810 ms | tok/sec 1130708.4390
for step 10013 | loss 3.383444 | norm 0.3072 | time 463.9580 ms | tok/sec 1130033.2611
for step 10014 | loss 3.362960 | norm 0.3186 | time 464.1809 ms | tok/sec 1129490.5664
for step 10015 | loss 3.320723 | norm 0.2925 | time 464.9036 ms | tok/sec 1127734.8849
for step 10016 | loss 3.398683 | norm 0.3524 | time 464.8302 ms | tok/sec 1127913.0421
for step 10017 | loss 3.315437 | norm 0.3348 | time 464.2994 ms | tok/sec 1129202.3087
for step 10018 | loss 3.270383 | norm 0.2968 | time 464.6423 ms | tok/sec 1128369.1034
for step 10019 | loss 3.308036 | norm 0.3034 | time 465.1520 ms | tok/sec 1127132.5751
for step 10020 | loss 3.268056 | norm 0.3381 | time 464.2448 ms | tok/sec 1129335.1093
for step 10021 | loss 3.278119 | norm 0.2793 | time 464.8995 ms | tok/sec 1127744.7168
for step 10022 | loss 3.316215 | norm 0.3098 | time 466.2595 ms | tok/sec 1124455.4224
for step 10023 | loss 3.256892 | norm 0.2947 | time 463.7749 ms | tok/sec 1130479.4156
for step 10024 | loss 3.249482 | norm 0.2874 | time 465.3251 ms | tok/sec 1126713.3037
for step 10025 | loss 3.266598 | norm 0.3159 | time 465.1651 ms | tok/sec 1127100.8012
for step 10026 | loss 3.292565 | norm 0.3016 | time 465.4069 ms | tok/sec 1126515.3267
for step 10027 | loss 3.236938 | norm 0.2988 | time 466.5983 ms | tok/sec 1123638.9655
for step 10028 | loss 3.304352 | norm 0.3133 | time 464.5615 ms | tok/sec 1128565.4158
for step 10029 | loss 3.368845 | norm 0.3244 | time 465.9238 ms | tok/sec 1125265.5811
for step 10030 | loss 3.336629 | norm 0.3362 | time 465.5907 ms | tok/sec 1126070.5651
for step 10031 | loss 3.356620 | norm 0.3628 | time 464.9560 ms | tok/sec 1127607.6641
for step 10032 | loss 3.351858 | norm 0.4800 | time 465.7826 ms | tok/sec 1125606.5648
for step 10033 | loss 3.302176 | norm 0.4387 | time 464.8583 ms | tok/sec 1127844.7804
for step 10034 | loss 3.310488 | norm 0.4005 | time 465.8272 ms | tok/sec 1125498.8331
for step 10035 | loss 3.335148 | norm 0.3272 | time 465.7104 ms | tok/sec 1125781.1685
for step 10036 | loss 3.344074 | norm 0.3734 | time 467.4835 ms | tok/sec 1121511.1913
for step 10037 | loss 3.332016 | norm 0.3423 | time 465.8372 ms | tok/sec 1125474.6395
for step 10038 | loss 3.363557 | norm 0.3273 | time 465.4696 ms | tok/sec 1126363.5722
for step 10039 | loss 3.366787 | norm 0.3535 | time 465.8451 ms | tok/sec 1125455.6310
for step 10040 | loss 3.367510 | norm 0.3059 | time 465.0271 ms | tok/sec 1127435.3836
for step 10041 | loss 3.388648 | norm 0.3377 | time 465.7009 ms | tok/sec 1125804.2225
for step 10042 | loss 3.329405 | norm 0.3284 | time 466.0454 ms | tok/sec 1124971.9939
for step 10043 | loss 3.337909 | norm 0.3078 | time 465.3475 ms | tok/sec 1126659.0407
for step 10044 | loss 3.356901 | norm 0.2876 | time 465.9457 ms | tok/sec 1125212.6090
for step 10045 | loss 3.271818 | norm 0.2999 | time 465.7464 ms | tok/sec 1125694.1480
for step 10046 | loss 3.301204 | norm 0.2815 | time 465.5519 ms | tok/sec 1126164.5645
for step 10047 | loss 3.305265 | norm 0.2985 | time 464.6716 ms | tok/sec 1128297.8919
for step 10048 | loss 3.346428 | norm 0.2918 | time 466.6793 ms | tok/sec 1123443.7891
for step 10049 | loss 3.299744 | norm 0.2975 | time 465.0264 ms | tok/sec 1127437.1177
for step 10050 | loss 3.331703 | norm 0.3474 | time 464.9069 ms | tok/sec 1127726.7882
for step 10051 | loss 3.318063 | norm 0.2833 | time 466.0332 ms | tok/sec 1125001.3458
for step 10052 | loss 3.301507 | norm 0.3375 | time 466.1679 ms | tok/sec 1124676.2591
for step 10053 | loss 3.300165 | norm 0.3008 | time 466.8376 ms | tok/sec 1123062.8166
for step 10054 | loss 3.285075 | norm 0.3269 | time 465.8511 ms | tok/sec 1125441.2310
for step 10055 | loss 3.296808 | norm 0.3062 | time 465.6115 ms | tok/sec 1126020.4000
for step 10056 | loss 3.325899 | norm 0.3048 | time 466.2871 ms | tok/sec 1124388.7284
for step 10057 | loss 3.261284 | norm 0.3166 | time 466.2249 ms | tok/sec 1124538.8011
for step 10058 | loss 3.270039 | norm 0.3146 | time 465.5738 ms | tok/sec 1126111.5077
for step 10059 | loss 3.303954 | norm 0.3118 | time 464.8342 ms | tok/sec 1127903.2073
for step 10060 | loss 3.284542 | norm 0.3439 | time 466.2316 ms | tok/sec 1124522.6994
for step 10061 | loss 3.279145 | norm 0.3367 | time 464.5410 ms | tok/sec 1128615.2287
for step 10062 | loss 3.295641 | norm 0.3410 | time 464.7126 ms | tok/sec 1128198.3266
for step 10063 | loss 3.380412 | norm 0.3363 | time 464.8724 ms | tok/sec 1127810.6527
for step 10064 | loss 3.477006 | norm 0.3976 | time 465.1942 ms | tok/sec 1127030.3273
for step 10065 | loss 3.335026 | norm 0.3692 | time 465.2417 ms | tok/sec 1126915.3928
for step 10066 | loss 3.316775 | norm 0.3409 | time 465.0826 ms | tok/sec 1127300.7178
for step 10067 | loss 3.327535 | norm 0.3343 | time 465.0393 ms | tok/sec 1127405.9047
for step 10068 | loss 3.348090 | norm 0.3360 | time 465.0245 ms | tok/sec 1127441.7421
for step 10069 | loss 3.350673 | norm 0.3177 | time 465.6072 ms | tok/sec 1126030.7786
for step 10070 | loss 3.331443 | norm 0.3618 | time 465.1229 ms | tok/sec 1127203.0618
for step 10071 | loss 3.315702 | norm 0.3103 | time 464.8931 ms | tok/sec 1127760.3325
for step 10072 | loss 3.331160 | norm 0.3484 | time 465.8887 ms | tok/sec 1125350.2318
for step 10073 | loss 3.326466 | norm 0.3516 | time 465.2665 ms | tok/sec 1126855.3359
for step 10074 | loss 3.369751 | norm 0.3397 | time 464.7465 ms | tok/sec 1128116.1406
for step 10075 | loss 3.333985 | norm 0.3334 | time 465.0347 ms | tok/sec 1127416.8869
for step 10076 | loss 3.300055 | norm 0.3627 | time 465.4322 ms | tok/sec 1126454.1583
for step 10077 | loss 3.314811 | norm 0.3065 | time 464.4833 ms | tok/sec 1128755.4233
for step 10078 | loss 3.295212 | norm 0.3070 | time 465.0621 ms | tok/sec 1127350.4190
for step 10079 | loss 3.333627 | norm 0.3107 | time 464.9076 ms | tok/sec 1127725.0532
for step 10080 | loss 3.324752 | norm 0.3061 | time 464.9770 ms | tok/sec 1127556.7839
for step 10081 | loss 3.361658 | norm 0.3327 | time 464.8776 ms | tok/sec 1127797.9276
for step 10082 | loss 3.347976 | norm 0.3156 | time 465.5471 ms | tok/sec 1126176.0993
for step 10083 | loss 3.331764 | norm 0.3123 | time 464.6659 ms | tok/sec 1128311.7861
for step 10084 | loss 3.343817 | norm 0.3051 | time 465.5395 ms | tok/sec 1126194.5554
for step 10085 | loss 3.290403 | norm 0.2860 | time 464.6204 ms | tok/sec 1128422.3732
for step 10086 | loss 3.266590 | norm 0.3336 | time 464.0248 ms | tok/sec 1129870.6882
for step 10087 | loss 3.277768 | norm 0.3579 | time 465.3330 ms | tok/sec 1126694.2533
for step 10088 | loss 3.293973 | norm 0.3062 | time 465.5542 ms | tok/sec 1126158.7972
for step 10089 | loss 3.277645 | norm 0.3282 | time 465.4000 ms | tok/sec 1126532.0626
for step 10090 | loss 3.288737 | norm 0.3574 | time 465.2553 ms | tok/sec 1126882.4762
for step 10091 | loss 3.303044 | norm 0.2874 | time 465.5740 ms | tok/sec 1126110.9310
for step 10092 | loss 3.282508 | norm 0.3426 | time 464.6041 ms | tok/sec 1128461.7497
for step 10093 | loss 3.268716 | norm 0.3005 | time 465.1668 ms | tok/sec 1127096.7574
for step 10094 | loss 3.328022 | norm 0.3431 | time 464.9358 ms | tok/sec 1127656.8142
for step 10095 | loss 3.260536 | norm 0.3252 | time 465.6711 ms | tok/sec 1125876.2724
Will loading at 0 from edu_fineweb10B/edufineweb_train_000054.npy
for step 10096 | loss 3.258337 | norm 0.3180 | time 1408.6940 ms | tok/sec 372180.1820
for step 10097 | loss 3.269309 | norm 0.3425 | time 464.1647 ms | tok/sec 1129530.0175
for step 10098 | loss 3.304443 | norm 0.2879 | time 463.8741 ms | tok/sec 1130237.7049
for step 10099 | loss 3.257108 | norm 0.3172 | time 464.2403 ms | tok/sec 1129346.1291
for step 10100 | loss 3.307795 | norm 0.3481 | time 464.0570 ms | tok/sec 1129792.3216
for step 10101 | loss 3.316396 | norm 0.2897 | time 464.3390 ms | tok/sec 1129106.0622
for step 10102 | loss 3.240823 | norm 0.3603 | time 465.4820 ms | tok/sec 1126333.5723
for step 10103 | loss 3.281888 | norm 0.3101 | time 464.7927 ms | tok/sec 1128003.8777
for step 10104 | loss 3.336913 | norm 0.3206 | time 464.8716 ms | tok/sec 1127812.3879
for step 10105 | loss 3.339812 | norm 0.3094 | time 464.5402 ms | tok/sec 1128616.9664
for step 10106 | loss 3.331341 | norm 0.3338 | time 464.5576 ms | tok/sec 1128574.6830
for step 10107 | loss 3.346037 | norm 0.2958 | time 464.9341 ms | tok/sec 1127660.8620
for step 10108 | loss 3.376803 | norm 0.3307 | time 465.4605 ms | tok/sec 1126385.4961
for step 10109 | loss 3.408741 | norm 0.2953 | time 465.7795 ms | tok/sec 1125614.0549
for step 10110 | loss 3.377209 | norm 0.3398 | time 465.1053 ms | tok/sec 1127245.8203
for step 10111 | loss 3.353143 | norm 0.3290 | time 464.5367 ms | tok/sec 1128625.6552
for step 10112 | loss 3.338647 | norm 0.3090 | time 465.5027 ms | tok/sec 1126283.3838
for step 10113 | loss 3.428824 | norm 0.3353 | time 465.5073 ms | tok/sec 1126272.4237
for step 10114 | loss 3.351754 | norm 0.2999 | time 465.6756 ms | tok/sec 1125865.3203
for step 10115 | loss 3.375082 | norm 0.4564 | time 464.9229 ms | tok/sec 1127688.0412
for step 10116 | loss 3.364082 | norm 0.3666 | time 464.4225 ms | tok/sec 1128903.1867
for step 10117 | loss 3.406981 | norm 0.3168 | time 465.3437 ms | tok/sec 1126668.2766
for step 10118 | loss 3.442183 | norm 0.3287 | time 465.1880 ms | tok/sec 1127045.3456
for step 10119 | loss 3.389182 | norm 0.3528 | time 465.4312 ms | tok/sec 1126456.4665
for step 10120 | loss 3.332603 | norm 0.3547 | time 464.7222 ms | tok/sec 1128175.1744
for step 10121 | loss 3.284432 | norm 0.3005 | time 464.8857 ms | tok/sec 1127778.2621
for step 10122 | loss 3.337444 | norm 0.3412 | time 464.8416 ms | tok/sec 1127885.2737
for step 10123 | loss 3.265955 | norm 0.3075 | time 464.6032 ms | tok/sec 1128464.0661
for step 10124 | loss 3.251693 | norm 0.3020 | time 464.4358 ms | tok/sec 1128870.7334
for step 10125 | loss 3.258393 | norm 0.2787 | time 465.3158 ms | tok/sec 1126735.8186
for step 10126 | loss 3.399273 | norm 0.3067 | time 464.4964 ms | tok/sec 1128723.5578
for step 10127 | loss 3.310899 | norm 0.2879 | time 465.4212 ms | tok/sec 1126480.7023
for step 10128 | loss 3.359156 | norm 0.2961 | time 464.8173 ms | tok/sec 1127944.2833
for step 10129 | loss 3.279851 | norm 0.2960 | time 464.3109 ms | tok/sec 1129174.4767
for step 10130 | loss 3.319304 | norm 0.3098 | time 465.7521 ms | tok/sec 1125680.3181
for step 10131 | loss 3.265155 | norm 0.2767 | time 465.0211 ms | tok/sec 1127449.8347
for step 10132 | loss 3.350575 | norm 0.3309 | time 466.3696 ms | tok/sec 1124189.8432
for step 10133 | loss 3.250808 | norm 0.3137 | time 464.0439 ms | tok/sec 1129824.2474
for step 10134 | loss 3.327086 | norm 0.3224 | time 464.7441 ms | tok/sec 1128121.9280
for step 10135 | loss 3.324883 | norm 0.3434 | time 464.9031 ms | tok/sec 1127736.0416
for step 10136 | loss 3.399821 | norm 0.3230 | time 464.5336 ms | tok/sec 1128633.1855
for step 10137 | loss 3.243083 | norm 0.2994 | time 465.0857 ms | tok/sec 1127293.2052
for step 10138 | loss 3.289076 | norm 0.3256 | time 465.3482 ms | tok/sec 1126657.3090
for step 10139 | loss 3.308506 | norm 0.3077 | time 465.0195 ms | tok/sec 1127453.8810
for step 10140 | loss 3.297500 | norm 0.3343 | time 465.1275 ms | tok/sec 1127192.0838
for step 10141 | loss 3.383225 | norm 0.3838 | time 465.4255 ms | tok/sec 1126470.3154
for step 10142 | loss 3.264994 | norm 0.3781 | time 465.9405 ms | tok/sec 1125225.2758
for step 10143 | loss 3.303100 | norm 0.3402 | time 464.4532 ms | tok/sec 1128828.4310
for step 10144 | loss 3.345228 | norm 0.4149 | time 466.2423 ms | tok/sec 1124496.8227
for step 10145 | loss 3.354145 | norm 0.3384 | time 464.8874 ms | tok/sec 1127774.2135
for step 10146 | loss 3.366633 | norm 0.3616 | time 465.9257 ms | tok/sec 1125260.9747
for step 10147 | loss 3.329167 | norm 0.3210 | time 464.9155 ms | tok/sec 1127705.9685
for step 10148 | loss 3.383321 | norm 0.3240 | time 464.9932 ms | tok/sec 1127517.4705
for step 10149 | loss 3.374759 | norm 0.4061 | time 464.7703 ms | tok/sec 1128058.2704
for step 10150 | loss 3.313349 | norm 0.3238 | time 465.8561 ms | tok/sec 1125429.1353
for step 10151 | loss 3.312879 | norm 0.3327 | time 464.1907 ms | tok/sec 1129466.7810
for step 10152 | loss 3.344465 | norm 0.3371 | time 465.3280 ms | tok/sec 1126706.3762
for step 10153 | loss 3.297519 | norm 0.3098 | time 465.2696 ms | tok/sec 1126847.8293
for step 10154 | loss 3.410564 | norm 0.3470 | time 464.8283 ms | tok/sec 1127917.6703
for step 10155 | loss 3.359830 | norm 0.3522 | time 464.8132 ms | tok/sec 1127954.1188
for step 10156 | loss 3.299094 | norm 0.3047 | time 464.7663 ms | tok/sec 1128068.1079
for step 10157 | loss 3.358936 | norm 0.3231 | time 464.5953 ms | tok/sec 1128483.1764
for step 10158 | loss 3.304743 | norm 0.3293 | time 465.8380 ms | tok/sec 1125472.9114
for step 10159 | loss 3.338738 | norm 0.3196 | time 464.9549 ms | tok/sec 1127610.5552
for step 10160 | loss 3.256878 | norm 0.3127 | time 465.6541 ms | tok/sec 1125917.2009
for step 10161 | loss 3.282177 | norm 0.3136 | time 465.1990 ms | tok/sec 1127018.7750
for step 10162 | loss 3.341708 | norm 0.3172 | time 464.9987 ms | tok/sec 1127504.1739
for step 10163 | loss 3.276678 | norm 0.3129 | time 464.9005 ms | tok/sec 1127742.4034
for step 10164 | loss 3.290635 | norm 0.3743 | time 464.2034 ms | tok/sec 1129436.0355
for step 10165 | loss 3.299129 | norm 0.3144 | time 465.4541 ms | tok/sec 1126401.0742
for step 10166 | loss 3.223221 | norm 0.3334 | time 464.4239 ms | tok/sec 1128899.7095
for step 10167 | loss 3.293136 | norm 0.3481 | time 465.4424 ms | tok/sec 1126429.3467
for step 10168 | loss 3.302228 | norm 0.3416 | time 465.3170 ms | tok/sec 1126732.9320
for step 10169 | loss 3.359360 | norm 0.2987 | time 465.6801 ms | tok/sec 1125854.3683
for step 10170 | loss 3.310198 | norm 0.3399 | time 466.3773 ms | tok/sec 1124171.4528
for step 10171 | loss 3.381281 | norm 0.3217 | time 465.9903 ms | tok/sec 1125104.9527
for step 10172 | loss 3.345948 | norm 0.3374 | time 466.0792 ms | tok/sec 1124890.2773
for step 10173 | loss 3.360063 | norm 0.3338 | time 465.0962 ms | tok/sec 1127267.7787
for step 10174 | loss 3.316173 | norm 0.3109 | time 464.8485 ms | tok/sec 1127868.4975
for step 10175 | loss 3.362043 | norm 0.3422 | time 466.4509 ms | tok/sec 1123993.9009
for step 10176 | loss 3.375589 | norm 0.3462 | time 465.3451 ms | tok/sec 1126664.8131
for step 10177 | loss 3.320770 | norm 0.3043 | time 465.7304 ms | tok/sec 1125732.7580
for step 10178 | loss 3.330134 | norm 0.3247 | time 465.2646 ms | tok/sec 1126859.9555
for step 10179 | loss 3.370872 | norm 0.3032 | time 466.8450 ms | tok/sec 1123045.0365
for step 10180 | loss 3.361025 | norm 0.3586 | time 465.3032 ms | tok/sec 1126766.4173
for step 10181 | loss 3.345053 | norm 0.3875 | time 464.9646 ms | tok/sec 1127586.8489
for step 10182 | loss 3.383846 | norm 0.3579 | time 465.6329 ms | tok/sec 1125968.5099
for step 10183 | loss 3.390432 | norm 0.3503 | time 465.4212 ms | tok/sec 1126480.7023
for step 10184 | loss 3.356426 | norm 0.3600 | time 465.0927 ms | tok/sec 1127276.4467
for step 10185 | loss 3.396433 | norm 0.3227 | time 465.0602 ms | tok/sec 1127355.0426
for step 10186 | loss 3.376379 | norm 0.3642 | time 465.7707 ms | tok/sec 1125635.3735
for step 10187 | loss 3.389059 | norm 0.3319 | time 465.6303 ms | tok/sec 1125974.8517
for step 10188 | loss 3.350029 | norm 0.3500 | time 465.4543 ms | tok/sec 1126400.4972
for step 10189 | loss 3.383330 | norm 0.3288 | time 465.6417 ms | tok/sec 1125947.1786
for step 10190 | loss 3.393697 | norm 0.3475 | time 465.1122 ms | tok/sec 1127229.0632
for step 10191 | loss 3.311642 | norm 0.3393 | time 466.2707 ms | tok/sec 1124428.3989
for step 10192 | loss 3.264196 | norm 0.2970 | time 465.6258 ms | tok/sec 1125985.8060
for step 10193 | loss 3.326937 | norm 0.3209 | time 465.8964 ms | tok/sec 1125331.8033
for step 10194 | loss 3.304344 | norm 0.3236 | time 465.8344 ms | tok/sec 1125481.5518
for step 10195 | loss 3.318448 | norm 0.3191 | time 465.8909 ms | tok/sec 1125345.0487
for step 10196 | loss 3.259118 | norm 0.3032 | time 465.1527 ms | tok/sec 1127130.8419
for step 10197 | loss 3.274562 | norm 0.3077 | time 465.2755 ms | tok/sec 1126833.3937
for step 10198 | loss 3.320989 | norm 0.3122 | time 465.1570 ms | tok/sec 1127120.4430
for step 10199 | loss 3.233323 | norm 0.3124 | time 466.1217 ms | tok/sec 1124787.8606
for step 10200 | loss 3.325185 | norm 0.3234 | time 466.1233 ms | tok/sec 1124783.8333
for step 10201 | loss 3.247845 | norm 0.3036 | time 465.4164 ms | tok/sec 1126492.2435
for step 10202 | loss 3.313639 | norm 0.2964 | time 465.3120 ms | tok/sec 1126745.0558
for step 10203 | loss 3.295561 | norm 0.3319 | time 465.8172 ms | tok/sec 1125523.0277
for step 10204 | loss 3.347414 | norm 0.3261 | time 465.5659 ms | tok/sec 1126130.5384
for step 10205 | loss 3.325947 | norm 0.3470 | time 465.7495 ms | tok/sec 1125686.6568
for step 10206 | loss 3.253102 | norm 0.3484 | time 465.6048 ms | tok/sec 1126036.5446
for step 10207 | loss 3.362927 | norm 0.3679 | time 465.6017 ms | tok/sec 1126044.0405
for step 10208 | loss 3.321162 | norm 0.3009 | time 465.6458 ms | tok/sec 1125937.3780
for step 10209 | loss 3.302369 | norm 0.3180 | time 465.1620 ms | tok/sec 1127108.3112
for step 10210 | loss 3.331290 | norm 0.3201 | time 465.0862 ms | tok/sec 1127292.0494
for step 10211 | loss 3.269690 | norm 0.3079 | time 465.1256 ms | tok/sec 1127196.7061
for step 10212 | loss 3.319276 | norm 0.3133 | time 466.1584 ms | tok/sec 1124699.2679
for step 10213 | loss 3.351067 | norm 0.3077 | time 464.4382 ms | tok/sec 1128864.9383
for step 10214 | loss 3.311152 | norm 0.3038 | time 465.2719 ms | tok/sec 1126842.0550
for step 10215 | loss 3.350323 | norm 0.3197 | time 465.0140 ms | tok/sec 1127467.1764
for step 10216 | loss 3.314599 | norm 0.3424 | time 465.6599 ms | tok/sec 1125903.3656
for step 10217 | loss 3.412950 | norm 0.3215 | time 464.7908 ms | tok/sec 1128008.5067
for step 10218 | loss 3.338856 | norm 0.3563 | time 464.4446 ms | tok/sec 1128849.2920
for step 10219 | loss 3.378111 | norm 0.3338 | time 464.6225 ms | tok/sec 1128417.1618
for step 10220 | loss 3.338133 | norm 0.3221 | time 464.6487 ms | tok/sec 1128353.4708
for step 10221 | loss 3.353578 | norm 0.3462 | time 464.7820 ms | tok/sec 1128029.9161
for step 10222 | loss 3.290193 | norm 0.2974 | time 464.9162 ms | tok/sec 1127704.2336
for step 10223 | loss 3.417572 | norm 0.3225 | time 464.5419 ms | tok/sec 1128612.9117
for step 10224 | loss 3.318706 | norm 0.2984 | time 464.4833 ms | tok/sec 1128755.4233
for step 10225 | loss 3.371955 | norm 0.3083 | time 465.9555 ms | tok/sec 1125189.0034
for step 10226 | loss 3.319120 | norm 0.3236 | time 465.3780 ms | tok/sec 1126585.1590
for step 10227 | loss 3.230978 | norm 0.2926 | time 465.4903 ms | tok/sec 1126313.3810
for step 10228 | loss 3.263335 | norm 0.3107 | time 465.7030 ms | tok/sec 1125799.0352
for step 10229 | loss 3.279356 | norm 0.3067 | time 464.6850 ms | tok/sec 1128265.4734
for step 10230 | loss 3.258165 | norm 0.3232 | time 465.9047 ms | tok/sec 1125311.6479
for step 10231 | loss 3.302803 | norm 0.3428 | time 465.7257 ms | tok/sec 1125744.2840
for step 10232 | loss 3.253230 | norm 0.2889 | time 465.8990 ms | tok/sec 1125325.4687
for step 10233 | loss 3.275559 | norm 0.3234 | time 466.2914 ms | tok/sec 1124378.3800
for step 10234 | loss 3.369147 | norm 0.3232 | time 465.9536 ms | tok/sec 1125193.6093
for step 10235 | loss 3.296071 | norm 0.3332 | time 465.2631 ms | tok/sec 1126863.4201
for step 10236 | loss 3.303930 | norm 0.3085 | time 464.5462 ms | tok/sec 1128602.4854
for step 10237 | loss 3.283545 | norm 0.3135 | time 465.3561 ms | tok/sec 1126638.2605
for step 10238 | loss 3.296396 | norm 0.3171 | time 465.3034 ms | tok/sec 1126765.8399
for step 10239 | loss 3.318480 | norm 0.3132 | time 465.6785 ms | tok/sec 1125858.4032
for step 10240 | loss 3.355236 | norm 0.3060 | time 465.5328 ms | tok/sec 1126210.7050
for step 10241 | loss 3.411704 | norm 0.3842 | time 465.6415 ms | tok/sec 1125947.7551
for step 10242 | loss 3.283481 | norm 0.3777 | time 464.4482 ms | tok/sec 1128840.5998
for step 10243 | loss 3.318591 | norm 0.3262 | time 464.6626 ms | tok/sec 1128319.8912
for step 10244 | loss 3.356009 | norm 0.3976 | time 464.8328 ms | tok/sec 1127906.6784
for step 10245 | loss 3.298702 | norm 0.3253 | time 465.2061 ms | tok/sec 1127001.4471
for step 10246 | loss 3.325376 | norm 0.3128 | time 465.8165 ms | tok/sec 1125524.7559
for step 10247 | loss 3.330775 | norm 0.3563 | time 464.6368 ms | tok/sec 1128382.4204
for step 10248 | loss 3.260259 | norm 0.3150 | time 464.2289 ms | tok/sec 1129373.9696
for step 10249 | loss 3.334131 | norm 0.3791 | time 465.7993 ms | tok/sec 1125566.2350
validation loss 3.3279
HellaSwag accuracy: 2769/10042=0.2757
> Hello, I'm a language model, and this is a model for modeling: the interface to the database is the real world database. You can't edit the
> Hello, I'm a language model, a model language (which is used to generate a model file) and a set of expressions that take a model file and
> Hello, I'm a language model, but once it's been explained, I'll be able to build an actual language model.
What do I say to
> Hello, I'm a language model, and I'm an English class, and while I'm only entering basic Spanish you can probably already find some of the basic
> Hello, I'm a language model, you could use a number you've used in the above example to generate the same integer. As you can imagine, here
> Hello, I'm a language model, and I can't seem like the only explanation for why I'm here...
My name is M, which is a
> > >>Hello, I'm a language model, so I'd like to try to model an object with a model. So now I need to tell you a little more
Hello, I'm a language model, and a great place to begin with.
So, I'll do it by writing a little bit of what language models
 Hello, I'm a language model, and I like to think of all the time we do a little science project that's a little crazy and it's fun Hello, I'm a language model, and I was a computer scientist. I knew something was wrong with my first language assignment. I want to make sure that> > 

Hello, I'm a language model, having the ability to work with your child's native language. I will tell you this by saying, "This app can
Hello, I'm a language model, and a model for all languages I'm writing in.
I'm a regular language, doesn't let anyone stop listening
> > > Hello, I'm a language model, so there's no problem. I only have one class name and it's very simple, right? "It's just
>>Hello, I'm a language model, of course. I'm not talking for anything, only writing about programming languages. Not for a computer user.
This
Hello, I'm a language model, I could tell any English, but there was a lot of talkin' on those languages. And some of these are
 Hello, I'm a language model, but what's the deal with that? To understand this language model for example, you need to know the basic parts of Hello, I'm a language model, so I should probably want to see this as another way to talk to my students. I'll be at least as useful> > 
>>
Hello, I'm a language model, and I've also covered the many subfields of 'language learning'. However, I've also covered the 'lingua
Hello, I'm a language model, and the function has some very nice aspects in addition to basic model and compiler issues.
The program has a really simple
 Hello, I'm a language model, and i am sure you would be amazed to know that it has a big vocabulary with both the word and verb forms of Hello, I'm a language model, to describe something you mean it in terms of another language or something specific. If something has a definition as another language,> > > 

Hello, I'm a language model, and one of the most famous of my books written by linguists, that's as great as I think. It just
Hello, I'm a language model, from a point of view.
Well, my best guess is, a language model is a computer program that analyzes
Hello, I'm a language model, and I'm not. So, we define the way our English is implemented then we will go through, and it should
> > > Hello, I'm a language model, in my programming. I'm a developer . I will do the rest:
- I will create an application on my
Hello, I'm a language model, and you want the algorithm to use one of the following methods:
For some languages of the world, the algorithm might
Hello, I'm a language model, and this one is a true translation, but it's important to note that the language model is not the only choice of
> > > Hello, I'm a language model, so I am not a language model I might not have even realized it yet. I don't see my model working with
Hello, I'm a language model, is the code in my own code.
First we have to do the keyword model and we look for a string containing
Hello, I'm a language model, and you are familiar with the various languages which speak them. This is the way of the language world, and what languages
> Hello, I'm a language model, and that does not work for me. As a mother, when I was on my way when I was working with someone> 
Hello, I'm a language model, and some of it's models in a way that is quite alien. There's something strange going on—the way that
for step 10250 | loss 3.387103 | norm 0.3507 | time 12984.3335 ms | tok/sec 40378.5069
for step 10251 | loss 3.409175 | norm 0.3653 | time 461.2014 ms | tok/sec 1136787.4569
for step 10252 | loss 3.392319 | norm 0.3742 | time 462.5568 ms | tok/sec 1133456.3794
for step 10253 | loss 3.344131 | norm 0.3196 | time 462.5301 ms | tok/sec 1133521.8164
for step 10254 | loss 3.406668 | norm 0.3407 | time 462.9228 ms | tok/sec 1132560.3037
for step 10255 | loss 3.376102 | norm 0.3209 | time 461.9999 ms | tok/sec 1134822.7732
for step 10256 | loss 3.357349 | norm 0.3372 | time 462.5850 ms | tok/sec 1133387.4451
for step 10257 | loss 3.390476 | norm 0.3276 | time 464.1259 ms | tok/sec 1129624.5954
for step 10258 | loss 3.368980 | norm 0.3254 | time 462.7481 ms | tok/sec 1132988.0249
for step 10259 | loss 3.364027 | norm 0.3099 | time 463.4972 ms | tok/sec 1131156.8724
for step 10260 | loss 3.272069 | norm 0.3115 | time 462.8444 ms | tok/sec 1132752.2423
for step 10261 | loss 3.340992 | norm 0.3102 | time 463.4452 ms | tok/sec 1131283.7314
for step 10262 | loss 3.375446 | norm 0.3222 | time 463.2189 ms | tok/sec 1131836.3068
for step 10263 | loss 3.244299 | norm 0.3226 | time 463.2256 ms | tok/sec 1131819.9955
for step 10264 | loss 3.235757 | norm 0.6198 | time 463.6195 ms | tok/sec 1130858.4588
for step 10265 | loss 3.357812 | norm 0.3155 | time 463.9566 ms | tok/sec 1130036.7454
for step 10266 | loss 3.260878 | norm 0.3238 | time 465.2040 ms | tok/sec 1127006.6454
for step 10267 | loss 3.310148 | norm 0.2996 | time 465.7087 ms | tok/sec 1125785.2028
for step 10268 | loss 3.287714 | norm 0.3169 | time 463.7089 ms | tok/sec 1130640.4198
for step 10269 | loss 3.297201 | norm 0.3309 | time 465.0748 ms | tok/sec 1127319.7887
for step 10270 | loss 3.334277 | norm 0.2765 | time 464.0443 ms | tok/sec 1129823.0865
for step 10271 | loss 3.286221 | norm 0.3045 | time 464.5660 ms | tok/sec 1128554.4113
for step 10272 | loss 3.316752 | norm 0.2817 | time 465.1556 ms | tok/sec 1127123.9093
for step 10273 | loss 3.294194 | norm 0.3010 | time 464.1085 ms | tok/sec 1129666.9575
for step 10274 | loss 3.334910 | norm 0.3196 | time 464.3393 ms | tok/sec 1129105.4825
for step 10275 | loss 3.339378 | norm 0.3433 | time 464.4575 ms | tok/sec 1128818.0007
for step 10276 | loss 3.298645 | norm 0.3682 | time 465.0354 ms | tok/sec 1127415.1528
for step 10277 | loss 3.327114 | norm 0.3134 | time 465.0919 ms | tok/sec 1127278.1803
for step 10278 | loss 3.347093 | norm 0.3000 | time 465.6155 ms | tok/sec 1126010.5982
for step 10279 | loss 3.342307 | norm 0.3326 | time 465.5013 ms | tok/sec 1126286.8449
for step 10280 | loss 3.311576 | norm 0.3052 | time 465.0095 ms | tok/sec 1127478.1598
for step 10281 | loss 3.326844 | norm 0.2905 | time 465.0660 ms | tok/sec 1127341.1719
for step 10282 | loss 3.325322 | norm 0.3225 | time 465.2023 ms | tok/sec 1127010.6886
for step 10283 | loss 3.343969 | norm 0.3127 | time 465.0600 ms | tok/sec 1127355.6206
for step 10284 | loss 3.356707 | norm 0.3402 | time 464.7026 ms | tok/sec 1128222.6375
for step 10285 | loss 3.363997 | norm 0.3192 | time 465.6878 ms | tok/sec 1125835.9233
Will loading at 0 from edu_fineweb10B/edufineweb_train_000055.npy
for step 10286 | loss 3.369827 | norm 0.3188 | time 1415.6537 ms | tok/sec 370350.4592
for step 10287 | loss 3.331336 | norm 0.3363 | time 463.4590 ms | tok/sec 1131249.9771
for step 10288 | loss 3.356593 | norm 0.3256 | time 464.5331 ms | tok/sec 1128634.3441
for step 10289 | loss 3.359711 | norm 0.3032 | time 464.1774 ms | tok/sec 1129499.2686
for step 10290 | loss 3.351571 | norm 0.3659 | time 463.6900 ms | tok/sec 1130686.3464
for step 10291 | loss 3.363193 | norm 0.3456 | time 464.8538 ms | tok/sec 1127855.7712
for step 10292 | loss 3.331264 | norm 0.3392 | time 465.5824 ms | tok/sec 1126090.7477
for step 10293 | loss 3.382464 | norm 0.3570 | time 464.7973 ms | tok/sec 1127992.8841
for step 10294 | loss 3.335852 | norm 0.3581 | time 464.2184 ms | tok/sec 1129399.4912
for step 10295 | loss 3.346672 | norm 0.3255 | time 465.1701 ms | tok/sec 1127088.6698
for step 10296 | loss 3.417485 | norm 0.3533 | time 465.2257 ms | tok/sec 1126954.0867
for step 10297 | loss 3.295791 | norm 0.3449 | time 464.8814 ms | tok/sec 1127788.6732
for step 10298 | loss 3.284114 | norm 0.3321 | time 464.7825 ms | tok/sec 1128028.7588
for step 10299 | loss 3.286525 | norm 0.3343 | time 464.9141 ms | tok/sec 1127709.4384
for step 10300 | loss 3.307901 | norm 0.3632 | time 464.8838 ms | tok/sec 1127782.8892
for step 10301 | loss 3.270491 | norm 0.3213 | time 465.8639 ms | tok/sec 1125410.1284
for step 10302 | loss 3.315391 | norm 0.3091 | time 464.3898 ms | tok/sec 1128982.5893
for step 10303 | loss 3.300054 | norm 0.3124 | time 464.3629 ms | tok/sec 1129048.0903
for step 10304 | loss 3.249929 | norm 0.3005 | time 464.6404 ms | tok/sec 1128373.7354
for step 10305 | loss 3.284279 | norm 0.3207 | time 465.6870 ms | tok/sec 1125837.6525
for step 10306 | loss 3.312662 | norm 0.3179 | time 465.2696 ms | tok/sec 1126847.8293
for step 10307 | loss 3.302190 | norm 0.2996 | time 464.8650 ms | tok/sec 1127828.5839
for step 10308 | loss 3.231653 | norm 0.3093 | time 465.6272 ms | tok/sec 1125982.3468
for step 10309 | loss 3.345454 | norm 0.2861 | time 464.6895 ms | tok/sec 1128254.4746
for step 10310 | loss 3.316822 | norm 0.3585 | time 465.1244 ms | tok/sec 1127199.5950
for step 10311 | loss 3.298725 | norm 0.2797 | time 465.1828 ms | tok/sec 1127058.0537
for step 10312 | loss 3.308118 | norm 0.3478 | time 465.0252 ms | tok/sec 1127440.0079
for step 10313 | loss 3.339648 | norm 0.3342 | time 466.3990 ms | tok/sec 1124119.1582
for step 10314 | loss 3.300580 | norm 0.3262 | time 465.5735 ms | tok/sec 1126112.0844
for step 10315 | loss 3.318399 | norm 0.3226 | time 465.6162 ms | tok/sec 1126008.8685
for step 10316 | loss 3.317632 | norm 0.3363 | time 465.4450 ms | tok/sec 1126422.9997
for step 10317 | loss 3.343932 | norm 0.3457 | time 465.5523 ms | tok/sec 1126163.4111
for step 10318 | loss 3.352123 | norm 0.3768 | time 464.7815 ms | tok/sec 1128031.0734
for step 10319 | loss 3.307796 | norm 0.3959 | time 464.4885 ms | tok/sec 1128742.6769
for step 10320 | loss 3.376524 | norm 0.3733 | time 465.2996 ms | tok/sec 1126775.0776
for step 10321 | loss 3.347160 | norm 0.3444 | time 464.7126 ms | tok/sec 1128198.3266
for step 10322 | loss 3.357095 | norm 0.3387 | time 465.1194 ms | tok/sec 1127211.7288
for step 10323 | loss 3.321890 | norm 0.3534 | time 464.6914 ms | tok/sec 1128249.8437
for step 10324 | loss 3.362209 | norm 0.3053 | time 464.7698 ms | tok/sec 1128059.4277
for step 10325 | loss 3.267676 | norm 0.3495 | time 464.0796 ms | tok/sec 1129737.1812
for step 10326 | loss 3.379500 | norm 0.3116 | time 465.0249 ms | tok/sec 1127440.5860
for step 10327 | loss 3.357873 | norm 0.3232 | time 465.0869 ms | tok/sec 1127290.3157
for step 10328 | loss 3.367779 | norm 0.3588 | time 465.8096 ms | tok/sec 1125541.4624
for step 10329 | loss 3.360322 | norm 0.3172 | time 465.3895 ms | tok/sec 1126557.4559
for step 10330 | loss 3.337914 | norm 0.3377 | time 465.2202 ms | tok/sec 1126967.3703
for step 10331 | loss 3.283799 | norm 0.3360 | time 465.5883 ms | tok/sec 1126076.3315
for step 10332 | loss 3.275667 | norm 0.3572 | time 464.7453 ms | tok/sec 1128119.0343
for step 10333 | loss 3.296113 | norm 0.3718 | time 464.4752 ms | tok/sec 1128775.1228
for step 10334 | loss 3.242693 | norm 0.3144 | time 465.0052 ms | tok/sec 1127488.5653
for step 10335 | loss 3.295100 | norm 0.3461 | time 464.8168 ms | tok/sec 1127945.4404
for step 10336 | loss 3.300050 | norm 0.2819 | time 465.1930 ms | tok/sec 1127033.2154
for step 10337 | loss 3.295230 | norm 0.3108 | time 465.6286 ms | tok/sec 1125978.8875
for step 10338 | loss 3.278868 | norm 0.3166 | time 465.8940 ms | tok/sec 1125337.5622
for step 10339 | loss 3.305704 | norm 0.3601 | time 465.5397 ms | tok/sec 1126193.9786
for step 10340 | loss 3.317177 | norm 0.2983 | time 464.7644 ms | tok/sec 1128072.7374
for step 10341 | loss 3.283060 | norm 0.3054 | time 465.2758 ms | tok/sec 1126832.8162
for step 10342 | loss 3.299876 | norm 0.3056 | time 464.8752 ms | tok/sec 1127803.7117
for step 10343 | loss 3.304374 | norm 0.3069 | time 465.5917 ms | tok/sec 1126068.2586
for step 10344 | loss 3.332666 | norm 0.3384 | time 465.1875 ms | tok/sec 1127046.5008
for step 10345 | loss 3.353877 | norm 0.3063 | time 465.4362 ms | tok/sec 1126444.3489
for step 10346 | loss 3.328801 | norm 0.3181 | time 465.5733 ms | tok/sec 1126112.6611
for step 10347 | loss 3.442607 | norm 0.3172 | time 464.4394 ms | tok/sec 1128862.0408
for step 10348 | loss 3.376698 | norm 0.3377 | time 465.0810 ms | tok/sec 1127304.7631
for step 10349 | loss 3.332357 | norm 0.3244 | time 464.7615 ms | tok/sec 1128079.6817
for step 10350 | loss 3.414605 | norm 0.3669 | time 464.7851 ms | tok/sec 1128022.3938
for step 10351 | loss 3.334024 | norm 0.3785 | time 465.1387 ms | tok/sec 1127164.9286
for step 10352 | loss 3.409606 | norm 0.3601 | time 465.5025 ms | tok/sec 1126283.9607
for step 10353 | loss 3.344148 | norm 0.3960 | time 464.7727 ms | tok/sec 1128052.4837
for step 10354 | loss 3.303021 | norm 0.3443 | time 465.2090 ms | tok/sec 1126994.5160
for step 10355 | loss 3.397261 | norm 0.3837 | time 465.4987 ms | tok/sec 1126293.1904
for step 10356 | loss 3.328045 | norm 0.3251 | time 464.5791 ms | tok/sec 1128522.5572
for step 10357 | loss 3.360317 | norm 0.3486 | time 464.6018 ms | tok/sec 1128467.5406
for step 10358 | loss 3.404069 | norm 0.3176 | time 464.5178 ms | tok/sec 1128671.4182
for step 10359 | loss 3.379446 | norm 0.3237 | time 465.5252 ms | tok/sec 1126229.1622
for step 10360 | loss 3.339866 | norm 0.3122 | time 466.0299 ms | tok/sec 1125009.4034
for step 10361 | loss 3.305539 | norm 0.3218 | time 465.2576 ms | tok/sec 1126876.7016
for step 10362 | loss 3.336171 | norm 0.3164 | time 465.5797 ms | tok/sec 1126097.0909
for step 10363 | loss 3.333632 | norm 0.3304 | time 465.0023 ms | tok/sec 1127495.5024
for step 10364 | loss 3.440456 | norm 0.3615 | time 464.7899 ms | tok/sec 1128010.8212
for step 10365 | loss 3.350604 | norm 0.3396 | time 464.7770 ms | tok/sec 1128042.0678
for step 10366 | loss 3.331642 | norm 0.3551 | time 465.4317 ms | tok/sec 1126455.3124
for step 10367 | loss 3.307603 | norm 0.3784 | time 465.2259 ms | tok/sec 1126953.5092
for step 10368 | loss 3.369994 | norm 0.3853 | time 465.9545 ms | tok/sec 1125191.3064
for step 10369 | loss 3.276530 | norm 0.3180 | time 464.8082 ms | tok/sec 1127966.2689
for step 10370 | loss 3.297966 | norm 0.3414 | time 465.4715 ms | tok/sec 1126358.9567
for step 10371 | loss 3.221657 | norm 0.5543 | time 468.3034 ms | tok/sec 1119547.6116
for step 10372 | loss 3.253988 | norm 0.3220 | time 464.4315 ms | tok/sec 1128881.1646
for step 10373 | loss 3.344218 | norm 0.3455 | time 465.7753 ms | tok/sec 1125624.4260
for step 10374 | loss 3.302319 | norm 0.3067 | time 464.9627 ms | tok/sec 1127591.4745
for step 10375 | loss 3.271652 | norm 0.3081 | time 464.6590 ms | tok/sec 1128328.5754
for step 10376 | loss 3.303459 | norm 0.2809 | time 465.4670 ms | tok/sec 1126369.9185
for step 10377 | loss 3.257217 | norm 0.3140 | time 465.6167 ms | tok/sec 1126007.7153
for step 10378 | loss 3.276026 | norm 0.2856 | time 465.3049 ms | tok/sec 1126762.3758
for step 10379 | loss 3.317560 | norm 0.3271 | time 465.0931 ms | tok/sec 1127275.2909
for step 10380 | loss 3.387684 | norm 0.3011 | time 464.6101 ms | tok/sec 1128447.2727
for step 10381 | loss 3.355384 | norm 0.3280 | time 464.4942 ms | tok/sec 1128728.7720
for step 10382 | loss 3.326309 | norm 0.3098 | time 465.4598 ms | tok/sec 1126387.2270
for step 10383 | loss 3.319058 | norm 0.3097 | time 465.1482 ms | tok/sec 1127141.8187
for step 10384 | loss 3.336272 | norm 0.3579 | time 465.0288 ms | tok/sec 1127431.3374
for step 10385 | loss 3.313139 | norm 0.3059 | time 465.3952 ms | tok/sec 1126543.6048
for step 10386 | loss 3.338556 | norm 0.3480 | time 464.7894 ms | tok/sec 1128011.9784
for step 10387 | loss 3.351702 | norm 0.3585 | time 465.0743 ms | tok/sec 1127320.9445
for step 10388 | loss 3.351509 | norm 0.3716 | time 465.1456 ms | tok/sec 1127148.1739
for step 10389 | loss 3.310562 | norm 0.3508 | time 464.9582 ms | tok/sec 1127602.4603
for step 10390 | loss 3.304925 | norm 0.3810 | time 465.3785 ms | tok/sec 1126584.0047
for step 10391 | loss 3.323956 | norm 0.3893 | time 464.9575 ms | tok/sec 1127604.1949
for step 10392 | loss 3.378873 | norm 0.3479 | time 465.6196 ms | tok/sec 1126000.7965
for step 10393 | loss 3.345583 | norm 0.3500 | time 466.2991 ms | tok/sec 1124359.9834
for step 10394 | loss 3.319453 | norm 0.3303 | time 464.3986 ms | tok/sec 1128961.1437
for step 10395 | loss 3.372369 | norm 0.3149 | time 465.4269 ms | tok/sec 1126466.8531
for step 10396 | loss 3.359185 | norm 0.3619 | time 464.5238 ms | tok/sec 1128656.9358
for step 10397 | loss 3.347891 | norm 0.3039 | time 465.5290 ms | tok/sec 1126219.9335
for step 10398 | loss 3.349323 | norm 0.3608 | time 464.8857 ms | tok/sec 1127778.2621
for step 10399 | loss 3.335259 | norm 0.3113 | time 465.4324 ms | tok/sec 1126453.5813
for step 10400 | loss 3.398014 | norm 0.3668 | time 465.5282 ms | tok/sec 1126221.6639
for step 10401 | loss 3.373770 | norm 0.3294 | time 464.7746 ms | tok/sec 1128047.8543
for step 10402 | loss 3.339962 | norm 0.3380 | time 464.1218 ms | tok/sec 1129634.4603
for step 10403 | loss 3.225595 | norm 0.3593 | time 464.3364 ms | tok/sec 1129112.4395
for step 10404 | loss 3.273141 | norm 0.3534 | time 465.2407 ms | tok/sec 1126917.7029
for step 10405 | loss 3.246038 | norm 0.3333 | time 465.1377 ms | tok/sec 1127167.2396
for step 10406 | loss 3.299081 | norm 0.3331 | time 465.1880 ms | tok/sec 1127045.3456
for step 10407 | loss 3.272120 | norm 0.3293 | time 463.9707 ms | tok/sec 1130002.4848
for step 10408 | loss 3.289521 | norm 0.3521 | time 464.7675 ms | tok/sec 1128065.2145
for step 10409 | loss 3.304329 | norm 0.3300 | time 465.0536 ms | tok/sec 1127371.2255
for step 10410 | loss 3.271560 | norm 0.3517 | time 464.8082 ms | tok/sec 1127966.2689
for step 10411 | loss 3.287330 | norm 0.3284 | time 464.8395 ms | tok/sec 1127890.4801
for step 10412 | loss 3.324241 | norm 0.3034 | time 465.0488 ms | tok/sec 1127382.7850
for step 10413 | loss 3.341082 | norm 0.3502 | time 464.4625 ms | tok/sec 1128805.8323
for step 10414 | loss 3.313286 | norm 0.3468 | time 464.6511 ms | tok/sec 1128347.6811
for step 10415 | loss 3.357444 | norm 0.3890 | time 465.1594 ms | tok/sec 1127114.6659
for step 10416 | loss 3.410182 | norm 0.3487 | time 464.9198 ms | tok/sec 1127695.5590
for step 10417 | loss 3.314543 | norm 0.3599 | time 465.6386 ms | tok/sec 1125954.6733
for step 10418 | loss 3.302188 | norm 0.3377 | time 465.5614 ms | tok/sec 1126141.4957
for step 10419 | loss 3.313179 | norm 0.3414 | time 464.4253 ms | tok/sec 1128896.2323
for step 10420 | loss 3.304316 | norm 0.3273 | time 465.2669 ms | tok/sec 1126854.1811
for step 10421 | loss 3.343417 | norm 0.3505 | time 465.5223 ms | tok/sec 1126236.0838
for step 10422 | loss 3.348320 | norm 0.3420 | time 464.6780 ms | tok/sec 1128282.2613
for step 10423 | loss 3.311972 | norm 0.3625 | time 465.7605 ms | tok/sec 1125660.1502
for step 10424 | loss 3.343069 | norm 0.3403 | time 465.2021 ms | tok/sec 1127011.2662
for step 10425 | loss 3.378377 | norm 0.3380 | time 464.8998 ms | tok/sec 1127744.1384
for step 10426 | loss 3.392121 | norm 0.3377 | time 465.2376 ms | tok/sec 1126925.2104
for step 10427 | loss 3.348309 | norm 0.3429 | time 464.7162 ms | tok/sec 1128189.6444
for step 10428 | loss 3.330939 | norm 0.3190 | time 465.1287 ms | tok/sec 1127189.1949
for step 10429 | loss 3.345348 | norm 0.3305 | time 465.6377 ms | tok/sec 1125956.9793
for step 10430 | loss 3.333227 | norm 0.3240 | time 465.5771 ms | tok/sec 1126103.4343
for step 10431 | loss 3.319194 | norm 0.3649 | time 464.5541 ms | tok/sec 1128583.3711
for step 10432 | loss 3.368891 | norm 0.3361 | time 464.6385 ms | tok/sec 1128378.3674
for step 10433 | loss 3.359986 | norm 0.3540 | time 465.1394 ms | tok/sec 1127163.1953
for step 10434 | loss 3.399837 | norm 0.3412 | time 464.6518 ms | tok/sec 1128345.9442
for step 10435 | loss 3.397186 | norm 0.3689 | time 465.2252 ms | tok/sec 1126955.2418
for step 10436 | loss 3.295507 | norm 0.3048 | time 464.6189 ms | tok/sec 1128425.8475
for step 10437 | loss 3.259712 | norm 0.3118 | time 465.5828 ms | tok/sec 1126089.5944
for step 10438 | loss 3.310529 | norm 0.3161 | time 465.1637 ms | tok/sec 1127104.2673
for step 10439 | loss 3.318629 | norm 0.3253 | time 465.3945 ms | tok/sec 1126545.3362
for step 10440 | loss 3.363688 | norm 0.3114 | time 464.7918 ms | tok/sec 1128006.1922
for step 10441 | loss 3.328945 | norm 0.3338 | time 464.9556 ms | tok/sec 1127608.8206
for step 10442 | loss 3.318113 | norm 0.3176 | time 465.8697 ms | tok/sec 1125396.3055
for step 10443 | loss 3.295439 | norm 0.3234 | time 465.1895 ms | tok/sec 1127041.8798
for step 10444 | loss 3.293716 | norm 0.3022 | time 465.7602 ms | tok/sec 1125660.7265
for step 10445 | loss 3.272467 | norm 0.3187 | time 466.0091 ms | tok/sec 1125059.4784
for step 10446 | loss 3.275452 | norm 0.3053 | time 465.7836 ms | tok/sec 1125604.2601
for step 10447 | loss 3.326095 | norm 0.3192 | time 464.1461 ms | tok/sec 1129575.2737
for step 10448 | loss 3.322863 | norm 0.3391 | time 464.8044 ms | tok/sec 1127975.5262
for step 10449 | loss 3.350799 | norm 0.3163 | time 465.6384 ms | tok/sec 1125955.2498
for step 10450 | loss 3.304172 | norm 0.3348 | time 465.3866 ms | tok/sec 1126564.3816
for step 10451 | loss 3.297910 | norm 0.3184 | time 466.5987 ms | tok/sec 1123637.8172
for step 10452 | loss 3.358887 | norm 0.3267 | time 464.8032 ms | tok/sec 1127978.4191
for step 10453 | loss 3.404334 | norm 0.3376 | time 466.3861 ms | tok/sec 1124150.1896
for step 10454 | loss 3.316939 | norm 0.2936 | time 466.3057 ms | tok/sec 1124343.8869
for step 10455 | loss 3.326216 | norm 0.3231 | time 465.7190 ms | tok/sec 1125760.4206
for step 10456 | loss 3.302961 | norm 0.2789 | time 465.7116 ms | tok/sec 1125778.2868
for step 10457 | loss 3.300871 | norm 0.3342 | time 465.7092 ms | tok/sec 1125784.0502
for step 10458 | loss 3.349794 | norm 0.2951 | time 465.3358 ms | tok/sec 1126687.3261
for step 10459 | loss 3.313699 | norm 0.3230 | time 465.8644 ms | tok/sec 1125408.9764
for step 10460 | loss 3.414316 | norm 0.3760 | time 465.8351 ms | tok/sec 1125479.8237
for step 10461 | loss 3.278039 | norm 0.3736 | time 465.9190 ms | tok/sec 1125277.0975
for step 10462 | loss 3.374568 | norm 0.3430 | time 465.6250 ms | tok/sec 1125987.5357
for step 10463 | loss 3.330555 | norm 0.3769 | time 465.5070 ms | tok/sec 1126273.0005
for step 10464 | loss 3.344424 | norm 0.3320 | time 465.9996 ms | tok/sec 1125082.5029
for step 10465 | loss 3.318575 | norm 0.3840 | time 465.9870 ms | tok/sec 1125113.0118
for step 10466 | loss 3.322116 | norm 0.3069 | time 465.2500 ms | tok/sec 1126895.1807
for step 10467 | loss 3.287018 | norm 0.3481 | time 465.7314 ms | tok/sec 1125730.4529
for step 10468 | loss 3.336324 | norm 0.3046 | time 464.3610 ms | tok/sec 1129052.7279
for step 10469 | loss 3.355937 | norm 0.3281 | time 464.9365 ms | tok/sec 1127655.0794
for step 10470 | loss 3.326762 | norm 0.2803 | time 464.2222 ms | tok/sec 1129390.2105
for step 10471 | loss 3.300969 | norm 0.3108 | time 464.8042 ms | tok/sec 1127976.1048
for step 10472 | loss 3.322714 | norm 0.2881 | time 464.2582 ms | tok/sec 1129302.6312
for step 10473 | loss 3.255699 | norm 0.3012 | time 464.0961 ms | tok/sec 1129697.1352
for step 10474 | loss 3.336611 | norm 0.3120 | time 464.7002 ms | tok/sec 1128228.4259
for step 10475 | loss 3.300206 | norm 0.2910 | time 465.5249 ms | tok/sec 1126229.7390
for step 10476 | loss 3.280994 | norm 0.3077 | time 464.1743 ms | tok/sec 1129506.8106
Will loading at 0 from edu_fineweb10B/edufineweb_train_000056.npy
for step 10477 | loss 3.326289 | norm 0.2986 | time 1419.8475 ms | tok/sec 369256.5605
for step 10478 | loss 3.343255 | norm 0.2966 | time 463.9623 ms | tok/sec 1130022.8086
for step 10479 | loss 3.273838 | norm 0.3256 | time 464.5145 ms | tok/sec 1128679.5285
for step 10480 | loss 3.258156 | norm 0.3274 | time 464.0319 ms | tok/sec 1129853.2725
for step 10481 | loss 3.247041 | norm 0.3336 | time 464.4818 ms | tok/sec 1128758.8996
for step 10482 | loss 3.271488 | norm 0.2977 | time 465.2874 ms | tok/sec 1126804.5235
for step 10483 | loss 3.259216 | norm 0.3304 | time 465.5368 ms | tok/sec 1126200.8998
for step 10484 | loss 3.288356 | norm 0.3561 | time 463.9492 ms | tok/sec 1130054.7475
for step 10485 | loss 3.273659 | norm 0.3160 | time 464.6680 ms | tok/sec 1128306.5757
for step 10486 | loss 3.260457 | norm 0.3627 | time 465.6911 ms | tok/sec 1125827.8539
for step 10487 | loss 3.291348 | norm 0.3141 | time 465.7967 ms | tok/sec 1125572.5723
for step 10488 | loss 3.346998 | norm 0.3409 | time 465.4343 ms | tok/sec 1126448.9651
for step 10489 | loss 3.238896 | norm 0.3167 | time 466.1496 ms | tok/sec 1124720.5519
for step 10490 | loss 3.286363 | norm 0.3315 | time 466.0664 ms | tok/sec 1124921.3513
for step 10491 | loss 3.326931 | norm 0.3411 | time 465.5592 ms | tok/sec 1126146.6861
for step 10492 | loss 3.334839 | norm 0.3121 | time 465.3208 ms | tok/sec 1126723.6951
for step 10493 | loss 3.268638 | norm 0.3435 | time 465.3964 ms | tok/sec 1126540.7193
for step 10494 | loss 3.430241 | norm 0.3395 | time 465.0350 ms | tok/sec 1127416.3088
for step 10495 | loss 3.375474 | norm 0.3167 | time 465.1992 ms | tok/sec 1127018.1974
for step 10496 | loss 3.349905 | norm 0.3142 | time 465.0640 ms | tok/sec 1127345.7955
for step 10497 | loss 3.377104 | norm 0.3181 | time 466.1233 ms | tok/sec 1124783.8333
for step 10498 | loss 3.353776 | norm 0.3254 | time 464.9508 ms | tok/sec 1127620.3849
for step 10499 | loss 3.348298 | norm 0.3039 | time 465.2119 ms | tok/sec 1126987.5851
validation loss 3.3234
HellaSwag accuracy: 2763/10042=0.2751
> Hello, I'm a language model, and the questions are pretty easy to test out at. My guess is that some of these questions were developed for my own
> Hello, I'm a language model, a model of reality. I am interested in the role I play in the human mind--including knowledge and understanding. I
> Hello, I'm a language model, but he doesn't understand the language. I'm not interested in how it's used in my language and how its structures
> Hello, I'm a language model, and I'm sure I did learn about two languages by interacting with two objects together. :)
So, I was wondering
> Hello, I'm a language model, so I need to look at the language I use in my daily life...
2.) We say it as if we
> Hello, I'm a language model, in other words, how about programming the model? In the same way that my problem is being able to understand the basics> > 
> Hello, I'm a language model, thanks.
1. I would like to introduce an I/O (in which is the same as I, as
Hello, I'm a language model, and I love to be here. I start working with my teacher and a guy goes in and speaks in the language model
> Hello, I'm a language model, and I know that you could just give another child a set of ideas at that time, and we'll go over some
> > Hello, I'm a language model, and I am a real-life model trying to model how to write software.
A language model can take a range
>>> Hello, I'm a language model, I learn very pretty easily through the IFL world. I can speak anywhere. They all belong to your class as a
Hello, I'm a language model, please: my name is William Cope. I work on my projects in the IJN to ensure my students and
 Hello, I'm a language model, but what about the code?
This program will start by typing any string (such as "1" in the input Hello, I'm a language model, but if you want to know the exact meaning in a language model, I want to share the article at https://enHello, I'm a language model, so why not?
As an Object-Based Engineer, you need to learn to write your code with this in mind
> > > 

> Hello, I'm a language model, and it got this to say: when i write down a comment to input text, i can find a text I'm
Hello, I'm a language model, and this will tell me where I'd be.
This post is intended for you to share with others
I would
Hello, I'm a language model, and I want to focus on languages not by choosing only the most useful ways to express the language.
The way we
>>> > Hello, I'm a language model, so I used this feature instead of 'I'. I used my computer to create a language model for my class, so
>  Hello, I'm a language model, and I'm a linguistically oriented team (CBT) leader, having never seen this language before
- a process Hello, I'm a language model, though, I'm more focused on more traditional dialects. I think language is not as complicated as you might think....Hello, I'm a language model, and as for that is very similar to what are used for. At this moment when we talk about learning models we are
Hello, I'm a language model, and my favorite model is the S-box, and I'm getting a 3-page review of models of 3-
> Hello, I'm a language model, but I wouldn't expect a lot of future work.
So it isn't a question of what type of language it


> Hello, I'm a language model, and can't help me understand how to do other languages. I would recommend this theory because most languages we use in modern
> > Hello, I'm a language model, and i know how not to teach any language to my students. My parents want my teaching to be easy, and if
Hello, I'm a language model, so I have a very simple concept - using the -O-M_F1 function to add another function to that
Hello, I'm a language model, and the second one is C2.
If you see a list of the words in the list, you would want
> > > Hello, I'm a language model, one that understands all the other languages.
All language classes are done in parallel
and are intended for a particular language
Hello, I'm a language model, but my intent is to help you learn English. For my final project, I am working and talking about this topic.
Hello, I'm a language model, and i've only created a set of templates out of my work. I am in the process of creating a framework of
> Hello, I'm a language model, and in my classes I see many things. It has an explanation that you can call a few lines using a variable,
for step 10500 | loss 3.386146 | norm 0.3193 | time 12887.4321 ms | tok/sec 40682.1154
for step 10501 | loss 3.392986 | norm 0.3550 | time 461.5436 ms | tok/sec 1135944.7849
for step 10502 | loss 3.370052 | norm 0.3210 | time 461.9210 ms | tok/sec 1135016.6511
for step 10503 | loss 3.412153 | norm 0.3174 | time 462.6386 ms | tok/sec 1133256.0258
for step 10504 | loss 3.351601 | norm 0.3197 | time 463.2535 ms | tok/sec 1131751.8425
for step 10505 | loss 3.350863 | norm 0.3503 | time 465.1415 ms | tok/sec 1127157.9955
for step 10506 | loss 3.253504 | norm 0.3129 | time 465.1601 ms | tok/sec 1127112.9328
for step 10507 | loss 3.301544 | norm 0.3160 | time 463.5167 ms | tok/sec 1131109.1622
for step 10508 | loss 3.293759 | norm 0.2929 | time 463.9795 ms | tok/sec 1129981.0005
for step 10509 | loss 3.273728 | norm 0.2987 | time 466.1424 ms | tok/sec 1124737.8097
for step 10510 | loss 3.253329 | norm 0.2963 | time 463.7270 ms | tok/sec 1130596.2408
for step 10511 | loss 3.258736 | norm 0.2946 | time 464.4847 ms | tok/sec 1128751.9470
for step 10512 | loss 3.264762 | norm 0.3064 | time 464.2274 ms | tok/sec 1129377.4497
for step 10513 | loss 3.211063 | norm 0.2883 | time 464.6227 ms | tok/sec 1128416.5828
for step 10514 | loss 3.278701 | norm 0.3088 | time 468.8873 ms | tok/sec 1118153.4826
for step 10515 | loss 3.281543 | norm 0.2842 | time 464.3066 ms | tok/sec 1129184.9135
for step 10516 | loss 3.301709 | norm 0.2892 | time 463.6946 ms | tok/sec 1130675.3004
for step 10517 | loss 3.274541 | norm 0.2995 | time 464.7446 ms | tok/sec 1128120.7705
for step 10518 | loss 3.328172 | norm 0.3078 | time 464.5705 ms | tok/sec 1128543.4069
for step 10519 | loss 3.225097 | norm 0.3229 | time 464.8705 ms | tok/sec 1127815.2800
for step 10520 | loss 3.273688 | norm 0.2906 | time 465.0564 ms | tok/sec 1127364.2899
for step 10521 | loss 3.336778 | norm 0.3343 | time 464.3877 ms | tok/sec 1128987.8059
for step 10522 | loss 3.313343 | norm 0.3339 | time 464.3593 ms | tok/sec 1129056.7857
for step 10523 | loss 3.324345 | norm 0.3177 | time 464.7229 ms | tok/sec 1128173.4381
for step 10524 | loss 3.406940 | norm 0.3238 | time 464.9878 ms | tok/sec 1127530.7673
for step 10525 | loss 3.298519 | norm 0.3661 | time 465.1449 ms | tok/sec 1127149.9071
for step 10526 | loss 3.420119 | norm 0.4003 | time 464.9532 ms | tok/sec 1127614.6027
for step 10527 | loss 3.341787 | norm 0.3623 | time 465.5252 ms | tok/sec 1126229.1622
for step 10528 | loss 3.425779 | norm 0.3866 | time 465.1718 ms | tok/sec 1127084.6261
for step 10529 | loss 3.341833 | norm 0.3824 | time 464.1557 ms | tok/sec 1129552.0649
for step 10530 | loss 3.364022 | norm 0.3604 | time 465.7388 ms | tok/sec 1125712.5883
for step 10531 | loss 3.397078 | norm 0.3965 | time 465.5080 ms | tok/sec 1126270.6932
for step 10532 | loss 3.368662 | norm 0.3800 | time 465.9524 ms | tok/sec 1125196.4880
for step 10533 | loss 3.325153 | norm 0.3898 | time 464.9401 ms | tok/sec 1127646.4056
for step 10534 | loss 3.396717 | norm 0.4172 | time 465.4050 ms | tok/sec 1126519.9434
for step 10535 | loss 3.357841 | norm 0.3404 | time 465.3332 ms | tok/sec 1126693.6761
for step 10536 | loss 3.380181 | norm 0.3643 | time 465.4229 ms | tok/sec 1126476.6629
for step 10537 | loss 3.377053 | norm 0.3191 | time 465.9805 ms | tok/sec 1125128.5547
for step 10538 | loss 3.367074 | norm 0.3287 | time 464.3900 ms | tok/sec 1128982.0097
for step 10539 | loss 3.321850 | norm 0.3206 | time 465.5240 ms | tok/sec 1126232.0462
for step 10540 | loss 3.359411 | norm 0.3332 | time 465.0192 ms | tok/sec 1127454.4591
for step 10541 | loss 3.308587 | norm 0.3242 | time 465.1518 ms | tok/sec 1127133.1528
for step 10542 | loss 3.299382 | norm 0.3443 | time 464.6077 ms | tok/sec 1128453.0635
for step 10543 | loss 3.282048 | norm 0.3321 | time 465.4815 ms | tok/sec 1126334.7261
for step 10544 | loss 3.328619 | norm 0.3091 | time 465.0004 ms | tok/sec 1127500.1272
for step 10545 | loss 3.304519 | norm 0.3519 | time 465.7266 ms | tok/sec 1125741.9788
for step 10546 | loss 3.256904 | norm 0.2885 | time 468.3387 ms | tok/sec 1119463.2618
for step 10547 | loss 3.308265 | norm 0.3254 | time 465.2157 ms | tok/sec 1126978.3440
for step 10548 | loss 3.252166 | norm 0.3057 | time 464.4639 ms | tok/sec 1128802.3557
for step 10549 | loss 3.320074 | norm 0.3173 | time 465.3342 ms | tok/sec 1126691.3670
for step 10550 | loss 3.288891 | norm 0.3235 | time 464.9258 ms | tok/sec 1127681.1017
for step 10551 | loss 3.365243 | norm 0.2960 | time 465.3151 ms | tok/sec 1126737.5506
for step 10552 | loss 3.302951 | norm 0.3265 | time 465.0936 ms | tok/sec 1127274.1352
for step 10553 | loss 3.326242 | norm 0.3162 | time 464.9103 ms | tok/sec 1127718.6915
for step 10554 | loss 3.306277 | norm 0.3099 | time 464.4477 ms | tok/sec 1128841.7588
for step 10555 | loss 3.299510 | norm 0.3121 | time 465.2081 ms | tok/sec 1126996.8264
for step 10556 | loss 3.312727 | norm 0.3178 | time 464.6890 ms | tok/sec 1128255.6324
for step 10557 | loss 3.274529 | norm 0.3104 | time 464.4320 ms | tok/sec 1128880.0056
for step 10558 | loss 3.305635 | norm 0.2893 | time 464.7171 ms | tok/sec 1128187.3292
for step 10559 | loss 3.331784 | norm 0.3117 | time 465.2288 ms | tok/sec 1126946.5788
for step 10560 | loss 3.317692 | norm 0.2901 | time 465.3296 ms | tok/sec 1126702.3352
for step 10561 | loss 3.322625 | norm 0.2890 | time 465.0016 ms | tok/sec 1127497.2367
for step 10562 | loss 3.289147 | norm 0.3646 | time 464.6347 ms | tok/sec 1128387.6315
for step 10563 | loss 3.234255 | norm 0.2942 | time 464.8046 ms | tok/sec 1127974.9476
for step 10564 | loss 3.374908 | norm 0.3278 | time 464.6764 ms | tok/sec 1128286.3136
for step 10565 | loss 3.394607 | norm 0.3307 | time 465.1425 ms | tok/sec 1127155.6845
for step 10566 | loss 3.325614 | norm 0.3444 | time 463.5422 ms | tok/sec 1131046.9123
for step 10567 | loss 3.385615 | norm 0.2984 | time 465.4951 ms | tok/sec 1126301.8434
for step 10568 | loss 3.370762 | norm 0.3109 | time 465.3161 ms | tok/sec 1126735.2413
for step 10569 | loss 3.335212 | norm 0.3048 | time 464.6766 ms | tok/sec 1128285.7347
for step 10570 | loss 3.388444 | norm 0.2857 | time 464.8321 ms | tok/sec 1127908.4140
for step 10571 | loss 3.304220 | norm 0.3076 | time 464.8015 ms | tok/sec 1127982.4693
for step 10572 | loss 3.347962 | norm 0.3027 | time 464.9272 ms | tok/sec 1127677.6320
for step 10573 | loss 3.326758 | norm 0.2740 | time 464.8263 ms | tok/sec 1127922.2986
for step 10574 | loss 3.336213 | norm 0.3094 | time 465.1129 ms | tok/sec 1127227.3297
for step 10575 | loss 3.385288 | norm 0.3173 | time 465.6601 ms | tok/sec 1125902.7892
for step 10576 | loss 3.355815 | norm 0.3591 | time 464.8020 ms | tok/sec 1127981.3121
for step 10577 | loss 3.299127 | norm 0.3370 | time 466.1694 ms | tok/sec 1124672.8078
for step 10578 | loss 3.339940 | norm 0.3358 | time 465.6918 ms | tok/sec 1125826.1247
for step 10579 | loss 3.283107 | norm 0.3108 | time 465.0238 ms | tok/sec 1127443.4762
for step 10580 | loss 3.259485 | norm 0.3573 | time 466.1937 ms | tok/sec 1124614.1400
for step 10581 | loss 3.287109 | norm 0.3131 | time 465.9197 ms | tok/sec 1125275.3700
for step 10582 | loss 3.364467 | norm 0.3725 | time 465.4031 ms | tok/sec 1126524.5602
for step 10583 | loss 3.329772 | norm 0.3248 | time 465.5249 ms | tok/sec 1126229.7390
for step 10584 | loss 3.262082 | norm 0.3835 | time 465.7445 ms | tok/sec 1125698.7580
for step 10585 | loss 3.298620 | norm 0.3230 | time 465.0617 ms | tok/sec 1127351.5749
for step 10586 | loss 3.298204 | norm 0.3500 | time 465.1043 ms | tok/sec 1127248.1317
for step 10587 | loss 3.303952 | norm 0.3084 | time 465.2853 ms | tok/sec 1126809.7201
for step 10588 | loss 3.283842 | norm 0.3086 | time 464.1817 ms | tok/sec 1129488.8260
for step 10589 | loss 3.291713 | norm 0.3167 | time 465.1763 ms | tok/sec 1127073.6504
for step 10590 | loss 3.330143 | norm 0.3229 | time 465.0493 ms | tok/sec 1127381.6290
for step 10591 | loss 3.261642 | norm 0.3076 | time 464.7396 ms | tok/sec 1128132.9241
for step 10592 | loss 3.335932 | norm 0.3148 | time 465.2669 ms | tok/sec 1126854.1811
for step 10593 | loss 3.278901 | norm 0.2992 | time 465.7931 ms | tok/sec 1125581.2143
for step 10594 | loss 3.342909 | norm 0.3380 | time 465.8458 ms | tok/sec 1125453.9030
for step 10595 | loss 3.303854 | norm 0.2830 | time 464.9498 ms | tok/sec 1127622.6978
for step 10596 | loss 3.252153 | norm 0.3156 | time 464.5119 ms | tok/sec 1128685.9009
for step 10597 | loss 3.299252 | norm 0.3325 | time 464.4499 ms | tok/sec 1128836.5435
for step 10598 | loss 3.305663 | norm 0.2821 | time 465.3676 ms | tok/sec 1126610.5548
for step 10599 | loss 3.323508 | norm 0.3689 | time 465.4016 ms | tok/sec 1126528.0228
for step 10600 | loss 3.332673 | norm 0.3126 | time 464.7555 ms | tok/sec 1128094.1492
for step 10601 | loss 3.326267 | norm 0.3266 | time 465.5492 ms | tok/sec 1126170.9086
for step 10602 | loss 3.367517 | norm 0.3161 | time 465.2410 ms | tok/sec 1126917.1253
for step 10603 | loss 3.385643 | norm 0.3258 | time 464.3512 ms | tok/sec 1129076.4958
for step 10604 | loss 3.352534 | norm 0.3101 | time 465.2812 ms | tok/sec 1126819.5358
for step 10605 | loss 3.411393 | norm 0.3415 | time 465.4510 ms | tok/sec 1126408.5749
for step 10606 | loss 3.319113 | norm 0.3175 | time 465.1337 ms | tok/sec 1127177.0616
for step 10607 | loss 3.382789 | norm 0.3024 | time 465.0033 ms | tok/sec 1127493.1900
for step 10608 | loss 3.407444 | norm 0.3247 | time 465.1060 ms | tok/sec 1127244.0868
for step 10609 | loss 3.300581 | norm 0.3002 | time 465.3883 ms | tok/sec 1126560.3416
for step 10610 | loss 3.388719 | norm 0.3340 | time 465.2746 ms | tok/sec 1126835.7033
for step 10611 | loss 3.335814 | norm 0.3076 | time 464.7675 ms | tok/sec 1128065.2145
for step 10612 | loss 3.356254 | norm 0.3094 | time 464.8747 ms | tok/sec 1127804.8685
for step 10613 | loss 3.280177 | norm 0.2974 | time 463.6617 ms | tok/sec 1130755.5339
for step 10614 | loss 3.312996 | norm 0.2951 | time 464.9041 ms | tok/sec 1127733.7282
for step 10615 | loss 3.305210 | norm 0.3411 | time 466.4340 ms | tok/sec 1124034.6927
for step 10616 | loss 3.292361 | norm 0.3005 | time 464.3390 ms | tok/sec 1129106.0622
for step 10617 | loss 3.306927 | norm 0.3095 | time 465.9858 ms | tok/sec 1125115.8901
for step 10618 | loss 3.247719 | norm 0.2993 | time 464.9436 ms | tok/sec 1127637.7319
for step 10619 | loss 3.314243 | norm 0.3127 | time 466.4471 ms | tok/sec 1124003.0932
for step 10620 | loss 3.298968 | norm 0.3283 | time 465.0817 ms | tok/sec 1127303.0294
for step 10621 | loss 3.255711 | norm 0.3172 | time 465.9610 ms | tok/sec 1125175.7617
for step 10622 | loss 3.264626 | norm 0.3060 | time 464.8590 ms | tok/sec 1127843.0451
for step 10623 | loss 3.328644 | norm 0.3427 | time 465.2493 ms | tok/sec 1126896.9131
for step 10624 | loss 3.303047 | norm 0.2974 | time 464.9239 ms | tok/sec 1127685.7280
for step 10625 | loss 3.292180 | norm 0.3408 | time 466.2023 ms | tok/sec 1124593.4352
for step 10626 | loss 3.309136 | norm 0.3017 | time 465.3466 ms | tok/sec 1126661.3496
for step 10627 | loss 3.380996 | norm 0.3355 | time 465.0025 ms | tok/sec 1127494.9243
for step 10628 | loss 3.295513 | norm 0.2966 | time 465.6887 ms | tok/sec 1125833.6178
for step 10629 | loss 3.258873 | norm 0.2931 | time 465.6799 ms | tok/sec 1125854.9447
for step 10630 | loss 3.372149 | norm 0.3133 | time 465.2231 ms | tok/sec 1126960.4397
for step 10631 | loss 3.328347 | norm 0.3049 | time 464.6156 ms | tok/sec 1128433.9542
for step 10632 | loss 3.352419 | norm 0.2971 | time 464.8669 ms | tok/sec 1127823.9565
for step 10633 | loss 3.280103 | norm 0.3095 | time 465.6131 ms | tok/sec 1126016.3640
for step 10634 | loss 3.342229 | norm 0.3595 | time 465.4028 ms | tok/sec 1126525.1373
for step 10635 | loss 3.406802 | norm 0.3379 | time 465.3902 ms | tok/sec 1126555.7245
for step 10636 | loss 3.408801 | norm 0.3652 | time 464.8020 ms | tok/sec 1127981.3121
for step 10637 | loss 3.387877 | norm 0.3608 | time 465.6646 ms | tok/sec 1125891.8364
for step 10638 | loss 3.353562 | norm 0.3680 | time 465.6148 ms | tok/sec 1126012.3279
for step 10639 | loss 3.400698 | norm 0.3643 | time 465.1327 ms | tok/sec 1127179.3727
for step 10640 | loss 3.382447 | norm 0.3276 | time 465.4546 ms | tok/sec 1126399.9203
for step 10641 | loss 3.393388 | norm 0.3522 | time 465.6169 ms | tok/sec 1126007.1388
for step 10642 | loss 3.299757 | norm 0.3044 | time 466.1999 ms | tok/sec 1124599.1864
for step 10643 | loss 3.317755 | norm 0.3770 | time 464.9851 ms | tok/sec 1127537.1268
for step 10644 | loss 3.346403 | norm 0.3612 | time 465.5495 ms | tok/sec 1126170.3319
for step 10645 | loss 3.321479 | norm 0.3146 | time 465.6150 ms | tok/sec 1126011.7513
for step 10646 | loss 3.314234 | norm 0.3461 | time 466.0525 ms | tok/sec 1124954.7289
for step 10647 | loss 3.305076 | norm 0.2932 | time 465.0490 ms | tok/sec 1127382.2070
for step 10648 | loss 3.290971 | norm 0.3251 | time 465.2858 ms | tok/sec 1126808.5653
for step 10649 | loss 3.292748 | norm 0.3178 | time 465.6136 ms | tok/sec 1126015.2108
for step 10650 | loss 3.273539 | norm 0.3006 | time 465.4374 ms | tok/sec 1126441.4639
for step 10651 | loss 3.262600 | norm 0.3051 | time 465.2989 ms | tok/sec 1126776.8096
for step 10652 | loss 3.240355 | norm 0.3064 | time 465.5902 ms | tok/sec 1126071.7184
for step 10653 | loss 3.278491 | norm 0.3099 | time 465.4000 ms | tok/sec 1126532.0626
for step 10654 | loss 3.276550 | norm 0.2862 | time 465.4338 ms | tok/sec 1126450.1192
for step 10655 | loss 3.324662 | norm 0.3146 | time 465.0869 ms | tok/sec 1127290.3157
for step 10656 | loss 3.283516 | norm 0.2771 | time 464.9336 ms | tok/sec 1127662.0186
for step 10657 | loss 3.354404 | norm 0.3013 | time 465.0300 ms | tok/sec 1127428.4473
for step 10658 | loss 3.275344 | norm 0.2896 | time 465.5654 ms | tok/sec 1126131.6918
for step 10659 | loss 3.368486 | norm 0.3709 | time 465.0984 ms | tok/sec 1127262.5779
for step 10660 | loss 3.290455 | norm 0.3044 | time 465.6529 ms | tok/sec 1125920.0833
for step 10661 | loss 3.309201 | norm 0.3251 | time 465.5876 ms | tok/sec 1126078.0614
for step 10662 | loss 3.350716 | norm 0.3817 | time 465.2469 ms | tok/sec 1126902.6880
for step 10663 | loss 3.408757 | norm 0.4378 | time 466.1186 ms | tok/sec 1124795.3398
for step 10664 | loss 3.313454 | norm 0.3881 | time 465.3945 ms | tok/sec 1126545.3362
for step 10665 | loss 3.325787 | norm 0.3540 | time 466.0895 ms | tok/sec 1124865.5345
for step 10666 | loss 3.381794 | norm 0.3987 | time 465.5199 ms | tok/sec 1126241.8519
Will loading at 0 from edu_fineweb10B/edufineweb_train_000057.npy
for step 10667 | loss 3.322581 | norm 0.3408 | time 1414.4523 ms | tok/sec 370665.0233
for step 10668 | loss 3.300545 | norm 0.3629 | time 463.4702 ms | tok/sec 1131222.6260
for step 10669 | loss 3.325140 | norm 0.3233 | time 463.9847 ms | tok/sec 1129968.2264
for step 10670 | loss 3.372788 | norm 0.3545 | time 465.5848 ms | tok/sec 1126084.9812
for step 10671 | loss 3.320233 | norm 0.3514 | time 464.0737 ms | tok/sec 1129751.6913
for step 10672 | loss 3.366381 | norm 0.3545 | time 465.1687 ms | tok/sec 1127092.1359
for step 10673 | loss 3.354531 | norm 0.3290 | time 465.1861 ms | tok/sec 1127049.9667
for step 10674 | loss 3.379147 | norm 0.3207 | time 464.4523 ms | tok/sec 1128830.7488
for step 10675 | loss 3.306674 | norm 0.3421 | time 464.7889 ms | tok/sec 1128013.1357
for step 10676 | loss 3.372260 | norm 0.2973 | time 466.2173 ms | tok/sec 1124557.2035
for step 10677 | loss 3.353192 | norm 0.3630 | time 465.3864 ms | tok/sec 1126564.9587
for step 10678 | loss 3.377663 | norm 0.3068 | time 465.7931 ms | tok/sec 1125581.2143
for step 10679 | loss 3.312433 | norm 0.3028 | time 465.4593 ms | tok/sec 1126388.3809
for step 10680 | loss 3.438620 | norm 0.3267 | time 464.8278 ms | tok/sec 1127918.8274
for step 10681 | loss 3.289798 | norm 0.3050 | time 464.9415 ms | tok/sec 1127642.9361
for step 10682 | loss 3.274397 | norm 0.3265 | time 465.4095 ms | tok/sec 1126508.9787
for step 10683 | loss 3.300768 | norm 0.3133 | time 464.2732 ms | tok/sec 1129266.0955
for step 10684 | loss 3.252020 | norm 0.3120 | time 465.9073 ms | tok/sec 1125305.3135
for step 10685 | loss 3.241345 | norm 0.2899 | time 466.4040 ms | tok/sec 1124107.0909
for step 10686 | loss 3.327903 | norm 0.3187 | time 465.4272 ms | tok/sec 1126466.2761
for step 10687 | loss 3.306752 | norm 0.2901 | time 464.6845 ms | tok/sec 1128266.6311
for step 10688 | loss 3.325524 | norm 0.3506 | time 465.7741 ms | tok/sec 1125627.3069
for step 10689 | loss 3.312940 | norm 0.3677 | time 465.1265 ms | tok/sec 1127194.3949
for step 10690 | loss 3.265332 | norm 0.3260 | time 465.2719 ms | tok/sec 1126842.0550
for step 10691 | loss 3.272880 | norm 0.3225 | time 465.1296 ms | tok/sec 1127186.8837
for step 10692 | loss 3.281645 | norm 0.3195 | time 464.3910 ms | tok/sec 1128979.6912
for step 10693 | loss 3.286535 | norm 0.3516 | time 465.6565 ms | tok/sec 1125911.4362
for step 10694 | loss 3.362599 | norm 0.3094 | time 465.3776 ms | tok/sec 1126586.3134
for step 10695 | loss 3.341073 | norm 0.3610 | time 465.7242 ms | tok/sec 1125747.7418
for step 10696 | loss 3.302963 | norm 0.3231 | time 464.9262 ms | tok/sec 1127679.9451
for step 10697 | loss 3.405328 | norm 0.4148 | time 465.0755 ms | tok/sec 1127318.0549
for step 10698 | loss 3.373461 | norm 0.3844 | time 464.5858 ms | tok/sec 1128506.3412
for step 10699 | loss 3.294436 | norm 0.3526 | time 465.6951 ms | tok/sec 1125818.0554
for step 10700 | loss 3.401918 | norm 0.4538 | time 465.4441 ms | tok/sec 1126425.3077
for step 10701 | loss 3.341722 | norm 0.4077 | time 465.7669 ms | tok/sec 1125644.5926
for step 10702 | loss 3.327101 | norm 0.3644 | time 464.8294 ms | tok/sec 1127914.7777
for step 10703 | loss 3.357501 | norm 0.3617 | time 464.7260 ms | tok/sec 1128165.9138
for step 10704 | loss 3.390535 | norm 0.3761 | time 464.9408 ms | tok/sec 1127644.6708
for step 10705 | loss 3.321917 | norm 0.3729 | time 464.9498 ms | tok/sec 1127622.6978
for step 10706 | loss 3.345420 | norm 0.3612 | time 465.1918 ms | tok/sec 1127036.1035
for step 10707 | loss 3.340703 | norm 0.3442 | time 465.2021 ms | tok/sec 1127011.2662
for step 10708 | loss 3.355683 | norm 0.3619 | time 464.2715 ms | tok/sec 1129270.1549
for step 10709 | loss 3.370899 | norm 0.3384 | time 465.7071 ms | tok/sec 1125789.2373
for step 10710 | loss 3.348642 | norm 0.3445 | time 466.0075 ms | tok/sec 1125063.5076
for step 10711 | loss 3.386227 | norm 0.3072 | time 465.3091 ms | tok/sec 1126751.9837
for step 10712 | loss 3.320143 | norm 0.3180 | time 465.3120 ms | tok/sec 1126745.0558
for step 10713 | loss 3.347986 | norm 0.2901 | time 464.6940 ms | tok/sec 1128243.4761
for step 10714 | loss 3.331130 | norm 0.3157 | time 465.3282 ms | tok/sec 1126705.7989
for step 10715 | loss 3.362030 | norm 0.2922 | time 465.5797 ms | tok/sec 1126097.0909
for step 10716 | loss 3.328526 | norm 0.3130 | time 464.7379 ms | tok/sec 1128136.9754
for step 10717 | loss 3.302383 | norm 0.3029 | time 465.2829 ms | tok/sec 1126815.4940
for step 10718 | loss 3.303593 | norm 0.3156 | time 465.7669 ms | tok/sec 1125644.5926
for step 10719 | loss 3.256980 | norm 0.3460 | time 465.7483 ms | tok/sec 1125689.5380
for step 10720 | loss 3.290299 | norm 0.3085 | time 464.9525 ms | tok/sec 1127616.3374
for step 10721 | loss 3.245741 | norm 0.3327 | time 466.4390 ms | tok/sec 1124022.6272
for step 10722 | loss 3.294272 | norm 0.3249 | time 464.9239 ms | tok/sec 1127685.7280
for step 10723 | loss 3.304430 | norm 0.3059 | time 464.7453 ms | tok/sec 1128119.0343
for step 10724 | loss 3.276829 | norm 0.2963 | time 465.5964 ms | tok/sec 1126056.7260
for step 10725 | loss 3.270520 | norm 0.3041 | time 464.8678 ms | tok/sec 1127821.6427
for step 10726 | loss 3.260068 | norm 0.2829 | time 465.0059 ms | tok/sec 1127486.8310
for step 10727 | loss 3.316898 | norm 0.3005 | time 465.2348 ms | tok/sec 1126932.1406
for step 10728 | loss 3.268984 | norm 0.3154 | time 465.2297 ms | tok/sec 1126944.2686
for step 10729 | loss 3.363436 | norm 0.3321 | time 465.0614 ms | tok/sec 1127352.1529
for step 10730 | loss 3.311295 | norm 0.3035 | time 464.8430 ms | tok/sec 1127881.8027
for step 10731 | loss 3.301865 | norm 0.3244 | time 465.3766 ms | tok/sec 1126588.6220
for step 10732 | loss 3.326683 | norm 0.3130 | time 465.6477 ms | tok/sec 1125932.7661
for step 10733 | loss 3.319449 | norm 0.3015 | time 465.6329 ms | tok/sec 1125968.5099
for step 10734 | loss 3.342617 | norm 0.3509 | time 465.5428 ms | tok/sec 1126186.4808
for step 10735 | loss 3.278284 | norm 0.2974 | time 466.0943 ms | tok/sec 1124854.0266
for step 10736 | loss 3.313413 | norm 0.3485 | time 464.4527 ms | tok/sec 1128829.5899
for step 10737 | loss 3.371288 | norm 0.3228 | time 464.8235 ms | tok/sec 1127929.2410
for step 10738 | loss 3.325104 | norm 0.3606 | time 470.4964 ms | tok/sec 1114329.4233
for step 10739 | loss 3.280931 | norm 0.2903 | time 464.6068 ms | tok/sec 1128455.3798
for step 10740 | loss 3.325737 | norm 0.3837 | time 466.1851 ms | tok/sec 1124634.8456
for step 10741 | loss 3.342686 | norm 0.3236 | time 465.7273 ms | tok/sec 1125740.2499
for step 10742 | loss 3.282590 | norm 0.3864 | time 464.0131 ms | tok/sec 1129899.1351
for step 10743 | loss 3.331712 | norm 0.3222 | time 464.8840 ms | tok/sec 1127782.3109
for step 10744 | loss 3.357671 | norm 0.3333 | time 464.4680 ms | tok/sec 1128792.5054
for step 10745 | loss 3.293800 | norm 0.3076 | time 465.2472 ms | tok/sec 1126902.1105
for step 10746 | loss 3.306102 | norm 0.3291 | time 464.8347 ms | tok/sec 1127902.0503
for step 10747 | loss 3.349361 | norm 0.3069 | time 465.6620 ms | tok/sec 1125898.1775
for step 10748 | loss 3.323514 | norm 0.3185 | time 465.7271 ms | tok/sec 1125740.8262
for step 10749 | loss 3.305217 | norm 0.3246 | time 465.7745 ms | tok/sec 1125626.1546
validation loss 3.3175
HellaSwag accuracy: 2762/10042=0.2750
> Hello, I'm a language model, and i am a member of the human body in my home village.
As a child, the language has been a
> Hello, I'm a language model, a model for an entire language (in addition to the native languages!)
If I're being paid for a new language
> Hello, I'm a language model, but these models are not just for me.
It's interesting, and I'm not sure it's the case that
> Hello, I'm a language model, and I'm really curious of the kind of language models around you. In each case, I want to say I'm
> Hello, I'm a language model, and I know that you don't get started or get involved with, for a long time, the idea was to model> > 
Hello, I'm a language model, so I don't know how to use and manipulate HTML. So I use that to work like this:
I can
Hello, I'm a language model, and I'm a student, so I decided to go into the language paradigm to show me how difficult this language model can
> > Hello, I'm a language model, so there's a lot of overlap, but it probably was the easiest one for me, as it helps the best of
> Hello, I'm a language model, has an idea to explain to children's. If a grammar is good, the students understand it and learn to analyze the
>>> Hello, I'm a language model, using several sources, including text in Portuguese. I'm pretty sure you can use many of those sources (though I think
 Hello, I'm a language model, learning, and it works out when you are learning the English grammar, but if you're writing a simple English sentence there Hello, I'm a language model, I'm a member of the English community of K-12 education. I'm doing so many of my classes that areHello, I'm a language model, so I didn't notice when I clicked on each list. This makes the list search simple, and it's a great
>>> 

 Hello, I'm a language model, but what is the point of this lesson? I'm a language model. I've been learning and developing new and more Hello, I'm a language model, and now I'm a language modeling editor, etc.
At the start, my language model was to be made useHello, I'm a language model, and I am here to help you
understand basic grammar and sentence structure. I am a regular speaker and will help
>>> > 

 Hello, I'm a language model, and this post discusses the way to get the most out of a language. I like to use a lot (and some Hello, I'm a language model, working with real world and the same person. Now, I am an actor, so I'm gonna need to translate anHello, I'm a language model, and I like to work on things like learning a few languages. I don't want to work on things because I'm
Hello, I'm a language model, and i’m going to find that i always refer to a text. I'm not so good at creating my
> > 

> Hello, I'm a language model, and I'm not trying to guess your definition. So I didn't hear 'how about'.
A few words like
Hello, I'm a language model, trying to solve problems where one is asked to take a question or perform an experiment where you are asked to analyze the data
> Hello, I'm a language model, but there is no specific model of your language (or any language!). So how do you know what particular grammatical features
> > Hello, I'm a language model, an advanced method, and I'm currently talking about object-orientation with a simple example: "Object" is not
> Hello, I'm a language model, and the best way to put all my ideas down to these two sentences is to use the grammar, usage, semantics...
Hello, I'm a language model, so you should know its syntax, usage, usage, and semantics. I'd be talking about the "language" language
> Hello, I'm a language model, and am looking forward to learning this language again.
I'm starting to get more interested after a term of...

> > Hello, I'm a language model, so I can use a lot of models out there, but I have done the work, I haven't seen that I
Hello, I'm a language model, and you have to worry about where you and your teammates are, and I know people who want to be there on a
Hello, I'm a language model, since I have taught many languages. I have taught it since I started teaching. When we started making presentations, I did
> > Hello, I'm a language model, but that question is a bit of a new one with the aim of bringing into a conversation more details. But, that
Hello, I'm a language model, and would like to offer some fun, but not so easy, tutorials and tutorials...
3. Free Dictionary of English
for step 10750 | loss 3.272405 | norm 0.3077 | time 12951.9103 ms | tok/sec 40479.5887
for step 10751 | loss 3.299222 | norm 0.3074 | time 462.0519 ms | tok/sec 1134695.1193
for step 10752 | loss 3.391222 | norm 0.3240 | time 462.6522 ms | tok/sec 1133222.7378
for step 10753 | loss 3.247783 | norm 0.3637 | time 463.2261 ms | tok/sec 1131818.8304
for step 10754 | loss 3.264004 | norm 0.3241 | time 462.2877 ms | tok/sec 1134116.3530
for step 10755 | loss 3.255544 | norm 0.3769 | time 463.3532 ms | tok/sec 1131508.4230
for step 10756 | loss 3.278701 | norm 0.3227 | time 463.6545 ms | tok/sec 1130772.9775
for step 10757 | loss 3.275530 | norm 0.3490 | time 464.2620 ms | tok/sec 1129293.3521
for step 10758 | loss 3.245572 | norm 0.3069 | time 464.5176 ms | tok/sec 1128671.9975
for step 10759 | loss 3.294232 | norm 0.3286 | time 464.1776 ms | tok/sec 1129498.6885
for step 10760 | loss 3.256142 | norm 0.2850 | time 464.1094 ms | tok/sec 1129664.6362
for step 10761 | loss 3.250644 | norm 0.3326 | time 464.0236 ms | tok/sec 1129873.5909
for step 10762 | loss 3.268342 | norm 0.3276 | time 465.1403 ms | tok/sec 1127160.8843
for step 10763 | loss 3.254932 | norm 0.3179 | time 463.3658 ms | tok/sec 1131477.5662
for step 10764 | loss 3.266433 | norm 0.3095 | time 464.2167 ms | tok/sec 1129403.5516
for step 10765 | loss 3.382457 | norm 0.3320 | time 463.4430 ms | tok/sec 1131288.9693
for step 10766 | loss 3.313625 | norm 0.3148 | time 463.7036 ms | tok/sec 1130653.2091
for step 10767 | loss 3.328953 | norm 0.3447 | time 464.2210 ms | tok/sec 1129393.1107
for step 10768 | loss 3.268575 | norm 0.3216 | time 464.1180 ms | tok/sec 1129643.7450
for step 10769 | loss 3.335430 | norm 0.3372 | time 467.2384 ms | tok/sec 1122099.4905
for step 10770 | loss 3.365655 | norm 0.3199 | time 464.3009 ms | tok/sec 1129198.8296
for step 10771 | loss 3.329989 | norm 0.3090 | time 465.5521 ms | tok/sec 1126163.9878
for step 10772 | loss 3.283180 | norm 0.2865 | time 464.9084 ms | tok/sec 1127723.3182
for step 10773 | loss 3.350987 | norm 0.3086 | time 464.6218 ms | tok/sec 1128418.8989
for step 10774 | loss 3.323144 | norm 0.3061 | time 464.5343 ms | tok/sec 1128631.4478
for step 10775 | loss 3.311940 | norm 0.3455 | time 464.7052 ms | tok/sec 1128216.2702
for step 10776 | loss 3.304458 | norm 0.3073 | time 465.0137 ms | tok/sec 1127467.7545
for step 10777 | loss 3.291555 | norm 0.3176 | time 464.3691 ms | tok/sec 1129033.0186
for step 10778 | loss 3.325214 | norm 0.3424 | time 464.2832 ms | tok/sec 1129241.7397
for step 10779 | loss 3.393593 | norm 0.3458 | time 464.7202 ms | tok/sec 1128179.8048
for step 10780 | loss 3.325836 | norm 0.3076 | time 464.9310 ms | tok/sec 1127668.3795
for step 10781 | loss 3.296835 | norm 0.3705 | time 464.7276 ms | tok/sec 1128161.8624
for step 10782 | loss 3.275718 | norm 0.3411 | time 465.4984 ms | tok/sec 1126293.7673
for step 10783 | loss 3.278303 | norm 0.3048 | time 464.4842 ms | tok/sec 1128753.1057
for step 10784 | loss 3.282479 | norm 0.3444 | time 464.9789 ms | tok/sec 1127552.1586
for step 10785 | loss 3.313235 | norm 0.2928 | time 464.7682 ms | tok/sec 1128063.4784
for step 10786 | loss 3.307351 | norm 0.3086 | time 464.8948 ms | tok/sec 1127756.2839
for step 10787 | loss 3.278605 | norm 0.2837 | time 465.1451 ms | tok/sec 1127149.3293
for step 10788 | loss 3.345159 | norm 0.3189 | time 465.2486 ms | tok/sec 1126898.6456
for step 10789 | loss 3.375785 | norm 0.3619 | time 466.9466 ms | tok/sec 1122800.7612
for step 10790 | loss 3.253081 | norm 0.4461 | time 464.8633 ms | tok/sec 1127832.6330
for step 10791 | loss 3.268632 | norm 0.3421 | time 464.9634 ms | tok/sec 1127589.7399
for step 10792 | loss 3.309118 | norm 0.3633 | time 464.4325 ms | tok/sec 1128878.8465
for step 10793 | loss 3.301968 | norm 0.3433 | time 464.4852 ms | tok/sec 1128750.7882
for step 10794 | loss 3.271586 | norm 0.3602 | time 465.4400 ms | tok/sec 1126435.1167
for step 10795 | loss 3.370399 | norm 0.3823 | time 464.5927 ms | tok/sec 1128489.5466
for step 10796 | loss 3.278523 | norm 0.4226 | time 464.7560 ms | tok/sec 1128092.9918
for step 10797 | loss 3.306596 | norm 0.3675 | time 464.9305 ms | tok/sec 1127669.5361
for step 10798 | loss 3.307161 | norm 0.4045 | time 465.2476 ms | tok/sec 1126900.9555
for step 10799 | loss 3.411910 | norm 0.4493 | time 469.3251 ms | tok/sec 1117110.5880
for step 10800 | loss 3.298760 | norm 0.3919 | time 464.3667 ms | tok/sec 1129038.8154
for step 10801 | loss 3.264121 | norm 0.3720 | time 464.7856 ms | tok/sec 1128021.2365
for step 10802 | loss 3.373131 | norm 0.3789 | time 465.2767 ms | tok/sec 1126830.5066
for step 10803 | loss 3.370570 | norm 0.3733 | time 465.3270 ms | tok/sec 1126708.6854
for step 10804 | loss 3.352371 | norm 0.3873 | time 465.2858 ms | tok/sec 1126808.5653
for step 10805 | loss 3.343930 | norm 0.3900 | time 464.9606 ms | tok/sec 1127596.6782
for step 10806 | loss 3.309095 | norm 0.3466 | time 464.5624 ms | tok/sec 1128563.0991
for step 10807 | loss 3.382398 | norm 0.3668 | time 464.5650 ms | tok/sec 1128556.7280
for step 10808 | loss 3.336208 | norm 0.3321 | time 465.1048 ms | tok/sec 1127246.9760
for step 10809 | loss 3.332289 | norm 0.3564 | time 465.1620 ms | tok/sec 1127108.3112
for step 10810 | loss 3.330082 | norm 0.3233 | time 463.9959 ms | tok/sec 1129940.9372
for step 10811 | loss 3.391131 | norm 0.3311 | time 465.3094 ms | tok/sec 1126751.4064
for step 10812 | loss 3.432087 | norm 0.3310 | time 465.1027 ms | tok/sec 1127252.1766
for step 10813 | loss 3.349106 | norm 0.3273 | time 463.6803 ms | tok/sec 1130710.1831
for step 10814 | loss 3.346601 | norm 0.3191 | time 465.7645 ms | tok/sec 1125650.3547
for step 10815 | loss 3.366220 | norm 0.3094 | time 464.7167 ms | tok/sec 1128188.4868
for step 10816 | loss 3.372778 | norm 0.3305 | time 465.3692 ms | tok/sec 1126606.5144
for step 10817 | loss 3.359882 | norm 0.3215 | time 464.5329 ms | tok/sec 1128634.9233
for step 10818 | loss 3.315173 | norm 0.3276 | time 465.2483 ms | tok/sec 1126899.2230
for step 10819 | loss 3.361219 | norm 0.2938 | time 465.1449 ms | tok/sec 1127149.9071
for step 10820 | loss 3.341369 | norm 0.3190 | time 464.9699 ms | tok/sec 1127574.1289
for step 10821 | loss 3.279724 | norm 0.3226 | time 465.1439 ms | tok/sec 1127152.2180
for step 10822 | loss 3.328945 | norm 0.3039 | time 464.5660 ms | tok/sec 1128554.4113
for step 10823 | loss 3.368676 | norm 0.3348 | time 465.1358 ms | tok/sec 1127171.8617
for step 10824 | loss 3.290123 | norm 0.3271 | time 465.9524 ms | tok/sec 1125196.4880
for step 10825 | loss 3.290279 | norm 0.3111 | time 464.6020 ms | tok/sec 1128466.9615
for step 10826 | loss 3.303652 | norm 0.3495 | time 465.5464 ms | tok/sec 1126177.8295
for step 10827 | loss 3.311106 | norm 0.3033 | time 466.3260 ms | tok/sec 1124295.0252
for step 10828 | loss 3.305839 | norm 0.3340 | time 464.4969 ms | tok/sec 1128722.3991
for step 10829 | loss 3.286378 | norm 0.3636 | time 464.8595 ms | tok/sec 1127841.8882
for step 10830 | loss 3.289389 | norm 0.3302 | time 464.9913 ms | tok/sec 1127522.0954
for step 10831 | loss 3.261811 | norm 0.3404 | time 464.1600 ms | tok/sec 1129541.6213
for step 10832 | loss 3.258330 | norm 0.2974 | time 464.9465 ms | tok/sec 1127630.7930
for step 10833 | loss 3.262020 | norm 0.3391 | time 464.4930 ms | tok/sec 1128731.6689
for step 10834 | loss 3.281869 | norm 0.3100 | time 466.0828 ms | tok/sec 1124881.6460
for step 10835 | loss 3.255436 | norm 0.3249 | time 465.9026 ms | tok/sec 1125316.8307
for step 10836 | loss 3.318885 | norm 0.3289 | time 464.5436 ms | tok/sec 1128608.8570
for step 10837 | loss 3.286571 | norm 0.3030 | time 465.6284 ms | tok/sec 1125979.4640
for step 10838 | loss 3.319755 | norm 0.3215 | time 465.1771 ms | tok/sec 1127071.9174
for step 10839 | loss 3.354804 | norm 0.3235 | time 464.7841 ms | tok/sec 1128024.7083
for step 10840 | loss 3.333977 | norm 0.3312 | time 465.1015 ms | tok/sec 1127255.0658
for step 10841 | loss 3.339652 | norm 0.3748 | time 464.5612 ms | tok/sec 1128565.9950
for step 10842 | loss 3.343971 | norm 0.3164 | time 465.7819 ms | tok/sec 1125608.2933
for step 10843 | loss 3.296598 | norm 0.3568 | time 464.5171 ms | tok/sec 1128673.1561
for step 10844 | loss 3.324167 | norm 0.2992 | time 465.5213 ms | tok/sec 1126238.3910
for step 10845 | loss 3.352555 | norm 0.3415 | time 464.4301 ms | tok/sec 1128884.6417
for step 10846 | loss 3.291722 | norm 0.3313 | time 464.9689 ms | tok/sec 1127576.4416
for step 10847 | loss 3.340934 | norm 0.2939 | time 465.6956 ms | tok/sec 1125816.9026
for step 10848 | loss 3.266973 | norm 0.3243 | time 466.0394 ms | tok/sec 1124986.3819
for step 10849 | loss 3.308619 | norm 0.2909 | time 465.2650 ms | tok/sec 1126858.8006
for step 10850 | loss 3.330808 | norm 0.3318 | time 464.8533 ms | tok/sec 1127856.9281
for step 10851 | loss 3.344838 | norm 0.2956 | time 465.1496 ms | tok/sec 1127138.3524
for step 10852 | loss 3.351420 | norm 0.3106 | time 464.6871 ms | tok/sec 1128260.2634
for step 10853 | loss 3.353992 | norm 0.3336 | time 465.0767 ms | tok/sec 1127315.1654
for step 10854 | loss 3.348606 | norm 0.3101 | time 464.1345 ms | tok/sec 1129603.7057
for step 10855 | loss 3.307570 | norm 0.3352 | time 465.6310 ms | tok/sec 1125973.1221
for step 10856 | loss 3.284816 | norm 0.3319 | time 465.8999 ms | tok/sec 1125323.1652
for step 10857 | loss 3.367178 | norm 0.3425 | time 464.4167 ms | tok/sec 1128917.0958
Will loading at 0 from edu_fineweb10B/edufineweb_train_000058.npy
for step 10858 | loss 3.314136 | norm 0.3447 | time 1400.4352 ms | tok/sec 374375.0489
for step 10859 | loss 3.319566 | norm 0.3223 | time 463.4252 ms | tok/sec 1131332.6204
for step 10860 | loss 3.334506 | norm 0.3458 | time 464.4747 ms | tok/sec 1128776.2817
for step 10861 | loss 3.319535 | norm 0.3618 | time 466.5525 ms | tok/sec 1123749.2127
for step 10862 | loss 3.272328 | norm 0.3324 | time 464.5677 ms | tok/sec 1128550.3570
for step 10863 | loss 3.239672 | norm 0.3285 | time 464.7129 ms | tok/sec 1128197.7478
for step 10864 | loss 3.312418 | norm 0.3558 | time 464.8674 ms | tok/sec 1127822.7996
for step 10865 | loss 3.309721 | norm 0.3112 | time 464.2735 ms | tok/sec 1129265.5156
for step 10866 | loss 3.247069 | norm 0.3797 | time 464.8178 ms | tok/sec 1127943.1262
for step 10867 | loss 3.282933 | norm 0.3355 | time 464.5066 ms | tok/sec 1128698.6461
for step 10868 | loss 3.291650 | norm 0.3751 | time 464.1669 ms | tok/sec 1129524.7959
for step 10869 | loss 3.238226 | norm 0.3222 | time 464.4492 ms | tok/sec 1128838.2819
for step 10870 | loss 3.288095 | norm 0.3502 | time 464.5329 ms | tok/sec 1128634.9233
for step 10871 | loss 3.259922 | norm 0.3206 | time 464.7880 ms | tok/sec 1128015.4502
for step 10872 | loss 3.325138 | norm 0.3256 | time 464.7400 ms | tok/sec 1128131.7666
for step 10873 | loss 3.364941 | norm 0.3330 | time 466.1665 ms | tok/sec 1124679.7103
for step 10874 | loss 3.313240 | norm 0.3048 | time 465.0154 ms | tok/sec 1127463.7080
for step 10875 | loss 3.297255 | norm 0.3050 | time 466.4154 ms | tok/sec 1124079.5095
for step 10876 | loss 3.283595 | norm 0.3128 | time 465.4849 ms | tok/sec 1126326.6495
for step 10877 | loss 3.338719 | norm 0.3214 | time 464.3540 ms | tok/sec 1129069.5392
for step 10878 | loss 3.365447 | norm 0.3131 | time 466.2621 ms | tok/sec 1124449.0976
for step 10879 | loss 3.311874 | norm 0.3140 | time 465.9038 ms | tok/sec 1125313.9514
for step 10880 | loss 3.418220 | norm 0.3143 | time 465.0683 ms | tok/sec 1127335.3926
for step 10881 | loss 3.336891 | norm 0.3521 | time 466.2375 ms | tok/sec 1124508.3233
for step 10882 | loss 3.304577 | norm 0.3176 | time 465.0912 ms | tok/sec 1127279.9139
for step 10883 | loss 3.321208 | norm 0.3506 | time 464.9694 ms | tok/sec 1127575.2853
for step 10884 | loss 3.323036 | norm 0.2971 | time 465.2548 ms | tok/sec 1126883.6312
for step 10885 | loss 3.319450 | norm 0.3649 | time 464.9830 ms | tok/sec 1127542.3301
for step 10886 | loss 3.287796 | norm 0.3071 | time 465.9584 ms | tok/sec 1125182.0947
for step 10887 | loss 3.335971 | norm 0.3742 | time 465.4677 ms | tok/sec 1126368.1877
for step 10888 | loss 3.282318 | norm 0.3205 | time 465.0140 ms | tok/sec 1127467.1764
for step 10889 | loss 3.303715 | norm 0.3333 | time 465.3087 ms | tok/sec 1126753.1384
for step 10890 | loss 3.281057 | norm 0.3329 | time 465.3935 ms | tok/sec 1126547.6447
for step 10891 | loss 3.340930 | norm 0.3259 | time 465.5612 ms | tok/sec 1126142.0724
for step 10892 | loss 3.309743 | norm 0.3399 | time 464.9410 ms | tok/sec 1127644.0926
for step 10893 | loss 3.288678 | norm 0.3142 | time 465.8356 ms | tok/sec 1125478.6717
for step 10894 | loss 3.331753 | norm 0.3219 | time 465.9581 ms | tok/sec 1125182.6704
for step 10895 | loss 3.333018 | norm 0.3435 | time 465.0288 ms | tok/sec 1127431.3374
for step 10896 | loss 3.281509 | norm 0.3772 | time 464.9787 ms | tok/sec 1127552.7368
for step 10897 | loss 3.272136 | norm 0.4632 | time 465.2789 ms | tok/sec 1126825.3099
for step 10898 | loss 3.284652 | norm 0.3623 | time 465.2798 ms | tok/sec 1126823.0002
for step 10899 | loss 3.269372 | norm 0.3831 | time 464.4551 ms | tok/sec 1128823.7953
for step 10900 | loss 3.263710 | norm 0.3864 | time 464.9899 ms | tok/sec 1127525.5642
for step 10901 | loss 3.332505 | norm 0.3368 | time 464.5584 ms | tok/sec 1128572.9454
for step 10902 | loss 3.233728 | norm 0.3582 | time 465.1082 ms | tok/sec 1127238.8863
for step 10903 | loss 3.286788 | norm 0.3606 | time 464.9551 ms | tok/sec 1127609.9770
for step 10904 | loss 3.285477 | norm 0.3493 | time 465.5015 ms | tok/sec 1126286.2681
for step 10905 | loss 3.228152 | norm 0.3452 | time 464.7255 ms | tok/sec 1128167.0714
for step 10906 | loss 3.277362 | norm 0.3204 | time 464.5412 ms | tok/sec 1128614.6494
for step 10907 | loss 3.297584 | norm 0.3615 | time 464.3154 ms | tok/sec 1129163.4602
for step 10908 | loss 3.311725 | norm 0.3122 | time 464.2973 ms | tok/sec 1129207.5273
for step 10909 | loss 3.326050 | norm 0.3534 | time 465.0133 ms | tok/sec 1127468.9106
for step 10910 | loss 3.299572 | norm 0.3194 | time 464.9858 ms | tok/sec 1127535.3924
for step 10911 | loss 3.412241 | norm 0.3481 | time 465.9934 ms | tok/sec 1125097.4693
for step 10912 | loss 3.314202 | norm 0.3128 | time 464.9158 ms | tok/sec 1127705.3902
for step 10913 | loss 3.336827 | norm 0.3357 | time 465.7106 ms | tok/sec 1125780.5921
for step 10914 | loss 3.309860 | norm 0.3171 | time 465.3420 ms | tok/sec 1126672.3173
for step 10915 | loss 3.294559 | norm 0.3612 | time 464.7202 ms | tok/sec 1128179.8048
for step 10916 | loss 3.327228 | norm 0.3110 | time 465.3289 ms | tok/sec 1126704.0671
for step 10917 | loss 3.325500 | norm 0.3658 | time 465.6842 ms | tok/sec 1125844.5693
for step 10918 | loss 3.418395 | norm 0.3129 | time 464.6020 ms | tok/sec 1128466.9615
for step 10919 | loss 3.373696 | norm 0.3785 | time 464.4732 ms | tok/sec 1128779.7581
for step 10920 | loss 3.307682 | norm 0.3089 | time 465.1651 ms | tok/sec 1127100.8012
for step 10921 | loss 3.227524 | norm 0.3542 | time 465.6429 ms | tok/sec 1125944.2961
for step 10922 | loss 3.284620 | norm 0.3193 | time 464.9560 ms | tok/sec 1127607.6641
for step 10923 | loss 3.335443 | norm 0.3219 | time 464.8678 ms | tok/sec 1127821.6427
for step 10924 | loss 3.306377 | norm 0.3513 | time 464.8428 ms | tok/sec 1127882.3812
for step 10925 | loss 3.310426 | norm 0.2975 | time 464.6940 ms | tok/sec 1128243.4761
for step 10926 | loss 3.306669 | norm 0.3168 | time 464.5987 ms | tok/sec 1128475.0689
for step 10927 | loss 3.311993 | norm 0.2992 | time 465.8298 ms | tok/sec 1125492.4966
for step 10928 | loss 3.286798 | norm 0.2879 | time 465.4443 ms | tok/sec 1126424.7307
for step 10929 | loss 3.374352 | norm 0.3167 | time 465.6672 ms | tok/sec 1125885.4955
for step 10930 | loss 3.312051 | norm 0.2878 | time 464.8407 ms | tok/sec 1127887.5876
for step 10931 | loss 3.283142 | norm 0.3397 | time 465.4164 ms | tok/sec 1126492.2435
for step 10932 | loss 3.329837 | norm 0.3048 | time 464.9627 ms | tok/sec 1127591.4745
for step 10933 | loss 3.243363 | norm 0.3400 | time 464.3950 ms | tok/sec 1128969.8378
for step 10934 | loss 3.261757 | norm 0.3253 | time 463.9175 ms | tok/sec 1130131.9890
for step 10935 | loss 3.266742 | norm 0.3023 | time 464.3440 ms | tok/sec 1129093.8876
for step 10936 | loss 3.272942 | norm 0.2846 | time 464.7467 ms | tok/sec 1128115.5619
for step 10937 | loss 3.300991 | norm 0.3233 | time 465.6072 ms | tok/sec 1126030.7786
for step 10938 | loss 3.311364 | norm 0.2912 | time 465.3363 ms | tok/sec 1126686.1715
for step 10939 | loss 3.268088 | norm 0.3200 | time 464.7624 ms | tok/sec 1128077.3669
for step 10940 | loss 3.297494 | norm 0.3422 | time 464.7882 ms | tok/sec 1128014.8716
for step 10941 | loss 3.313461 | norm 0.3051 | time 465.3296 ms | tok/sec 1126702.3352
for step 10942 | loss 3.307232 | norm 0.3215 | time 465.4138 ms | tok/sec 1126498.5913
for step 10943 | loss 3.287755 | norm 0.3255 | time 465.0543 ms | tok/sec 1127369.4916
for step 10944 | loss 3.271843 | norm 0.3528 | time 464.6676 ms | tok/sec 1128307.7336
for step 10945 | loss 3.284501 | norm 0.3193 | time 465.5395 ms | tok/sec 1126194.5554
for step 10946 | loss 3.341877 | norm 0.3333 | time 465.5123 ms | tok/sec 1126260.3101
for step 10947 | loss 3.215311 | norm 0.3135 | time 465.7025 ms | tok/sec 1125800.1880
for step 10948 | loss 3.261146 | norm 0.3253 | time 468.1907 ms | tok/sec 1119817.2744
for step 10949 | loss 3.296300 | norm 0.2985 | time 464.9141 ms | tok/sec 1127709.4384
for step 10950 | loss 3.282994 | norm 0.3553 | time 466.5105 ms | tok/sec 1123850.2916
for step 10951 | loss 3.325727 | norm 0.3734 | time 464.7818 ms | tok/sec 1128030.4947
for step 10952 | loss 3.278281 | norm 0.3428 | time 465.3516 ms | tok/sec 1126649.2277
for step 10953 | loss 3.273618 | norm 0.3170 | time 464.9279 ms | tok/sec 1127675.8971
for step 10954 | loss 3.243783 | norm 0.3342 | time 465.7779 ms | tok/sec 1125618.0881
for step 10955 | loss 3.302700 | norm 0.3205 | time 465.0512 ms | tok/sec 1127377.0052
for step 10956 | loss 3.348198 | norm 0.3193 | time 465.3926 ms | tok/sec 1126549.9532
for step 10957 | loss 3.311095 | norm 0.3634 | time 465.9643 ms | tok/sec 1125167.7017
for step 10958 | loss 3.298343 | norm 0.3113 | time 464.8674 ms | tok/sec 1127822.7996
for step 10959 | loss 3.269941 | norm 0.3333 | time 465.0123 ms | tok/sec 1127471.2229
for step 10960 | loss 3.320316 | norm 0.3357 | time 466.5725 ms | tok/sec 1123700.9769
for step 10961 | loss 3.322352 | norm 0.3304 | time 465.6758 ms | tok/sec 1125864.7438
for step 10962 | loss 3.287941 | norm 0.3725 | time 465.1058 ms | tok/sec 1127244.6646
for step 10963 | loss 3.328552 | norm 0.3003 | time 464.6840 ms | tok/sec 1128267.7889
for step 10964 | loss 3.343244 | norm 0.3184 | time 465.3969 ms | tok/sec 1126539.5650
for step 10965 | loss 3.333367 | norm 0.2947 | time 464.7956 ms | tok/sec 1127996.9344
for step 10966 | loss 3.340057 | norm 0.3174 | time 465.8539 ms | tok/sec 1125434.3192
for step 10967 | loss 3.274901 | norm 0.2853 | time 464.5860 ms | tok/sec 1128505.7621
for step 10968 | loss 3.250057 | norm 0.3210 | time 464.4513 ms | tok/sec 1128833.0667
for step 10969 | loss 3.304095 | norm 0.3194 | time 465.0457 ms | tok/sec 1127390.2988
for step 10970 | loss 3.297117 | norm 0.2955 | time 464.6659 ms | tok/sec 1128311.7861
for step 10971 | loss 3.254914 | norm 0.3018 | time 466.0146 ms | tok/sec 1125046.2398
for step 10972 | loss 3.283892 | norm 0.3089 | time 465.8322 ms | tok/sec 1125486.7362
for step 10973 | loss 3.274830 | norm 0.3057 | time 465.2205 ms | tok/sec 1126966.7928
for step 10974 | loss 3.248209 | norm 0.3158 | time 466.3453 ms | tok/sec 1124248.4668
for step 10975 | loss 3.259501 | norm 0.3086 | time 465.8394 ms | tok/sec 1125469.4553
for step 10976 | loss 3.221150 | norm 0.3051 | time 465.5600 ms | tok/sec 1126144.9560
for step 10977 | loss 3.315905 | norm 0.3381 | time 465.4322 ms | tok/sec 1126454.1583
for step 10978 | loss 3.234804 | norm 0.3053 | time 465.4007 ms | tok/sec 1126530.3312
for step 10979 | loss 3.323630 | norm 0.3457 | time 465.2820 ms | tok/sec 1126817.8036
for step 10980 | loss 3.284596 | norm 0.3168 | time 465.5645 ms | tok/sec 1126133.9986
for step 10981 | loss 3.293262 | norm 0.3603 | time 465.1914 ms | tok/sec 1127037.2587
for step 10982 | loss 3.295623 | norm 0.3369 | time 466.3472 ms | tok/sec 1124243.8686
for step 10983 | loss 3.367234 | norm 0.3595 | time 464.7608 ms | tok/sec 1128081.4178
for step 10984 | loss 3.309424 | norm 0.3374 | time 464.8757 ms | tok/sec 1127802.5549
for step 10985 | loss 3.308238 | norm 0.3698 | time 465.6878 ms | tok/sec 1125835.9233
for step 10986 | loss 3.322023 | norm 0.3383 | time 465.5180 ms | tok/sec 1126246.4664
for step 10987 | loss 3.329147 | norm 0.3256 | time 465.2479 ms | tok/sec 1126900.3780
for step 10988 | loss 3.294240 | norm 0.3385 | time 464.5598 ms | tok/sec 1128569.4702
for step 10989 | loss 3.292204 | norm 0.3269 | time 466.1045 ms | tok/sec 1124829.2853
for step 10990 | loss 3.270779 | norm 0.3337 | time 463.9866 ms | tok/sec 1129963.5813
for step 10991 | loss 3.341544 | norm 0.3635 | time 465.3890 ms | tok/sec 1126558.6102
for step 10992 | loss 3.377559 | norm 0.3434 | time 465.4467 ms | tok/sec 1126418.9607
for step 10993 | loss 3.326778 | norm 0.3580 | time 465.5428 ms | tok/sec 1126186.4808
for step 10994 | loss 3.328426 | norm 0.3217 | time 465.4508 ms | tok/sec 1126409.1519
for step 10995 | loss 3.328484 | norm 0.3392 | time 465.3625 ms | tok/sec 1126622.6758
for step 10996 | loss 3.255691 | norm 0.3401 | time 465.4396 ms | tok/sec 1126436.2707
for step 10997 | loss 3.358508 | norm 0.3387 | time 465.5044 ms | tok/sec 1126279.3458
for step 10998 | loss 3.342351 | norm 0.3042 | time 464.9298 ms | tok/sec 1127671.2709
for step 10999 | loss 3.290619 | norm 0.3301 | time 465.2276 ms | tok/sec 1126949.4664
validation loss 3.3107
HellaSwag accuracy: 2790/10042=0.2778
> Hello, I'm a language model, and i am a research professor at King Abdullah University of Agriculture, in Saudi Arabia. It has to do with the way
> Hello, I'm a language model, which means I understand the language as if it were a noun. I'm learning myself an MLE language model, which
> Hello, I'm a language model, I usually don't know what I'm talking about. It means you have to know what to expect, what your best
> Hello, I'm a language model, and I'm just trying to work for and to create videos on their topics before I want to work on them.

> Hello, I'm a language model, what does it mean. Thanks to our friend, we have created this website that helps you learn how to write a perfect
> Hello, I'm a language model, and I think that this model can be found in my other blog. Thanks for the explanation, the blog, and all
> Hello, I'm a language model, so I need to model for the future for the school.
I also had a little background to start working on this> 
Hello, I'm a language model, I can think of several things that I don't know about the future, it's one of those things! That is
>>> >  Hello, I'm a language model, and I understand that you will be able to make this one of the few "so many" programming languages.
As Hello, I'm a language model, and we use it to help my clients get to know the language, and we use that technique to really do what weHello, I'm a language model, we use the “Hello Word” box at the top of the window when they're not on it. The
Hello, I'm a language model, and I can't wait to write. Any suggestions?
- When creating and using language models and frameworks like the C


> > > Hello, I'm a language model, but am not looking like your own and i'm not looking that way.
Your goal is to try and answer your
>>Hello, I'm a language model, so to speak, you have to.
- (to get your reader)
The only way to learn a language
Hello, I'm a language model, just come on and say the right letter when I read all the words.
There's also a good video to tell
 Hello, I'm a language model, I think I'm just a huge person with lots of experience. This isn't just a matter of "what happens to >> > 
Hello, I'm a language model, and i are working on it. Since this language model we only work with a subset of languages from a subset of them
 Hello, I'm a language model, having to learn what I'm learning so that you can work out what I want you to learn and what my goals areHello, I'm a language model, so I use this technique here. So I will look at each language.
And in this case, I'll find
Hello, I'm a language model, and I am so happy to write, but I cannot understand the difference; I can't understand the difference, since there
> 
> > Hello, I'm a language model,
(this is probably more about that), where I am going to do that).<|endoftext|>A couple of centuries after the
Hello, I'm a language model, and one of the most important aspects of this approach is that it should be able to do better in more formal and practical
> Hello, I'm a language model, from a point of view (say what I'd like to say) with a little bit of effort, but I wanted
> Hello, I'm a language model, I'm so interested in the language design processes.
I think we had a lot of discussions on this, but what
> Hello, I'm a language model, and you know that from the book by the authors, I'm pretty sure that the idea is to get the students working
> Hello, I'm a language model, and I'm not really familiar with C++, so I prefer C . They can create dynamic expressions such as , so
> Hello, I'm a language model, so I am using it.<|endoftext|>Hiroshima, Japan – May 19, 2017 – (WAP) Japan
Hello, I'm a language model, thanks for posting a lot of comments. I have two of the words that came up: "When I was in high
> > Hello, I'm a language model, and the other is a visual simulation.
If you are interested, please submit your request to: firstname.last
Hello, I'm a language model, I like "I" because I don't know whether you really know what languages are and, after that, you may
> Hello, I'm a language model, and you'll hear us the complete sentences by going to the web and clicking on the "comments" button. There's
> Hello, I'm a language model, and someone who wants to express yourself in the real world: I don't know you, but I see it as a
for step 11000 | loss 3.337385 | norm 0.2949 | time 12872.9217 ms | tok/sec 40727.9724
for step 11001 | loss 3.265254 | norm 0.3497 | time 461.7159 ms | tok/sec 1135520.6922
for step 11002 | loss 3.268110 | norm 0.3194 | time 463.2843 ms | tok/sec 1131676.7090
for step 11003 | loss 3.271291 | norm 0.2914 | time 462.6029 ms | tok/sec 1133343.6353
for step 11004 | loss 3.268276 | norm 0.3079 | time 462.7554 ms | tok/sec 1132969.9292
for step 11005 | loss 3.330138 | norm 0.3275 | time 463.8937 ms | tok/sec 1130190.0722
for step 11006 | loss 3.305734 | norm 0.2905 | time 462.7464 ms | tok/sec 1132992.1112
for step 11007 | loss 3.278903 | norm 0.3430 | time 463.2659 ms | tok/sec 1131721.5550
for step 11008 | loss 3.301551 | norm 0.3001 | time 463.2230 ms | tok/sec 1131826.4034
for step 11009 | loss 3.277009 | norm 0.3433 | time 463.2037 ms | tok/sec 1131873.5916
for step 11010 | loss 3.284027 | norm 0.3134 | time 464.1042 ms | tok/sec 1129677.4035
for step 11011 | loss 3.268383 | norm 0.3359 | time 463.9356 ms | tok/sec 1130087.8497
for step 11012 | loss 3.231128 | norm 0.3191 | time 463.8331 ms | tok/sec 1130337.6305
for step 11013 | loss 3.257219 | norm 0.3240 | time 464.7405 ms | tok/sec 1128130.6091
for step 11014 | loss 3.244953 | norm 0.3430 | time 463.3865 ms | tok/sec 1131426.9183
for step 11015 | loss 3.340787 | norm 0.3528 | time 464.0598 ms | tok/sec 1129785.3562
for step 11016 | loss 3.293518 | norm 0.3172 | time 464.0892 ms | tok/sec 1129713.9658
for step 11017 | loss 3.279115 | norm 0.3422 | time 463.8646 ms | tok/sec 1130260.9418
for step 11018 | loss 3.304276 | norm 0.3018 | time 464.2613 ms | tok/sec 1129295.0919
for step 11019 | loss 3.311881 | norm 0.3497 | time 464.2467 ms | tok/sec 1129330.4695
for step 11020 | loss 3.293324 | norm 0.3009 | time 464.0465 ms | tok/sec 1129817.8621
for step 11021 | loss 3.328954 | norm 0.3448 | time 465.3120 ms | tok/sec 1126745.0558
for step 11022 | loss 3.294712 | norm 0.3085 | time 464.5102 ms | tok/sec 1128689.9562
for step 11023 | loss 3.265868 | norm 0.3485 | time 464.6895 ms | tok/sec 1128254.4746
for step 11024 | loss 3.298128 | norm 0.3468 | time 464.8051 ms | tok/sec 1127973.7904
for step 11025 | loss 3.312237 | norm 0.3390 | time 465.3025 ms | tok/sec 1126768.1493
for step 11026 | loss 3.399014 | norm 0.3807 | time 464.9861 ms | tok/sec 1127534.8143
for step 11027 | loss 3.355223 | norm 0.3192 | time 464.2370 ms | tok/sec 1129354.2491
for step 11028 | loss 3.315696 | norm 0.3285 | time 464.1442 ms | tok/sec 1129579.9155
for step 11029 | loss 3.339475 | norm 0.3171 | time 466.4700 ms | tok/sec 1123947.9420
for step 11030 | loss 3.327099 | norm 0.3400 | time 464.7372 ms | tok/sec 1128138.7117
for step 11031 | loss 3.332694 | norm 0.3442 | time 464.4103 ms | tok/sec 1128932.7440
for step 11032 | loss 3.313995 | norm 0.3447 | time 465.2321 ms | tok/sec 1126938.4934
for step 11033 | loss 3.323380 | norm 0.3199 | time 465.8036 ms | tok/sec 1125555.8649
for step 11034 | loss 3.311868 | norm 0.3545 | time 465.1632 ms | tok/sec 1127105.4227
for step 11035 | loss 3.281324 | norm 0.3281 | time 464.3512 ms | tok/sec 1129076.4958
for step 11036 | loss 3.281203 | norm 0.3583 | time 464.6311 ms | tok/sec 1128396.3167
for step 11037 | loss 3.304997 | norm 0.3591 | time 464.8747 ms | tok/sec 1127804.8685
for step 11038 | loss 3.274240 | norm 0.3438 | time 464.3304 ms | tok/sec 1129126.9336
for step 11039 | loss 3.296080 | norm 0.3314 | time 465.1012 ms | tok/sec 1127255.6437
for step 11040 | loss 3.169433 | norm 0.3310 | time 465.6770 ms | tok/sec 1125861.8617
for step 11041 | loss 3.291317 | norm 0.3393 | time 464.3035 ms | tok/sec 1129192.4514
for step 11042 | loss 3.247684 | norm 0.3391 | time 465.4405 ms | tok/sec 1126433.9627
for step 11043 | loss 3.294827 | norm 0.3112 | time 464.7384 ms | tok/sec 1128135.8179
for step 11044 | loss 3.318021 | norm 0.3604 | time 464.9324 ms | tok/sec 1127664.9099
for step 11045 | loss 3.283411 | norm 0.3105 | time 464.2270 ms | tok/sec 1129378.6098
for step 11046 | loss 3.230425 | norm 0.3124 | time 463.8896 ms | tok/sec 1130199.9469
for step 11047 | loss 3.293191 | norm 0.3055 | time 465.0609 ms | tok/sec 1127353.3088
Will loading at 0 from edu_fineweb10B/edufineweb_train_000059.npy
for step 11048 | loss 3.237647 | norm 0.2807 | time 1419.5831 ms | tok/sec 369325.3368
for step 11049 | loss 3.366812 | norm 0.3612 | time 464.1886 ms | tok/sec 1129472.0021
for step 11050 | loss 3.302434 | norm 0.3571 | time 464.1981 ms | tok/sec 1129448.7976
for step 11051 | loss 3.298212 | norm 0.3111 | time 464.0546 ms | tok/sec 1129798.1261
for step 11052 | loss 3.262298 | norm 0.3496 | time 463.4061 ms | tok/sec 1131379.1853
for step 11053 | loss 3.264399 | norm 0.2836 | time 464.4475 ms | tok/sec 1128842.3383
for step 11054 | loss 3.305784 | norm 0.3174 | time 464.8983 ms | tok/sec 1127747.6085
for step 11055 | loss 3.330828 | norm 0.3045 | time 464.3898 ms | tok/sec 1128982.5893
for step 11056 | loss 3.323303 | norm 0.3139 | time 464.3033 ms | tok/sec 1129193.0312
for step 11057 | loss 3.274062 | norm 0.3207 | time 463.9196 ms | tok/sec 1130126.7618
for step 11058 | loss 3.312268 | norm 0.3178 | time 465.2503 ms | tok/sec 1126894.6032
for step 11059 | loss 3.280190 | norm 0.3421 | time 466.0211 ms | tok/sec 1125030.6991
for step 11060 | loss 3.294871 | norm 0.3581 | time 464.5460 ms | tok/sec 1128603.0647
for step 11061 | loss 3.295365 | norm 0.3182 | time 465.1031 ms | tok/sec 1127251.0209
for step 11062 | loss 3.372163 | norm 0.3129 | time 464.9181 ms | tok/sec 1127699.6072
for step 11063 | loss 3.338708 | norm 0.3966 | time 465.5843 ms | tok/sec 1126086.1345
for step 11064 | loss 3.318240 | norm 0.2890 | time 466.5813 ms | tok/sec 1123679.7314
for step 11065 | loss 3.287181 | norm 0.3492 | time 465.6405 ms | tok/sec 1125950.0612
for step 11066 | loss 3.275462 | norm 0.3122 | time 465.6441 ms | tok/sec 1125941.4135
for step 11067 | loss 3.349566 | norm 0.3210 | time 464.7651 ms | tok/sec 1128071.0013
for step 11068 | loss 3.285269 | norm 0.2980 | time 465.8103 ms | tok/sec 1125539.7341
for step 11069 | loss 3.366972 | norm 0.3390 | time 465.7986 ms | tok/sec 1125567.9633
for step 11070 | loss 3.327079 | norm 0.2931 | time 465.1787 ms | tok/sec 1127067.8738
for step 11071 | loss 3.308094 | norm 0.3259 | time 466.3813 ms | tok/sec 1124161.6831
for step 11072 | loss 3.272320 | norm 0.3216 | time 464.9875 ms | tok/sec 1127531.3455
for step 11073 | loss 3.252002 | norm 0.2850 | time 465.8902 ms | tok/sec 1125346.7764
for step 11074 | loss 3.267121 | norm 0.3146 | time 465.6253 ms | tok/sec 1125986.9591
for step 11075 | loss 3.274368 | norm 0.3205 | time 465.4999 ms | tok/sec 1126290.3061
for step 11076 | loss 3.291081 | norm 0.3242 | time 465.7748 ms | tok/sec 1125625.5784
for step 11077 | loss 3.221642 | norm 0.3252 | time 465.4582 ms | tok/sec 1126391.2657
for step 11078 | loss 3.288210 | norm 0.3196 | time 466.3904 ms | tok/sec 1124139.8456
for step 11079 | loss 3.209611 | norm 0.3006 | time 465.2977 ms | tok/sec 1126779.6964
for step 11080 | loss 3.272914 | norm 0.3037 | time 465.4069 ms | tok/sec 1126515.3267
for step 11081 | loss 3.326333 | norm 0.3004 | time 464.9839 ms | tok/sec 1127540.0175
for step 11082 | loss 3.357507 | norm 0.3190 | time 465.9364 ms | tok/sec 1125235.0640
for step 11083 | loss 3.285287 | norm 0.3209 | time 466.0132 ms | tok/sec 1125049.6933
for step 11084 | loss 3.281383 | norm 0.3391 | time 465.7991 ms | tok/sec 1125566.8111
for step 11085 | loss 3.326922 | norm 0.3149 | time 465.8039 ms | tok/sec 1125555.2888
for step 11086 | loss 3.346227 | norm 0.3587 | time 465.9982 ms | tok/sec 1125085.9567
for step 11087 | loss 3.300605 | norm 0.3281 | time 466.7940 ms | tok/sec 1123167.7877
for step 11088 | loss 3.318449 | norm 0.3211 | time 464.9189 ms | tok/sec 1127697.8722
for step 11089 | loss 3.313400 | norm 0.3258 | time 465.9798 ms | tok/sec 1125130.2817
for step 11090 | loss 3.329624 | norm 0.3346 | time 464.7880 ms | tok/sec 1128015.4502
for step 11091 | loss 3.287049 | norm 0.3528 | time 465.6508 ms | tok/sec 1125925.2717
for step 11092 | loss 3.316263 | norm 0.3291 | time 465.2741 ms | tok/sec 1126836.8582
for step 11093 | loss 3.321750 | norm 0.3629 | time 465.6584 ms | tok/sec 1125906.8244
for step 11094 | loss 3.322518 | norm 0.3447 | time 466.2786 ms | tok/sec 1124409.4257
for step 11095 | loss 3.280266 | norm 0.3482 | time 466.7315 ms | tok/sec 1123318.1084
for step 11096 | loss 3.307511 | norm 0.3356 | time 465.5817 ms | tok/sec 1126092.4777
for step 11097 | loss 3.319911 | norm 0.3315 | time 465.2219 ms | tok/sec 1126963.3275
for step 11098 | loss 3.341379 | norm 0.3378 | time 465.1000 ms | tok/sec 1127258.5329
for step 11099 | loss 3.320621 | norm 0.3210 | time 465.4567 ms | tok/sec 1126394.7275
for step 11100 | loss 3.293249 | norm 0.3171 | time 465.8825 ms | tok/sec 1125365.2053
for step 11101 | loss 3.306543 | norm 0.3285 | time 465.1155 ms | tok/sec 1127220.9738
for step 11102 | loss 3.348221 | norm 0.2995 | time 465.4703 ms | tok/sec 1126361.8414
for step 11103 | loss 3.363448 | norm 0.3345 | time 466.1100 ms | tok/sec 1124816.0521
for step 11104 | loss 3.360363 | norm 0.3285 | time 465.8759 ms | tok/sec 1125381.3311
for step 11105 | loss 3.334786 | norm 0.3202 | time 465.5979 ms | tok/sec 1126053.2663
for step 11106 | loss 3.285532 | norm 0.3463 | time 464.5314 ms | tok/sec 1128638.3989
for step 11107 | loss 3.354795 | norm 0.3227 | time 465.4088 ms | tok/sec 1126510.7100
for step 11108 | loss 3.326016 | norm 0.3574 | time 464.6924 ms | tok/sec 1128247.5282
for step 11109 | loss 3.317195 | norm 0.3480 | time 465.0223 ms | tok/sec 1127446.9444
for step 11110 | loss 3.229336 | norm 0.3425 | time 464.6702 ms | tok/sec 1128301.3654
for step 11111 | loss 3.278546 | norm 0.3272 | time 465.1027 ms | tok/sec 1127252.1766
for step 11112 | loss 3.249029 | norm 0.3296 | time 465.2574 ms | tok/sec 1126877.2791
for step 11113 | loss 3.261113 | norm 0.3151 | time 465.5046 ms | tok/sec 1126278.7690
for step 11114 | loss 3.218824 | norm 0.3271 | time 464.9353 ms | tok/sec 1127657.9707
for step 11115 | loss 3.233400 | norm 0.3171 | time 465.0948 ms | tok/sec 1127271.2458
for step 11116 | loss 3.231659 | norm 0.3488 | time 465.3513 ms | tok/sec 1126649.8050
for step 11117 | loss 3.273772 | norm 0.3238 | time 464.3884 ms | tok/sec 1128986.0670
for step 11118 | loss 3.282132 | norm 0.3534 | time 464.6437 ms | tok/sec 1128365.6295
for step 11119 | loss 3.260976 | norm 0.3257 | time 464.8120 ms | tok/sec 1127957.0117
for step 11120 | loss 3.359320 | norm 0.3444 | time 464.5164 ms | tok/sec 1128674.8940
for step 11121 | loss 3.294136 | norm 0.3296 | time 465.0626 ms | tok/sec 1127349.2631
for step 11122 | loss 3.309890 | norm 0.3764 | time 465.4312 ms | tok/sec 1126456.4665
for step 11123 | loss 3.290276 | norm 0.3258 | time 465.1344 ms | tok/sec 1127175.3283
for step 11124 | loss 3.308668 | norm 0.3505 | time 465.2746 ms | tok/sec 1126835.7033
for step 11125 | loss 3.279058 | norm 0.3011 | time 465.1513 ms | tok/sec 1127134.3083
for step 11126 | loss 3.295462 | norm 0.3507 | time 465.1000 ms | tok/sec 1127258.5329
for step 11127 | loss 3.346752 | norm 0.3111 | time 465.9646 ms | tok/sec 1125167.1260
for step 11128 | loss 3.346712 | norm 0.3476 | time 465.5352 ms | tok/sec 1126204.9372
for step 11129 | loss 3.377499 | norm 0.3203 | time 465.0195 ms | tok/sec 1127453.8810
for step 11130 | loss 3.277827 | norm 0.3663 | time 466.2530 ms | tok/sec 1124470.9472
for step 11131 | loss 3.333618 | norm 0.3445 | time 464.9215 ms | tok/sec 1127691.5109
for step 11132 | loss 3.313191 | norm 0.3549 | time 464.5448 ms | tok/sec 1128605.9608
for step 11133 | loss 3.297686 | norm 0.3475 | time 463.8991 ms | tok/sec 1130176.7125
for step 11134 | loss 3.327206 | norm 0.3527 | time 464.5951 ms | tok/sec 1128483.7555
for step 11135 | loss 3.421232 | norm 0.3295 | time 465.3165 ms | tok/sec 1126734.0867
for step 11136 | loss 3.291866 | norm 0.3427 | time 464.2184 ms | tok/sec 1129399.4912
for step 11137 | loss 3.332636 | norm 0.3471 | time 465.1740 ms | tok/sec 1127079.4270
for step 11138 | loss 3.314643 | norm 0.3292 | time 465.1630 ms | tok/sec 1127106.0004
for step 11139 | loss 3.356996 | norm 0.3343 | time 464.7648 ms | tok/sec 1128071.5800
for step 11140 | loss 3.360992 | norm 0.3256 | time 464.7427 ms | tok/sec 1128125.4004
for step 11141 | loss 3.323774 | norm 0.3129 | time 464.4127 ms | tok/sec 1128926.9483
for step 11142 | loss 3.323817 | norm 0.3200 | time 464.7901 ms | tok/sec 1128010.2425
for step 11143 | loss 3.244115 | norm 0.3085 | time 465.0116 ms | tok/sec 1127472.9571
for step 11144 | loss 3.320327 | norm 0.3059 | time 464.8864 ms | tok/sec 1127776.5270
for step 11145 | loss 3.237491 | norm 0.3118 | time 465.6103 ms | tok/sec 1126023.2830
for step 11146 | loss 3.297431 | norm 0.3030 | time 464.7818 ms | tok/sec 1128030.4947
for step 11147 | loss 3.264604 | norm 0.2891 | time 465.1148 ms | tok/sec 1127222.7072
for step 11148 | loss 3.246495 | norm 0.2852 | time 465.7459 ms | tok/sec 1125695.3005
for step 11149 | loss 3.255754 | norm 0.2973 | time 465.3785 ms | tok/sec 1126584.0047
for step 11150 | loss 3.222121 | norm 0.2792 | time 464.7803 ms | tok/sec 1128033.9666
for step 11151 | loss 3.300807 | norm 0.3178 | time 465.4629 ms | tok/sec 1126379.7266
for step 11152 | loss 3.312750 | norm 0.3249 | time 465.4403 ms | tok/sec 1126434.5397
for step 11153 | loss 3.275983 | norm 0.2916 | time 465.5938 ms | tok/sec 1126063.0689
for step 11154 | loss 3.283076 | norm 0.3306 | time 465.7958 ms | tok/sec 1125574.8768
for step 11155 | loss 3.290828 | norm 0.3383 | time 464.7915 ms | tok/sec 1128006.7708
for step 11156 | loss 3.321426 | norm 0.3247 | time 464.5107 ms | tok/sec 1128688.7975
for step 11157 | loss 3.365657 | norm 0.3665 | time 465.3857 ms | tok/sec 1126566.6901
for step 11158 | loss 3.312135 | norm 0.3277 | time 465.9822 ms | tok/sec 1125124.5250
for step 11159 | loss 3.328231 | norm 0.3243 | time 465.4553 ms | tok/sec 1126398.1894
for step 11160 | loss 3.313666 | norm 0.3258 | time 464.9627 ms | tok/sec 1127591.4745
for step 11161 | loss 3.310145 | norm 0.3157 | time 465.0466 ms | tok/sec 1127387.9868
for step 11162 | loss 3.293094 | norm 0.3471 | time 465.4970 ms | tok/sec 1126297.2285
for step 11163 | loss 3.268429 | norm 0.3048 | time 465.2817 ms | tok/sec 1126818.3810
for step 11164 | loss 3.297481 | norm 0.3492 | time 465.1322 ms | tok/sec 1127180.5282
for step 11165 | loss 3.349692 | norm 0.3373 | time 465.5979 ms | tok/sec 1126053.2663
for step 11166 | loss 3.302234 | norm 0.3174 | time 465.4648 ms | tok/sec 1126375.1110
for step 11167 | loss 3.315927 | norm 0.3369 | time 464.7532 ms | tok/sec 1128099.9364
for step 11168 | loss 3.359571 | norm 0.3230 | time 464.9496 ms | tok/sec 1127623.2760
for step 11169 | loss 3.332864 | norm 0.3765 | time 464.8664 ms | tok/sec 1127825.1133
for step 11170 | loss 3.340284 | norm 0.3357 | time 465.7111 ms | tok/sec 1125779.4394
for step 11171 | loss 3.299265 | norm 0.3662 | time 466.2232 ms | tok/sec 1124542.8266
for step 11172 | loss 3.322572 | norm 0.4361 | time 464.5548 ms | tok/sec 1128581.6335
for step 11173 | loss 3.404740 | norm 0.3438 | time 465.3180 ms | tok/sec 1126730.6228
for step 11174 | loss 3.297152 | norm 0.3494 | time 465.5662 ms | tok/sec 1126129.9617
for step 11175 | loss 3.335356 | norm 0.3284 | time 466.0788 ms | tok/sec 1124891.4282
for step 11176 | loss 3.307389 | norm 0.3221 | time 467.8195 ms | tok/sec 1120705.8566
for step 11177 | loss 3.276680 | norm 0.3313 | time 464.9911 ms | tok/sec 1127522.6736
for step 11178 | loss 3.254868 | norm 0.3074 | time 465.3914 ms | tok/sec 1126552.8388
for step 11179 | loss 3.287727 | norm 0.3281 | time 464.7725 ms | tok/sec 1128053.0623
for step 11180 | loss 3.242524 | norm 0.3089 | time 465.0786 ms | tok/sec 1127310.5421
for step 11181 | loss 3.281160 | norm 0.3338 | time 465.6785 ms | tok/sec 1125858.4032
for step 11182 | loss 3.257112 | norm 0.2817 | time 465.0302 ms | tok/sec 1127427.8692
for step 11183 | loss 3.276550 | norm 0.3385 | time 465.5864 ms | tok/sec 1126080.9446
for step 11184 | loss 3.323919 | norm 0.3104 | time 465.3649 ms | tok/sec 1126616.9039
for step 11185 | loss 3.250137 | norm 0.3095 | time 465.2479 ms | tok/sec 1126900.3780
for step 11186 | loss 3.239611 | norm 0.3035 | time 465.1411 ms | tok/sec 1127159.1510
for step 11187 | loss 3.303651 | norm 0.3320 | time 464.4158 ms | tok/sec 1128919.4141
for step 11188 | loss 3.343678 | norm 0.3176 | time 466.1832 ms | tok/sec 1124639.4469
for step 11189 | loss 3.291880 | norm 0.3312 | time 464.9551 ms | tok/sec 1127609.9770
for step 11190 | loss 3.307921 | norm 0.3419 | time 464.7160 ms | tok/sec 1128190.2233
for step 11191 | loss 3.344537 | norm 0.3391 | time 465.5707 ms | tok/sec 1126119.0046
for step 11192 | loss 3.269662 | norm 0.3800 | time 465.3456 ms | tok/sec 1126663.6586
for step 11193 | loss 3.328152 | norm 0.3277 | time 465.2343 ms | tok/sec 1126933.2957
for step 11194 | loss 3.297379 | norm 0.3546 | time 464.7005 ms | tok/sec 1128227.8471
for step 11195 | loss 3.381527 | norm 0.3745 | time 464.3519 ms | tok/sec 1129074.7567
for step 11196 | loss 3.332079 | norm 0.3141 | time 464.8809 ms | tok/sec 1127789.8300
for step 11197 | loss 3.295284 | norm 0.3332 | time 463.8860 ms | tok/sec 1130208.6601
for step 11198 | loss 3.283332 | norm 0.3203 | time 465.2722 ms | tok/sec 1126841.4776
for step 11199 | loss 3.308424 | norm 0.3521 | time 465.3270 ms | tok/sec 1126708.6854
for step 11200 | loss 3.310375 | norm 0.3268 | time 465.5635 ms | tok/sec 1126136.3054
for step 11201 | loss 3.310952 | norm 0.3861 | time 464.4842 ms | tok/sec 1128753.1057
for step 11202 | loss 3.407337 | norm 0.3492 | time 464.5472 ms | tok/sec 1128600.1685
for step 11203 | loss 3.309983 | norm 0.3413 | time 464.5402 ms | tok/sec 1128616.9664
for step 11204 | loss 3.413575 | norm 0.3266 | time 465.0116 ms | tok/sec 1127472.9571
for step 11205 | loss 3.293980 | norm 0.3188 | time 464.1318 ms | tok/sec 1129610.0886
for step 11206 | loss 3.327176 | norm 0.3062 | time 465.2414 ms | tok/sec 1126915.9703
for step 11207 | loss 3.321692 | norm 0.2976 | time 464.5560 ms | tok/sec 1128578.7374
for step 11208 | loss 3.302190 | norm 0.3042 | time 464.2575 ms | tok/sec 1129304.3710
for step 11209 | loss 3.309441 | norm 0.2935 | time 465.4217 ms | tok/sec 1126479.5482
for step 11210 | loss 3.322896 | norm 0.2877 | time 465.3788 ms | tok/sec 1126583.4275
for step 11211 | loss 3.282691 | norm 0.3143 | time 465.7245 ms | tok/sec 1125747.1655
for step 11212 | loss 3.265459 | norm 0.3102 | time 465.1470 ms | tok/sec 1127144.7074
for step 11213 | loss 3.259717 | norm 0.3219 | time 464.7300 ms | tok/sec 1128156.0746
for step 11214 | loss 3.220167 | norm 0.3004 | time 464.9787 ms | tok/sec 1127552.7368
for step 11215 | loss 3.270008 | norm 0.3189 | time 465.0605 ms | tok/sec 1127354.4647
for step 11216 | loss 3.269452 | norm 0.2929 | time 465.6253 ms | tok/sec 1125986.9591
for step 11217 | loss 3.267390 | norm 0.3037 | time 465.7810 ms | tok/sec 1125610.5979
for step 11218 | loss 3.279313 | norm 0.2917 | time 465.7059 ms | tok/sec 1125792.1190
for step 11219 | loss 3.250543 | norm 0.2947 | time 464.8800 ms | tok/sec 1127792.1436
for step 11220 | loss 3.277167 | norm 0.3239 | time 465.6818 ms | tok/sec 1125850.3334
for step 11221 | loss 3.255344 | norm 0.3061 | time 466.0578 ms | tok/sec 1124942.0682
for step 11222 | loss 3.302278 | norm 0.3714 | time 466.2902 ms | tok/sec 1124381.2546
for step 11223 | loss 3.251670 | norm 0.3252 | time 464.8094 ms | tok/sec 1127963.3760
for step 11224 | loss 3.343715 | norm 0.3818 | time 465.8272 ms | tok/sec 1125498.8331
for step 11225 | loss 3.284894 | norm 0.3223 | time 464.7474 ms | tok/sec 1128113.8257
for step 11226 | loss 3.327976 | norm 0.3431 | time 464.7028 ms | tok/sec 1128222.0586
for step 11227 | loss 3.324169 | norm 0.3291 | time 466.0597 ms | tok/sec 1124937.4643
for step 11228 | loss 3.338346 | norm 0.3359 | time 465.4925 ms | tok/sec 1126308.1891
for step 11229 | loss 3.377902 | norm 0.3919 | time 466.2788 ms | tok/sec 1124408.8508
for step 11230 | loss 3.377683 | norm 0.3455 | time 464.1824 ms | tok/sec 1129487.0855
for step 11231 | loss 3.277160 | norm 0.3500 | time 464.7028 ms | tok/sec 1128222.0586
for step 11232 | loss 3.358314 | norm 0.3278 | time 464.8738 ms | tok/sec 1127807.1822
for step 11233 | loss 3.331221 | norm 0.3418 | time 464.1871 ms | tok/sec 1129475.4829
for step 11234 | loss 3.314646 | norm 0.3169 | time 466.6090 ms | tok/sec 1123613.1294
for step 11235 | loss 3.301175 | norm 0.3470 | time 465.2581 ms | tok/sec 1126875.5467
for step 11236 | loss 3.356380 | norm 0.3278 | time 465.1499 ms | tok/sec 1127137.7746
for step 11237 | loss 3.323796 | norm 0.3339 | time 464.8795 ms | tok/sec 1127793.3004
for step 11238 | loss 3.331296 | norm 0.3481 | time 465.0502 ms | tok/sec 1127379.3171
Will loading at 0 from edu_fineweb10B/edufineweb_train_000060.npy
for step 11239 | loss 3.311090 | norm 0.2916 | time 1410.2874 ms | tok/sec 371759.6905
for step 11240 | loss 3.339840 | norm 0.3423 | time 463.6457 ms | tok/sec 1130794.4920
for step 11241 | loss 3.329700 | norm 0.3232 | time 464.1042 ms | tok/sec 1129677.4035
for step 11242 | loss 3.290129 | norm 0.3092 | time 464.1068 ms | tok/sec 1129671.0198
for step 11243 | loss 3.320736 | norm 0.3521 | time 465.3447 ms | tok/sec 1126665.9676
for step 11244 | loss 3.304798 | norm 0.2856 | time 465.3845 ms | tok/sec 1126569.5758
for step 11245 | loss 3.315552 | norm 0.3287 | time 465.3914 ms | tok/sec 1126552.8388
for step 11246 | loss 3.311787 | norm 0.3129 | time 464.7312 ms | tok/sec 1128153.1807
for step 11247 | loss 3.288489 | norm 0.3060 | time 465.6708 ms | tok/sec 1125876.8489
for step 11248 | loss 3.253380 | norm 0.2962 | time 465.4973 ms | tok/sec 1126296.6516
for step 11249 | loss 3.245845 | norm 0.3143 | time 466.3856 ms | tok/sec 1124151.3389
validation loss 3.3055
HellaSwag accuracy: 2788/10042=0.2776
> Hello, I'm a language model, and if I'm just trying to grasp it's language, which is very powerful. The word processor is an interface between
> Hello, I'm a language model, a model, and a model that allows you to express oneself through other languages. I call this "model" because it
> Hello, I'm a language model, but let's get started.
I'm a language model student and it's a great idea, because it shows how
> Hello, I'm a language model, and I'm using the program, not yet. I try to read your blog, a story about a place called "
> Hello, I'm a language model, an abstraction, and this example should help.
What are you a student studying?
Here are some examples of an
>>  Hello, I'm a language model, and I'm not really comfortable with a little bit of stuff. I need to dig down more if that is what youHello, I'm a language model, and I want to try it in the past, to make sure I'll be a successful model, so what I'm

> Hello, I'm a language model, but a great model. It was great to watch a video, as it gives you a better understanding of it! I> 
> Hello, I'm a language model, teaching of language. I am very little interested in any meaningful language development. I use the English language at my level at
> Hello, I'm a language model, so I'd like to give you some help with the problem.
This is a bit off-putting.

> > Hello, I'm a language model, and i'm not interested in grammar, so i'm not interested in grammar or grammar theory altogether. At times, they
> > Hello, I'm a language model, and I can't understand all this and understand many things.
Here's how to create a basic grammar and vocabulary.
Hello, I'm a language model, and I am studying Chinese. Because a lot of questions are being asked (and I am not a professional language trainer).
Hello, I'm a language model, my name is on Facebook, there is a video tutorial to help you teach the game. I'm an architect, so
Hello, I'm a language model, but i don't like to focus on words, especially the ones of which I have no idea, for example, if
> > > > Hello, I'm a language model, so why not?
I don't know what else a language model is, but the key to its success is a
> Hello, I'm a language model, though, I'm still going to stick with using the first base of the language. I'm very happy with the program
Hello, I'm a language model,
so, for example, you'll have a simple language model,
and if you're interested in doing a good
Hello, I'm a language model, I read only, it means that, the "language" in my head is to learn as I make it through my
> Hello, I'm a language model, writing software as I read them. This is, of course, as I work it. But when you've got a>> > Hello, I'm a language model, so I do it when I'm working on programming. I do a lot of programming and I'm not a good job

 Hello, I'm a language model, and I'm a person with an infinite curiosity about language. Here are the differences in each type of tongue.<|endoftext|>BHello, I'm a language model, this is definitely an advanced language model: they are complex, complex, expressive, and useful, so this model is suitable
Hello, I'm a language model, and it came just to get me more confused when I spoke one language of the world.
Language models are all around
> > 
> Hello, I'm a language model, and for the last few weeks, I have started to get this book. It turns out that I just recently got another
Hello, I'm a language model, so I would love to help explain these concepts to you.
Here are a few example sentences for a language model you
> Hello, I'm a language model, so I've used this to understand everything I've learned.
So far, it's not going to be totally precise
> Hello, I'm a language model, and this isn't a theory or a model, but a model, and I'm not a model, but something to
> Hello, I'm a language model, one that has no limit; it's not your favorite, but I'm an outsider to it when I'm in education
> Hello, I'm a language model, but for learning a foreign language, I just need to use this language model. If I go the foreign language, I
Hello, I'm a language model, and you've worked
on something. Your mind is going to be very, very interesting. :)
- Why would
> Hello, I'm a language model, and want to know what it means. I'll start this lesson with an example language that is native english, and then
for step 11250 | loss 3.210785 | norm 0.2971 | time 12943.2418 ms | tok/sec 40506.6989
for step 11251 | loss 3.285508 | norm 0.2965 | time 462.4419 ms | tok/sec 1133738.0456
for step 11252 | loss 3.253642 | norm 0.3193 | time 462.0836 ms | tok/sec 1134617.2529
for step 11253 | loss 3.298649 | norm 0.3093 | time 462.5585 ms | tok/sec 1133452.2899
for step 11254 | loss 3.311194 | norm 0.3261 | time 462.9991 ms | tok/sec 1132373.6781
for step 11255 | loss 3.256840 | norm 0.3439 | time 462.7991 ms | tok/sec 1132863.1180
for step 11256 | loss 3.312307 | norm 0.3702 | time 463.0196 ms | tok/sec 1132323.5330
for step 11257 | loss 3.298409 | norm 0.3958 | time 463.4449 ms | tok/sec 1131284.3134
for step 11258 | loss 3.268589 | norm 0.3563 | time 463.9256 ms | tok/sec 1130112.2420
for step 11259 | loss 3.366515 | norm 0.3703 | time 463.8221 ms | tok/sec 1130364.3577
for step 11260 | loss 3.377655 | norm 0.3635 | time 464.2208 ms | tok/sec 1129393.6907
for step 11261 | loss 3.291652 | norm 0.4039 | time 464.3953 ms | tok/sec 1128969.2581
for step 11262 | loss 3.294246 | norm 0.3423 | time 464.5939 ms | tok/sec 1128486.6510
for step 11263 | loss 3.403662 | norm 0.3754 | time 465.2631 ms | tok/sec 1126863.4201
for step 11264 | loss 3.269388 | norm 0.3634 | time 464.9782 ms | tok/sec 1127553.8931
for step 11265 | loss 3.345642 | norm 0.4081 | time 465.1361 ms | tok/sec 1127171.2839
for step 11266 | loss 3.343866 | norm 0.3418 | time 464.0999 ms | tok/sec 1129687.8496
for step 11267 | loss 3.290596 | norm 0.3576 | time 464.6893 ms | tok/sec 1128255.0535
for step 11268 | loss 3.374501 | norm 0.3621 | time 466.0995 ms | tok/sec 1124841.3681
for step 11269 | loss 3.265927 | norm 0.3354 | time 464.9293 ms | tok/sec 1127672.4275
for step 11270 | loss 3.355067 | norm 0.3547 | time 465.0848 ms | tok/sec 1127295.5167
for step 11271 | loss 3.358824 | norm 0.3813 | time 465.6870 ms | tok/sec 1125837.6525
for step 11272 | loss 3.300858 | norm 0.3418 | time 465.2371 ms | tok/sec 1126926.3655
for step 11273 | loss 3.344585 | norm 0.3430 | time 465.0950 ms | tok/sec 1127270.6680
for step 11274 | loss 3.331494 | norm 0.3886 | time 464.6480 ms | tok/sec 1128355.2078
for step 11275 | loss 3.286628 | norm 0.3110 | time 465.2114 ms | tok/sec 1126988.7403
for step 11276 | loss 3.357497 | norm 0.3829 | time 465.1799 ms | tok/sec 1127064.9855
for step 11277 | loss 3.359196 | norm 0.3453 | time 464.9618 ms | tok/sec 1127593.7873
for step 11278 | loss 3.304391 | norm 0.3663 | time 466.6238 ms | tok/sec 1123577.5350
for step 11279 | loss 3.310118 | norm 0.3293 | time 465.2026 ms | tok/sec 1127010.1110
for step 11280 | loss 3.356900 | norm 0.3564 | time 464.8695 ms | tok/sec 1127817.5937
for step 11281 | loss 3.351845 | norm 0.2961 | time 465.1160 ms | tok/sec 1127219.8181
for step 11282 | loss 3.297291 | norm 0.3503 | time 465.1737 ms | tok/sec 1127080.0047
for step 11283 | loss 3.260049 | norm 0.3364 | time 465.0652 ms | tok/sec 1127342.9058
for step 11284 | loss 3.288559 | norm 0.3161 | time 464.7744 ms | tok/sec 1128048.4330
for step 11285 | loss 3.287260 | norm 0.3636 | time 465.7886 ms | tok/sec 1125592.1609
for step 11286 | loss 3.221038 | norm 0.2998 | time 465.6420 ms | tok/sec 1125946.6021
for step 11287 | loss 3.280703 | norm 0.3183 | time 466.2828 ms | tok/sec 1124399.0769
for step 11288 | loss 3.250316 | norm 0.3099 | time 465.1835 ms | tok/sec 1127056.3207
for step 11289 | loss 3.400615 | norm 0.3026 | time 465.6522 ms | tok/sec 1125921.8128
for step 11290 | loss 3.283520 | norm 0.2975 | time 465.0745 ms | tok/sec 1127320.3666
for step 11291 | loss 3.282688 | norm 0.2869 | time 464.1850 ms | tok/sec 1129480.7040
for step 11292 | loss 3.332847 | norm 0.3620 | time 464.5221 ms | tok/sec 1128660.9909
for step 11293 | loss 3.311775 | norm 0.3234 | time 464.9789 ms | tok/sec 1127552.1586
for step 11294 | loss 3.341169 | norm 0.3583 | time 465.1477 ms | tok/sec 1127142.9742
for step 11295 | loss 3.343266 | norm 0.3758 | time 469.7607 ms | tok/sec 1116074.7349
for step 11296 | loss 3.292322 | norm 0.3166 | time 465.1015 ms | tok/sec 1127255.0658
for step 11297 | loss 3.402864 | norm 0.4138 | time 464.5116 ms | tok/sec 1128686.4803
for step 11298 | loss 3.325282 | norm 0.4147 | time 464.2692 ms | tok/sec 1129275.9541
for step 11299 | loss 3.352876 | norm 0.3745 | time 465.4830 ms | tok/sec 1126331.2647
for step 11300 | loss 3.288364 | norm 0.3952 | time 465.2026 ms | tok/sec 1127010.1110
for step 11301 | loss 3.291154 | norm 0.3776 | time 465.4706 ms | tok/sec 1126361.2644
for step 11302 | loss 3.311642 | norm 0.3213 | time 463.9485 ms | tok/sec 1130056.4897
for step 11303 | loss 3.341388 | norm 0.3267 | time 463.7976 ms | tok/sec 1130424.2082
for step 11304 | loss 3.335906 | norm 0.3687 | time 464.1495 ms | tok/sec 1129567.1505
for step 11305 | loss 3.322002 | norm 0.3351 | time 466.9693 ms | tok/sec 1122746.3011
for step 11306 | loss 3.261404 | norm 0.3296 | time 464.6366 ms | tok/sec 1128382.9994
for step 11307 | loss 3.314667 | norm 0.3263 | time 465.4334 ms | tok/sec 1126451.2732
for step 11308 | loss 3.271565 | norm 0.3312 | time 464.8628 ms | tok/sec 1127833.7899
for step 11309 | loss 3.250825 | norm 0.3019 | time 465.4284 ms | tok/sec 1126463.3909
for step 11310 | loss 3.309571 | norm 0.3096 | time 465.9414 ms | tok/sec 1125222.9727
for step 11311 | loss 3.366979 | norm 0.2951 | time 464.4771 ms | tok/sec 1128770.4876
for step 11312 | loss 3.250609 | norm 0.2995 | time 466.2211 ms | tok/sec 1124548.0022
for step 11313 | loss 3.254892 | norm 0.2974 | time 464.9763 ms | tok/sec 1127558.5184
for step 11314 | loss 3.286084 | norm 0.2978 | time 465.5468 ms | tok/sec 1126176.6760
for step 11315 | loss 3.281337 | norm 0.2852 | time 466.4428 ms | tok/sec 1124013.4346
for step 11316 | loss 3.295377 | norm 0.3046 | time 465.5073 ms | tok/sec 1126272.4237
for step 11317 | loss 3.297474 | norm 0.2881 | time 465.6565 ms | tok/sec 1125911.4362
for step 11318 | loss 3.230305 | norm 0.2858 | time 465.1721 ms | tok/sec 1127084.0484
for step 11319 | loss 3.270253 | norm 0.2803 | time 465.4896 ms | tok/sec 1126315.1116
for step 11320 | loss 3.282490 | norm 0.2875 | time 466.2333 ms | tok/sec 1124518.6740
for step 11321 | loss 3.304953 | norm 0.3076 | time 466.0032 ms | tok/sec 1125073.8686
for step 11322 | loss 3.219597 | norm 0.2833 | time 465.4386 ms | tok/sec 1126438.5788
for step 11323 | loss 3.236218 | norm 0.2917 | time 465.5969 ms | tok/sec 1126055.5728
for step 11324 | loss 3.259678 | norm 0.3153 | time 465.6687 ms | tok/sec 1125882.0368
for step 11325 | loss 3.226411 | norm 0.3154 | time 465.2622 ms | tok/sec 1126865.7299
for step 11326 | loss 3.248035 | norm 0.2934 | time 465.3282 ms | tok/sec 1126705.7989
for step 11327 | loss 3.283893 | norm 0.3374 | time 464.8435 ms | tok/sec 1127880.6457
for step 11328 | loss 3.246547 | norm 0.2840 | time 465.7872 ms | tok/sec 1125595.6178
for step 11329 | loss 3.261416 | norm 0.3208 | time 465.7800 ms | tok/sec 1125612.9026
for step 11330 | loss 3.362779 | norm 0.2984 | time 464.5600 ms | tok/sec 1128568.8910
for step 11331 | loss 3.394616 | norm 0.3147 | time 465.2050 ms | tok/sec 1127004.3350
for step 11332 | loss 3.284807 | norm 0.3721 | time 465.2989 ms | tok/sec 1126776.8096
for step 11333 | loss 3.380808 | norm 0.3035 | time 464.8380 ms | tok/sec 1127893.9512
for step 11334 | loss 3.330212 | norm 0.4097 | time 465.2393 ms | tok/sec 1126921.1679
for step 11335 | loss 3.338083 | norm 0.3272 | time 465.3935 ms | tok/sec 1126547.6447
for step 11336 | loss 3.329510 | norm 0.3496 | time 464.6032 ms | tok/sec 1128464.0661
for step 11337 | loss 3.363271 | norm 0.3551 | time 465.6303 ms | tok/sec 1125974.8517
for step 11338 | loss 3.307124 | norm 0.4113 | time 464.9396 ms | tok/sec 1127647.5621
for step 11339 | loss 3.346507 | norm 0.3737 | time 466.1877 ms | tok/sec 1124628.5188
for step 11340 | loss 3.332175 | norm 0.3807 | time 465.5721 ms | tok/sec 1126115.5445
for step 11341 | loss 3.324236 | norm 0.3557 | time 465.5399 ms | tok/sec 1126193.4019
for step 11342 | loss 3.288931 | norm 0.3989 | time 464.9780 ms | tok/sec 1127554.4713
for step 11343 | loss 3.277003 | norm 0.3601 | time 464.9243 ms | tok/sec 1127684.5714
for step 11344 | loss 3.268532 | norm 0.3655 | time 465.4217 ms | tok/sec 1126479.5482
for step 11345 | loss 3.296699 | norm 0.3240 | time 465.5154 ms | tok/sec 1126252.8114
for step 11346 | loss 3.311446 | norm 0.3546 | time 464.9684 ms | tok/sec 1127577.5980
for step 11347 | loss 3.299631 | norm 0.3177 | time 465.6420 ms | tok/sec 1125946.6021
for step 11348 | loss 3.295308 | norm 0.3339 | time 465.2121 ms | tok/sec 1126987.0075
for step 11349 | loss 3.244849 | norm 0.3139 | time 465.1055 ms | tok/sec 1127245.2425
for step 11350 | loss 3.265627 | norm 0.3249 | time 465.7905 ms | tok/sec 1125587.5518
for step 11351 | loss 3.221465 | norm 0.3199 | time 466.0044 ms | tok/sec 1125070.9905
for step 11352 | loss 3.391274 | norm 0.3262 | time 465.7109 ms | tok/sec 1125780.0158
for step 11353 | loss 3.339600 | norm 0.2882 | time 465.6153 ms | tok/sec 1126011.1748
for step 11354 | loss 3.243037 | norm 0.3482 | time 465.6510 ms | tok/sec 1125924.6952
for step 11355 | loss 3.300823 | norm 0.3070 | time 465.6372 ms | tok/sec 1125958.1324
for step 11356 | loss 3.288480 | norm 0.3189 | time 465.7440 ms | tok/sec 1125699.9105
for step 11357 | loss 3.249060 | norm 0.3104 | time 465.6024 ms | tok/sec 1126042.3107
for step 11358 | loss 3.280675 | norm 0.2986 | time 465.8842 ms | tok/sec 1125361.1739
for step 11359 | loss 3.215637 | norm 0.2970 | time 465.8425 ms | tok/sec 1125461.9671
for step 11360 | loss 3.337035 | norm 0.2868 | time 465.4894 ms | tok/sec 1126315.6885
for step 11361 | loss 3.256709 | norm 0.2877 | time 466.1660 ms | tok/sec 1124680.8608
for step 11362 | loss 3.266910 | norm 0.3034 | time 465.8849 ms | tok/sec 1125359.4462
for step 11363 | loss 3.241837 | norm 0.2897 | time 465.1253 ms | tok/sec 1127197.2839
for step 11364 | loss 3.268956 | norm 0.2883 | time 465.3435 ms | tok/sec 1126668.8538
for step 11365 | loss 3.241151 | norm 0.3342 | time 465.3575 ms | tok/sec 1126634.7972
for step 11366 | loss 3.321232 | norm 0.3084 | time 466.7156 ms | tok/sec 1123356.5557
for step 11367 | loss 3.273671 | norm 0.3036 | time 465.9016 ms | tok/sec 1125319.1341
for step 11368 | loss 3.322588 | norm 0.2862 | time 464.8683 ms | tok/sec 1127820.4859
for step 11369 | loss 3.339907 | norm 0.3515 | time 466.2244 ms | tok/sec 1124539.9512
for step 11370 | loss 3.295451 | norm 0.3269 | time 465.4310 ms | tok/sec 1126457.0435
for step 11371 | loss 3.324965 | norm 0.3410 | time 465.5619 ms | tok/sec 1126140.3423
for step 11372 | loss 3.330403 | norm 0.3548 | time 465.8394 ms | tok/sec 1125469.4553
for step 11373 | loss 3.265365 | norm 0.3568 | time 464.4730 ms | tok/sec 1128780.3375
for step 11374 | loss 3.263069 | norm 0.3336 | time 466.0554 ms | tok/sec 1124947.8230
for step 11375 | loss 3.351554 | norm 0.3597 | time 464.8066 ms | tok/sec 1127970.3189
for step 11376 | loss 3.315494 | norm 0.3559 | time 464.4051 ms | tok/sec 1128945.4947
for step 11377 | loss 3.269387 | norm 0.3610 | time 465.3149 ms | tok/sec 1126738.1279
for step 11378 | loss 3.281675 | norm 0.3429 | time 465.2889 ms | tok/sec 1126801.0592
for step 11379 | loss 3.251055 | norm 0.3374 | time 465.9219 ms | tok/sec 1125270.1876
for step 11380 | loss 3.341202 | norm 0.2986 | time 464.8335 ms | tok/sec 1127904.9429
for step 11381 | loss 3.240523 | norm 0.3680 | time 467.0336 ms | tok/sec 1122591.5487
for step 11382 | loss 3.609994 | norm 0.6080 | time 465.7254 ms | tok/sec 1125744.8603
for step 11383 | loss 3.292880 | norm 0.4757 | time 465.1899 ms | tok/sec 1127040.7245
for step 11384 | loss 3.298531 | norm 0.3887 | time 465.4884 ms | tok/sec 1126317.9961
for step 11385 | loss 3.330842 | norm 0.4160 | time 465.9810 ms | tok/sec 1125127.4034
for step 11386 | loss 3.286426 | norm 0.3652 | time 466.3107 ms | tok/sec 1124331.8148
for step 11387 | loss 3.339358 | norm 0.3505 | time 465.4944 ms | tok/sec 1126303.5740
for step 11388 | loss 3.268487 | norm 0.3773 | time 465.3864 ms | tok/sec 1126564.9587
for step 11389 | loss 3.351553 | norm 0.3525 | time 465.4772 ms | tok/sec 1126345.1105
for step 11390 | loss 3.267852 | norm 0.3637 | time 465.6689 ms | tok/sec 1125881.4604
for step 11391 | loss 3.291039 | norm 0.3381 | time 465.6425 ms | tok/sec 1125945.4491
for step 11392 | loss 3.310137 | norm 0.3543 | time 465.3013 ms | tok/sec 1126771.0361
for step 11393 | loss 3.284484 | norm 0.3221 | time 465.3592 ms | tok/sec 1126630.7567
for step 11394 | loss 3.289556 | norm 0.3028 | time 464.9425 ms | tok/sec 1127640.6231
for step 11395 | loss 3.239776 | norm 0.3019 | time 465.2750 ms | tok/sec 1126834.5485
for step 11396 | loss 3.309997 | norm 0.3078 | time 464.7651 ms | tok/sec 1128071.0013
for step 11397 | loss 3.291480 | norm 0.3203 | time 465.2348 ms | tok/sec 1126932.1406
for step 11398 | loss 3.269356 | norm 0.3232 | time 464.0646 ms | tok/sec 1129773.7474
for step 11399 | loss 3.243845 | norm 0.2884 | time 465.8825 ms | tok/sec 1125365.2053
for step 11400 | loss 3.285995 | norm 0.3259 | time 465.2660 ms | tok/sec 1126856.4908
for step 11401 | loss 3.288970 | norm 0.3568 | time 465.5213 ms | tok/sec 1126238.3910
for step 11402 | loss 3.328029 | norm 0.3369 | time 466.3870 ms | tok/sec 1124147.8909
for step 11403 | loss 3.292598 | norm 0.3423 | time 466.1059 ms | tok/sec 1124825.8332
for step 11404 | loss 3.301099 | norm 0.3441 | time 466.5160 ms | tok/sec 1123837.0814
for step 11405 | loss 3.323686 | norm 0.3286 | time 464.9892 ms | tok/sec 1127527.2986
for step 11406 | loss 3.265624 | norm 0.3239 | time 466.5279 ms | tok/sec 1123808.3646
for step 11407 | loss 3.260629 | norm 0.3016 | time 465.1074 ms | tok/sec 1127240.6198
for step 11408 | loss 3.285300 | norm 0.3288 | time 465.5719 ms | tok/sec 1126116.1211
for step 11409 | loss 3.290446 | norm 0.3069 | time 465.5440 ms | tok/sec 1126183.5970
for step 11410 | loss 3.258035 | norm 0.3304 | time 465.7433 ms | tok/sec 1125701.6393
for step 11411 | loss 3.271210 | norm 0.2968 | time 465.8709 ms | tok/sec 1125393.4258
for step 11412 | loss 3.262704 | norm 0.3762 | time 465.4474 ms | tok/sec 1126417.2297
for step 11413 | loss 3.326185 | norm 0.3485 | time 465.8694 ms | tok/sec 1125396.8814
for step 11414 | loss 3.248064 | norm 0.3746 | time 465.7476 ms | tok/sec 1125691.2667
for step 11415 | loss 3.268821 | norm 0.3507 | time 465.0850 ms | tok/sec 1127294.9388
for step 11416 | loss 3.307916 | norm 0.3474 | time 465.4641 ms | tok/sec 1126376.8418
for step 11417 | loss 3.343406 | norm 0.3209 | time 465.1299 ms | tok/sec 1127186.3060
for step 11418 | loss 3.300295 | norm 0.3221 | time 465.4653 ms | tok/sec 1126373.9571
for step 11419 | loss 3.316913 | norm 0.3377 | time 464.9997 ms | tok/sec 1127501.8615
for step 11420 | loss 3.320479 | norm 0.2900 | time 465.7254 ms | tok/sec 1125744.8603
for step 11421 | loss 3.321103 | norm 0.3365 | time 465.3275 ms | tok/sec 1126707.5308
for step 11422 | loss 3.305753 | norm 0.3162 | time 466.2476 ms | tok/sec 1124484.1723
for step 11423 | loss 3.265786 | norm 0.3339 | time 466.6066 ms | tok/sec 1123618.8706
for step 11424 | loss 3.250202 | norm 0.3053 | time 465.5683 ms | tok/sec 1126124.7714
for step 11425 | loss 3.224086 | norm 0.3154 | time 466.2261 ms | tok/sec 1124535.9257
for step 11426 | loss 3.250450 | norm 0.3101 | time 465.6866 ms | tok/sec 1125838.8053
for step 11427 | loss 3.200092 | norm 0.3042 | time 466.0230 ms | tok/sec 1125026.0946
for step 11428 | loss 3.300955 | norm 0.2873 | time 464.4613 ms | tok/sec 1128808.7295
Will loading at 0 from edu_fineweb10B/edufineweb_train_000061.npy
for step 11429 | loss 3.254179 | norm 0.2935 | time 2574.1813 ms | tok/sec 203671.7446
for step 11430 | loss 3.272830 | norm 0.3067 | time 483.1138 ms | tok/sec 1085226.7876
for step 11431 | loss 3.237923 | norm 0.2862 | time 463.5854 ms | tok/sec 1130941.6265
for step 11432 | loss 3.258036 | norm 0.3109 | time 464.1714 ms | tok/sec 1129513.7726
for step 11433 | loss 3.187114 | norm 0.3256 | time 464.7312 ms | tok/sec 1128153.1807
for step 11434 | loss 3.274485 | norm 0.3233 | time 468.0765 ms | tok/sec 1120090.4903
for step 11435 | loss 3.246103 | norm 0.3679 | time 463.1987 ms | tok/sec 1131885.8262
for step 11436 | loss 3.309203 | norm 0.3814 | time 464.8194 ms | tok/sec 1127939.0763
for step 11437 | loss 3.263188 | norm 0.3315 | time 464.6778 ms | tok/sec 1128282.8402
for step 11438 | loss 3.294988 | norm 0.3519 | time 464.4184 ms | tok/sec 1128913.0390
for step 11439 | loss 3.299444 | norm 0.3120 | time 464.8242 ms | tok/sec 1127927.5054
for step 11440 | loss 3.310650 | norm 0.3768 | time 465.8668 ms | tok/sec 1125403.2169
for step 11441 | loss 3.305059 | norm 0.3026 | time 465.4777 ms | tok/sec 1126343.9567
for step 11442 | loss 3.298193 | norm 0.3825 | time 466.4907 ms | tok/sec 1123897.9658
for step 11443 | loss 3.320321 | norm 0.2816 | time 464.9451 ms | tok/sec 1127634.2625
for step 11444 | loss 3.294644 | norm 0.3269 | time 464.8893 ms | tok/sec 1127769.5864
for step 11445 | loss 3.269274 | norm 0.3201 | time 464.7706 ms | tok/sec 1128057.6917
for step 11446 | loss 3.414928 | norm 0.3550 | time 465.9758 ms | tok/sec 1125140.0683
for step 11447 | loss 3.300111 | norm 0.3718 | time 466.6789 ms | tok/sec 1123444.9370
for step 11448 | loss 3.304300 | norm 0.2877 | time 464.9689 ms | tok/sec 1127576.4416
for step 11449 | loss 3.290049 | norm 0.3566 | time 466.1720 ms | tok/sec 1124666.4806
for step 11450 | loss 3.301220 | norm 0.3067 | time 464.8235 ms | tok/sec 1127929.2410
for step 11451 | loss 3.204991 | norm 0.3301 | time 464.9012 ms | tok/sec 1127740.6683
for step 11452 | loss 3.323851 | norm 0.3236 | time 465.8036 ms | tok/sec 1125555.8649
for step 11453 | loss 3.259199 | norm 0.3245 | time 465.2281 ms | tok/sec 1126948.3114
for step 11454 | loss 3.281637 | norm 0.3164 | time 466.4123 ms | tok/sec 1124086.9794
for step 11455 | loss 3.250613 | norm 0.3013 | time 465.4033 ms | tok/sec 1126523.9831
for step 11456 | loss 3.241630 | norm 0.3067 | time 465.6737 ms | tok/sec 1125869.9317
for step 11457 | loss 3.322821 | norm 0.3025 | time 465.5850 ms | tok/sec 1126084.4045
for step 11458 | loss 3.291357 | norm 0.3027 | time 465.4784 ms | tok/sec 1126342.2259
for step 11459 | loss 3.285070 | norm 0.3253 | time 465.6618 ms | tok/sec 1125898.7539
for step 11460 | loss 3.271297 | norm 0.3047 | time 466.5897 ms | tok/sec 1123659.6352
for step 11461 | loss 3.287167 | norm 0.3188 | time 465.7304 ms | tok/sec 1125732.7580
for step 11462 | loss 3.258980 | norm 0.3168 | time 465.4491 ms | tok/sec 1126413.1908
for step 11463 | loss 3.250536 | norm 0.3252 | time 466.1326 ms | tok/sec 1124761.3964
for step 11464 | loss 3.306857 | norm 0.3043 | time 466.0439 ms | tok/sec 1124975.4470
for step 11465 | loss 3.284126 | norm 0.3120 | time 465.6963 ms | tok/sec 1125815.1735
for step 11466 | loss 3.257985 | norm 0.3122 | time 465.8363 ms | tok/sec 1125476.9436
for step 11467 | loss 3.261884 | norm 0.3060 | time 466.3246 ms | tok/sec 1124298.4741
for step 11468 | loss 3.232098 | norm 0.3130 | time 465.9495 ms | tok/sec 1125203.3969
for step 11469 | loss 3.262815 | norm 0.3460 | time 465.9288 ms | tok/sec 1125253.4892
for step 11470 | loss 3.259598 | norm 0.3313 | time 465.6911 ms | tok/sec 1125827.8539
for step 11471 | loss 3.268391 | norm 0.3155 | time 465.4622 ms | tok/sec 1126381.4574
for step 11472 | loss 3.305152 | norm 0.3413 | time 468.5733 ms | tok/sec 1118902.7723
for step 11473 | loss 3.291498 | norm 0.3114 | time 465.8575 ms | tok/sec 1125425.6795
for step 11474 | loss 3.289230 | norm 0.3896 | time 466.3811 ms | tok/sec 1124162.2578
for step 11475 | loss 3.295669 | norm 0.3202 | time 466.4207 ms | tok/sec 1124066.8685
for step 11476 | loss 3.319387 | norm 0.3376 | time 466.3882 ms | tok/sec 1124145.0175
for step 11477 | loss 3.267584 | norm 0.3240 | time 465.7300 ms | tok/sec 1125733.9106
for step 11478 | loss 3.200739 | norm 0.3734 | time 466.9433 ms | tok/sec 1122808.7873
for step 11479 | loss 3.298440 | norm 0.3661 | time 466.3172 ms | tok/sec 1124316.2939
for step 11480 | loss 3.332165 | norm 0.3528 | time 465.8108 ms | tok/sec 1125538.5819
for step 11481 | loss 3.258397 | norm 0.3637 | time 466.2704 ms | tok/sec 1124428.9738
for step 11482 | loss 3.388049 | norm 0.4078 | time 465.9562 ms | tok/sec 1125187.2762
for step 11483 | loss 3.226173 | norm 0.4204 | time 466.7051 ms | tok/sec 1123381.8061
for step 11484 | loss 3.347028 | norm 0.3907 | time 466.5699 ms | tok/sec 1123707.2932
for step 11485 | loss 3.340328 | norm 0.3965 | time 466.3074 ms | tok/sec 1124339.8628
for step 11486 | loss 3.300596 | norm 0.3738 | time 466.4369 ms | tok/sec 1124027.7981
for step 11487 | loss 3.281291 | norm 0.3524 | time 465.4455 ms | tok/sec 1126421.8457
for step 11488 | loss 3.323245 | norm 0.3707 | time 466.7513 ms | tok/sec 1123270.4834
for step 11489 | loss 3.254212 | norm 0.3782 | time 465.8496 ms | tok/sec 1125444.6870
for step 11490 | loss 3.328682 | norm 0.3814 | time 465.7893 ms | tok/sec 1125590.4325
for step 11491 | loss 3.284755 | norm 0.3627 | time 466.4311 ms | tok/sec 1124041.5873
for step 11492 | loss 3.317548 | norm 0.3544 | time 466.1901 ms | tok/sec 1124622.7672
for step 11493 | loss 3.278058 | norm 0.3488 | time 465.5898 ms | tok/sec 1126072.8716
for step 11494 | loss 3.231838 | norm 0.3641 | time 465.6520 ms | tok/sec 1125922.3893
for step 11495 | loss 3.239846 | norm 0.3451 | time 465.6210 ms | tok/sec 1125997.3371
for step 11496 | loss 3.247898 | norm 0.3072 | time 465.2286 ms | tok/sec 1126947.1563
for step 11497 | loss 3.298798 | norm 0.3302 | time 466.4919 ms | tok/sec 1123895.0938
for step 11498 | loss 3.266498 | norm 0.3100 | time 465.2965 ms | tok/sec 1126782.5832
for step 11499 | loss 3.240104 | norm 0.2984 | time 465.7953 ms | tok/sec 1125576.0291
validation loss 3.3006
HellaSwag accuracy: 2755/10042=0.2743
> Hello, I'm a language model, and I can't say how I might use an expression instead. I'm the one with the "I" in my
> Hello, I'm a language model, and you are able to model, edit, and make inferences.
This helps an examiner the most.
I
> Hello, I'm a language model, I usually start with a simple sentence and then I think, hey, how is it that we all do this to turn
> Hello, I'm a language model, and I'm also an active participant in planning and doing design activities at I2C which may interest you. As a
> Hello, I'm a language model, and I want to understand a thing in languages but I'm not even 100% sure if I ever read an English dictionary
> Hello, I'm a language model, so when I'm writing, the term "langualts" doesn't mean that the machine has many "models
> Hello, I'm a language model, so I really don't get excited to talk about it. You have to be aware that the language model is not your
> Hello, I'm a language model, and here's what I mean: "A way to describe a computer, or make something of it's own" from
> Hello, I'm a language model, so I need to add and delete all this. This is what I'll discuss in future on Chapter 13.
The
> Hello, I'm a language model, here we go, let's assume that a program written in Python is valid if a text string is an array by default
> Hello, I'm a language model, but some of you probably can't find any great books on my list.
Most of you might feel like writing your
> Hello, I'm a language model, and it tells that what we want to see on a screen like a radio, our eyes, but we can't really
> > Hello, I'm a language model, this blog is a series of articles which are based on my work at CELT. CELT has a focus
Hello, I'm a language model, and we are doing it the way we do. And, you know, we all know that there are three things to
> > > Hello, I'm a language model, and I'll be talking to you right now. I'm actually a mother and husband, and how are we going to
Hello, I'm a language model, and I don't even have an idea until I am in the computer!
Let's say I am trying to break
> Hello, I'm a language model, because I learn from each other. Now instead of only playing games, I'm learning from other languages and learning from me
> > Hello, I'm a language model, I can model it using the English language in the UML. For me, the English language isn't just one,
Hello, I'm a language model, using EMC. I don't worry about the vocabulary from the source code, as this is a new language model with
> Hello, I'm a language model, and I'll probably want to start at the first letter of my favorite song but I'm not sure how this one is
> > Hello, I'm a language model, so I'll explain what I am really learning here.
A model describes an algorithm which attempts to perform a mathematical problem
> Hello, I'm a language model, and there are so many people. But the good news is, languages are easier to model. There are different models,
Hello, I'm a language model, and I am one of the main actors on the entire network, and am very excited about the way you can interact with
> Hello, I'm a language model, and I don't want to get confused with that. I'm trying to figure out the math behind how languages work.
> > Hello, I'm a language model, and i am a social engineer. I learned at an early age that I use it as often as my mom was working
Hello, I'm a language model, and you've been running this website from the comfort of your own home.
If you're not a native speaker,
Hello, I'm a language model, at the age of 16 and I’m a language learner. I’m here to help you as
> Hello, I'm a language model, thanks for sharing!
How do you model an event with a message?
It looks like…
The model,
> Hello, I'm a language model, and I'm a student and a scientist.
A Language Model
Many reasons I find this essay easy. But a
> Hello, I'm a language model, and this article is about real data and real data. How do I know how to use real data in my projects or
> Hello, I'm a language model, and you know there's a little bit of logic to it, but you also know there's only a little hint I
> Hello, I'm a language model, and as a beginner, I don't have to learn all languages in this world. And my English would probably be too
for step 11500 | loss 3.231241 | norm 0.3280 | time 12443.9704 ms | tok/sec 42131.8905
for step 11501 | loss 3.303721 | norm 0.3100 | time 462.5883 ms | tok/sec 1133379.2670
for step 11502 | loss 3.232736 | norm 0.3290 | time 463.7687 ms | tok/sec 1130494.5260
for step 11503 | loss 3.220622 | norm 0.2952 | time 463.0833 ms | tok/sec 1132167.8781
for step 11504 | loss 3.247832 | norm 0.3384 | time 464.0100 ms | tok/sec 1129906.6825
for step 11505 | loss 3.335512 | norm 0.3681 | time 463.9418 ms | tok/sec 1130072.7502
for step 11506 | loss 3.331645 | norm 0.3219 | time 463.4664 ms | tok/sec 1131231.9369
for step 11507 | loss 3.296167 | norm 0.3079 | time 463.0995 ms | tok/sec 1132128.2424
for step 11508 | loss 3.238374 | norm 0.3099 | time 463.6207 ms | tok/sec 1130855.5511
for step 11509 | loss 3.406719 | norm 0.3115 | time 464.1001 ms | tok/sec 1129687.2693
for step 11510 | loss 3.299068 | norm 0.3180 | time 463.3365 ms | tok/sec 1131549.1797
for step 11511 | loss 3.285217 | norm 0.3339 | time 463.8221 ms | tok/sec 1130364.3577
for step 11512 | loss 3.248137 | norm 0.2897 | time 463.6807 ms | tok/sec 1130709.0204
for step 11513 | loss 3.387978 | norm 0.3512 | time 464.5116 ms | tok/sec 1128686.4803
for step 11514 | loss 3.339494 | norm 0.3613 | time 464.9992 ms | tok/sec 1127503.0177
for step 11515 | loss 3.306101 | norm 0.3483 | time 464.4299 ms | tok/sec 1128885.2212
for step 11516 | loss 3.319303 | norm 0.3412 | time 464.6461 ms | tok/sec 1128359.8396
for step 11517 | loss 3.283219 | norm 0.3552 | time 464.7703 ms | tok/sec 1128058.2704
for step 11518 | loss 3.301624 | norm 0.3278 | time 465.1186 ms | tok/sec 1127213.4622
for step 11519 | loss 3.331166 | norm 0.3309 | time 465.1494 ms | tok/sec 1127138.9301
for step 11520 | loss 3.283648 | norm 0.3277 | time 464.3373 ms | tok/sec 1129110.1205
for step 11521 | loss 3.283163 | norm 0.3460 | time 465.1186 ms | tok/sec 1127213.4622
for step 11522 | loss 3.272058 | norm 0.3111 | time 464.8764 ms | tok/sec 1127800.8196
for step 11523 | loss 3.314183 | norm 0.3144 | time 464.9343 ms | tok/sec 1127660.2838
for step 11524 | loss 3.317080 | norm 0.3207 | time 465.2405 ms | tok/sec 1126918.2804
for step 11525 | loss 3.264966 | norm 0.3202 | time 465.5731 ms | tok/sec 1126113.2377
for step 11526 | loss 3.316694 | norm 0.3305 | time 464.7439 ms | tok/sec 1128122.5067
for step 11527 | loss 3.319087 | norm 0.3320 | time 466.3863 ms | tok/sec 1124149.6149
for step 11528 | loss 3.267701 | norm 0.3365 | time 464.7386 ms | tok/sec 1128135.2391
for step 11529 | loss 3.417942 | norm 0.4092 | time 465.5921 ms | tok/sec 1126067.1053
for step 11530 | loss 3.264159 | norm 0.3089 | time 466.4466 ms | tok/sec 1124004.2422
for step 11531 | loss 3.265476 | norm 0.3084 | time 465.0784 ms | tok/sec 1127311.1200
for step 11532 | loss 3.213320 | norm 0.2989 | time 465.5731 ms | tok/sec 1126113.2377
for step 11533 | loss 3.261446 | norm 0.3030 | time 465.3387 ms | tok/sec 1126680.3989
for step 11534 | loss 3.326406 | norm 0.3092 | time 465.3120 ms | tok/sec 1126745.0558
for step 11535 | loss 3.284068 | norm 0.3250 | time 465.6496 ms | tok/sec 1125928.1541
for step 11536 | loss 3.234633 | norm 0.3028 | time 464.2413 ms | tok/sec 1129343.8092
for step 11537 | loss 3.215694 | norm 0.2959 | time 465.2822 ms | tok/sec 1126817.2262
for step 11538 | loss 3.266916 | norm 0.3182 | time 465.7242 ms | tok/sec 1125747.7418
for step 11539 | loss 3.242233 | norm 0.3354 | time 466.4779 ms | tok/sec 1123928.9850
for step 11540 | loss 3.194786 | norm 0.3552 | time 464.6943 ms | tok/sec 1128242.8973
for step 11541 | loss 3.312019 | norm 0.3758 | time 465.1828 ms | tok/sec 1127058.0537
for step 11542 | loss 3.341925 | norm 0.4063 | time 466.7153 ms | tok/sec 1123357.1296
for step 11543 | loss 3.271875 | norm 0.3513 | time 465.1263 ms | tok/sec 1127194.9727
for step 11544 | loss 3.340680 | norm 0.3414 | time 465.1132 ms | tok/sec 1127226.7519
for step 11545 | loss 3.335850 | norm 0.3415 | time 465.7037 ms | tok/sec 1125797.3062
for step 11546 | loss 3.286756 | norm 0.3560 | time 465.9114 ms | tok/sec 1125295.5241
for step 11547 | loss 3.329921 | norm 0.3176 | time 464.1924 ms | tok/sec 1129462.7202
for step 11548 | loss 3.264056 | norm 0.3428 | time 465.9574 ms | tok/sec 1125184.3976
for step 11549 | loss 3.330364 | norm 0.3507 | time 465.0538 ms | tok/sec 1127370.6475
for step 11550 | loss 3.330973 | norm 0.3888 | time 464.7803 ms | tok/sec 1128033.9666
for step 11551 | loss 3.305421 | norm 0.3353 | time 464.6268 ms | tok/sec 1128406.7391
for step 11552 | loss 3.289501 | norm 0.3927 | time 465.4531 ms | tok/sec 1126403.3821
for step 11553 | loss 3.328297 | norm 0.3161 | time 464.6120 ms | tok/sec 1128442.6402
for step 11554 | loss 3.333139 | norm 0.3397 | time 465.2631 ms | tok/sec 1126863.4201
for step 11555 | loss 3.277421 | norm 0.3052 | time 464.3645 ms | tok/sec 1129044.0325
for step 11556 | loss 3.300606 | norm 0.3523 | time 465.4841 ms | tok/sec 1126328.3802
for step 11557 | loss 3.267295 | norm 0.3122 | time 464.9453 ms | tok/sec 1127633.6842
for step 11558 | loss 3.306860 | norm 0.3010 | time 465.4503 ms | tok/sec 1126410.3059
for step 11559 | loss 3.302308 | norm 0.3308 | time 466.1200 ms | tok/sec 1124791.8879
for step 11560 | loss 3.312576 | norm 0.3198 | time 465.1661 ms | tok/sec 1127098.4904
for step 11561 | loss 3.287783 | norm 0.3213 | time 465.6532 ms | tok/sec 1125919.5068
for step 11562 | loss 3.376999 | norm 0.3020 | time 464.7563 ms | tok/sec 1128092.4131
for step 11563 | loss 3.247150 | norm 0.3178 | time 464.9875 ms | tok/sec 1127531.3455
for step 11564 | loss 3.292938 | norm 0.3230 | time 464.7388 ms | tok/sec 1128134.6604
for step 11565 | loss 3.320052 | norm 0.3357 | time 465.6286 ms | tok/sec 1125978.8875
for step 11566 | loss 3.334610 | norm 0.4284 | time 465.7140 ms | tok/sec 1125772.5234
for step 11567 | loss 3.277732 | norm 0.3029 | time 465.3072 ms | tok/sec 1126756.6024
for step 11568 | loss 3.264166 | norm 0.3700 | time 465.1544 ms | tok/sec 1127126.7979
for step 11569 | loss 3.265573 | norm 0.3119 | time 465.2023 ms | tok/sec 1127010.6886
for step 11570 | loss 3.244760 | norm 0.3061 | time 465.2655 ms | tok/sec 1126857.6457
for step 11571 | loss 3.229419 | norm 0.2994 | time 464.7777 ms | tok/sec 1128040.3318
for step 11572 | loss 3.256252 | norm 0.3174 | time 464.4938 ms | tok/sec 1128729.9308
for step 11573 | loss 3.255009 | norm 0.3159 | time 465.5399 ms | tok/sec 1126193.4019
for step 11574 | loss 3.238388 | norm 0.2837 | time 465.2123 ms | tok/sec 1126986.4300
for step 11575 | loss 3.321378 | norm 0.3090 | time 465.7912 ms | tok/sec 1125585.8234
for step 11576 | loss 3.287953 | norm 0.2966 | time 465.4982 ms | tok/sec 1126294.3441
for step 11577 | loss 3.326081 | norm 0.3413 | time 465.3900 ms | tok/sec 1126556.3016
for step 11578 | loss 3.362118 | norm 0.3201 | time 464.3784 ms | tok/sec 1129010.4118
for step 11579 | loss 3.271679 | norm 0.3561 | time 465.7838 ms | tok/sec 1125603.6840
for step 11580 | loss 3.218395 | norm 0.3347 | time 465.2786 ms | tok/sec 1126825.8873
for step 11581 | loss 3.300208 | norm 0.3236 | time 465.2047 ms | tok/sec 1127004.9126
for step 11582 | loss 3.324781 | norm 0.3629 | time 465.3234 ms | tok/sec 1126717.3448
for step 11583 | loss 3.313929 | norm 0.3203 | time 466.0604 ms | tok/sec 1124935.7379
for step 11584 | loss 3.351205 | norm 0.3383 | time 465.5464 ms | tok/sec 1126177.8295
for step 11585 | loss 3.350136 | norm 0.3287 | time 465.2147 ms | tok/sec 1126980.6542
for step 11586 | loss 3.441541 | norm 0.4132 | time 464.6828 ms | tok/sec 1128270.6834
for step 11587 | loss 3.248022 | norm 0.3683 | time 465.3578 ms | tok/sec 1126634.2200
for step 11588 | loss 3.251952 | norm 0.3407 | time 465.0064 ms | tok/sec 1127485.6749
for step 11589 | loss 3.323951 | norm 0.3865 | time 465.6556 ms | tok/sec 1125913.7421
for step 11590 | loss 3.301659 | norm 0.3320 | time 465.2889 ms | tok/sec 1126801.0592
for step 11591 | loss 3.325225 | norm 0.3446 | time 466.2669 ms | tok/sec 1124437.5982
for step 11592 | loss 3.304740 | norm 0.3282 | time 465.5092 ms | tok/sec 1126267.8090
for step 11593 | loss 3.292867 | norm 0.3418 | time 465.1656 ms | tok/sec 1127099.6458
for step 11594 | loss 3.304200 | norm 0.3367 | time 465.5910 ms | tok/sec 1126069.9885
for step 11595 | loss 3.392132 | norm 0.3438 | time 466.2807 ms | tok/sec 1124404.2513
for step 11596 | loss 3.288305 | norm 0.3250 | time 465.0061 ms | tok/sec 1127486.2529
for step 11597 | loss 3.316135 | norm 0.3410 | time 465.2822 ms | tok/sec 1126817.2262
for step 11598 | loss 3.307259 | norm 0.3351 | time 465.6062 ms | tok/sec 1126033.0850
for step 11599 | loss 3.300618 | norm 0.3384 | time 465.6410 ms | tok/sec 1125948.9081
for step 11600 | loss 3.229348 | norm 0.3380 | time 465.3494 ms | tok/sec 1126654.4228
for step 11601 | loss 3.220888 | norm 0.2945 | time 465.5931 ms | tok/sec 1126064.7988
for step 11602 | loss 3.274679 | norm 0.3068 | time 464.8938 ms | tok/sec 1127758.5974
for step 11603 | loss 3.245467 | norm 0.3208 | time 465.0948 ms | tok/sec 1127271.2458
for step 11604 | loss 3.281192 | norm 0.3225 | time 465.4038 ms | tok/sec 1126522.8289
for step 11605 | loss 3.255529 | norm 0.3113 | time 464.9997 ms | tok/sec 1127501.8615
for step 11606 | loss 3.230632 | norm 0.2900 | time 466.1264 ms | tok/sec 1124776.3542
for step 11607 | loss 3.309757 | norm 0.3123 | time 465.5988 ms | tok/sec 1126050.9598
for step 11608 | loss 3.272920 | norm 0.2985 | time 466.0347 ms | tok/sec 1124997.8925
for step 11609 | loss 3.236248 | norm 0.3034 | time 465.8074 ms | tok/sec 1125546.6473
for step 11610 | loss 3.220219 | norm 0.3042 | time 465.0538 ms | tok/sec 1127370.6475
for step 11611 | loss 3.241840 | norm 0.2940 | time 465.6551 ms | tok/sec 1125914.8950
for step 11612 | loss 3.318728 | norm 0.3119 | time 465.4183 ms | tok/sec 1126487.6270
for step 11613 | loss 3.302959 | norm 0.3099 | time 465.4465 ms | tok/sec 1126419.5377
for step 11614 | loss 3.242519 | norm 0.3185 | time 466.3775 ms | tok/sec 1124170.8781
for step 11615 | loss 3.328984 | norm 0.3324 | time 465.5030 ms | tok/sec 1126282.8070
for step 11616 | loss 3.296586 | norm 0.3358 | time 466.0015 ms | tok/sec 1125077.8979
for step 11617 | loss 3.314621 | norm 0.3329 | time 465.4830 ms | tok/sec 1126331.2647
for step 11618 | loss 3.217944 | norm 0.3507 | time 465.1456 ms | tok/sec 1127148.1739
for step 11619 | loss 3.234527 | norm 0.3429 | time 465.1959 ms | tok/sec 1127026.2840
Will loading at 0 from edu_fineweb10B/edufineweb_train_000062.npy
for step 11620 | loss 3.273074 | norm 0.3490 | time 2585.2749 ms | tok/sec 202797.7732
for step 11621 | loss 3.304395 | norm 0.3437 | time 463.1102 ms | tok/sec 1132102.0145
for step 11622 | loss 3.235516 | norm 0.3476 | time 462.5881 ms | tok/sec 1133379.8512
for step 11623 | loss 3.284283 | norm 0.3432 | time 464.1593 ms | tok/sec 1129543.3619
for step 11624 | loss 3.311615 | norm 0.3449 | time 463.4953 ms | tok/sec 1131161.5273
for step 11625 | loss 3.302468 | norm 0.3339 | time 464.4384 ms | tok/sec 1128864.3588
for step 11626 | loss 3.314055 | norm 0.3033 | time 464.8962 ms | tok/sec 1127752.8137
for step 11627 | loss 3.307789 | norm 0.3481 | time 464.8683 ms | tok/sec 1127820.4859
for step 11628 | loss 3.325254 | norm 0.3262 | time 464.7961 ms | tok/sec 1127995.7771
for step 11629 | loss 3.271101 | norm 0.3290 | time 464.8552 ms | tok/sec 1127852.3004
for step 11630 | loss 3.283721 | norm 0.3121 | time 464.7717 ms | tok/sec 1128054.7983
for step 11631 | loss 3.324407 | norm 0.3233 | time 464.8538 ms | tok/sec 1127855.7712
for step 11632 | loss 3.308035 | norm 0.3468 | time 465.7154 ms | tok/sec 1125769.0655
for step 11633 | loss 3.310552 | norm 0.3100 | time 464.0980 ms | tok/sec 1129692.4924
for step 11634 | loss 3.242978 | norm 0.3027 | time 465.5223 ms | tok/sec 1126236.0838
for step 11635 | loss 3.228537 | norm 0.2886 | time 465.0373 ms | tok/sec 1127410.5287
for step 11636 | loss 3.315946 | norm 0.3081 | time 465.3821 ms | tok/sec 1126575.3473
for step 11637 | loss 3.282606 | norm 0.2885 | time 465.9910 ms | tok/sec 1125103.2257
for step 11638 | loss 3.278104 | norm 0.2842 | time 466.1262 ms | tok/sec 1124776.9296
for step 11639 | loss 3.256141 | norm 0.2888 | time 466.9905 ms | tok/sec 1122695.2854
for step 11640 | loss 3.252748 | norm 0.2839 | time 466.3076 ms | tok/sec 1124339.2879
for step 11641 | loss 3.254890 | norm 0.3008 | time 466.3272 ms | tok/sec 1124292.1511
for step 11642 | loss 3.215014 | norm 0.3018 | time 466.5895 ms | tok/sec 1123660.2093
for step 11643 | loss 3.290336 | norm 0.3251 | time 466.2573 ms | tok/sec 1124460.5973
for step 11644 | loss 3.290629 | norm 0.2976 | time 466.6166 ms | tok/sec 1123594.7578
for step 11645 | loss 3.252458 | norm 0.3226 | time 466.4338 ms | tok/sec 1124035.2672
for step 11646 | loss 3.338812 | norm 0.3003 | time 465.2858 ms | tok/sec 1126808.5653
for step 11647 | loss 3.305359 | norm 0.3246 | time 466.1825 ms | tok/sec 1124641.1725
for step 11648 | loss 3.298366 | norm 0.2931 | time 466.4686 ms | tok/sec 1123951.3887
for step 11649 | loss 3.304814 | norm 0.3215 | time 465.5068 ms | tok/sec 1126273.5774
for step 11650 | loss 3.289047 | norm 0.3162 | time 468.8699 ms | tok/sec 1118194.9887
for step 11651 | loss 3.339431 | norm 0.3286 | time 465.3840 ms | tok/sec 1126570.7301
for step 11652 | loss 3.405540 | norm 0.3688 | time 465.9941 ms | tok/sec 1125095.7424
for step 11653 | loss 3.268073 | norm 0.3173 | time 465.7488 ms | tok/sec 1125688.3855
for step 11654 | loss 3.254551 | norm 0.3378 | time 482.2600 ms | tok/sec 1087148.0367
for step 11655 | loss 3.303212 | norm 0.3089 | time 464.3421 ms | tok/sec 1129098.5255
for step 11656 | loss 3.315210 | norm 0.3406 | time 464.2451 ms | tok/sec 1129334.5294
for step 11657 | loss 3.367446 | norm 0.3341 | time 464.3235 ms | tok/sec 1129143.7471
for step 11658 | loss 3.284589 | norm 0.3649 | time 465.7843 ms | tok/sec 1125602.5317
for step 11659 | loss 3.266868 | norm 0.3181 | time 463.8236 ms | tok/sec 1130360.8715
for step 11660 | loss 3.349253 | norm 0.3600 | time 464.3605 ms | tok/sec 1129053.8873
for step 11661 | loss 3.303479 | norm 0.3231 | time 464.1354 ms | tok/sec 1129601.3847
for step 11662 | loss 3.289183 | norm 0.3298 | time 464.0830 ms | tok/sec 1129729.0557
for step 11663 | loss 3.311601 | norm 0.3264 | time 463.9680 ms | tok/sec 1130008.8722
for step 11664 | loss 3.258887 | norm 0.3187 | time 465.3192 ms | tok/sec 1126727.7362
for step 11665 | loss 3.240190 | norm 0.3314 | time 465.3945 ms | tok/sec 1126545.3362
for step 11666 | loss 3.443173 | norm 0.4262 | time 465.3652 ms | tok/sec 1126616.3267
for step 11667 | loss 3.268688 | norm 0.3799 | time 465.2123 ms | tok/sec 1126986.4300
for step 11668 | loss 3.243482 | norm 0.3519 | time 464.3521 ms | tok/sec 1129074.1770
for step 11669 | loss 3.327842 | norm 0.3554 | time 465.1668 ms | tok/sec 1127096.7574
for step 11670 | loss 3.342092 | norm 0.3626 | time 464.6211 ms | tok/sec 1128420.6361
for step 11671 | loss 3.247573 | norm 0.3453 | time 465.0898 ms | tok/sec 1127283.3812
for step 11672 | loss 3.310898 | norm 0.3352 | time 465.8475 ms | tok/sec 1125449.8710
for step 11673 | loss 3.252115 | norm 0.3440 | time 465.6105 ms | tok/sec 1126022.7064
for step 11674 | loss 3.248969 | norm 0.3253 | time 465.0152 ms | tok/sec 1127464.2861
for step 11675 | loss 3.300050 | norm 0.3326 | time 464.8139 ms | tok/sec 1127952.3831
for step 11676 | loss 3.234355 | norm 0.3292 | time 466.5942 ms | tok/sec 1123648.7261
for step 11677 | loss 3.291153 | norm 0.3108 | time 465.6761 ms | tok/sec 1125864.1674
for step 11678 | loss 3.310275 | norm 0.2993 | time 465.6773 ms | tok/sec 1125861.2853
for step 11679 | loss 3.247307 | norm 0.3047 | time 465.2424 ms | tok/sec 1126913.6603
for step 11680 | loss 3.276475 | norm 0.2746 | time 466.3558 ms | tok/sec 1124223.1774
for step 11681 | loss 3.222136 | norm 0.3147 | time 465.2984 ms | tok/sec 1126777.9643
for step 11682 | loss 3.243334 | norm 0.3120 | time 466.0823 ms | tok/sec 1124882.7968
for step 11683 | loss 3.332987 | norm 0.3029 | time 466.5658 ms | tok/sec 1123717.0550
for step 11684 | loss 3.287384 | norm 0.3229 | time 465.8997 ms | tok/sec 1125323.7411
for step 11685 | loss 3.309430 | norm 0.3267 | time 466.4800 ms | tok/sec 1123923.8150
for step 11686 | loss 3.260908 | norm 0.3092 | time 465.7123 ms | tok/sec 1125776.5578
for step 11687 | loss 3.425015 | norm 0.3488 | time 465.5120 ms | tok/sec 1126260.8870
for step 11688 | loss 3.310038 | norm 0.3410 | time 465.7574 ms | tok/sec 1125667.6411
for step 11689 | loss 3.293952 | norm 0.3436 | time 465.6663 ms | tok/sec 1125887.8013
for step 11690 | loss 3.288312 | norm 0.3094 | time 465.7488 ms | tok/sec 1125688.3855
for step 11691 | loss 3.296236 | norm 0.3564 | time 466.0766 ms | tok/sec 1124896.6071
for step 11692 | loss 3.311607 | norm 0.3424 | time 465.6601 ms | tok/sec 1125902.7892
for step 11693 | loss 3.311580 | norm 0.3274 | time 465.8210 ms | tok/sec 1125513.8106
for step 11694 | loss 3.320315 | norm 0.3426 | time 465.9941 ms | tok/sec 1125095.7424
for step 11695 | loss 3.333788 | norm 0.2724 | time 465.5795 ms | tok/sec 1126097.6676
for step 11696 | loss 3.291047 | norm 0.3492 | time 465.9946 ms | tok/sec 1125094.5911
for step 11697 | loss 3.287313 | norm 0.3419 | time 465.8642 ms | tok/sec 1125409.5524
for step 11698 | loss 3.318265 | norm 0.3239 | time 465.9417 ms | tok/sec 1125222.3969
for step 11699 | loss 3.264787 | norm 0.3047 | time 465.6358 ms | tok/sec 1125961.5915
for step 11700 | loss 3.274207 | norm 0.3280 | time 465.9135 ms | tok/sec 1125290.3416
for step 11701 | loss 3.246777 | norm 0.3008 | time 465.2426 ms | tok/sec 1126913.0828
for step 11702 | loss 3.270983 | norm 0.3134 | time 465.9953 ms | tok/sec 1125092.8642
for step 11703 | loss 3.281341 | norm 0.3270 | time 466.0091 ms | tok/sec 1125059.4784
for step 11704 | loss 3.369557 | norm 0.3090 | time 466.6529 ms | tok/sec 1123507.5010
for step 11705 | loss 3.256572 | norm 0.3199 | time 465.3330 ms | tok/sec 1126694.2533
for step 11706 | loss 3.256444 | norm 0.2966 | time 465.4622 ms | tok/sec 1126381.4574
for step 11707 | loss 3.264237 | norm 0.3470 | time 466.0623 ms | tok/sec 1124931.1341
for step 11708 | loss 3.264564 | norm 0.3062 | time 465.2390 ms | tok/sec 1126921.7454
for step 11709 | loss 3.220481 | norm 0.2988 | time 465.9250 ms | tok/sec 1125262.7021
for step 11710 | loss 3.289754 | norm 0.3221 | time 465.7626 ms | tok/sec 1125654.9643
for step 11711 | loss 3.233986 | norm 0.3969 | time 465.7309 ms | tok/sec 1125731.6055
for step 11712 | loss 3.276593 | norm 0.2900 | time 465.2750 ms | tok/sec 1126834.5485
for step 11713 | loss 3.274229 | norm 0.3259 | time 465.1847 ms | tok/sec 1127053.4325
for step 11714 | loss 3.216832 | norm 0.2968 | time 465.3697 ms | tok/sec 1126605.3601
for step 11715 | loss 3.269773 | norm 0.3016 | time 465.9505 ms | tok/sec 1125201.0940
for step 11716 | loss 3.248320 | norm 0.3732 | time 465.9050 ms | tok/sec 1125311.0721
for step 11717 | loss 3.317164 | norm 0.3130 | time 465.8360 ms | tok/sec 1125477.5196
for step 11718 | loss 3.325781 | norm 0.3215 | time 465.9381 ms | tok/sec 1125231.0335
for step 11719 | loss 3.367721 | norm 0.3451 | time 465.8165 ms | tok/sec 1125524.7559
for step 11720 | loss 3.273094 | norm 0.3219 | time 465.5955 ms | tok/sec 1126059.0325
for step 11721 | loss 3.358619 | norm 0.3482 | time 465.5859 ms | tok/sec 1126082.0979
for step 11722 | loss 3.310244 | norm 0.3245 | time 466.2559 ms | tok/sec 1124464.0472
for step 11723 | loss 3.293953 | norm 0.3680 | time 465.4467 ms | tok/sec 1126418.9607
for step 11724 | loss 3.337398 | norm 0.3215 | time 465.3645 ms | tok/sec 1126618.0583
for step 11725 | loss 3.331477 | norm 0.3140 | time 465.4458 ms | tok/sec 1126421.2687
for step 11726 | loss 3.317303 | norm 0.3581 | time 466.2161 ms | tok/sec 1124560.0790
for step 11727 | loss 3.312274 | norm 0.3100 | time 465.1856 ms | tok/sec 1127051.1219
for step 11728 | loss 3.282583 | norm 0.3219 | time 465.9483 ms | tok/sec 1125206.2757
for step 11729 | loss 3.304613 | norm 0.3413 | time 465.2839 ms | tok/sec 1126813.1844
for step 11730 | loss 3.339037 | norm 0.2925 | time 466.0275 ms | tok/sec 1125015.1589
for step 11731 | loss 3.318807 | norm 0.3034 | time 465.6231 ms | tok/sec 1125992.1481
for step 11732 | loss 3.259301 | norm 0.2982 | time 465.3642 ms | tok/sec 1126618.6355
for step 11733 | loss 3.257777 | norm 0.3124 | time 465.5626 ms | tok/sec 1126138.6122
for step 11734 | loss 3.283766 | norm 0.3165 | time 465.1940 ms | tok/sec 1127030.9049
for step 11735 | loss 3.310907 | norm 0.3284 | time 466.6440 ms | tok/sec 1123528.7399
for step 11736 | loss 3.365590 | norm 0.3196 | time 465.7800 ms | tok/sec 1125612.9026
for step 11737 | loss 3.282786 | norm 0.2959 | time 465.8699 ms | tok/sec 1125395.7296
for step 11738 | loss 3.320238 | norm 0.2837 | time 466.2130 ms | tok/sec 1124567.5552
for step 11739 | loss 3.339096 | norm 0.3069 | time 466.0671 ms | tok/sec 1124919.6249
for step 11740 | loss 3.261343 | norm 0.3071 | time 466.8634 ms | tok/sec 1123000.8756
for step 11741 | loss 3.267466 | norm 0.2941 | time 465.6422 ms | tok/sec 1125946.0256
for step 11742 | loss 3.270932 | norm 0.3000 | time 465.5175 ms | tok/sec 1126247.6200
for step 11743 | loss 3.196021 | norm 0.3247 | time 465.7934 ms | tok/sec 1125580.6381
for step 11744 | loss 3.320422 | norm 0.2861 | time 465.8158 ms | tok/sec 1125526.4842
for step 11745 | loss 3.233740 | norm 0.3146 | time 466.2359 ms | tok/sec 1124512.3486
for step 11746 | loss 3.269047 | norm 0.3046 | time 466.3577 ms | tok/sec 1124218.5795
for step 11747 | loss 3.289649 | norm 0.2991 | time 465.3714 ms | tok/sec 1126601.3198
for step 11748 | loss 3.226290 | norm 0.3044 | time 465.7283 ms | tok/sec 1125737.9447
for step 11749 | loss 3.227248 | norm 0.2828 | time 464.7303 ms | tok/sec 1128155.4958
validation loss 3.2963
HellaSwag accuracy: 2778/10042=0.2766
> Hello, I'm a language model, and I think that language is a game, you're supposed to be able to do that by having a few games.
> Hello, I'm a language model, a model, and I'm looking for languages that make this a game. This game for kids lets them learn how to
> Hello, I'm a language model, but are you really interested in learning?
I'm really like this, yes, I know you have to use that
> Hello, I'm a language model, and I'm really excited of how my learners can really move towards real time problem solving rather than trying to get things done
> Hello, I'm a language model, and I know that you've already found our book, if you think in detail, it's probably not an easy one> 
Hello, I'm a language model, so I need to keep reading. I think you'll find that the way they define and work together are a good way
> Hello, I'm a language model, so to speak, and I've designed a model before me, that basically makes it sound pretty good too.
First
> Hello, I'm a language model, who wants to practice, and i also get excited! To be able to write code to be written and then we need> 
Hello, I'm a language model, so I use this:
There's a difference between the word and the word model:
- The word is the
> Hello, I'm a language model, I got it on from friends and it works on the real.
I've spent my 20s to get into a> 
Hello, I'm a language model, and will be going to be the last one or two weeks. You know that because your teacher has taken a class (
> Hello, I'm a language model, and it always needs to be modeled if there aren't different model languages from different languages. I would love to explain that
> Hello, I'm a language model, because this is a lot of learning.
I hope that the below post provides some useful hints and tips to help you
> Hello, I'm a language model, and I don't need a translation, my translation software is not good (I don't know what to write here).
> Hello, I'm a language model, but I am a teacher and have taught myself so much. So my question and answer are:
Are you using the
> Hello, I'm a language model, and am trying to write a code that represents my own language.
I mean, you learn a language as a system> 
>Hello, I'm a language model, but this isn't an easy task for me to do, because that's the best way to solve it.
There
 Hello, I'm a language model, and I'm not. This means you do things differently than that, maybe every time or every time... I am learning
> > Hello, I'm a language model, really I know the words of English and I actually learn English by putting the objects you know. I like them, I
Hello, I'm a language model, and the most important thing to notice is that your model has a model that is not a regular model, but one where
> > Hello, I'm a language model, so I can use modeling to express that. This is a tool where I'll show them how to model a language or
Hello, I'm a language model, and you'll soon love it.
For language models, here's what a data model looks like:
What if
> Hello, I'm a language model, since I have never heard how it works, so why would I call it my brain model, after all? I do> 
Hello, I'm a language model, and how to make it's job to do, but i don't have any reason to doubt that: I am a
> Hello, I'm a language model, and this article does not even cover anything.
If someone asks me, "Are you a language model in your school
> Hello, I'm a language model, but I found myself working in a bunch of languages during the course of my career. I was able to get my own
> Hello, I'm a language model, and I understand the importance of the translation/translation in real life. How do I learn what translation works for? I> 
Hello, I'm a language model, so I'll explain it in the chapter titled "The Language of Languages." The reason I said these language models is because
> Hello, I'm a language model, for the purposes of this course...so what I learn comes from the fact that one or more of the elements in each
> Hello, I'm a language model, but that way you don't have to make complicated equations for all of your applications. It gives you the freedom you need> 
Hello, I'm a language model, and I am from Southern California, where I am always on my team from the University of California, Berkeley, California (
> Hello, I'm a language model, so you don't have to be like your colleagues.
However I understand that it's not enough to just understand some
for step 11750 | loss 3.280367 | norm 0.3339 | time 12407.0849 ms | tok/sec 42257.1460
for step 11751 | loss 3.266860 | norm 0.3193 | time 463.1431 ms | tok/sec 1132021.5898
for step 11752 | loss 3.282028 | norm 0.3345 | time 464.0977 ms | tok/sec 1129693.0728
for step 11753 | loss 3.335218 | norm 0.3239 | time 462.6873 ms | tok/sec 1133136.8987
for step 11754 | loss 3.294219 | norm 0.3519 | time 464.8824 ms | tok/sec 1127786.3596
for step 11755 | loss 3.334587 | norm 0.3285 | time 463.1600 ms | tok/sec 1131980.2163
for step 11756 | loss 3.304978 | norm 0.4100 | time 463.7616 ms | tok/sec 1130511.9615
for step 11757 | loss 3.332486 | norm 0.3205 | time 463.8364 ms | tok/sec 1130329.4963
for step 11758 | loss 3.262289 | norm 0.3646 | time 463.6850 ms | tok/sec 1130698.5553
for step 11759 | loss 3.268757 | norm 0.3675 | time 463.6796 ms | tok/sec 1130711.9273
for step 11760 | loss 3.363850 | norm 0.4111 | time 464.6394 ms | tok/sec 1128376.0514
for step 11761 | loss 3.374178 | norm 0.3655 | time 464.4413 ms | tok/sec 1128857.4049
for step 11762 | loss 3.271109 | norm 0.3497 | time 463.6061 ms | tok/sec 1130891.0265
for step 11763 | loss 3.355687 | norm 0.3409 | time 465.4875 ms | tok/sec 1126320.3036
for step 11764 | loss 3.284664 | norm 0.3688 | time 464.7939 ms | tok/sec 1128000.9846
for step 11765 | loss 3.275859 | norm 0.3569 | time 465.2503 ms | tok/sec 1126894.6032
for step 11766 | loss 3.319440 | norm 0.3933 | time 464.9470 ms | tok/sec 1127629.6366
for step 11767 | loss 3.247503 | norm 0.3775 | time 464.6959 ms | tok/sec 1128238.8453
for step 11768 | loss 3.288911 | norm 0.3692 | time 465.6959 ms | tok/sec 1125816.3262
for step 11769 | loss 3.264656 | norm 0.3556 | time 465.4446 ms | tok/sec 1126424.1537
for step 11770 | loss 3.258681 | norm 0.3993 | time 465.3280 ms | tok/sec 1126706.3762
for step 11771 | loss 3.248393 | norm 0.3422 | time 466.0976 ms | tok/sec 1124845.9712
for step 11772 | loss 3.330555 | norm 0.3784 | time 464.5970 ms | tok/sec 1128479.1226
for step 11773 | loss 3.301384 | norm 0.3355 | time 465.6615 ms | tok/sec 1125899.3304
for step 11774 | loss 3.277252 | norm 0.3593 | time 466.0220 ms | tok/sec 1125028.3968
for step 11775 | loss 3.304081 | norm 0.3365 | time 464.5913 ms | tok/sec 1128493.0213
for step 11776 | loss 3.282153 | norm 0.3081 | time 465.0588 ms | tok/sec 1127358.5103
for step 11777 | loss 3.256190 | norm 0.3075 | time 465.0776 ms | tok/sec 1127312.8537
for step 11778 | loss 3.249487 | norm 0.3113 | time 465.1916 ms | tok/sec 1127036.6811
for step 11779 | loss 3.260315 | norm 0.3348 | time 466.1379 ms | tok/sec 1124748.7400
for step 11780 | loss 3.363498 | norm 0.3288 | time 465.9703 ms | tok/sec 1125153.3091
for step 11781 | loss 3.296880 | norm 0.3459 | time 465.7595 ms | tok/sec 1125662.4551
for step 11782 | loss 3.238751 | norm 0.3298 | time 466.1417 ms | tok/sec 1124739.5356
for step 11783 | loss 3.269402 | norm 0.3632 | time 465.1301 ms | tok/sec 1127185.7282
for step 11784 | loss 3.255044 | norm 0.3019 | time 465.5805 ms | tok/sec 1126095.3610
for step 11785 | loss 3.277311 | norm 0.3209 | time 465.5716 ms | tok/sec 1126116.6978
for step 11786 | loss 3.213595 | norm 0.3468 | time 466.9313 ms | tok/sec 1122837.4530
for step 11787 | loss 3.257339 | norm 0.3104 | time 465.6284 ms | tok/sec 1125979.4640
for step 11788 | loss 3.246906 | norm 0.3237 | time 465.8520 ms | tok/sec 1125438.9271
for step 11789 | loss 3.290793 | norm 0.3228 | time 467.0162 ms | tok/sec 1122633.3850
for step 11790 | loss 3.309028 | norm 0.3288 | time 465.4579 ms | tok/sec 1126391.8427
for step 11791 | loss 3.207248 | norm 0.3371 | time 465.5690 ms | tok/sec 1126123.0414
for step 11792 | loss 3.373158 | norm 0.3365 | time 465.6248 ms | tok/sec 1125988.1122
for step 11793 | loss 3.272348 | norm 0.3465 | time 466.6033 ms | tok/sec 1123626.9085
for step 11794 | loss 3.332509 | norm 0.3336 | time 465.5728 ms | tok/sec 1126113.8144
for step 11795 | loss 3.441814 | norm 0.3399 | time 465.1768 ms | tok/sec 1127072.4950
for step 11796 | loss 3.297309 | norm 0.3319 | time 465.4143 ms | tok/sec 1126497.4371
for step 11797 | loss 3.316145 | norm 0.3010 | time 465.0011 ms | tok/sec 1127498.3929
for step 11798 | loss 3.266947 | norm 0.3648 | time 465.4398 ms | tok/sec 1126435.6937
for step 11799 | loss 3.289920 | norm 0.2994 | time 465.9536 ms | tok/sec 1125193.6093
for step 11800 | loss 3.298375 | norm 0.3413 | time 465.0207 ms | tok/sec 1127450.9908
for step 11801 | loss 3.262630 | norm 0.3306 | time 465.6222 ms | tok/sec 1125994.4543
for step 11802 | loss 3.311137 | norm 0.3091 | time 465.4562 ms | tok/sec 1126395.8815
for step 11803 | loss 3.293605 | norm 0.3342 | time 464.5855 ms | tok/sec 1128506.9203
for step 11804 | loss 3.288971 | norm 0.3126 | time 465.6947 ms | tok/sec 1125819.2081
for step 11805 | loss 3.314884 | norm 0.3555 | time 465.6036 ms | tok/sec 1126039.4276
for step 11806 | loss 3.290148 | norm 0.3371 | time 465.7009 ms | tok/sec 1125804.2225
for step 11807 | loss 3.296377 | norm 0.3141 | time 465.5612 ms | tok/sec 1126142.0724
for step 11808 | loss 3.264856 | norm 0.2974 | time 465.8008 ms | tok/sec 1125562.7783
for step 11809 | loss 3.248315 | norm 0.3142 | time 464.6559 ms | tok/sec 1128336.1018
Will loading at 0 from edu_fineweb10B/edufineweb_train_000063.npy
for step 11810 | loss 3.269267 | norm 0.3205 | time 2710.4087 ms | tok/sec 193435.0352
for step 11811 | loss 3.229215 | norm 0.2944 | time 470.7119 ms | tok/sec 1113819.1922
for step 11812 | loss 3.255743 | norm 0.3256 | time 464.0191 ms | tok/sec 1129884.6212
for step 11813 | loss 3.251669 | norm 0.3091 | time 464.0329 ms | tok/sec 1129850.9504
for step 11814 | loss 3.244044 | norm 0.2989 | time 464.2832 ms | tok/sec 1129241.7397
for step 11815 | loss 3.257020 | norm 0.3126 | time 465.2932 ms | tok/sec 1126790.6664
for step 11816 | loss 3.225777 | norm 0.2891 | time 465.2572 ms | tok/sec 1126877.8565
for step 11817 | loss 3.245310 | norm 0.3148 | time 465.8570 ms | tok/sec 1125426.8314
for step 11818 | loss 3.288243 | norm 0.3027 | time 465.6425 ms | tok/sec 1125945.4491
for step 11819 | loss 3.281783 | norm 0.3160 | time 465.0435 ms | tok/sec 1127395.5007
for step 11820 | loss 3.222365 | norm 0.2938 | time 465.2426 ms | tok/sec 1126913.0828
for step 11821 | loss 3.226704 | norm 0.3667 | time 465.0006 ms | tok/sec 1127499.5491
for step 11822 | loss 3.241531 | norm 0.3437 | time 465.0958 ms | tok/sec 1127268.9344
for step 11823 | loss 3.349649 | norm 0.3626 | time 466.1565 ms | tok/sec 1124703.8698
for step 11824 | loss 3.399262 | norm 0.3957 | time 465.1227 ms | tok/sec 1127203.6396
for step 11825 | loss 3.245196 | norm 0.3688 | time 466.7680 ms | tok/sec 1123230.3208
for step 11826 | loss 3.277061 | norm 0.3308 | time 465.6739 ms | tok/sec 1125869.3553
for step 11827 | loss 3.256600 | norm 0.3712 | time 465.0578 ms | tok/sec 1127360.8222
for step 11828 | loss 3.313251 | norm 0.3261 | time 466.5048 ms | tok/sec 1123864.0765
for step 11829 | loss 3.352018 | norm 0.3845 | time 465.7602 ms | tok/sec 1125660.7265
for step 11830 | loss 3.319542 | norm 0.3532 | time 466.7060 ms | tok/sec 1123379.5105
for step 11831 | loss 3.270018 | norm 0.3282 | time 466.0497 ms | tok/sec 1124961.6348
for step 11832 | loss 3.252128 | norm 0.3067 | time 466.2642 ms | tok/sec 1124443.9229
for step 11833 | loss 3.291729 | norm 0.3226 | time 465.1093 ms | tok/sec 1127235.9971
for step 11834 | loss 3.278742 | norm 0.3339 | time 465.6041 ms | tok/sec 1126038.2744
for step 11835 | loss 3.275755 | norm 0.3166 | time 466.2144 ms | tok/sec 1124564.1046
for step 11836 | loss 3.301422 | norm 0.3122 | time 465.6096 ms | tok/sec 1126025.0127
for step 11837 | loss 3.314983 | norm 0.3430 | time 465.9748 ms | tok/sec 1125142.3710
for step 11838 | loss 3.316274 | norm 0.3005 | time 465.9569 ms | tok/sec 1125185.5490
for step 11839 | loss 3.335462 | norm 0.3531 | time 466.3010 ms | tok/sec 1124355.3843
for step 11840 | loss 3.313178 | norm 0.3482 | time 465.9917 ms | tok/sec 1125101.4988
for step 11841 | loss 3.285457 | norm 0.3354 | time 465.8356 ms | tok/sec 1125478.6717
for step 11842 | loss 3.343193 | norm 0.3547 | time 466.1114 ms | tok/sec 1124812.6000
for step 11843 | loss 3.271765 | norm 0.3003 | time 465.3022 ms | tok/sec 1126768.7267
for step 11844 | loss 3.324821 | norm 0.3284 | time 465.2843 ms | tok/sec 1126812.0296
for step 11845 | loss 3.245708 | norm 0.3270 | time 465.8730 ms | tok/sec 1125388.2423
for step 11846 | loss 3.405653 | norm 0.3443 | time 465.5633 ms | tok/sec 1126136.8821
for step 11847 | loss 3.244631 | norm 0.3858 | time 465.1103 ms | tok/sec 1127233.6858
for step 11848 | loss 3.199032 | norm 0.3247 | time 466.3923 ms | tok/sec 1124135.2483
for step 11849 | loss 3.241199 | norm 0.3505 | time 466.2120 ms | tok/sec 1124569.8556
for step 11850 | loss 3.265408 | norm 0.3317 | time 466.0976 ms | tok/sec 1124845.9712
for step 11851 | loss 3.327057 | norm 0.3716 | time 466.1007 ms | tok/sec 1124838.4912
for step 11852 | loss 3.290653 | norm 0.3367 | time 466.0285 ms | tok/sec 1125012.8567
for step 11853 | loss 3.223595 | norm 0.3398 | time 465.8167 ms | tok/sec 1125524.1799
for step 11854 | loss 3.209963 | norm 0.3042 | time 465.9510 ms | tok/sec 1125199.9425
for step 11855 | loss 3.195500 | norm 0.3332 | time 466.4035 ms | tok/sec 1124108.2402
for step 11856 | loss 3.260872 | norm 0.3079 | time 465.5032 ms | tok/sec 1126282.2301
for step 11857 | loss 3.329156 | norm 0.4003 | time 467.1261 ms | tok/sec 1122369.2385
for step 11858 | loss 3.301877 | norm 0.3929 | time 465.7094 ms | tok/sec 1125783.4738
for step 11859 | loss 3.274902 | norm 0.3337 | time 465.7316 ms | tok/sec 1125729.8766
for step 11860 | loss 3.262778 | norm 0.3148 | time 466.3384 ms | tok/sec 1124265.1354
for step 11861 | loss 3.312992 | norm 0.3046 | time 465.5607 ms | tok/sec 1126143.2259
for step 11862 | loss 3.353438 | norm 0.3139 | time 466.3265 ms | tok/sec 1124293.8755
for step 11863 | loss 3.265647 | norm 0.3109 | time 465.5762 ms | tok/sec 1126105.7410
for step 11864 | loss 3.286273 | norm 0.2838 | time 464.8888 ms | tok/sec 1127770.7432
for step 11865 | loss 3.247718 | norm 0.3185 | time 465.1136 ms | tok/sec 1127225.5963
for step 11866 | loss 3.221129 | norm 0.3283 | time 465.8449 ms | tok/sec 1125456.2070
for step 11867 | loss 3.316097 | norm 0.3186 | time 465.4891 ms | tok/sec 1126316.2654
for step 11868 | loss 3.286094 | norm 0.3024 | time 465.4393 ms | tok/sec 1126436.8477
for step 11869 | loss 3.263643 | norm 0.3099 | time 465.0033 ms | tok/sec 1127493.1900
for step 11870 | loss 3.268351 | norm 0.3313 | time 464.7355 ms | tok/sec 1128142.7630
for step 11871 | loss 3.270931 | norm 0.2992 | time 465.7164 ms | tok/sec 1125766.7602
for step 11872 | loss 3.314873 | norm 0.3030 | time 464.8066 ms | tok/sec 1127970.3189
for step 11873 | loss 3.257939 | norm 0.3178 | time 464.9525 ms | tok/sec 1127616.3374
for step 11874 | loss 3.338932 | norm 0.2974 | time 465.4553 ms | tok/sec 1126398.1894
for step 11875 | loss 3.282983 | norm 0.3184 | time 465.1561 ms | tok/sec 1127122.7539
for step 11876 | loss 3.256336 | norm 0.3219 | time 465.2178 ms | tok/sec 1126973.1459
for step 11877 | loss 3.365668 | norm 0.3588 | time 465.1699 ms | tok/sec 1127089.2475
for step 11878 | loss 3.354320 | norm 0.3510 | time 464.9458 ms | tok/sec 1127632.5278
for step 11879 | loss 3.305068 | norm 0.3793 | time 465.6534 ms | tok/sec 1125918.9304
for step 11880 | loss 3.247398 | norm 0.3223 | time 465.2779 ms | tok/sec 1126827.6195
for step 11881 | loss 3.302782 | norm 0.3285 | time 465.3497 ms | tok/sec 1126653.8456
for step 11882 | loss 3.293122 | norm 0.3327 | time 464.8895 ms | tok/sec 1127769.0080
for step 11883 | loss 3.256017 | norm 0.3224 | time 465.1294 ms | tok/sec 1127187.4615
for step 11884 | loss 3.263525 | norm 0.3104 | time 464.6070 ms | tok/sec 1128454.8007
for step 11885 | loss 3.239483 | norm 0.3152 | time 465.3699 ms | tok/sec 1126604.7829
for step 11886 | loss 3.250373 | norm 0.2892 | time 465.6785 ms | tok/sec 1125858.4032
for step 11887 | loss 3.186536 | norm 0.3023 | time 465.7264 ms | tok/sec 1125742.5551
for step 11888 | loss 3.244305 | norm 0.2689 | time 465.3568 ms | tok/sec 1126636.5288
for step 11889 | loss 3.297665 | norm 0.3308 | time 466.0709 ms | tok/sec 1124910.4176
for step 11890 | loss 3.272871 | norm 0.2935 | time 466.0971 ms | tok/sec 1124847.1219
for step 11891 | loss 3.274103 | norm 0.3513 | time 465.7512 ms | tok/sec 1125682.6231
for step 11892 | loss 3.311281 | norm 0.3204 | time 465.2393 ms | tok/sec 1126921.1679
for step 11893 | loss 3.287081 | norm 0.3258 | time 470.0880 ms | tok/sec 1115297.5493
for step 11894 | loss 3.273658 | norm 0.3118 | time 465.4312 ms | tok/sec 1126456.4665
for step 11895 | loss 3.259036 | norm 0.3361 | time 465.6880 ms | tok/sec 1125835.3469
for step 11896 | loss 3.342475 | norm 0.3355 | time 465.0884 ms | tok/sec 1127286.8484
for step 11897 | loss 3.289447 | norm 0.3414 | time 464.9851 ms | tok/sec 1127537.1268
for step 11898 | loss 3.266159 | norm 0.3160 | time 465.4028 ms | tok/sec 1126525.1373
for step 11899 | loss 3.362030 | norm 0.3473 | time 465.7495 ms | tok/sec 1125686.6568
for step 11900 | loss 3.293983 | norm 0.3797 | time 465.7655 ms | tok/sec 1125648.0498
for step 11901 | loss 3.297107 | norm 0.3383 | time 466.1167 ms | tok/sec 1124799.9425
for step 11902 | loss 3.327271 | norm 0.3617 | time 466.0387 ms | tok/sec 1124988.1085
for step 11903 | loss 3.278748 | norm 0.3327 | time 465.9460 ms | tok/sec 1125212.0332
for step 11904 | loss 3.312376 | norm 0.3404 | time 465.3947 ms | tok/sec 1126544.7591
for step 11905 | loss 3.285369 | norm 0.3491 | time 466.6207 ms | tok/sec 1123584.9982
for step 11906 | loss 3.314379 | norm 0.3346 | time 466.1429 ms | tok/sec 1124736.6592
for step 11907 | loss 3.307265 | norm 0.3216 | time 466.4121 ms | tok/sec 1124087.5540
for step 11908 | loss 3.328937 | norm 0.3357 | time 465.4644 ms | tok/sec 1126376.2649
for step 11909 | loss 3.322245 | norm 0.3091 | time 466.2247 ms | tok/sec 1124539.3761
for step 11910 | loss 3.293021 | norm 0.3244 | time 465.1389 ms | tok/sec 1127164.3508
for step 11911 | loss 3.284629 | norm 0.3041 | time 466.2859 ms | tok/sec 1124391.6030
for step 11912 | loss 3.262496 | norm 0.3227 | time 465.8794 ms | tok/sec 1125372.6922
for step 11913 | loss 3.285918 | norm 0.3006 | time 466.3770 ms | tok/sec 1124172.0275
for step 11914 | loss 3.280149 | norm 0.3205 | time 464.7698 ms | tok/sec 1128059.4277
for step 11915 | loss 3.206247 | norm 0.2986 | time 465.5199 ms | tok/sec 1126241.8519
for step 11916 | loss 3.269347 | norm 0.3698 | time 465.9152 ms | tok/sec 1125286.3107
for step 11917 | loss 3.264443 | norm 0.3411 | time 465.4768 ms | tok/sec 1126346.2644
for step 11918 | loss 3.223833 | norm 0.3421 | time 467.0804 ms | tok/sec 1122479.2367
for step 11919 | loss 3.294516 | norm 0.3615 | time 465.9085 ms | tok/sec 1125302.4343
for step 11920 | loss 3.221332 | norm 0.3211 | time 465.9629 ms | tok/sec 1125171.1560
for step 11921 | loss 3.259807 | norm 0.3382 | time 466.9495 ms | tok/sec 1122793.8817
for step 11922 | loss 3.281456 | norm 0.2941 | time 465.3995 ms | tok/sec 1126533.2168
for step 11923 | loss 3.241102 | norm 0.3224 | time 465.7397 ms | tok/sec 1125710.2832
for step 11924 | loss 3.227003 | norm 0.3055 | time 465.4665 ms | tok/sec 1126371.0724
for step 11925 | loss 3.252666 | norm 0.3094 | time 466.3169 ms | tok/sec 1124316.8687
for step 11926 | loss 3.315822 | norm 0.3323 | time 465.6205 ms | tok/sec 1125998.4903
for step 11927 | loss 3.315179 | norm 0.2794 | time 465.6734 ms | tok/sec 1125870.5081
for step 11928 | loss 3.265601 | norm 0.3251 | time 466.7871 ms | tok/sec 1123184.4243
for step 11929 | loss 3.305328 | norm 0.3031 | time 465.2352 ms | tok/sec 1126930.9856
for step 11930 | loss 3.270334 | norm 0.3143 | time 465.9920 ms | tok/sec 1125100.9232
for step 11931 | loss 3.268473 | norm 0.3412 | time 465.5964 ms | tok/sec 1126056.7260
for step 11932 | loss 3.257532 | norm 0.3609 | time 466.2931 ms | tok/sec 1124374.3557
for step 11933 | loss 3.331005 | norm 0.3372 | time 465.0233 ms | tok/sec 1127444.6323
for step 11934 | loss 3.286428 | norm 0.3127 | time 465.6587 ms | tok/sec 1125906.2479
for step 11935 | loss 3.298408 | norm 0.3394 | time 465.4157 ms | tok/sec 1126493.9747
for step 11936 | loss 3.290138 | norm 0.3204 | time 465.7903 ms | tok/sec 1125588.1279
for step 11937 | loss 3.242762 | norm 0.3601 | time 465.2963 ms | tok/sec 1126783.1606
for step 11938 | loss 3.357867 | norm 0.3161 | time 466.0690 ms | tok/sec 1124915.0212
for step 11939 | loss 3.280509 | norm 0.3243 | time 465.6527 ms | tok/sec 1125920.6598
for step 11940 | loss 3.292767 | norm 0.3227 | time 465.7161 ms | tok/sec 1125767.3365
for step 11941 | loss 3.228944 | norm 0.2887 | time 465.0717 ms | tok/sec 1127327.3016
for step 11942 | loss 3.314323 | norm 0.3137 | time 465.0254 ms | tok/sec 1127439.4299
for step 11943 | loss 3.285982 | norm 0.3457 | time 466.7923 ms | tok/sec 1123171.8034
for step 11944 | loss 3.304533 | norm 0.3085 | time 465.4071 ms | tok/sec 1126514.7496
for step 11945 | loss 3.246727 | norm 0.3348 | time 465.2448 ms | tok/sec 1126907.8854
for step 11946 | loss 3.245337 | norm 0.3589 | time 465.9343 ms | tok/sec 1125240.2460
for step 11947 | loss 3.201045 | norm 0.3631 | time 465.5554 ms | tok/sec 1126155.9136
for step 11948 | loss 3.225769 | norm 0.3240 | time 465.6835 ms | tok/sec 1125846.2985
for step 11949 | loss 3.241369 | norm 0.3488 | time 465.2719 ms | tok/sec 1126842.0550
for step 11950 | loss 3.265333 | norm 0.3207 | time 465.9963 ms | tok/sec 1125090.5617
for step 11951 | loss 3.301203 | norm 0.3627 | time 464.5481 ms | tok/sec 1128597.8516
for step 11952 | loss 3.226301 | norm 0.3048 | time 465.1492 ms | tok/sec 1127139.5078
for step 11953 | loss 3.240959 | norm 0.3656 | time 465.6940 ms | tok/sec 1125820.9373
for step 11954 | loss 3.209956 | norm 0.3073 | time 465.7567 ms | tok/sec 1125669.3698
for step 11955 | loss 3.267166 | norm 0.3367 | time 465.1129 ms | tok/sec 1127227.3297
for step 11956 | loss 3.239236 | norm 0.3003 | time 465.4980 ms | tok/sec 1126294.9210
for step 11957 | loss 3.266328 | norm 0.3193 | time 465.5249 ms | tok/sec 1126229.7390
for step 11958 | loss 3.204986 | norm 0.2817 | time 465.6372 ms | tok/sec 1125958.1324
for step 11959 | loss 3.265340 | norm 0.2917 | time 465.3981 ms | tok/sec 1126536.6794
for step 11960 | loss 3.241830 | norm 0.2911 | time 465.6358 ms | tok/sec 1125961.5915
for step 11961 | loss 3.321957 | norm 0.3390 | time 465.8144 ms | tok/sec 1125529.9406
for step 11962 | loss 3.296250 | norm 0.3208 | time 465.7435 ms | tok/sec 1125701.0630
for step 11963 | loss 3.333052 | norm 0.3732 | time 465.5013 ms | tok/sec 1126286.8449
for step 11964 | loss 3.329530 | norm 0.3285 | time 465.6770 ms | tok/sec 1125861.8617
for step 11965 | loss 3.292569 | norm 0.3793 | time 465.5116 ms | tok/sec 1126262.0406
for step 11966 | loss 3.257972 | norm 0.3641 | time 465.0750 ms | tok/sec 1127319.2107
for step 11967 | loss 3.277874 | norm 0.3618 | time 466.1052 ms | tok/sec 1124827.5592
for step 11968 | loss 3.273604 | norm 0.3603 | time 466.0597 ms | tok/sec 1124937.4643
for step 11969 | loss 3.334127 | norm 0.3424 | time 465.4188 ms | tok/sec 1126486.4729
for step 11970 | loss 3.414330 | norm 0.3766 | time 465.8093 ms | tok/sec 1125542.0385
for step 11971 | loss 3.343643 | norm 0.3287 | time 465.4558 ms | tok/sec 1126397.0354
for step 11972 | loss 3.281125 | norm 0.3769 | time 465.7085 ms | tok/sec 1125785.7792
for step 11973 | loss 3.290287 | norm 0.3321 | time 466.4955 ms | tok/sec 1123886.4777
for step 11974 | loss 3.282524 | norm 0.3303 | time 465.2491 ms | tok/sec 1126897.4906
for step 11975 | loss 3.236571 | norm 0.3562 | time 465.5135 ms | tok/sec 1126257.4260
for step 11976 | loss 3.252764 | norm 0.2958 | time 465.9774 ms | tok/sec 1125136.0385
for step 11977 | loss 3.249857 | norm 0.3191 | time 465.5528 ms | tok/sec 1126162.2576
for step 11978 | loss 3.238913 | norm 0.3600 | time 465.4958 ms | tok/sec 1126300.1128
for step 11979 | loss 3.260907 | norm 0.4101 | time 465.5643 ms | tok/sec 1126134.5753
for step 11980 | loss 3.244467 | norm 0.2943 | time 466.9201 ms | tok/sec 1122864.4001
for step 11981 | loss 3.260825 | norm 0.3656 | time 465.3561 ms | tok/sec 1126638.2605
for step 11982 | loss 3.301721 | norm 0.3386 | time 466.1169 ms | tok/sec 1124799.3671
for step 11983 | loss 3.251174 | norm 0.3544 | time 466.0895 ms | tok/sec 1124865.5345
for step 11984 | loss 3.235289 | norm 0.3290 | time 465.1859 ms | tok/sec 1127050.5443
for step 11985 | loss 3.256850 | norm 0.3540 | time 465.5459 ms | tok/sec 1126178.9830
for step 11986 | loss 3.209349 | norm 0.3280 | time 465.2545 ms | tok/sec 1126884.2086
for step 11987 | loss 3.205934 | norm 0.3246 | time 465.8372 ms | tok/sec 1125474.6395
for step 11988 | loss 3.271626 | norm 0.2963 | time 465.6243 ms | tok/sec 1125989.2654
for step 11989 | loss 3.214665 | norm 0.3237 | time 465.5764 ms | tok/sec 1126105.1643
for step 11990 | loss 3.212102 | norm 0.3148 | time 465.4224 ms | tok/sec 1126477.8170
for step 11991 | loss 3.246962 | norm 0.3209 | time 466.1326 ms | tok/sec 1124761.3964
for step 11992 | loss 3.218595 | norm 0.3262 | time 465.9088 ms | tok/sec 1125301.8584
for step 11993 | loss 3.253387 | norm 0.2970 | time 465.4038 ms | tok/sec 1126522.8289
for step 11994 | loss 3.263993 | norm 0.3051 | time 465.9159 ms | tok/sec 1125284.5832
for step 11995 | loss 3.272135 | norm 0.3213 | time 465.4837 ms | tok/sec 1126329.5340
for step 11996 | loss 3.304317 | norm 0.3010 | time 466.1057 ms | tok/sec 1124826.4085
for step 11997 | loss 3.303589 | norm 0.3246 | time 466.2366 ms | tok/sec 1124510.6234
for step 11998 | loss 3.292130 | norm 0.2943 | time 466.1279 ms | tok/sec 1124772.9024
for step 11999 | loss 3.356676 | norm 0.3050 | time 465.9669 ms | tok/sec 1125161.3689
validation loss 3.2881
HellaSwag accuracy: 2823/10042=0.2811
> Hello, I'm a language model, and have nothing to do with what those people thought about learning.
I really don't want that to sound like a
> Hello, I'm a language model, not an algorithm. I'm also a system, not algorithms.
A few short time ago, we were talking about
> Hello, I'm a language model, but most language models are about the same as the language you learn at the computer.
If you have any feedback on
> Hello, I'm a language model, and I'm looking at everything from an audio model to spoken dialogue. If I'm getting interested in the difference between a
> Hello, I'm a language model, and I can't figure out a perfect equation about how things work; here we have to learn to understand and use some
> Hello, I'm a language model, so there's no reason to be so confused with them. I have so many languages that we don't understand, and
> Hello, I'm a language model, so I do understand its use. I'm working on a very large database, so I'm not sure how it gets
> Hello, I'm a language model, and in my language modeling course I have been dealing with the structure of the world as a whole, to be a living
> Hello, I'm a language model, so I will be going to be using something called "language modeling" rather than "programming."
Let's start
> Hello, I'm a language model, now that's already being used: my friend in linguistics, in business. In English I'm writing, all of
> Hello, I'm a language model, I'm a social media consultant, and I'm a personified who plays a huge role in the growth of communication and
> Hello, I'm a language model, and I know that's been a long subject of translation for me. How do you translate what I'm doing? I> 
>>Hello, I'm a language model, and the future may depend on the particular case we do....[not,]
- I want to know when and
 Hello, I'm a language model, just about it. If you didn't understand it yourself, then I'll get you back!
2) The grammar>  Hello, I'm a language model, and I'm not just one person. And, yes, there's great support for such a model," says Ture> 
Hello, I'm a language model, let me start with an "i"), we can translate anything into anything, and anything that comes from another language model:

Hello, I'm a language model, and am not going to explain the details. I'm a model, so it's not a simple model!
If>> > 
>  Hello, I'm a language model, but for me, there's a huge scope to which you can expand the range of languages you may want to take onHello, I'm a language model, and I think it's important to understand one of its components.
You can learn about it from Wikipedia or from the
Hello, I'm a language model, and I've no idea what happens. Please answer these questions and tell me what I've learned.
- If a
> Hello, I'm a language model, and this isn't a difficult job.
If you liked this article, please give it a quick review on any similar

> > Hello, I'm a language model, so I believe the more I see our approach to processing that information to form a model.
In my mind, a
> > Hello, I'm a language model, and my native language is Spanish (with no accent). I've spoken Spanish to Spanish speakers and I used them here for
Hello, I'm a language model, so you are a language interpreter, really mean - I'm not an individual, and I'm so afraid of you reading
> Hello, I'm a language model, and you have to spend a lot of time teaching the language so that I am also able to show how it helps with
Hello, I'm a language model, really... the most challenging, I know... well, you ask me why I can't understand anything at work... I
> Hello, I'm a language model, so I'll have to do my version at a later date. So there's no need for someone to say what language
> > Hello, I'm a language model, and my learning theory was based on my reading, writing, and mathematics background of my students own age.<|endoftext|>(By
> Hello, I'm a language model, and most of our readers have not heard the word “he,” that has literally translated some words, and
Hello, I'm a language model, so I don’t have any problems with the model. In the past few decades, I have been working on
Hello, I'm a language model, and we could get it from the "f" sign in your browser. :)
If not it is because the website
> Hello, I'm a language model, in the field of computational linguistics.
Hello, I'm a language model, in the field of computational linguistics
for step 12000 | loss 3.320484 | norm 0.2998 | time 12545.9504 ms | tok/sec 41789.4207
Will loading at 0 from edu_fineweb10B/edufineweb_train_000064.npy
for step 12001 | loss 3.299636 | norm 0.3116 | time 2570.2515 ms | tok/sec 203983.1539
for step 12002 | loss 3.235757 | norm 0.3202 | time 461.5943 ms | tok/sec 1135819.8118
for step 12003 | loss 3.299629 | norm 0.3027 | time 462.6601 ms | tok/sec 1133203.4666
for step 12004 | loss 3.295230 | norm 0.3132 | time 463.3563 ms | tok/sec 1131500.8542
for step 12005 | loss 3.197311 | norm 0.3180 | time 465.0979 ms | tok/sec 1127263.7336
for step 12006 | loss 3.247951 | norm 0.3302 | time 463.4321 ms | tok/sec 1131315.7416
for step 12007 | loss 3.276295 | norm 0.3541 | time 463.0854 ms | tok/sec 1132162.6320
for step 12008 | loss 3.277013 | norm 0.3449 | time 464.3228 ms | tok/sec 1129145.4865
for step 12009 | loss 3.289335 | norm 0.2996 | time 464.2143 ms | tok/sec 1129409.3521
for step 12010 | loss 3.213569 | norm 0.3332 | time 463.3601 ms | tok/sec 1131491.5389
for step 12011 | loss 3.268822 | norm 0.3119 | time 463.8436 ms | tok/sec 1130312.0664
for step 12012 | loss 3.193004 | norm 0.3265 | time 463.6240 ms | tok/sec 1130847.4095
for step 12013 | loss 3.220725 | norm 0.3042 | time 463.9282 ms | tok/sec 1130105.8534
for step 12014 | loss 3.311523 | norm 0.2965 | time 463.6290 ms | tok/sec 1130835.1973
for step 12015 | loss 3.276987 | norm 0.2894 | time 464.6001 ms | tok/sec 1128471.5943
for step 12016 | loss 3.208028 | norm 0.3004 | time 464.1500 ms | tok/sec 1129565.9901
for step 12017 | loss 3.304247 | norm 0.2945 | time 464.2200 ms | tok/sec 1129395.4309
for step 12018 | loss 3.277851 | norm 0.2834 | time 465.0457 ms | tok/sec 1127390.2988
for step 12019 | loss 3.253407 | norm 0.3057 | time 465.0316 ms | tok/sec 1127424.4011
for step 12020 | loss 3.277244 | norm 0.2664 | time 464.4063 ms | tok/sec 1128942.5968
for step 12021 | loss 3.240708 | norm 0.3085 | time 464.8619 ms | tok/sec 1127836.1037
for step 12022 | loss 3.220169 | norm 0.2884 | time 464.2141 ms | tok/sec 1129409.9322
for step 12023 | loss 3.298047 | norm 0.3100 | time 464.8106 ms | tok/sec 1127960.4831
for step 12024 | loss 3.238677 | norm 0.3183 | time 465.9019 ms | tok/sec 1125318.5583
for step 12025 | loss 3.193047 | norm 0.3286 | time 466.0978 ms | tok/sec 1124845.3958
for step 12026 | loss 3.194311 | norm 0.3023 | time 465.2944 ms | tok/sec 1126787.7796
for step 12027 | loss 3.255189 | norm 0.3573 | time 465.8425 ms | tok/sec 1125461.9671
for step 12028 | loss 3.233670 | norm 0.2776 | time 464.7794 ms | tok/sec 1128036.2812
for step 12029 | loss 3.333354 | norm 0.3561 | time 464.9796 ms | tok/sec 1127550.4242
for step 12030 | loss 3.258793 | norm 0.3092 | time 465.6239 ms | tok/sec 1125990.4185
for step 12031 | loss 3.314295 | norm 0.3369 | time 465.6816 ms | tok/sec 1125850.9098
for step 12032 | loss 3.316255 | norm 0.3091 | time 466.2147 ms | tok/sec 1124563.5295
for step 12033 | loss 3.260112 | norm 0.3297 | time 464.9804 ms | tok/sec 1127548.6897
for step 12034 | loss 3.313409 | norm 0.3500 | time 466.4369 ms | tok/sec 1124027.7981
for step 12035 | loss 3.263695 | norm 0.3440 | time 464.7171 ms | tok/sec 1128187.3292
for step 12036 | loss 3.266838 | norm 0.2983 | time 465.4534 ms | tok/sec 1126402.8051
for step 12037 | loss 3.311707 | norm 0.3375 | time 466.4018 ms | tok/sec 1124112.2626
for step 12038 | loss 3.239801 | norm 0.3117 | time 465.6277 ms | tok/sec 1125981.1937
for step 12039 | loss 3.345218 | norm 0.3295 | time 464.6697 ms | tok/sec 1128302.5233
for step 12040 | loss 3.252135 | norm 0.3178 | time 465.4822 ms | tok/sec 1126332.9954
for step 12041 | loss 3.267269 | norm 0.3367 | time 465.4293 ms | tok/sec 1126461.0827
for step 12042 | loss 3.265694 | norm 0.2851 | time 464.7853 ms | tok/sec 1128021.8151
for step 12043 | loss 3.263345 | norm 0.3593 | time 464.9737 ms | tok/sec 1127564.8782
for step 12044 | loss 3.267349 | norm 0.2929 | time 466.5198 ms | tok/sec 1123827.8919
for step 12045 | loss 3.218652 | norm 0.3113 | time 465.6920 ms | tok/sec 1125825.5483
for step 12046 | loss 3.249543 | norm 0.3233 | time 465.5337 ms | tok/sec 1126208.3978
for step 12047 | loss 3.233225 | norm 0.3086 | time 464.6635 ms | tok/sec 1128317.5755
for step 12048 | loss 3.254902 | norm 0.3068 | time 465.9476 ms | tok/sec 1125208.0029
for step 12049 | loss 3.288935 | norm 0.3330 | time 468.6472 ms | tok/sec 1118726.3115
for step 12050 | loss 3.271074 | norm 0.2983 | time 465.6806 ms | tok/sec 1125853.2155
for step 12051 | loss 3.295494 | norm 0.3055 | time 465.1084 ms | tok/sec 1127238.3084
for step 12052 | loss 3.223031 | norm 0.3104 | time 465.8921 ms | tok/sec 1125342.1693
for step 12053 | loss 3.242112 | norm 0.3140 | time 465.3258 ms | tok/sec 1126711.5718
for step 12054 | loss 3.312765 | norm 0.3088 | time 465.7300 ms | tok/sec 1125733.9106
for step 12055 | loss 3.257325 | norm 0.3191 | time 464.8139 ms | tok/sec 1127952.3831
for step 12056 | loss 3.285444 | norm 0.3105 | time 467.9971 ms | tok/sec 1120280.5081
for step 12057 | loss 3.247869 | norm 0.2891 | time 465.5275 ms | tok/sec 1126223.3942
for step 12058 | loss 3.255276 | norm 0.2824 | time 465.6091 ms | tok/sec 1126026.1659
for step 12059 | loss 3.259099 | norm 0.2929 | time 465.9038 ms | tok/sec 1125313.9514
for step 12060 | loss 3.229382 | norm 0.2966 | time 465.1124 ms | tok/sec 1127228.4854
for step 12061 | loss 3.220055 | norm 0.3017 | time 465.2054 ms | tok/sec 1127003.1798
for step 12062 | loss 3.253913 | norm 0.2958 | time 465.2779 ms | tok/sec 1126827.6195
for step 12063 | loss 3.295772 | norm 0.3260 | time 466.1822 ms | tok/sec 1124641.7476
for step 12064 | loss 3.301277 | norm 0.3078 | time 465.9593 ms | tok/sec 1125179.7918
for step 12065 | loss 3.303691 | norm 0.2940 | time 464.9131 ms | tok/sec 1127711.7517
for step 12066 | loss 3.354504 | norm 0.3634 | time 464.9551 ms | tok/sec 1127609.9770
for step 12067 | loss 3.357592 | norm 0.3622 | time 465.4193 ms | tok/sec 1126485.3187
for step 12068 | loss 3.300073 | norm 0.3202 | time 464.0398 ms | tok/sec 1129834.1158
for step 12069 | loss 3.363760 | norm 0.3915 | time 465.2932 ms | tok/sec 1126790.6664
for step 12070 | loss 3.265724 | norm 0.3415 | time 466.0954 ms | tok/sec 1124851.1496
for step 12071 | loss 3.254374 | norm 0.3552 | time 465.0767 ms | tok/sec 1127315.1654
for step 12072 | loss 3.342026 | norm 0.3542 | time 465.5013 ms | tok/sec 1126286.8449
for step 12073 | loss 3.323205 | norm 0.3683 | time 465.3187 ms | tok/sec 1126728.8909
for step 12074 | loss 3.262001 | norm 0.3463 | time 465.4050 ms | tok/sec 1126519.9434
for step 12075 | loss 3.262347 | norm 0.3291 | time 465.9278 ms | tok/sec 1125255.7924
for step 12076 | loss 3.254146 | norm 0.3124 | time 465.7738 ms | tok/sec 1125627.8831
for step 12077 | loss 3.217767 | norm 0.3213 | time 465.3642 ms | tok/sec 1126618.6355
for step 12078 | loss 3.280456 | norm 0.3389 | time 465.4121 ms | tok/sec 1126502.6308
for step 12079 | loss 3.259032 | norm 0.2990 | time 465.0254 ms | tok/sec 1127439.4299
for step 12080 | loss 3.266913 | norm 0.3272 | time 465.3504 ms | tok/sec 1126652.1139
for step 12081 | loss 3.306057 | norm 0.3008 | time 465.5828 ms | tok/sec 1126089.5944
for step 12082 | loss 3.256297 | norm 0.3014 | time 464.3879 ms | tok/sec 1128987.2263
for step 12083 | loss 3.231552 | norm 0.2847 | time 465.0507 ms | tok/sec 1127378.1612
for step 12084 | loss 3.282898 | norm 0.2897 | time 465.1241 ms | tok/sec 1127200.1728
for step 12085 | loss 3.303701 | norm 0.3463 | time 465.3914 ms | tok/sec 1126552.8388
for step 12086 | loss 3.235463 | norm 0.3243 | time 465.5030 ms | tok/sec 1126282.8070
for step 12087 | loss 3.262619 | norm 0.3035 | time 465.6980 ms | tok/sec 1125811.1389
for step 12088 | loss 3.307637 | norm 0.2984 | time 465.3852 ms | tok/sec 1126567.8444
for step 12089 | loss 3.198473 | norm 0.3278 | time 465.1275 ms | tok/sec 1127192.0838
for step 12090 | loss 3.228131 | norm 0.3210 | time 465.1558 ms | tok/sec 1127123.3316
for step 12091 | loss 3.239251 | norm 0.3156 | time 467.9964 ms | tok/sec 1120282.2203
for step 12092 | loss 3.256224 | norm 0.3227 | time 464.7708 ms | tok/sec 1128057.1130
for step 12093 | loss 3.243662 | norm 0.2978 | time 465.5073 ms | tok/sec 1126272.4237
for step 12094 | loss 3.227404 | norm 0.3200 | time 465.0617 ms | tok/sec 1127351.5749
for step 12095 | loss 3.212646 | norm 0.2791 | time 464.6282 ms | tok/sec 1128403.2650
for step 12096 | loss 3.220236 | norm 0.3126 | time 464.9746 ms | tok/sec 1127562.5655
for step 12097 | loss 3.293590 | norm 0.3512 | time 464.4997 ms | tok/sec 1128715.4469
for step 12098 | loss 3.309771 | norm 0.3449 | time 465.3423 ms | tok/sec 1126671.7401
for step 12099 | loss 3.267305 | norm 0.3446 | time 465.3637 ms | tok/sec 1126619.7898
for step 12100 | loss 3.289697 | norm 0.3395 | time 465.7130 ms | tok/sec 1125774.8288
for step 12101 | loss 3.252272 | norm 0.3230 | time 465.4665 ms | tok/sec 1126371.0724
for step 12102 | loss 3.250547 | norm 0.3584 | time 465.9853 ms | tok/sec 1125117.0414
for step 12103 | loss 3.277322 | norm 0.3375 | time 465.0128 ms | tok/sec 1127470.0668
for step 12104 | loss 3.298131 | norm 0.3245 | time 465.0881 ms | tok/sec 1127287.4263
for step 12105 | loss 3.273645 | norm 0.3401 | time 465.2369 ms | tok/sec 1126926.9430
for step 12106 | loss 3.342338 | norm 0.3125 | time 465.9755 ms | tok/sec 1125140.6439
for step 12107 | loss 3.263127 | norm 0.3395 | time 464.9231 ms | tok/sec 1127687.4629
for step 12108 | loss 3.235040 | norm 0.3137 | time 465.4012 ms | tok/sec 1126529.1770
for step 12109 | loss 3.259723 | norm 0.3068 | time 465.5194 ms | tok/sec 1126243.0055
for step 12110 | loss 3.271239 | norm 0.3379 | time 465.8210 ms | tok/sec 1125513.8106
for step 12111 | loss 3.283277 | norm 0.3096 | time 465.3854 ms | tok/sec 1126567.2673
for step 12112 | loss 3.250714 | norm 0.3195 | time 465.9290 ms | tok/sec 1125252.9134
for step 12113 | loss 3.245634 | norm 0.3254 | time 465.4753 ms | tok/sec 1126349.7259
for step 12114 | loss 3.271439 | norm 0.3260 | time 466.1746 ms | tok/sec 1124660.1535
for step 12115 | loss 3.246972 | norm 0.3337 | time 465.9314 ms | tok/sec 1125247.1555
for step 12116 | loss 3.245557 | norm 0.3730 | time 464.8645 ms | tok/sec 1127829.7408
for step 12117 | loss 3.297921 | norm 0.3382 | time 465.1582 ms | tok/sec 1127117.5545
for step 12118 | loss 3.230496 | norm 0.3737 | time 465.7123 ms | tok/sec 1125776.5578
for step 12119 | loss 3.250015 | norm 0.3310 | time 465.6060 ms | tok/sec 1126033.6616
for step 12120 | loss 3.319014 | norm 0.3644 | time 465.0118 ms | tok/sec 1127472.3790
for step 12121 | loss 3.268109 | norm 0.3075 | time 465.4896 ms | tok/sec 1126315.1116
for step 12122 | loss 3.214626 | norm 0.5464 | time 465.5242 ms | tok/sec 1126231.4694
for step 12123 | loss 3.247808 | norm 0.3211 | time 465.4288 ms | tok/sec 1126462.2368
for step 12124 | loss 3.386667 | norm 0.3694 | time 465.9061 ms | tok/sec 1125308.1928
for step 12125 | loss 3.242743 | norm 0.3344 | time 465.0385 ms | tok/sec 1127407.6387
for step 12126 | loss 3.247373 | norm 0.3655 | time 465.1022 ms | tok/sec 1127253.3323
for step 12127 | loss 3.219837 | norm 0.3134 | time 465.2889 ms | tok/sec 1126801.0592
for step 12128 | loss 3.201762 | norm 0.3945 | time 465.4512 ms | tok/sec 1126407.9980
for step 12129 | loss 3.202029 | norm 0.2942 | time 464.7005 ms | tok/sec 1128227.8471
for step 12130 | loss 3.242107 | norm 0.3323 | time 464.9577 ms | tok/sec 1127603.6167
for step 12131 | loss 3.270612 | norm 0.3124 | time 464.5360 ms | tok/sec 1128627.3929
for step 12132 | loss 3.287719 | norm 0.3330 | time 465.2829 ms | tok/sec 1126815.4940
for step 12133 | loss 3.295855 | norm 0.3672 | time 465.7195 ms | tok/sec 1125759.2680
for step 12134 | loss 3.260356 | norm 0.3272 | time 465.2910 ms | tok/sec 1126795.8628
for step 12135 | loss 3.404121 | norm 0.3266 | time 465.5333 ms | tok/sec 1126209.5514
for step 12136 | loss 3.302584 | norm 0.3223 | time 465.6565 ms | tok/sec 1125911.4362
for step 12137 | loss 3.325133 | norm 0.4227 | time 465.5356 ms | tok/sec 1126203.7837
for step 12138 | loss 3.259907 | norm 0.3618 | time 465.6634 ms | tok/sec 1125894.7187
for step 12139 | loss 3.312605 | norm 0.3180 | time 464.5057 ms | tok/sec 1128700.9634
for step 12140 | loss 3.307429 | norm 0.3579 | time 465.6725 ms | tok/sec 1125872.8138
for step 12141 | loss 3.302162 | norm 0.3309 | time 464.7067 ms | tok/sec 1128212.7972
for step 12142 | loss 3.323947 | norm 0.3902 | time 464.7288 ms | tok/sec 1128158.9685
for step 12143 | loss 3.288661 | norm 0.3315 | time 464.2780 ms | tok/sec 1129254.4974
for step 12144 | loss 3.262679 | norm 0.3693 | time 465.3723 ms | tok/sec 1126599.0111
for step 12145 | loss 3.334532 | norm 0.3459 | time 465.3704 ms | tok/sec 1126603.6285
for step 12146 | loss 3.259218 | norm 0.3134 | time 464.6318 ms | tok/sec 1128394.5796
for step 12147 | loss 3.279159 | norm 0.3298 | time 465.6510 ms | tok/sec 1125924.6952
for step 12148 | loss 3.259075 | norm 0.3204 | time 466.0966 ms | tok/sec 1124848.2727
for step 12149 | loss 3.288883 | norm 0.2937 | time 464.5720 ms | tok/sec 1128539.9319
for step 12150 | loss 3.266214 | norm 0.3588 | time 466.5647 ms | tok/sec 1123719.9261
for step 12151 | loss 3.342639 | norm 0.3302 | time 466.2242 ms | tok/sec 1124540.5263
for step 12152 | loss 3.245300 | norm 0.3770 | time 465.4207 ms | tok/sec 1126481.8564
for step 12153 | loss 3.229653 | norm 0.3298 | time 464.7141 ms | tok/sec 1128194.8537
for step 12154 | loss 3.307309 | norm 0.3698 | time 466.1801 ms | tok/sec 1124646.9242
for step 12155 | loss 3.213706 | norm 0.3405 | time 464.9541 ms | tok/sec 1127612.2898
for step 12156 | loss 3.227114 | norm 0.3445 | time 464.9699 ms | tok/sec 1127574.1289
for step 12157 | loss 3.251190 | norm 0.3644 | time 465.6324 ms | tok/sec 1125969.6629
for step 12158 | loss 3.188787 | norm 0.3500 | time 465.6539 ms | tok/sec 1125917.7774
for step 12159 | loss 3.246477 | norm 0.3133 | time 465.7712 ms | tok/sec 1125634.2211
for step 12160 | loss 3.252596 | norm 0.3450 | time 466.0313 ms | tok/sec 1125005.9501
for step 12161 | loss 3.327814 | norm 0.3174 | time 464.5948 ms | tok/sec 1128484.3346
for step 12162 | loss 3.221941 | norm 0.3545 | time 465.5139 ms | tok/sec 1126256.2723
for step 12163 | loss 3.293305 | norm 0.3556 | time 465.3809 ms | tok/sec 1126578.2331
for step 12164 | loss 3.226687 | norm 0.3515 | time 463.5086 ms | tok/sec 1131128.9440
for step 12165 | loss 3.260783 | norm 0.3148 | time 464.9296 ms | tok/sec 1127671.8492
for step 12166 | loss 3.318256 | norm 0.3336 | time 464.8185 ms | tok/sec 1127941.3905
for step 12167 | loss 3.300227 | norm 0.3543 | time 465.3487 ms | tok/sec 1126656.1545
for step 12168 | loss 3.299718 | norm 0.3675 | time 464.9744 ms | tok/sec 1127563.1437
for step 12169 | loss 3.316224 | norm 0.3790 | time 465.5077 ms | tok/sec 1126271.2700
for step 12170 | loss 3.307752 | norm 0.3487 | time 465.3568 ms | tok/sec 1126636.5288
for step 12171 | loss 3.217718 | norm 0.3376 | time 465.4303 ms | tok/sec 1126458.7746
for step 12172 | loss 3.280882 | norm 0.3257 | time 465.4877 ms | tok/sec 1126319.7268
for step 12173 | loss 3.271909 | norm 0.3533 | time 464.6645 ms | tok/sec 1128315.2597
for step 12174 | loss 3.289670 | norm 0.3242 | time 464.8912 ms | tok/sec 1127764.9594
for step 12175 | loss 3.318094 | norm 0.3229 | time 465.8790 ms | tok/sec 1125373.8441
for step 12176 | loss 3.225514 | norm 0.3273 | time 465.7857 ms | tok/sec 1125599.0747
for step 12177 | loss 3.258781 | norm 0.3243 | time 465.3623 ms | tok/sec 1126623.2530
for step 12178 | loss 3.213114 | norm 0.3196 | time 465.2007 ms | tok/sec 1127014.7318
for step 12179 | loss 3.255734 | norm 0.3050 | time 465.0137 ms | tok/sec 1127467.7545
for step 12180 | loss 3.229850 | norm 0.3173 | time 464.4537 ms | tok/sec 1128827.2720
for step 12181 | loss 3.248547 | norm 0.2875 | time 464.8554 ms | tok/sec 1127851.7219
for step 12182 | loss 3.330223 | norm 0.3109 | time 466.0585 ms | tok/sec 1124940.3417
for step 12183 | loss 3.265358 | norm 0.2747 | time 465.0676 ms | tok/sec 1127337.1264
for step 12184 | loss 3.288269 | norm 0.2794 | time 464.6220 ms | tok/sec 1128418.3199
for step 12185 | loss 3.295201 | norm 0.2974 | time 466.0320 ms | tok/sec 1125004.2235
for step 12186 | loss 3.321182 | norm 0.3296 | time 465.1704 ms | tok/sec 1127088.0921
for step 12187 | loss 3.278670 | norm 0.3283 | time 465.2491 ms | tok/sec 1126897.4906
for step 12188 | loss 3.295928 | norm 0.2913 | time 465.3807 ms | tok/sec 1126578.8103
for step 12189 | loss 3.268530 | norm 0.3266 | time 464.9715 ms | tok/sec 1127570.0817
for step 12190 | loss 3.254490 | norm 0.3670 | time 464.8573 ms | tok/sec 1127847.0942
Will loading at 0 from edu_fineweb10B/edufineweb_train_000065.npy
for step 12191 | loss 3.228027 | norm 0.2791 | time 2531.4479 ms | tok/sec 207109.9321
for step 12192 | loss 3.233858 | norm 0.3391 | time 474.4961 ms | tok/sec 1104936.3126
for step 12193 | loss 3.250813 | norm 0.3097 | time 463.2709 ms | tok/sec 1131709.3239
for step 12194 | loss 3.220460 | norm 0.3087 | time 465.1527 ms | tok/sec 1127130.8419
for step 12195 | loss 3.256135 | norm 0.3337 | time 464.1387 ms | tok/sec 1129593.2611
for step 12196 | loss 3.272164 | norm 0.3029 | time 464.2870 ms | tok/sec 1129232.4615
for step 12197 | loss 3.293024 | norm 0.3004 | time 464.0350 ms | tok/sec 1129845.7258
for step 12198 | loss 3.226208 | norm 0.2876 | time 464.0572 ms | tok/sec 1129791.7411
for step 12199 | loss 3.289124 | norm 0.3080 | time 464.4616 ms | tok/sec 1128808.1501
for step 12200 | loss 3.239420 | norm 0.2873 | time 464.6692 ms | tok/sec 1128303.6811
for step 12201 | loss 3.278965 | norm 0.3278 | time 464.9153 ms | tok/sec 1127706.5469
for step 12202 | loss 3.268505 | norm 0.3248 | time 464.8345 ms | tok/sec 1127902.6288
for step 12203 | loss 3.306917 | norm 0.3301 | time 465.1253 ms | tok/sec 1127197.2839
for step 12204 | loss 3.295666 | norm 0.3164 | time 465.4834 ms | tok/sec 1126330.1109
for step 12205 | loss 3.286532 | norm 0.3427 | time 464.3707 ms | tok/sec 1129028.9609
for step 12206 | loss 3.272779 | norm 0.3041 | time 465.0133 ms | tok/sec 1127468.9106
for step 12207 | loss 3.258528 | norm 0.3311 | time 466.4171 ms | tok/sec 1124075.4874
for step 12208 | loss 3.292180 | norm 0.3065 | time 465.1606 ms | tok/sec 1127111.7774
for step 12209 | loss 3.241975 | norm 0.3312 | time 466.6297 ms | tok/sec 1123563.1831
for step 12210 | loss 3.295105 | norm 0.3272 | time 465.1763 ms | tok/sec 1127073.6504
for step 12211 | loss 3.257088 | norm 0.3301 | time 465.7178 ms | tok/sec 1125763.3022
for step 12212 | loss 3.270285 | norm 0.3321 | time 464.5836 ms | tok/sec 1128511.5534
for step 12213 | loss 3.267452 | norm 0.3422 | time 465.8246 ms | tok/sec 1125505.1697
for step 12214 | loss 3.268102 | norm 0.3403 | time 465.3032 ms | tok/sec 1126766.4173
for step 12215 | loss 3.303766 | norm 0.3333 | time 465.0621 ms | tok/sec 1127350.4190
for step 12216 | loss 3.287435 | norm 0.3448 | time 466.2521 ms | tok/sec 1124473.2472
for step 12217 | loss 3.259693 | norm 0.2980 | time 465.1711 ms | tok/sec 1127086.3591
for step 12218 | loss 3.251562 | norm 0.3204 | time 464.9951 ms | tok/sec 1127512.8455
for step 12219 | loss 3.232614 | norm 0.2983 | time 466.2328 ms | tok/sec 1124519.8241
for step 12220 | loss 3.241434 | norm 0.3239 | time 466.6097 ms | tok/sec 1123611.4070
for step 12221 | loss 3.239840 | norm 0.3026 | time 465.9615 ms | tok/sec 1125174.6103
for step 12222 | loss 3.266560 | norm 0.3242 | time 464.8983 ms | tok/sec 1127747.6085
for step 12223 | loss 3.217062 | norm 0.3225 | time 465.6708 ms | tok/sec 1125876.8489
for step 12224 | loss 3.219940 | norm 0.3106 | time 465.2865 ms | tok/sec 1126806.8331
for step 12225 | loss 3.228003 | norm 0.3307 | time 465.0977 ms | tok/sec 1127264.3115
for step 12226 | loss 3.252889 | norm 0.2870 | time 465.3420 ms | tok/sec 1126672.3173
for step 12227 | loss 3.255268 | norm 0.3589 | time 465.7950 ms | tok/sec 1125576.6052
for step 12228 | loss 3.244916 | norm 0.3017 | time 465.2061 ms | tok/sec 1127001.4471
for step 12229 | loss 3.316578 | norm 0.3316 | time 465.7679 ms | tok/sec 1125642.2878
for step 12230 | loss 3.242445 | norm 0.3074 | time 465.7345 ms | tok/sec 1125722.9612
for step 12231 | loss 3.277534 | norm 0.3123 | time 465.1501 ms | tok/sec 1127137.1969
for step 12232 | loss 3.255147 | norm 0.3340 | time 465.4381 ms | tok/sec 1126439.7328
for step 12233 | loss 3.247286 | norm 0.3040 | time 465.6007 ms | tok/sec 1126046.3469
for step 12234 | loss 3.290735 | norm 0.3059 | time 465.0230 ms | tok/sec 1127445.2103
for step 12235 | loss 3.264919 | norm 0.3198 | time 465.3566 ms | tok/sec 1126637.1061
for step 12236 | loss 3.288046 | norm 0.3342 | time 466.9032 ms | tok/sec 1122905.1099
for step 12237 | loss 3.319741 | norm 0.3363 | time 465.5104 ms | tok/sec 1126264.9248
for step 12238 | loss 3.257004 | norm 0.3108 | time 466.0614 ms | tok/sec 1124933.4360
for step 12239 | loss 3.326025 | norm 0.3677 | time 465.0781 ms | tok/sec 1127311.6979
for step 12240 | loss 3.266210 | norm 0.3474 | time 467.7978 ms | tok/sec 1120757.8340
for step 12241 | loss 3.275603 | norm 0.3200 | time 464.9944 ms | tok/sec 1127514.5799
for step 12242 | loss 3.298193 | norm 0.3774 | time 465.1086 ms | tok/sec 1127237.7306
for step 12243 | loss 3.331185 | norm 0.2991 | time 465.2743 ms | tok/sec 1126836.2807
for step 12244 | loss 3.348129 | norm 0.3637 | time 466.1865 ms | tok/sec 1124631.3946
for step 12245 | loss 3.291929 | norm 0.3174 | time 465.9181 ms | tok/sec 1125279.4008
for step 12246 | loss 3.266875 | norm 0.3266 | time 464.8790 ms | tok/sec 1127794.4572
for step 12247 | loss 3.252943 | norm 0.3203 | time 465.1108 ms | tok/sec 1127232.5302
for step 12248 | loss 3.224437 | norm 0.3207 | time 464.8015 ms | tok/sec 1127982.4693
for step 12249 | loss 3.264269 | norm 0.3162 | time 465.9340 ms | tok/sec 1125240.8218
validation loss 3.2841
HellaSwag accuracy: 2817/10042=0.2805
> Hello, I'm a language model, and this's a big one. All I care is that as you're developing your skills, and you should be able
> Hello, I'm a language model, a model that explains how to build the code. I would like to do an original test for my language model, but
> Hello, I'm a language model, but your job is to keep the language model in the box at hand and simplify it. If you want a way to
> Hello, I'm a language model, and I'm just a guy, that takes a lot of time to develop something. At the time of writing this article
> Hello, I'm a language model, so I want to model something like the world, which is not a computer-like representation you can build in the real
> > Hello, I'm a language model, to use the C++ C Language framework.
First, I'm talking about programming as an abstract syntax, programming in
Hello, I'm a language model, and I know that you should start thinking what these terms mean, especially during an interview and an analysis, or when a
> Hello, I'm a language model, I see them I like. I think it's a good language modeling tool because that's what we talk about now,
> Hello, I'm a language model, so my translation is a little complicated—I've found plenty of other explanations, and some helpful links (and links to> 
Hello, I'm a language model, and it plays quite a big role in this process.... it gets very similar to the way we communicate.
A good
> > Hello, I'm a language model, learning an English language with more input and understanding. If you are curious to get in touch with me, please feel free
Hello, I'm a language model, so I could ask "Does your machine have three types of computers"
- If I were to ask "Does my
> Hello, I'm a language model, and I don't care what kind of content you use. I think of it like the one with a name, and>>
 Hello, I'm a language model, and like I've been working with a few kids, I'm always amazed how smart kids look. For example, an>  Hello, I'm a language model, and I'm a person of color and I am an international group. These 3 groups live here in Europe and in Mexico> 
Hello, I'm a language model, but i have a little problem. It depends. In this case, you have two pieces of paper and one piece of

> Hello, I'm a language model, and if you don't like this, just give me a call back to them.
I'm still at writing a
> > Hello, I'm a language model, and am a graduate student in linguistics at the University of Wisconsin-Milwaukee. I'm also a writer, researcher
> > Hello, I'm a language model, has I needed to teach this course as I start to teach my clients how to understand a new language, especially English.
Hello, I'm a language model, and this isn't a free version, I'm actually reading it, and I'm not sure if I'm writing about
> Hello, I'm a language model, and I love to do some research on me.
A major part of my problem is that languages don’t
Hello, I'm a language model, and i'm interested in the fact that each time I use a particular language in my head which is only possible between "
> > Hello, I'm a language model, because I wish it was easy to see I'm always getting more accurate responses.
I have to explain that I need
> Hello, I'm a language model, and you know that at the time I was doing a lot of things at an event which was done on the internet that
Hello, I'm a language model, and you've created an awesome and fun topic: how to teach a new language like English in context. I don't
> Hello, I'm a language model, teaching myself new words.
For all of the "learning" concepts, I do use them in their native tongue;
> > Hello, I'm a language model, so I've got to be very accurate in my math. This model takes you on a ride-on and goes off
> Hello, I'm a language model, since I have just started this blog.
So here I am going to write two things. Here is a link to
Hello, I'm a language model, and as a student, I like to learn new vocabulary to help students understand the sounds we create. Because of my background
> Hello, I'm a language model, and I am actually creating a small language dictionary. A few weeks ago, I wrote about the problem in a little paragraph
Hello, I'm a language model, but for good reason I am not a specific language model. You can't read or write as the models in this example
> Hello, I'm a language model, my mother is a professional trainer and now I need to show my daughter exactly how to speak. As she is having difficulties
for step 12250 | loss 3.228925 | norm 0.3350 | time 12480.9783 ms | tok/sec 42006.9637
for step 12251 | loss 3.292744 | norm 0.2774 | time 461.9401 ms | tok/sec 1134969.7864
for step 12252 | loss 3.233047 | norm 0.3136 | time 462.0645 ms | tok/sec 1134664.0886
for step 12253 | loss 3.259772 | norm 0.2903 | time 462.5344 ms | tok/sec 1133511.2992
for step 12254 | loss 3.233419 | norm 0.3077 | time 462.4710 ms | tok/sec 1133666.7393
for step 12255 | loss 3.344185 | norm 0.3349 | time 463.3904 ms | tok/sec 1131417.6042
for step 12256 | loss 3.280166 | norm 0.3647 | time 462.5492 ms | tok/sec 1133475.0749
for step 12257 | loss 3.311458 | norm 0.2793 | time 463.3446 ms | tok/sec 1131529.3832
for step 12258 | loss 3.256050 | norm 0.3487 | time 462.7745 ms | tok/sec 1132923.2333
for step 12259 | loss 3.259983 | norm 0.2885 | time 463.1226 ms | tok/sec 1132071.7082
for step 12260 | loss 3.285437 | norm 0.3039 | time 463.2537 ms | tok/sec 1131751.2600
for step 12261 | loss 3.337823 | norm 0.3209 | time 463.2406 ms | tok/sec 1131783.2967
for step 12262 | loss 3.228408 | norm 0.3253 | time 464.2134 ms | tok/sec 1129411.6724
for step 12263 | loss 3.258489 | norm 0.3123 | time 463.5341 ms | tok/sec 1131066.6919
for step 12264 | loss 3.279760 | norm 0.3140 | time 464.0994 ms | tok/sec 1129689.0103
for step 12265 | loss 3.265124 | norm 0.3348 | time 464.3009 ms | tok/sec 1129198.8296
for step 12266 | loss 3.270269 | norm 0.3715 | time 464.6652 ms | tok/sec 1128313.5229
for step 12267 | loss 3.235118 | norm 0.3731 | time 464.3786 ms | tok/sec 1129009.8322
for step 12268 | loss 3.269212 | norm 0.3032 | time 463.7809 ms | tok/sec 1130464.8868
for step 12269 | loss 3.266477 | norm 0.3677 | time 465.1506 ms | tok/sec 1127136.0414
for step 12270 | loss 3.251311 | norm 0.3366 | time 464.9050 ms | tok/sec 1127731.4148
for step 12271 | loss 3.347465 | norm 0.3676 | time 463.9561 ms | tok/sec 1130037.9068
for step 12272 | loss 3.321768 | norm 0.3528 | time 466.0022 ms | tok/sec 1125076.1711
for step 12273 | loss 3.292743 | norm 0.3895 | time 464.7670 ms | tok/sec 1128066.3718
for step 12274 | loss 3.249755 | norm 0.3481 | time 464.4489 ms | tok/sec 1128838.8614
for step 12275 | loss 3.321417 | norm 0.4055 | time 464.5998 ms | tok/sec 1128472.1734
for step 12276 | loss 3.296079 | norm 0.3404 | time 465.1611 ms | tok/sec 1127110.6220
for step 12277 | loss 3.249810 | norm 0.3557 | time 465.4980 ms | tok/sec 1126294.9210
for step 12278 | loss 3.266349 | norm 0.3223 | time 464.8521 ms | tok/sec 1127859.8204
for step 12279 | loss 3.308851 | norm 0.3924 | time 465.2443 ms | tok/sec 1126909.0404
for step 12280 | loss 3.289010 | norm 0.3767 | time 465.2319 ms | tok/sec 1126939.0709
for step 12281 | loss 3.285438 | norm 0.3283 | time 464.8979 ms | tok/sec 1127748.7652
for step 12282 | loss 3.332391 | norm 0.4546 | time 465.5666 ms | tok/sec 1126128.8083
for step 12283 | loss 3.325288 | norm 0.3618 | time 465.9245 ms | tok/sec 1125263.8537
for step 12284 | loss 3.213818 | norm 0.3297 | time 466.4598 ms | tok/sec 1123972.6444
for step 12285 | loss 3.239262 | norm 0.3941 | time 465.3127 ms | tok/sec 1126743.3238
for step 12286 | loss 3.290433 | norm 0.3357 | time 466.2039 ms | tok/sec 1124589.4093
for step 12287 | loss 3.294338 | norm 0.3385 | time 464.4108 ms | tok/sec 1128931.5849
for step 12288 | loss 3.240386 | norm 0.3142 | time 464.8623 ms | tok/sec 1127834.9468
for step 12289 | loss 3.255511 | norm 0.3212 | time 465.2426 ms | tok/sec 1126913.0828
for step 12290 | loss 3.265339 | norm 0.3035 | time 465.8113 ms | tok/sec 1125537.4298
for step 12291 | loss 3.232447 | norm 0.3143 | time 465.4424 ms | tok/sec 1126429.3467
for step 12292 | loss 3.272823 | norm 0.2980 | time 465.5137 ms | tok/sec 1126256.8492
for step 12293 | loss 3.224648 | norm 0.3372 | time 465.3382 ms | tok/sec 1126681.5534
for step 12294 | loss 3.261140 | norm 0.2957 | time 465.3268 ms | tok/sec 1126709.2627
for step 12295 | loss 3.270766 | norm 0.3300 | time 465.4703 ms | tok/sec 1126361.8414
for step 12296 | loss 3.253823 | norm 0.3091 | time 466.1441 ms | tok/sec 1124733.7829
for step 12297 | loss 3.289832 | norm 0.3384 | time 464.5212 ms | tok/sec 1128663.3080
for step 12298 | loss 3.258704 | norm 0.2914 | time 465.7035 ms | tok/sec 1125797.8825
for step 12299 | loss 3.249009 | norm 0.3186 | time 465.0087 ms | tok/sec 1127479.8940
for step 12300 | loss 3.256842 | norm 0.3365 | time 464.7775 ms | tok/sec 1128040.9104
for step 12301 | loss 3.270459 | norm 0.3385 | time 464.9532 ms | tok/sec 1127614.6027
for step 12302 | loss 3.217625 | norm 0.3224 | time 464.5972 ms | tok/sec 1128478.5435
for step 12303 | loss 3.283229 | norm 0.3067 | time 464.9286 ms | tok/sec 1127674.1623
for step 12304 | loss 3.285228 | norm 0.3087 | time 465.3969 ms | tok/sec 1126539.5650
for step 12305 | loss 3.308968 | norm 0.3342 | time 465.5333 ms | tok/sec 1126209.5514
for step 12306 | loss 3.242456 | norm 0.3005 | time 465.2486 ms | tok/sec 1126898.6456
for step 12307 | loss 3.248378 | norm 0.3440 | time 465.2543 ms | tok/sec 1126884.7861
for step 12308 | loss 3.328850 | norm 0.2853 | time 466.1114 ms | tok/sec 1124812.6000
for step 12309 | loss 3.235000 | norm 0.3246 | time 464.6277 ms | tok/sec 1128404.4230
for step 12310 | loss 3.300541 | norm 0.3262 | time 465.0764 ms | tok/sec 1127315.7433
for step 12311 | loss 3.262649 | norm 0.3455 | time 464.5460 ms | tok/sec 1128603.0647
for step 12312 | loss 3.281702 | norm 0.3265 | time 464.1731 ms | tok/sec 1129509.7114
for step 12313 | loss 3.248502 | norm 0.3006 | time 464.9458 ms | tok/sec 1127632.5278
for step 12314 | loss 3.230607 | norm 0.3315 | time 464.9243 ms | tok/sec 1127684.5714
for step 12315 | loss 3.277797 | norm 0.2996 | time 465.1289 ms | tok/sec 1127188.6171
for step 12316 | loss 3.234150 | norm 0.3038 | time 465.0941 ms | tok/sec 1127272.9794
for step 12317 | loss 3.304444 | norm 0.3307 | time 464.9761 ms | tok/sec 1127559.0965
for step 12318 | loss 3.303305 | norm 0.3033 | time 465.9066 ms | tok/sec 1125307.0411
for step 12319 | loss 3.267970 | norm 0.3329 | time 464.7148 ms | tok/sec 1128193.1173
for step 12320 | loss 3.254333 | norm 0.3136 | time 464.5870 ms | tok/sec 1128503.4455
for step 12321 | loss 3.270257 | norm 0.3362 | time 465.6000 ms | tok/sec 1126048.0768
for step 12322 | loss 3.272368 | norm 0.3276 | time 464.9124 ms | tok/sec 1127713.4867
for step 12323 | loss 3.270591 | norm 0.3102 | time 464.9620 ms | tok/sec 1127593.2091
for step 12324 | loss 3.252397 | norm 0.3462 | time 465.1320 ms | tok/sec 1127181.1060
for step 12325 | loss 3.306426 | norm 0.3244 | time 465.0178 ms | tok/sec 1127457.9274
for step 12326 | loss 3.452419 | norm 0.4192 | time 465.1344 ms | tok/sec 1127175.3283
for step 12327 | loss 3.233136 | norm 0.4406 | time 464.9706 ms | tok/sec 1127572.3944
for step 12328 | loss 3.241338 | norm 0.3552 | time 465.8194 ms | tok/sec 1125517.8431
for step 12329 | loss 3.204101 | norm 0.3735 | time 465.2214 ms | tok/sec 1126964.4826
for step 12330 | loss 3.233874 | norm 0.3484 | time 465.5688 ms | tok/sec 1126123.6181
for step 12331 | loss 3.206207 | norm 0.3435 | time 465.3704 ms | tok/sec 1126603.6285
for step 12332 | loss 3.252722 | norm 0.3800 | time 465.3943 ms | tok/sec 1126545.9133
for step 12333 | loss 3.209157 | norm 0.3350 | time 465.3106 ms | tok/sec 1126748.5197
for step 12334 | loss 3.223487 | norm 0.3362 | time 464.9243 ms | tok/sec 1127684.5714
for step 12335 | loss 3.244393 | norm 0.3383 | time 465.0047 ms | tok/sec 1127489.7215
for step 12336 | loss 3.232460 | norm 0.3683 | time 464.5391 ms | tok/sec 1128619.8627
for step 12337 | loss 3.290828 | norm 0.3638 | time 465.9853 ms | tok/sec 1125117.0414
for step 12338 | loss 3.284052 | norm 0.3567 | time 464.8156 ms | tok/sec 1127948.3332
for step 12339 | loss 3.253311 | norm 0.3370 | time 465.9224 ms | tok/sec 1125269.0360
for step 12340 | loss 3.329632 | norm 0.3424 | time 464.5014 ms | tok/sec 1128711.3915
for step 12341 | loss 3.381768 | norm 0.3565 | time 464.9146 ms | tok/sec 1127708.2818
for step 12342 | loss 3.267645 | norm 0.3789 | time 464.5236 ms | tok/sec 1128657.5151
for step 12343 | loss 3.238268 | norm 0.3334 | time 464.5107 ms | tok/sec 1128688.7975
for step 12344 | loss 3.278705 | norm 0.3447 | time 465.1392 ms | tok/sec 1127163.7730
for step 12345 | loss 3.273860 | norm 0.3913 | time 465.0724 ms | tok/sec 1127325.5678
for step 12346 | loss 3.278362 | norm 0.3134 | time 464.9515 ms | tok/sec 1127618.6503
for step 12347 | loss 3.301373 | norm 0.4006 | time 465.5023 ms | tok/sec 1126284.5375
for step 12348 | loss 3.308988 | norm 0.2990 | time 465.4288 ms | tok/sec 1126462.2368
for step 12349 | loss 3.268526 | norm 0.3535 | time 465.5912 ms | tok/sec 1126069.4118
for step 12350 | loss 3.271163 | norm 0.3156 | time 464.8879 ms | tok/sec 1127773.0567
for step 12351 | loss 3.271711 | norm 0.3260 | time 464.7808 ms | tok/sec 1128032.8093
for step 12352 | loss 3.289552 | norm 0.3282 | time 465.3978 ms | tok/sec 1126537.2566
for step 12353 | loss 3.326843 | norm 0.3191 | time 465.0745 ms | tok/sec 1127320.3666
for step 12354 | loss 3.215375 | norm 0.3025 | time 465.2007 ms | tok/sec 1127014.7318
for step 12355 | loss 3.354831 | norm 0.3426 | time 465.1604 ms | tok/sec 1127112.3551
for step 12356 | loss 3.228426 | norm 0.3492 | time 465.8370 ms | tok/sec 1125475.2155
for step 12357 | loss 3.302555 | norm 0.3091 | time 466.2755 ms | tok/sec 1124416.8999
for step 12358 | loss 3.215302 | norm 0.3316 | time 464.9246 ms | tok/sec 1127683.9931
for step 12359 | loss 3.257826 | norm 0.3115 | time 465.3761 ms | tok/sec 1126589.7763
for step 12360 | loss 3.228139 | norm 0.3566 | time 465.2402 ms | tok/sec 1126918.8579
for step 12361 | loss 3.304041 | norm 0.3202 | time 465.0557 ms | tok/sec 1127366.0238
for step 12362 | loss 3.278962 | norm 0.3257 | time 464.8845 ms | tok/sec 1127781.1541
for step 12363 | loss 3.270209 | norm 0.3034 | time 465.3475 ms | tok/sec 1126659.0407
for step 12364 | loss 3.229472 | norm 0.2906 | time 464.2341 ms | tok/sec 1129361.2092
for step 12365 | loss 3.311347 | norm 0.3449 | time 465.5771 ms | tok/sec 1126103.4343
for step 12366 | loss 3.223994 | norm 0.2952 | time 465.6358 ms | tok/sec 1125961.5915
for step 12367 | loss 3.285286 | norm 0.3071 | time 465.2648 ms | tok/sec 1126859.3780
for step 12368 | loss 3.254390 | norm 0.3017 | time 465.8589 ms | tok/sec 1125422.2236
for step 12369 | loss 3.209767 | norm 0.2963 | time 464.9262 ms | tok/sec 1127679.9451
for step 12370 | loss 3.283771 | norm 0.2965 | time 465.0626 ms | tok/sec 1127349.2631
for step 12371 | loss 3.321136 | norm 0.3067 | time 465.3015 ms | tok/sec 1126770.4587
for step 12372 | loss 3.308475 | norm 0.3022 | time 466.8715 ms | tok/sec 1122981.3771
for step 12373 | loss 3.261340 | norm 0.3265 | time 465.9996 ms | tok/sec 1125082.5029
for step 12374 | loss 3.274065 | norm 0.3206 | time 465.5468 ms | tok/sec 1126176.6760
for step 12375 | loss 3.301416 | norm 0.3205 | time 466.4257 ms | tok/sec 1124054.8024
for step 12376 | loss 3.256383 | norm 0.3472 | time 464.8473 ms | tok/sec 1127871.3899
for step 12377 | loss 3.250716 | norm 0.2959 | time 465.5249 ms | tok/sec 1126229.7390
for step 12378 | loss 3.248421 | norm 0.3353 | time 465.8661 ms | tok/sec 1125404.9447
for step 12379 | loss 3.394633 | norm 0.3446 | time 466.4369 ms | tok/sec 1124027.7981
for step 12380 | loss 3.310200 | norm 0.3179 | time 465.6098 ms | tok/sec 1126024.4361
for step 12381 | loss 3.312258 | norm 0.3562 | time 466.4662 ms | tok/sec 1123957.1334
Will loading at 0 from edu_fineweb10B/edufineweb_train_000066.npy
for step 12382 | loss 3.258211 | norm 0.3176 | time 2629.8461 ms | tok/sec 199360.7157
for step 12383 | loss 3.277242 | norm 0.3167 | time 463.9595 ms | tok/sec 1130029.7769
for step 12384 | loss 3.311731 | norm 0.3204 | time 465.0803 ms | tok/sec 1127306.4968
for step 12385 | loss 3.305592 | norm 0.3325 | time 465.4200 ms | tok/sec 1126483.5876
for step 12386 | loss 3.284003 | norm 0.2975 | time 464.1111 ms | tok/sec 1129660.5740
for step 12387 | loss 3.273817 | norm 0.3369 | time 463.9726 ms | tok/sec 1129997.8395
for step 12388 | loss 3.237512 | norm 0.2988 | time 464.8921 ms | tok/sec 1127762.6459
for step 12389 | loss 3.279021 | norm 0.3147 | time 464.8528 ms | tok/sec 1127858.0850
for step 12390 | loss 3.236842 | norm 0.2971 | time 463.8855 ms | tok/sec 1130209.8219
for step 12391 | loss 3.285208 | norm 0.3019 | time 464.7150 ms | tok/sec 1128192.5385
for step 12392 | loss 3.256203 | norm 0.3160 | time 464.9894 ms | tok/sec 1127526.7204
for step 12393 | loss 3.241685 | norm 0.3082 | time 464.8907 ms | tok/sec 1127766.1162
for step 12394 | loss 3.323386 | norm 0.3172 | time 465.6458 ms | tok/sec 1125937.3780
for step 12395 | loss 3.250904 | norm 0.3087 | time 464.9546 ms | tok/sec 1127611.1334
for step 12396 | loss 3.250555 | norm 0.3067 | time 465.2271 ms | tok/sec 1126950.6215
for step 12397 | loss 3.242185 | norm 0.3270 | time 465.3337 ms | tok/sec 1126692.5215
for step 12398 | loss 3.289220 | norm 0.3022 | time 465.0245 ms | tok/sec 1127441.7421
for step 12399 | loss 3.263241 | norm 0.3029 | time 465.1585 ms | tok/sec 1127116.9768
for step 12400 | loss 3.279504 | norm 0.3077 | time 467.1428 ms | tok/sec 1122329.1404
for step 12401 | loss 3.234756 | norm 0.3613 | time 464.7932 ms | tok/sec 1128002.7205
for step 12402 | loss 3.319705 | norm 0.2992 | time 466.2070 ms | tok/sec 1124581.9328
for step 12403 | loss 3.225421 | norm 0.3332 | time 466.6049 ms | tok/sec 1123622.8896
for step 12404 | loss 3.253475 | norm 0.3481 | time 465.1830 ms | tok/sec 1127057.4760
for step 12405 | loss 3.257066 | norm 0.3260 | time 465.8906 ms | tok/sec 1125345.6246
for step 12406 | loss 3.218995 | norm 0.3615 | time 464.7174 ms | tok/sec 1128186.7504
for step 12407 | loss 3.255877 | norm 0.3562 | time 465.5142 ms | tok/sec 1126255.6955
for step 12408 | loss 3.260131 | norm 0.3621 | time 466.0485 ms | tok/sec 1124964.5124
for step 12409 | loss 3.334220 | norm 0.4378 | time 465.9386 ms | tok/sec 1125229.8820
for step 12410 | loss 3.343663 | norm 0.4072 | time 465.8179 ms | tok/sec 1125521.2995
for step 12411 | loss 3.254014 | norm 0.3932 | time 466.0749 ms | tok/sec 1124900.6351
for step 12412 | loss 3.272366 | norm 0.3525 | time 465.4868 ms | tok/sec 1126322.0343
for step 12413 | loss 3.270121 | norm 0.3366 | time 465.5991 ms | tok/sec 1126050.3832
for step 12414 | loss 3.289666 | norm 0.3539 | time 465.8179 ms | tok/sec 1125521.2995
for step 12415 | loss 3.276690 | norm 0.3116 | time 466.9509 ms | tok/sec 1122790.4420
for step 12416 | loss 3.301854 | norm 0.3776 | time 465.8635 ms | tok/sec 1125411.2803
for step 12417 | loss 3.346913 | norm 0.3168 | time 465.4002 ms | tok/sec 1126531.4855
for step 12418 | loss 3.303535 | norm 0.3667 | time 466.0757 ms | tok/sec 1124898.9088
for step 12419 | loss 3.292627 | norm 0.3217 | time 465.7698 ms | tok/sec 1125637.6783
for step 12420 | loss 3.242193 | norm 0.3815 | time 465.9057 ms | tok/sec 1125309.3445
for step 12421 | loss 3.215686 | norm 0.3211 | time 465.8155 ms | tok/sec 1125527.0602
for step 12422 | loss 3.312452 | norm 0.3268 | time 465.4660 ms | tok/sec 1126372.2263
for step 12423 | loss 3.240946 | norm 0.3387 | time 464.8976 ms | tok/sec 1127749.3436
for step 12424 | loss 3.252332 | norm 0.3348 | time 465.7028 ms | tok/sec 1125799.6116
for step 12425 | loss 3.277896 | norm 0.3209 | time 466.2650 ms | tok/sec 1124442.1980
for step 12426 | loss 3.306310 | norm 0.3179 | time 465.4880 ms | tok/sec 1126319.1499
for step 12427 | loss 3.248088 | norm 0.3207 | time 466.1529 ms | tok/sec 1124712.4984
for step 12428 | loss 3.223547 | norm 0.2854 | time 465.1420 ms | tok/sec 1127156.8400
for step 12429 | loss 3.298734 | norm 0.3771 | time 466.2523 ms | tok/sec 1124472.6722
for step 12430 | loss 3.283273 | norm 0.2994 | time 466.0168 ms | tok/sec 1125041.0595
for step 12431 | loss 3.267495 | norm 0.3542 | time 465.8496 ms | tok/sec 1125444.6870
for step 12432 | loss 3.244437 | norm 0.3130 | time 465.7211 ms | tok/sec 1125755.2338
for step 12433 | loss 3.257197 | norm 0.3311 | time 465.6549 ms | tok/sec 1125915.4715
for step 12434 | loss 3.218231 | norm 0.3588 | time 466.1524 ms | tok/sec 1124713.6489
for step 12435 | loss 3.178785 | norm 0.3143 | time 465.9278 ms | tok/sec 1125255.7924
for step 12436 | loss 3.262640 | norm 0.3619 | time 465.2674 ms | tok/sec 1126853.0262
for step 12437 | loss 3.355888 | norm 0.3465 | time 465.7233 ms | tok/sec 1125750.0470
for step 12438 | loss 3.263082 | norm 0.3496 | time 465.9843 ms | tok/sec 1125119.3440
for step 12439 | loss 3.304125 | norm 0.3209 | time 465.3580 ms | tok/sec 1126633.6428
for step 12440 | loss 3.255862 | norm 0.3614 | time 465.4164 ms | tok/sec 1126492.2435
for step 12441 | loss 3.257660 | norm 0.3206 | time 465.6215 ms | tok/sec 1125996.1840
for step 12442 | loss 3.231451 | norm 0.3290 | time 465.5855 ms | tok/sec 1126083.2512
for step 12443 | loss 3.243879 | norm 0.3406 | time 464.6270 ms | tok/sec 1128406.1601
for step 12444 | loss 3.290026 | norm 0.3346 | time 464.6347 ms | tok/sec 1128387.6315
for step 12445 | loss 3.308450 | norm 0.3128 | time 464.8054 ms | tok/sec 1127973.2118
for step 12446 | loss 3.320272 | norm 0.3235 | time 465.1167 ms | tok/sec 1127218.0847
for step 12447 | loss 3.231623 | norm 0.3241 | time 464.9944 ms | tok/sec 1127514.5799
for step 12448 | loss 3.292120 | norm 0.3331 | time 465.6482 ms | tok/sec 1125931.6131
for step 12449 | loss 3.354342 | norm 0.3090 | time 465.4186 ms | tok/sec 1126487.0499
for step 12450 | loss 3.244171 | norm 0.3572 | time 464.5591 ms | tok/sec 1128571.2078
for step 12451 | loss 3.300946 | norm 0.3192 | time 464.4666 ms | tok/sec 1128795.9819
for step 12452 | loss 3.221908 | norm 0.3641 | time 466.0342 ms | tok/sec 1124999.0436
for step 12453 | loss 3.293568 | norm 0.3502 | time 465.5092 ms | tok/sec 1126267.8090
for step 12454 | loss 3.306053 | norm 0.3427 | time 464.9353 ms | tok/sec 1127657.9707
for step 12455 | loss 3.306777 | norm 0.3572 | time 465.2483 ms | tok/sec 1126899.2230
for step 12456 | loss 3.285513 | norm 0.2938 | time 464.7555 ms | tok/sec 1128094.1492
for step 12457 | loss 3.209269 | norm 0.3728 | time 465.5671 ms | tok/sec 1126127.6549
for step 12458 | loss 3.253338 | norm 0.3186 | time 464.4949 ms | tok/sec 1128727.0340
for step 12459 | loss 3.266913 | norm 0.3233 | time 464.9725 ms | tok/sec 1127567.7690
for step 12460 | loss 3.250007 | norm 0.3027 | time 464.7524 ms | tok/sec 1128101.6725
for step 12461 | loss 3.230900 | norm 0.3098 | time 465.3835 ms | tok/sec 1126571.8844
for step 12462 | loss 3.256919 | norm 0.3122 | time 465.0800 ms | tok/sec 1127307.0747
for step 12463 | loss 3.264756 | norm 0.3986 | time 465.3172 ms | tok/sec 1126732.3547
for step 12464 | loss 3.198443 | norm 0.3027 | time 464.8943 ms | tok/sec 1127757.4406
for step 12465 | loss 3.271616 | norm 0.3410 | time 466.3420 ms | tok/sec 1124256.5136
for step 12466 | loss 3.243057 | norm 0.3086 | time 465.9071 ms | tok/sec 1125305.8894
for step 12467 | loss 3.241997 | norm 0.2983 | time 464.3383 ms | tok/sec 1129107.8015
for step 12468 | loss 3.220292 | norm 0.2907 | time 465.2574 ms | tok/sec 1126877.2791
for step 12469 | loss 3.262604 | norm 0.3075 | time 465.6382 ms | tok/sec 1125955.8263
for step 12470 | loss 3.244753 | norm 0.3123 | time 466.1682 ms | tok/sec 1124675.6839
for step 12471 | loss 3.258136 | norm 0.3003 | time 466.3899 ms | tok/sec 1124140.9949
for step 12472 | loss 3.285704 | norm 0.3074 | time 464.7336 ms | tok/sec 1128147.3931
for step 12473 | loss 3.237679 | norm 0.3233 | time 465.2340 ms | tok/sec 1126933.8732
for step 12474 | loss 3.284807 | norm 0.3286 | time 465.1504 ms | tok/sec 1127136.6192
for step 12475 | loss 3.222046 | norm 0.2917 | time 465.0145 ms | tok/sec 1127466.0203
for step 12476 | loss 3.255648 | norm 0.3240 | time 464.4856 ms | tok/sec 1128749.6294
for step 12477 | loss 3.261227 | norm 0.3051 | time 465.2083 ms | tok/sec 1126996.2488
for step 12478 | loss 3.304114 | norm 0.3307 | time 464.9651 ms | tok/sec 1127585.6926
for step 12479 | loss 3.298497 | norm 0.3115 | time 465.0106 ms | tok/sec 1127475.2694
for step 12480 | loss 3.330001 | norm 0.3383 | time 465.2247 ms | tok/sec 1126956.3969
for step 12481 | loss 3.323056 | norm 0.3961 | time 466.0451 ms | tok/sec 1124972.5695
for step 12482 | loss 3.287210 | norm 0.3138 | time 465.0152 ms | tok/sec 1127464.2861
for step 12483 | loss 3.295398 | norm 0.3430 | time 464.3192 ms | tok/sec 1129154.1834
for step 12484 | loss 3.388565 | norm 0.3425 | time 464.6873 ms | tok/sec 1128259.6845
for step 12485 | loss 3.303893 | norm 0.3365 | time 465.7123 ms | tok/sec 1125776.5578
for step 12486 | loss 3.322949 | norm 0.3475 | time 464.9622 ms | tok/sec 1127592.6309
for step 12487 | loss 3.288320 | norm 0.3279 | time 465.1144 ms | tok/sec 1127223.8628
for step 12488 | loss 3.298114 | norm 0.3547 | time 465.2960 ms | tok/sec 1126783.7380
for step 12489 | loss 3.220623 | norm 0.3056 | time 465.1179 ms | tok/sec 1127215.1956
for step 12490 | loss 3.212358 | norm 0.3490 | time 465.4884 ms | tok/sec 1126317.9961
for step 12491 | loss 3.264400 | norm 0.3272 | time 465.9321 ms | tok/sec 1125245.4281
for step 12492 | loss 3.258011 | norm 0.3304 | time 465.3757 ms | tok/sec 1126590.9307
for step 12493 | loss 3.224966 | norm 0.3391 | time 465.9252 ms | tok/sec 1125262.1263
for step 12494 | loss 3.279414 | norm 0.3283 | time 465.0691 ms | tok/sec 1127333.6588
for step 12495 | loss 3.248152 | norm 0.3380 | time 465.5089 ms | tok/sec 1126268.3858
for step 12496 | loss 3.237303 | norm 0.3333 | time 465.0822 ms | tok/sec 1127301.8736
for step 12497 | loss 3.206989 | norm 0.3120 | time 465.3788 ms | tok/sec 1126583.4275
for step 12498 | loss 3.247662 | norm 0.3408 | time 465.3556 ms | tok/sec 1126639.4149
for step 12499 | loss 3.223096 | norm 0.2900 | time 466.0351 ms | tok/sec 1124996.7415
validation loss 3.2818
HellaSwag accuracy: 2796/10042=0.2784
> Hello, I'm a language model, and the fact that I've been sitting at my computer this year is very relevant. If you can't see the screen
> Hello, I'm a language model, a model language!
I'm very serious about writing up my essay. What is an essay I can write?

> Hello, I'm a language model, but why am I going to use the word "interrogations?"?
I'm a newbie. So,
> Hello, I'm a language model, and I'm looking at ways to solve more complex problems because I'm doing both the very old and new problems of the
> Hello, I'm a language model, and I'm a real English speaker. While I'm studying English, or being linguistically educated, English has a tendency
> Hello, I'm a language model, for years now, I've got tons of examples of C++/C++ examples, so I am just starting my
> Hello, I'm a language model, and I've worked with a model where we are always using a different key. I've used a set of 'keys
> Hello, I'm a language model, here is my first grader. Is an Italian, I'm an English, and my first grader is a bilingual
> Hello, I'm a language model, so I know that you should be using something similar, but this is where I fall back to becoming a second language.
> Hello, I'm a language model, and I know that you need to think out loud when you hear my question and I don't even think of it when> 
Hello, I'm a language model, isn't it helpful?
A computer's input can be a very hard-to-understand way of reading text
> Hello, I'm a language model, so we're going to try a better way. "In the last blog entry, you learned how to write a good> 
>>Hello, I'm a language model, I'm a research language programmer, I'm a language model researcher. I'm learning the concepts from each other as well
>  Hello, I'm a language model, and I'm not. And that's something that needs to be addressed like no two things should be used to create, Hello, I'm a language model, so I do want to start that topic with what I'm teaching you.
This is the first time I have any> > Hello, I'm a language model, but i've never written a perfect grammar review, so I'm looking at a few examples of this. So to clarify


>Hello, I'm a language model, and this forum I'm sharing is open and anonymous, and all my own articles are posted to Facebook and Twitter. If
> Hello, I'm a language model, don't worry if my new machine is a language model or if you have the word "real" in the dictionary as
>  Hello, I'm a language model, since languages are so small to learn and interact is much harder to read. If that is your goal, the best way> Hello, I'm a language model, let's say it's time for example, let's say I'm on the front page of a website:
Now
> Hello, I'm a language model, and the only thing I knew when I was a kid was that the more I learned to read and write, which led

Hello, I'm a language model, and for the purposes of this blog post I prefer to use the words "determinants" or "indefuses
> Hello, I'm a language model, and I want to do a project for programming this website. I'm really interested in how the project works for me.
> > Hello, I'm a language model, how to graph. I'm a writer using a framework to graph my entire life. I'm also writing about my passion
> Hello, I'm a language model, and you'll use whatever language you're writing right now. That's a huge job. But... I'm working hard
Hello, I'm a language model, so this could have started with I2C and it's working at the point that that's a little bit different from
> Hello, I'm a language model, but for a lot of languages you'd rather forget about the basics.
This is a good example of where your model
> > Hello, I'm a language model, so I'll give you a very important task for this question. So far, so good. Well, this question needs
> Hello, I'm a language model, and how I think about language learning, and I want this data in the form of text, graphs, or tables.
Hello, I'm a language model, is a lot easier to solve. I've tried all my life, but...it was just boring, I was never
> Hello, I'm a language model, and i'm not the first person I know, but I'm using it in a lot of different languages to use:
Hello, I'm a language model, but if I'm trying to write a story, please read.
I really wanted to draw as many elements in my
for step 12500 | loss 3.264585 | norm 0.3630 | time 12526.6480 ms | tok/sec 41853.8142
for step 12501 | loss 3.250862 | norm 0.3047 | time 462.8973 ms | tok/sec 1132622.7204
for step 12502 | loss 3.241439 | norm 0.3273 | time 463.0239 ms | tok/sec 1132313.0380
for step 12503 | loss 3.249111 | norm 0.3097 | time 464.1311 ms | tok/sec 1129611.8294
for step 12504 | loss 3.266126 | norm 0.3253 | time 463.3353 ms | tok/sec 1131552.0910
for step 12505 | loss 3.211068 | norm 1.3751 | time 463.5630 ms | tok/sec 1130996.3029
for step 12506 | loss 3.248205 | norm 0.3840 | time 464.3171 ms | tok/sec 1129159.4016
for step 12507 | loss 3.270972 | norm 0.3481 | time 465.2267 ms | tok/sec 1126951.7766
for step 12508 | loss 3.253111 | norm 0.3554 | time 463.5291 ms | tok/sec 1131078.9090
for step 12509 | loss 3.276047 | norm 0.3520 | time 464.1278 ms | tok/sec 1129619.9532
for step 12510 | loss 3.211208 | norm 0.3536 | time 463.9313 ms | tok/sec 1130098.3034
for step 12511 | loss 3.295868 | norm 0.3384 | time 464.4706 ms | tok/sec 1128786.1317
for step 12512 | loss 3.213047 | norm 0.3385 | time 464.9844 ms | tok/sec 1127538.8613
for step 12513 | loss 3.285875 | norm 0.3409 | time 464.6242 ms | tok/sec 1128413.1085
for step 12514 | loss 3.312450 | norm 0.3517 | time 465.2736 ms | tok/sec 1126838.0130
for step 12515 | loss 3.298643 | norm 0.3261 | time 465.0934 ms | tok/sec 1127274.7130
for step 12516 | loss 3.306351 | norm 0.3386 | time 465.2598 ms | tok/sec 1126871.5045
for step 12517 | loss 3.279202 | norm 0.3093 | time 464.8366 ms | tok/sec 1127897.4222
for step 12518 | loss 3.326657 | norm 0.3340 | time 465.5182 ms | tok/sec 1126245.8896
for step 12519 | loss 3.300937 | norm 0.3136 | time 466.0356 ms | tok/sec 1124995.5904
for step 12520 | loss 3.261598 | norm 0.3407 | time 464.7877 ms | tok/sec 1128016.0288
for step 12521 | loss 3.236770 | norm 0.3382 | time 465.5030 ms | tok/sec 1126282.8070
for step 12522 | loss 3.284280 | norm 0.3277 | time 465.0183 ms | tok/sec 1127456.7713
for step 12523 | loss 3.301798 | norm 0.3178 | time 465.7691 ms | tok/sec 1125639.4069
for step 12524 | loss 3.266968 | norm 0.3104 | time 465.0617 ms | tok/sec 1127351.5749
for step 12525 | loss 3.286903 | norm 0.3168 | time 465.3928 ms | tok/sec 1126549.3761
for step 12526 | loss 3.254866 | norm 0.2916 | time 466.4662 ms | tok/sec 1123957.1334
for step 12527 | loss 3.237289 | norm 0.2914 | time 465.7655 ms | tok/sec 1125648.0498
for step 12528 | loss 3.342248 | norm 0.3085 | time 465.9219 ms | tok/sec 1125270.1876
for step 12529 | loss 3.258150 | norm 0.3227 | time 466.4586 ms | tok/sec 1123975.5169
for step 12530 | loss 3.256907 | norm 0.2802 | time 465.3432 ms | tok/sec 1126669.4311
for step 12531 | loss 3.299081 | norm 0.3142 | time 466.0699 ms | tok/sec 1124912.7194
for step 12532 | loss 3.256766 | norm 0.2884 | time 466.4469 ms | tok/sec 1124003.6677
for step 12533 | loss 3.288988 | norm 0.2990 | time 465.4760 ms | tok/sec 1126347.9951
for step 12534 | loss 3.327705 | norm 0.3827 | time 466.4311 ms | tok/sec 1124041.5873
for step 12535 | loss 3.224185 | norm 0.3234 | time 466.2430 ms | tok/sec 1124495.0976
for step 12536 | loss 3.231449 | norm 0.2900 | time 465.7831 ms | tok/sec 1125605.4124
for step 12537 | loss 3.240773 | norm 0.3001 | time 466.5408 ms | tok/sec 1123777.3521
for step 12538 | loss 3.258883 | norm 0.2987 | time 465.3769 ms | tok/sec 1126588.0448
for step 12539 | loss 3.252042 | norm 0.2971 | time 465.5828 ms | tok/sec 1126089.5944
for step 12540 | loss 3.244247 | norm 0.2939 | time 465.9395 ms | tok/sec 1125227.5789
for step 12541 | loss 3.242424 | norm 0.2927 | time 465.9233 ms | tok/sec 1125266.7328
for step 12542 | loss 3.287981 | norm 0.2912 | time 465.6644 ms | tok/sec 1125892.4129
for step 12543 | loss 3.265363 | norm 0.3314 | time 466.0928 ms | tok/sec 1124857.4789
for step 12544 | loss 3.259779 | norm 0.2969 | time 466.3777 ms | tok/sec 1124170.3034
for step 12545 | loss 3.317693 | norm 0.2993 | time 465.4715 ms | tok/sec 1126358.9567
for step 12546 | loss 3.250238 | norm 0.3018 | time 466.1362 ms | tok/sec 1124752.7670
for step 12547 | loss 3.241352 | norm 0.2736 | time 466.4681 ms | tok/sec 1123952.5377
for step 12548 | loss 3.200297 | norm 0.3551 | time 466.1565 ms | tok/sec 1124703.8698
for step 12549 | loss 3.226669 | norm 0.2862 | time 466.6610 ms | tok/sec 1123487.9849
for step 12550 | loss 3.328371 | norm 0.3523 | time 466.5446 ms | tok/sec 1123768.1636
for step 12551 | loss 3.288688 | norm 0.3171 | time 465.3089 ms | tok/sec 1126752.5611
for step 12552 | loss 3.209989 | norm 0.3190 | time 466.1224 ms | tok/sec 1124786.1346
for step 12553 | loss 3.279578 | norm 0.3165 | time 467.6981 ms | tok/sec 1120996.6496
for step 12554 | loss 3.226364 | norm 0.3485 | time 465.5602 ms | tok/sec 1126144.3793
for step 12555 | loss 3.258223 | norm 0.3112 | time 465.3769 ms | tok/sec 1126588.0448
for step 12556 | loss 3.306868 | norm 0.3101 | time 466.1763 ms | tok/sec 1124656.1271
for step 12557 | loss 3.339897 | norm 0.3332 | time 465.8990 ms | tok/sec 1125325.4687
for step 12558 | loss 3.280955 | norm 0.3303 | time 466.1763 ms | tok/sec 1124656.1271
for step 12559 | loss 3.260339 | norm 0.3300 | time 465.9801 ms | tok/sec 1125129.7061
for step 12560 | loss 3.274881 | norm 0.3436 | time 465.4946 ms | tok/sec 1126302.9972
for step 12561 | loss 3.284771 | norm 0.3129 | time 465.4267 ms | tok/sec 1126467.4301
for step 12562 | loss 3.260238 | norm 0.3217 | time 465.7378 ms | tok/sec 1125714.8933
for step 12563 | loss 3.263304 | norm 0.3212 | time 466.0611 ms | tok/sec 1124934.0115
for step 12564 | loss 3.313947 | norm 0.3323 | time 465.8790 ms | tok/sec 1125373.8441
for step 12565 | loss 3.269040 | norm 0.3564 | time 465.6925 ms | tok/sec 1125824.3955
for step 12566 | loss 3.299595 | norm 0.3058 | time 466.2318 ms | tok/sec 1124522.1243
for step 12567 | loss 3.252323 | norm 0.3159 | time 465.5344 ms | tok/sec 1126206.6675
for step 12568 | loss 3.243609 | norm 0.3016 | time 465.6012 ms | tok/sec 1126045.1937
for step 12569 | loss 3.285192 | norm 0.2853 | time 465.7991 ms | tok/sec 1125566.8111
for step 12570 | loss 3.291337 | norm 0.3366 | time 465.9138 ms | tok/sec 1125289.7657
for step 12571 | loss 3.273268 | norm 0.3210 | time 466.4295 ms | tok/sec 1124045.6093
Will loading at 0 from edu_fineweb10B/edufineweb_train_000067.npy
for step 12572 | loss 3.274067 | norm 0.3259 | time 2602.7000 ms | tok/sec 201440.0434
for step 12573 | loss 3.259646 | norm 0.3243 | time 483.9926 ms | tok/sec 1083256.2840
for step 12574 | loss 3.254202 | norm 0.3618 | time 464.2794 ms | tok/sec 1129251.0180
for step 12575 | loss 3.234918 | norm 0.3110 | time 467.5527 ms | tok/sec 1121345.3429
for step 12576 | loss 3.280863 | norm 0.3254 | time 465.1291 ms | tok/sec 1127188.0393
for step 12577 | loss 3.229178 | norm 0.3168 | time 464.7939 ms | tok/sec 1128000.9846
for step 12578 | loss 3.243997 | norm 0.2933 | time 464.2406 ms | tok/sec 1129345.5491
for step 12579 | loss 3.264532 | norm 0.2995 | time 464.4992 ms | tok/sec 1128716.6056
for step 12580 | loss 3.219358 | norm 0.3098 | time 465.6491 ms | tok/sec 1125929.3071
for step 12581 | loss 3.237513 | norm 0.2960 | time 464.8633 ms | tok/sec 1127832.6330
for step 12582 | loss 3.291552 | norm 0.3377 | time 465.6625 ms | tok/sec 1125897.0245
for step 12583 | loss 3.259561 | norm 0.3100 | time 465.5552 ms | tok/sec 1126156.4903
for step 12584 | loss 3.236063 | norm 0.2774 | time 465.8182 ms | tok/sec 1125520.7234
for step 12585 | loss 3.302338 | norm 0.3714 | time 465.7698 ms | tok/sec 1125637.6783
for step 12586 | loss 3.215276 | norm 0.3168 | time 465.2297 ms | tok/sec 1126944.2686
for step 12587 | loss 3.245408 | norm 0.3556 | time 465.5635 ms | tok/sec 1126136.3054
for step 12588 | loss 3.253012 | norm 0.2935 | time 465.8248 ms | tok/sec 1125504.5936
for step 12589 | loss 3.284391 | norm 0.3393 | time 464.7794 ms | tok/sec 1128036.2812
for step 12590 | loss 3.271953 | norm 0.3302 | time 465.6911 ms | tok/sec 1125827.8539
for step 12591 | loss 3.305482 | norm 0.3147 | time 465.9271 ms | tok/sec 1125257.5198
for step 12592 | loss 3.215955 | norm 0.3259 | time 465.5101 ms | tok/sec 1126265.5016
for step 12593 | loss 3.216517 | norm 0.3179 | time 465.9071 ms | tok/sec 1125305.8894
for step 12594 | loss 3.237732 | norm 0.3174 | time 466.1884 ms | tok/sec 1124626.7933
for step 12595 | loss 3.256191 | norm 0.3410 | time 465.4553 ms | tok/sec 1126398.1894
for step 12596 | loss 3.284638 | norm 0.3270 | time 465.4567 ms | tok/sec 1126394.7275
for step 12597 | loss 3.225688 | norm 0.3512 | time 465.4605 ms | tok/sec 1126385.4961
for step 12598 | loss 3.259441 | norm 0.3221 | time 466.3932 ms | tok/sec 1124132.9497
for step 12599 | loss 3.190193 | norm 0.3268 | time 465.7564 ms | tok/sec 1125669.9460
for step 12600 | loss 3.268013 | norm 0.3243 | time 465.8093 ms | tok/sec 1125542.0385
for step 12601 | loss 3.264936 | norm 0.2832 | time 466.4764 ms | tok/sec 1123932.4317
for step 12602 | loss 3.244901 | norm 0.3012 | time 465.9350 ms | tok/sec 1125238.5186
for step 12603 | loss 3.287141 | norm 0.3075 | time 466.1589 ms | tok/sec 1124698.1174
for step 12604 | loss 3.248603 | norm 0.2891 | time 465.3716 ms | tok/sec 1126600.7426
for step 12605 | loss 3.269639 | norm 0.3448 | time 465.3664 ms | tok/sec 1126613.4407
for step 12606 | loss 3.237137 | norm 0.2881 | time 465.3018 ms | tok/sec 1126769.8814
for step 12607 | loss 3.249906 | norm 0.3093 | time 466.2445 ms | tok/sec 1124491.6475
for step 12608 | loss 3.301638 | norm 0.3124 | time 465.6460 ms | tok/sec 1125936.8015
for step 12609 | loss 3.248206 | norm 0.3376 | time 465.8632 ms | tok/sec 1125411.8562
for step 12610 | loss 3.267647 | norm 0.3140 | time 464.4928 ms | tok/sec 1128732.2482
for step 12611 | loss 3.227398 | norm 0.3358 | time 464.7729 ms | tok/sec 1128051.9050
for step 12612 | loss 3.250849 | norm 0.3400 | time 464.9687 ms | tok/sec 1127577.0198
for step 12613 | loss 3.258433 | norm 0.3104 | time 465.0984 ms | tok/sec 1127262.5779
for step 12614 | loss 3.263895 | norm 0.3337 | time 465.1296 ms | tok/sec 1127186.8837
for step 12615 | loss 3.264973 | norm 0.3070 | time 465.6074 ms | tok/sec 1126030.2020
for step 12616 | loss 3.253838 | norm 0.3129 | time 465.3695 ms | tok/sec 1126605.9373
for step 12617 | loss 3.289623 | norm 0.3129 | time 465.2643 ms | tok/sec 1126860.5329
for step 12618 | loss 3.270997 | norm 0.2778 | time 465.0450 ms | tok/sec 1127392.0327
for step 12619 | loss 3.307632 | norm 0.3508 | time 464.1304 ms | tok/sec 1129613.5702
for step 12620 | loss 3.275805 | norm 0.3075 | time 465.5845 ms | tok/sec 1126085.5578
for step 12621 | loss 3.263363 | norm 0.3261 | time 465.4517 ms | tok/sec 1126406.8440
for step 12622 | loss 3.275863 | norm 0.3277 | time 466.7437 ms | tok/sec 1123288.8444
for step 12623 | loss 3.209725 | norm 0.3143 | time 466.1911 ms | tok/sec 1124620.4666
for step 12624 | loss 3.280372 | norm 0.3253 | time 466.2220 ms | tok/sec 1124545.7019
for step 12625 | loss 3.244533 | norm 0.3107 | time 465.0722 ms | tok/sec 1127326.1458
for step 12626 | loss 3.247313 | norm 0.3035 | time 464.8988 ms | tok/sec 1127746.4518
for step 12627 | loss 3.312120 | norm 0.3099 | time 465.4512 ms | tok/sec 1126407.9980
for step 12628 | loss 3.319809 | norm 0.3107 | time 465.4837 ms | tok/sec 1126329.5340
for step 12629 | loss 3.308920 | norm 0.3220 | time 464.6270 ms | tok/sec 1128406.1601
for step 12630 | loss 3.299134 | norm 0.3127 | time 465.2128 ms | tok/sec 1126985.2748
for step 12631 | loss 3.310622 | norm 0.3327 | time 465.3347 ms | tok/sec 1126690.2124
for step 12632 | loss 3.232134 | norm 0.3146 | time 465.3642 ms | tok/sec 1126618.6355
for step 12633 | loss 3.284423 | norm 0.3478 | time 465.8163 ms | tok/sec 1125525.3320
for step 12634 | loss 3.250798 | norm 0.3293 | time 465.5964 ms | tok/sec 1126056.7260
for step 12635 | loss 3.224055 | norm 0.3181 | time 465.8756 ms | tok/sec 1125381.9071
for step 12636 | loss 3.269293 | norm 0.3209 | time 465.7364 ms | tok/sec 1125718.3510
for step 12637 | loss 3.262455 | norm 0.3169 | time 465.9989 ms | tok/sec 1125084.2298
for step 12638 | loss 3.270660 | norm 0.2922 | time 465.9004 ms | tok/sec 1125322.0135
for step 12639 | loss 3.288859 | norm 0.3110 | time 465.5020 ms | tok/sec 1126285.1144
for step 12640 | loss 3.248105 | norm 0.3065 | time 465.5762 ms | tok/sec 1126105.7410
for step 12641 | loss 3.300719 | norm 0.3057 | time 464.8325 ms | tok/sec 1127907.2569
for step 12642 | loss 3.259970 | norm 0.3111 | time 466.4147 ms | tok/sec 1124081.2333
for step 12643 | loss 3.233519 | norm 0.3049 | time 464.3984 ms | tok/sec 1128961.7233
for step 12644 | loss 3.250654 | norm 0.2983 | time 465.9259 ms | tok/sec 1125260.3989
for step 12645 | loss 3.235034 | norm 0.3153 | time 466.1515 ms | tok/sec 1124715.9499
for step 12646 | loss 3.194317 | norm 0.2787 | time 465.7164 ms | tok/sec 1125766.7602
for step 12647 | loss 3.247841 | norm 0.3180 | time 467.0222 ms | tok/sec 1122619.0571
for step 12648 | loss 3.269161 | norm 0.2947 | time 465.0338 ms | tok/sec 1127419.1989
for step 12649 | loss 3.208713 | norm 0.3227 | time 465.9684 ms | tok/sec 1125157.9147
for step 12650 | loss 3.293257 | norm 0.3509 | time 465.7450 ms | tok/sec 1125697.6055
for step 12651 | loss 3.265252 | norm 0.2923 | time 465.2047 ms | tok/sec 1127004.9126
for step 12652 | loss 3.260892 | norm 0.3680 | time 465.7369 ms | tok/sec 1125717.1984
for step 12653 | loss 3.235260 | norm 0.3293 | time 465.5049 ms | tok/sec 1126278.1922
for step 12654 | loss 3.244318 | norm 0.3538 | time 465.8039 ms | tok/sec 1125555.2888
for step 12655 | loss 3.192954 | norm 0.3232 | time 465.8613 ms | tok/sec 1125416.4639
for step 12656 | loss 3.278736 | norm 0.3306 | time 465.6544 ms | tok/sec 1125916.6245
for step 12657 | loss 3.276678 | norm 0.3524 | time 465.4164 ms | tok/sec 1126492.2435
for step 12658 | loss 3.219800 | norm 0.2973 | time 465.4446 ms | tok/sec 1126424.1537
for step 12659 | loss 3.198345 | norm 0.2988 | time 464.7629 ms | tok/sec 1128076.2095
for step 12660 | loss 3.261107 | norm 0.3068 | time 466.2256 ms | tok/sec 1124537.0759
for step 12661 | loss 3.275026 | norm 0.3212 | time 465.3103 ms | tok/sec 1126749.0971
for step 12662 | loss 3.261703 | norm 0.2846 | time 465.0402 ms | tok/sec 1127403.5927
for step 12663 | loss 3.279446 | norm 0.3084 | time 465.4820 ms | tok/sec 1126333.5723
for step 12664 | loss 3.272998 | norm 0.3003 | time 464.7865 ms | tok/sec 1128018.9220
for step 12665 | loss 3.283635 | norm 0.2879 | time 464.0746 ms | tok/sec 1129749.3697
for step 12666 | loss 3.252208 | norm 0.3278 | time 465.0354 ms | tok/sec 1127415.1528
for step 12667 | loss 3.288496 | norm 0.2886 | time 465.9059 ms | tok/sec 1125308.7686
for step 12668 | loss 3.217296 | norm 0.3284 | time 465.0650 ms | tok/sec 1127343.4837
for step 12669 | loss 3.241900 | norm 0.2991 | time 465.6510 ms | tok/sec 1125924.6952
for step 12670 | loss 3.205042 | norm 0.3562 | time 465.0888 ms | tok/sec 1127285.6927
for step 12671 | loss 3.240290 | norm 0.3585 | time 465.5640 ms | tok/sec 1126135.1520
for step 12672 | loss 3.235705 | norm 0.3686 | time 466.0370 ms | tok/sec 1124992.1372
for step 12673 | loss 3.289005 | norm 0.3457 | time 465.7304 ms | tok/sec 1125732.7580
for step 12674 | loss 3.248739 | norm 0.3328 | time 466.1920 ms | tok/sec 1124618.1660
for step 12675 | loss 3.229815 | norm 0.3341 | time 465.8360 ms | tok/sec 1125477.5196
for step 12676 | loss 3.226781 | norm 0.3128 | time 465.3401 ms | tok/sec 1126676.9354
for step 12677 | loss 3.272640 | norm 0.3324 | time 465.6136 ms | tok/sec 1126015.2108
for step 12678 | loss 3.265349 | norm 0.3042 | time 464.9093 ms | tok/sec 1127721.0049
for step 12679 | loss 3.274772 | norm 0.3202 | time 466.3460 ms | tok/sec 1124246.7425
for step 12680 | loss 3.255847 | norm 0.3189 | time 465.2047 ms | tok/sec 1127004.9126
for step 12681 | loss 3.242003 | norm 0.3320 | time 465.2281 ms | tok/sec 1126948.3114
for step 12682 | loss 3.230848 | norm 0.3063 | time 465.4586 ms | tok/sec 1126390.1118
for step 12683 | loss 3.259983 | norm 0.3556 | time 465.5623 ms | tok/sec 1126139.1889
for step 12684 | loss 3.317822 | norm 0.3052 | time 465.0080 ms | tok/sec 1127481.6283
for step 12685 | loss 3.268969 | norm 0.2942 | time 466.0189 ms | tok/sec 1125035.8793
for step 12686 | loss 3.277186 | norm 0.3715 | time 465.7989 ms | tok/sec 1125567.3872
for step 12687 | loss 3.273359 | norm 0.2976 | time 465.1563 ms | tok/sec 1127122.1762
for step 12688 | loss 3.263024 | norm 0.3107 | time 465.6887 ms | tok/sec 1125833.6178
for step 12689 | loss 3.307433 | norm 0.3191 | time 466.1627 ms | tok/sec 1124688.9138
for step 12690 | loss 3.260741 | norm 0.3026 | time 466.3372 ms | tok/sec 1124268.0093
for step 12691 | loss 3.201118 | norm 0.3023 | time 466.1746 ms | tok/sec 1124660.1535
for step 12692 | loss 3.281432 | norm 0.3229 | time 465.9348 ms | tok/sec 1125239.0944
for step 12693 | loss 3.287606 | norm 0.3090 | time 466.0010 ms | tok/sec 1125079.0492
for step 12694 | loss 3.273541 | norm 0.3287 | time 465.7886 ms | tok/sec 1125592.1609
for step 12695 | loss 3.246835 | norm 0.3326 | time 466.0034 ms | tok/sec 1125073.2930
for step 12696 | loss 3.287181 | norm 0.3326 | time 465.7242 ms | tok/sec 1125747.7418
for step 12697 | loss 3.270447 | norm 0.3128 | time 467.4447 ms | tok/sec 1121604.4311
for step 12698 | loss 3.287627 | norm 0.3364 | time 465.8580 ms | tok/sec 1125424.5275
for step 12699 | loss 3.257301 | norm 0.3027 | time 465.6909 ms | tok/sec 1125828.4302
for step 12700 | loss 3.466103 | norm 0.4317 | time 465.2491 ms | tok/sec 1126897.4906
for step 12701 | loss 3.200213 | norm 0.3365 | time 465.9452 ms | tok/sec 1125213.7605
for step 12702 | loss 3.269751 | norm 0.3752 | time 464.9456 ms | tok/sec 1127633.1060
for step 12703 | loss 3.286330 | norm 0.3567 | time 465.5807 ms | tok/sec 1126094.7843
for step 12704 | loss 3.223519 | norm 0.3937 | time 465.9011 ms | tok/sec 1125320.2859
for step 12705 | loss 3.281481 | norm 0.3291 | time 465.7333 ms | tok/sec 1125725.8426
for step 12706 | loss 3.197424 | norm 0.3498 | time 465.9004 ms | tok/sec 1125322.0135
for step 12707 | loss 3.285488 | norm 0.3571 | time 465.7209 ms | tok/sec 1125755.8101
for step 12708 | loss 3.286870 | norm 0.3458 | time 466.1105 ms | tok/sec 1124814.9014
for step 12709 | loss 3.280753 | norm 0.3347 | time 465.8267 ms | tok/sec 1125499.9852
for step 12710 | loss 3.262260 | norm 0.3064 | time 465.8754 ms | tok/sec 1125382.4830
for step 12711 | loss 3.269834 | norm 0.3179 | time 466.2011 ms | tok/sec 1124596.3108
for step 12712 | loss 3.242576 | norm 0.3073 | time 465.7710 ms | tok/sec 1125634.7973
for step 12713 | loss 3.351051 | norm 0.3680 | time 465.2264 ms | tok/sec 1126952.3541
for step 12714 | loss 3.259642 | norm 0.3372 | time 465.6632 ms | tok/sec 1125895.2952
for step 12715 | loss 3.237031 | norm 0.3161 | time 465.3001 ms | tok/sec 1126773.9228
for step 12716 | loss 3.250651 | norm 0.3286 | time 464.8659 ms | tok/sec 1127826.2702
for step 12717 | loss 3.249084 | norm 0.3106 | time 465.1287 ms | tok/sec 1127189.1949
for step 12718 | loss 3.222511 | norm 0.3284 | time 467.1226 ms | tok/sec 1122377.8313
for step 12719 | loss 3.264020 | norm 0.2973 | time 465.3096 ms | tok/sec 1126750.8291
for step 12720 | loss 3.257076 | norm 0.2892 | time 465.5230 ms | tok/sec 1126234.3534
for step 12721 | loss 3.277838 | norm 0.2925 | time 465.6637 ms | tok/sec 1125894.1423
for step 12722 | loss 3.322162 | norm 0.3217 | time 465.8630 ms | tok/sec 1125412.4322
for step 12723 | loss 3.271067 | norm 0.2949 | time 464.9725 ms | tok/sec 1127567.7690
for step 12724 | loss 3.281221 | norm 0.3082 | time 464.7331 ms | tok/sec 1128148.5506
for step 12725 | loss 3.273379 | norm 0.2836 | time 466.0814 ms | tok/sec 1124885.0985
for step 12726 | loss 3.228117 | norm 0.2952 | time 465.6191 ms | tok/sec 1126001.9496
for step 12727 | loss 3.205943 | norm 0.2961 | time 465.6823 ms | tok/sec 1125849.1806
for step 12728 | loss 3.296788 | norm 0.3839 | time 465.4810 ms | tok/sec 1126335.8799
for step 12729 | loss 3.377822 | norm 0.3277 | time 466.0130 ms | tok/sec 1125050.2689
for step 12730 | loss 3.272413 | norm 0.3310 | time 465.4510 ms | tok/sec 1126408.5749
for step 12731 | loss 3.347071 | norm 0.3464 | time 466.2716 ms | tok/sec 1124426.0991
for step 12732 | loss 3.274015 | norm 0.2972 | time 466.5213 ms | tok/sec 1123824.4458
for step 12733 | loss 3.290409 | norm 0.3083 | time 466.1036 ms | tok/sec 1124831.5868
for step 12734 | loss 3.299908 | norm 0.3091 | time 466.2385 ms | tok/sec 1124506.0232
for step 12735 | loss 3.284561 | norm 0.3029 | time 465.6923 ms | tok/sec 1125824.9719
for step 12736 | loss 3.313391 | norm 0.3128 | time 465.3630 ms | tok/sec 1126621.5214
for step 12737 | loss 3.456663 | norm 0.3373 | time 465.5759 ms | tok/sec 1126106.3176
for step 12738 | loss 3.258081 | norm 0.2933 | time 465.5342 ms | tok/sec 1126207.2443
for step 12739 | loss 3.290927 | norm 0.3267 | time 465.8968 ms | tok/sec 1125330.6516
for step 12740 | loss 3.247856 | norm 0.3065 | time 465.7681 ms | tok/sec 1125641.7116
for step 12741 | loss 3.191186 | norm 0.3160 | time 465.3268 ms | tok/sec 1126709.2627
for step 12742 | loss 3.247443 | norm 0.3019 | time 465.8403 ms | tok/sec 1125467.1512
for step 12743 | loss 3.257974 | norm 0.3235 | time 465.1835 ms | tok/sec 1127056.3207
for step 12744 | loss 3.263608 | norm 0.3166 | time 465.1363 ms | tok/sec 1127170.7061
for step 12745 | loss 3.243218 | norm 0.3110 | time 465.7588 ms | tok/sec 1125664.1838
for step 12746 | loss 3.226377 | norm 0.3119 | time 464.7977 ms | tok/sec 1127991.7269
for step 12747 | loss 3.296964 | norm 0.3229 | time 465.8406 ms | tok/sec 1125466.5752
for step 12748 | loss 3.273969 | norm 0.3357 | time 465.3144 ms | tok/sec 1126739.2825
for step 12749 | loss 3.260913 | norm 0.2982 | time 464.9084 ms | tok/sec 1127723.3182
validation loss 3.2752
HellaSwag accuracy: 2819/10042=0.2807
> Hello, I'm a language model, and the following is a little bit long:
This is your first post of my "I Am a Language" series
> Hello, I'm a language model, a model, a model of language in many ways. When I try to work on developing my skill level, I'm
> Hello, I'm a language model, I also am a language model, I am a language model programmer and the programming language is one of the languages for communicating
> Hello, I'm a language model, and I'm using the world class framework "The Language that Made this Language Better". Like some other languages, I'm
> Hello, I'm a language model, and I'd like to show you how simple it is, by using real-world examples. :)<|endoftext|>A new report
> Hello, I'm a language model, and I can't wait to write any good words and help my business more easily. It's also important for me because> 
Hello, I'm a language model, someone working to make it. This would be a different issue, of course, which would be a great way of dealing
> Hello, I'm a language model, so my friends and I understand how those languages have moved! I can have any language in my house on this subject!> 
Hello, I'm a language model, and I am here to help you build better models of your language. There are a few things that I would want to
>>>  Hello, I'm a language model, so I need to work with the context before writing the code.
For example, a web page has an HTML element Hello, I'm a language model, do I just want to work with. There is a lot of work to be done on this type of language, withHello, I'm a language model, so I guess it's more like "I see this, then we go to the dictionary."
Of course, at


> Hello, I'm a language model, and, now, I'm going to learn several languages. I'm going to see if I can create a model here> 
Hello, I'm a language model, my main goal is to understand our understanding of the world through the arts. In our previous blog post, we created this
> Hello, I'm a language model, what a language is; i.e., a person or group of persons is a language.
Why do we choose> 
Hello, I'm a language model, but...
Posted By: admin in this post.
A bit of a learning curve though, where you want to
>>  Hello, I'm a language model, which is not a formal model but a special kind of model.
Let's start from a basic understanding and have a> Hello, I'm a language model, for me, not a language. What is a language and why is it so important to me?
Let's talk

Hello, I'm a language model, and it could really become a language guide if someone uses the "I got to think about the topic," he said in> 
> > Hello, I'm a language model, because I're trying to create a text (and hence a lot of code) that's a bit like this:

Hello, I'm a language model, and am just trying to make sense of a sentence.
The most common question is: "What is "me"
Hello, I'm a language model, and I'm not. The following sections should be used for students who attend ELA lessons and English teacher training programs...
> > > Hello, I'm a language model, so I am going to be able to go back and re-think them in terms of what to do in language models
Hello, I'm a language model, and am really pleased to see my students work with the language model in the spring of 2017 :)
Last but not least
> Hello, I'm a language model, and the most important thing to be done is to teach, model and teach.
I'm a language model; my
> Hello, I'm a language model, I like it. The language model is all the things I think we have in mind when we use it in different contexts
> Hello, I'm a language model, and my father is the Director of the company. As a student, I always wondered, as my mother taught, the
Hello, I'm a language model, and you have to spend a lot of time at home in school.
If you're like me and want to make
> Hello, I'm a language model, now what should be one sentence and another sentence when you write this topic sentence: (I am a human rights lawyer,> 
Hello, I'm a language model, and like to have a strong, friendly, or supportive response.
When I don't go to dinner with you,
> Hello, I'm a language model, and you've been having a rough idea of what I'm doing on my English and then I'm not sure if what
> Hello, I'm a language model, one who, at the turn of the century, created and analyzed a number of other linguistic and textual systems.
And
for step 12750 | loss 3.237777 | norm 0.3302 | time 12523.8481 ms | tok/sec 41863.1716
for step 12751 | loss 3.304935 | norm 0.3071 | time 461.7078 ms | tok/sec 1135540.6286
for step 12752 | loss 3.264278 | norm 0.3341 | time 463.5007 ms | tok/sec 1131148.1447
for step 12753 | loss 3.243723 | norm 0.2947 | time 464.3939 ms | tok/sec 1128972.7358
for step 12754 | loss 3.261016 | norm 0.3153 | time 464.8390 ms | tok/sec 1127891.6371
for step 12755 | loss 3.254074 | norm 0.2878 | time 463.8619 ms | tok/sec 1130267.3322
for step 12756 | loss 3.352264 | norm 0.3109 | time 465.0085 ms | tok/sec 1127480.4721
for step 12757 | loss 3.250887 | norm 0.3158 | time 464.2406 ms | tok/sec 1129345.5491
for step 12758 | loss 3.274308 | norm 0.3012 | time 464.7381 ms | tok/sec 1128136.3966
for step 12759 | loss 3.208085 | norm 0.2752 | time 463.8882 ms | tok/sec 1130203.4322
for step 12760 | loss 3.291083 | norm 0.3019 | time 463.7485 ms | tok/sec 1130543.9281
for step 12761 | loss 3.222262 | norm 0.2948 | time 464.3838 ms | tok/sec 1128997.0800
for step 12762 | loss 3.264817 | norm 0.2880 | time 463.8495 ms | tok/sec 1130297.5419
Will loading at 0 from edu_fineweb10B/edufineweb_train_000068.npy
for step 12763 | loss 3.358976 | norm 0.3290 | time 2619.7355 ms | tok/sec 200130.1292
for step 12764 | loss 3.237901 | norm 0.3293 | time 464.4763 ms | tok/sec 1128772.2258
for step 12765 | loss 3.246269 | norm 0.3094 | time 463.8457 ms | tok/sec 1130306.8376
for step 12766 | loss 3.268712 | norm 0.3183 | time 463.6242 ms | tok/sec 1130846.8280
for step 12767 | loss 3.179957 | norm 0.3193 | time 463.4361 ms | tok/sec 1131305.8473
for step 12768 | loss 3.233944 | norm 0.3111 | time 464.3979 ms | tok/sec 1128962.8825
for step 12769 | loss 3.309413 | norm 0.3183 | time 463.3002 ms | tok/sec 1131637.6902
for step 12770 | loss 3.247988 | norm 0.3091 | time 463.6402 ms | tok/sec 1130807.8663
for step 12771 | loss 3.306642 | norm 0.3079 | time 465.5387 ms | tok/sec 1126196.2857
for step 12772 | loss 3.225214 | norm 0.3179 | time 463.9511 ms | tok/sec 1130050.1017
for step 12773 | loss 3.280885 | norm 0.3110 | time 464.4425 ms | tok/sec 1128854.5074
for step 12774 | loss 3.223466 | norm 0.3090 | time 464.9096 ms | tok/sec 1127720.4265
for step 12775 | loss 3.229742 | norm 0.2937 | time 465.3618 ms | tok/sec 1126624.4075
for step 12776 | loss 3.320332 | norm 0.2979 | time 465.4875 ms | tok/sec 1126320.3036
for step 12777 | loss 3.273380 | norm 0.3073 | time 464.8595 ms | tok/sec 1127841.8882
for step 12778 | loss 3.244544 | norm 0.3018 | time 465.8041 ms | tok/sec 1125554.7127
for step 12779 | loss 3.264626 | norm 0.3002 | time 465.0407 ms | tok/sec 1127402.4367
for step 12780 | loss 3.232903 | norm 0.2829 | time 465.2545 ms | tok/sec 1126884.2086
for step 12781 | loss 3.300810 | norm 0.3275 | time 464.9701 ms | tok/sec 1127573.5507
for step 12782 | loss 3.259786 | norm 0.2935 | time 465.0223 ms | tok/sec 1127446.9444
for step 12783 | loss 3.262960 | norm 0.3075 | time 465.2259 ms | tok/sec 1126953.5092
for step 12784 | loss 3.304242 | norm 0.3395 | time 464.7892 ms | tok/sec 1128012.5570
for step 12785 | loss 3.267998 | norm 0.2988 | time 464.5543 ms | tok/sec 1128582.7919
for step 12786 | loss 3.368684 | norm 0.3311 | time 465.3833 ms | tok/sec 1126572.4616
for step 12787 | loss 3.186040 | norm 0.3305 | time 465.2812 ms | tok/sec 1126819.5358
for step 12788 | loss 3.215358 | norm 0.3038 | time 465.9553 ms | tok/sec 1125189.5792
for step 12789 | loss 3.223445 | norm 0.3162 | time 465.6029 ms | tok/sec 1126041.1574
for step 12790 | loss 3.270421 | norm 0.2903 | time 465.0753 ms | tok/sec 1127318.6328
for step 12791 | loss 3.204103 | norm 0.3265 | time 465.9576 ms | tok/sec 1125183.8219
for step 12792 | loss 3.236672 | norm 0.2760 | time 466.1305 ms | tok/sec 1124766.5740
for step 12793 | loss 3.254306 | norm 0.2826 | time 465.5864 ms | tok/sec 1126080.9446
for step 12794 | loss 3.233149 | norm 0.3030 | time 465.6885 ms | tok/sec 1125834.1942
for step 12795 | loss 3.248655 | norm 0.3105 | time 465.1287 ms | tok/sec 1127189.1949
for step 12796 | loss 3.248264 | norm 0.2881 | time 465.3494 ms | tok/sec 1126654.4228
for step 12797 | loss 3.316377 | norm 0.3272 | time 465.5282 ms | tok/sec 1126221.6639
for step 12798 | loss 3.260076 | norm 0.3217 | time 465.6050 ms | tok/sec 1126035.9680
for step 12799 | loss 3.252082 | norm 0.3219 | time 465.0381 ms | tok/sec 1127408.7947
for step 12800 | loss 3.290862 | norm 0.3596 | time 465.5662 ms | tok/sec 1126129.9617
for step 12801 | loss 3.225179 | norm 0.3537 | time 465.7917 ms | tok/sec 1125584.6711
for step 12802 | loss 3.263462 | norm 0.5314 | time 465.8613 ms | tok/sec 1125416.4639
for step 12803 | loss 3.325775 | norm 0.4434 | time 464.9062 ms | tok/sec 1127728.5232
for step 12804 | loss 3.266722 | norm 0.4146 | time 465.0800 ms | tok/sec 1127307.0747
for step 12805 | loss 3.224493 | norm 0.3878 | time 465.8062 ms | tok/sec 1125549.5278
for step 12806 | loss 3.228789 | norm 0.3973 | time 465.4009 ms | tok/sec 1126529.7541
for step 12807 | loss 3.288805 | norm 0.3573 | time 465.0636 ms | tok/sec 1127346.9514
for step 12808 | loss 3.289701 | norm 0.3753 | time 465.5826 ms | tok/sec 1126090.1710
for step 12809 | loss 3.322144 | norm 0.3432 | time 465.6410 ms | tok/sec 1125948.9081
for step 12810 | loss 3.243758 | norm 0.3180 | time 465.6169 ms | tok/sec 1126007.1388
for step 12811 | loss 3.260437 | norm 0.3560 | time 465.3409 ms | tok/sec 1126675.2036
for step 12812 | loss 3.343197 | norm 0.3374 | time 464.9501 ms | tok/sec 1127622.1196
for step 12813 | loss 3.286182 | norm 0.3357 | time 465.0264 ms | tok/sec 1127437.1177
for step 12814 | loss 3.287985 | norm 0.3396 | time 465.8034 ms | tok/sec 1125556.4410
for step 12815 | loss 3.293354 | norm 0.3267 | time 465.9910 ms | tok/sec 1125103.2257
for step 12816 | loss 3.240111 | norm 0.3113 | time 466.4953 ms | tok/sec 1123887.0521
for step 12817 | loss 3.314312 | norm 0.3301 | time 466.6264 ms | tok/sec 1123571.2201
for step 12818 | loss 3.211520 | norm 0.3124 | time 464.3424 ms | tok/sec 1129097.9458
for step 12819 | loss 3.248294 | norm 0.3141 | time 466.2960 ms | tok/sec 1124367.4570
for step 12820 | loss 3.259692 | norm 0.3252 | time 466.1131 ms | tok/sec 1124808.5726
for step 12821 | loss 3.325142 | norm 0.3294 | time 465.2870 ms | tok/sec 1126805.6783
for step 12822 | loss 3.209700 | norm 0.3244 | time 465.5957 ms | tok/sec 1126058.4559
for step 12823 | loss 3.222631 | norm 0.3117 | time 465.6227 ms | tok/sec 1125993.3012
for step 12824 | loss 3.243418 | norm 0.3026 | time 465.9829 ms | tok/sec 1125122.7980
for step 12825 | loss 3.251933 | norm 0.2995 | time 465.6610 ms | tok/sec 1125900.4833
for step 12826 | loss 3.173885 | norm 0.2941 | time 465.8146 ms | tok/sec 1125529.3646
for step 12827 | loss 3.207184 | norm 0.2990 | time 465.3733 ms | tok/sec 1126596.7024
for step 12828 | loss 3.267204 | norm 0.3203 | time 465.8325 ms | tok/sec 1125486.1601
for step 12829 | loss 3.241582 | norm 0.2526 | time 464.9124 ms | tok/sec 1127713.4867
for step 12830 | loss 3.214687 | norm 0.3063 | time 465.9019 ms | tok/sec 1125318.5583
for step 12831 | loss 3.246270 | norm 0.2842 | time 465.1804 ms | tok/sec 1127063.8302
for step 12832 | loss 3.272442 | norm 0.3093 | time 465.0264 ms | tok/sec 1127437.1177
for step 12833 | loss 3.218308 | norm 0.2935 | time 465.6072 ms | tok/sec 1126030.7786
for step 12834 | loss 3.233046 | norm 0.3376 | time 465.7526 ms | tok/sec 1125679.1657
for step 12835 | loss 3.309195 | norm 0.3249 | time 465.0588 ms | tok/sec 1127358.5103
for step 12836 | loss 3.364509 | norm 0.3542 | time 465.1520 ms | tok/sec 1127132.5751
for step 12837 | loss 3.290754 | norm 0.3374 | time 465.8477 ms | tok/sec 1125449.2950
for step 12838 | loss 3.247970 | norm 0.3287 | time 465.1387 ms | tok/sec 1127164.9286
for step 12839 | loss 3.298573 | norm 0.3433 | time 465.7454 ms | tok/sec 1125696.4530
for step 12840 | loss 3.278252 | norm 0.2985 | time 465.3232 ms | tok/sec 1126717.9221
for step 12841 | loss 3.272959 | norm 0.3202 | time 465.5814 ms | tok/sec 1126093.0543
for step 12842 | loss 3.277925 | norm 0.3109 | time 465.0276 ms | tok/sec 1127434.2276
for step 12843 | loss 3.196532 | norm 0.3535 | time 465.4691 ms | tok/sec 1126364.7260
for step 12844 | loss 3.357316 | norm 0.3463 | time 465.4539 ms | tok/sec 1126401.6512
for step 12845 | loss 3.292386 | norm 0.3429 | time 465.3928 ms | tok/sec 1126549.3761
for step 12846 | loss 3.228244 | norm 0.3684 | time 465.9338 ms | tok/sec 1125241.3976
for step 12847 | loss 3.221851 | norm 0.3001 | time 466.7332 ms | tok/sec 1123314.0917
for step 12848 | loss 3.258800 | norm 0.3672 | time 465.9894 ms | tok/sec 1125107.2553
for step 12849 | loss 3.264718 | norm 0.3222 | time 465.5950 ms | tok/sec 1126060.1857
for step 12850 | loss 3.288114 | norm 0.3663 | time 465.5321 ms | tok/sec 1126212.4353
for step 12851 | loss 3.257651 | norm 0.3099 | time 465.0426 ms | tok/sec 1127397.8127
for step 12852 | loss 3.275498 | norm 0.3112 | time 465.9574 ms | tok/sec 1125184.3976
for step 12853 | loss 3.223930 | norm 0.3170 | time 468.4005 ms | tok/sec 1119315.6801
for step 12854 | loss 3.296897 | norm 0.3168 | time 465.6014 ms | tok/sec 1126044.6171
for step 12855 | loss 3.232157 | norm 0.3093 | time 484.3929 ms | tok/sec 1082361.0754
for step 12856 | loss 3.358276 | norm 0.3212 | time 463.5701 ms | tok/sec 1130978.8524
for step 12857 | loss 3.254993 | norm 0.3063 | time 463.3803 ms | tok/sec 1131442.0540
for step 12858 | loss 3.263826 | norm 0.3173 | time 462.4643 ms | tok/sec 1133683.1039
for step 12859 | loss 3.191238 | norm 0.3099 | time 464.0870 ms | tok/sec 1129719.1892
for step 12860 | loss 3.176894 | norm 0.3154 | time 464.5183 ms | tok/sec 1128670.2596
for step 12861 | loss 3.287412 | norm 0.3188 | time 464.3743 ms | tok/sec 1129020.2660
for step 12862 | loss 3.237880 | norm 0.3320 | time 463.9146 ms | tok/sec 1130138.9586
for step 12863 | loss 3.205190 | norm 0.3167 | time 464.1371 ms | tok/sec 1129597.3229
for step 12864 | loss 3.172861 | norm 0.3293 | time 464.5889 ms | tok/sec 1128498.8125
for step 12865 | loss 3.201742 | norm 0.3018 | time 465.7462 ms | tok/sec 1125694.7242
for step 12866 | loss 3.291374 | norm 0.3171 | time 464.0992 ms | tok/sec 1129689.5907
for step 12867 | loss 3.227781 | norm 0.3194 | time 464.7844 ms | tok/sec 1128024.1297
for step 12868 | loss 3.224564 | norm 0.3233 | time 464.8018 ms | tok/sec 1127981.8907
for step 12869 | loss 3.206612 | norm 0.3200 | time 465.3149 ms | tok/sec 1126738.1279
for step 12870 | loss 3.273441 | norm 0.3406 | time 465.3704 ms | tok/sec 1126603.6285
for step 12871 | loss 3.254782 | norm 0.3104 | time 465.6878 ms | tok/sec 1125835.9233
for step 12872 | loss 3.282766 | norm 0.3259 | time 465.0757 ms | tok/sec 1127317.4770
for step 12873 | loss 3.314254 | norm 0.3829 | time 465.4312 ms | tok/sec 1126456.4665
for step 12874 | loss 3.257599 | norm 0.4050 | time 466.0537 ms | tok/sec 1124951.8514
for step 12875 | loss 3.422008 | norm 0.4009 | time 466.1641 ms | tok/sec 1124685.4625
for step 12876 | loss 3.231506 | norm 0.4094 | time 466.5542 ms | tok/sec 1123745.1929
for step 12877 | loss 3.238712 | norm 0.3478 | time 465.7407 ms | tok/sec 1125707.9781
for step 12878 | loss 3.277231 | norm 0.3796 | time 466.3205 ms | tok/sec 1124308.2461
for step 12879 | loss 3.276139 | norm 0.3659 | time 465.8978 ms | tok/sec 1125328.3481
for step 12880 | loss 3.299175 | norm 0.3582 | time 466.5003 ms | tok/sec 1123874.9898
for step 12881 | loss 3.266423 | norm 0.3941 | time 466.2921 ms | tok/sec 1124376.6553
for step 12882 | loss 3.259903 | norm 0.3360 | time 466.1376 ms | tok/sec 1124749.3153
for step 12883 | loss 3.278622 | norm 0.3408 | time 465.9882 ms | tok/sec 1125110.1335
for step 12884 | loss 3.280998 | norm 0.3434 | time 466.6548 ms | tok/sec 1123502.9089
for step 12885 | loss 3.307314 | norm 0.3255 | time 465.6339 ms | tok/sec 1125966.2037
for step 12886 | loss 3.303223 | norm 0.3457 | time 466.0959 ms | tok/sec 1124849.9989
for step 12887 | loss 3.245864 | norm 0.3148 | time 466.0628 ms | tok/sec 1124929.9832
for step 12888 | loss 3.284555 | norm 0.3411 | time 466.1472 ms | tok/sec 1124726.3044
for step 12889 | loss 3.236887 | norm 0.3078 | time 466.3632 ms | tok/sec 1124205.3606
for step 12890 | loss 3.278189 | norm 0.3171 | time 465.9336 ms | tok/sec 1125241.9734
for step 12891 | loss 3.235049 | norm 0.2981 | time 465.7180 ms | tok/sec 1125762.7259
for step 12892 | loss 3.304348 | norm 0.3324 | time 465.1599 ms | tok/sec 1127113.5105
for step 12893 | loss 3.292683 | norm 0.2885 | time 466.0242 ms | tok/sec 1125023.2168
for step 12894 | loss 3.260581 | norm 0.3338 | time 465.4357 ms | tok/sec 1126445.5030
for step 12895 | loss 3.340098 | norm 0.3467 | time 464.6852 ms | tok/sec 1128264.8945
for step 12896 | loss 3.270791 | norm 0.3275 | time 465.5025 ms | tok/sec 1126283.9607
for step 12897 | loss 3.189768 | norm 0.3101 | time 465.2963 ms | tok/sec 1126783.1606
for step 12898 | loss 3.248843 | norm 0.3038 | time 466.5692 ms | tok/sec 1123709.0159
for step 12899 | loss 3.267516 | norm 0.3400 | time 465.5066 ms | tok/sec 1126274.1542
for step 12900 | loss 3.179858 | norm 0.2877 | time 464.8817 ms | tok/sec 1127788.0948
for step 12901 | loss 3.180741 | norm 0.3176 | time 464.6213 ms | tok/sec 1128420.0570
for step 12902 | loss 3.202587 | norm 0.3016 | time 465.4992 ms | tok/sec 1126292.0367
for step 12903 | loss 3.231658 | norm 0.2887 | time 465.3285 ms | tok/sec 1126705.2217
for step 12904 | loss 3.215007 | norm 0.3148 | time 464.9243 ms | tok/sec 1127684.5714
for step 12905 | loss 3.270013 | norm 0.2970 | time 466.0358 ms | tok/sec 1124995.0149
for step 12906 | loss 3.257709 | norm 0.3090 | time 466.7742 ms | tok/sec 1123215.4041
for step 12907 | loss 3.338388 | norm 0.3089 | time 465.7254 ms | tok/sec 1125744.8603
for step 12908 | loss 3.282249 | norm 0.3256 | time 466.1036 ms | tok/sec 1124831.5868
for step 12909 | loss 3.278986 | norm 0.3000 | time 466.1999 ms | tok/sec 1124599.1864
for step 12910 | loss 3.288823 | norm 0.3256 | time 465.3690 ms | tok/sec 1126607.0916
for step 12911 | loss 3.239332 | norm 0.3286 | time 465.8592 ms | tok/sec 1125421.6477
for step 12912 | loss 3.441547 | norm 0.3596 | time 465.4646 ms | tok/sec 1126375.6879
for step 12913 | loss 3.281153 | norm 0.3352 | time 465.6780 ms | tok/sec 1125859.5560
for step 12914 | loss 3.278074 | norm 0.3367 | time 464.8230 ms | tok/sec 1127930.3981
for step 12915 | loss 3.323017 | norm 0.3285 | time 466.3017 ms | tok/sec 1124353.6597
for step 12916 | loss 3.293165 | norm 0.3239 | time 465.3623 ms | tok/sec 1126623.2530
for step 12917 | loss 3.264399 | norm 0.3066 | time 465.2512 ms | tok/sec 1126892.2933
for step 12918 | loss 3.255703 | norm 0.3112 | time 465.5330 ms | tok/sec 1126210.1282
for step 12919 | loss 3.272967 | norm 0.3340 | time 465.7679 ms | tok/sec 1125642.2878
for step 12920 | loss 3.253866 | norm 0.3354 | time 464.4327 ms | tok/sec 1128878.2670
for step 12921 | loss 3.249416 | norm 0.3182 | time 465.9317 ms | tok/sec 1125246.5797
for step 12922 | loss 3.276315 | norm 0.3239 | time 466.5408 ms | tok/sec 1123777.3521
for step 12923 | loss 3.232831 | norm 0.3108 | time 466.5713 ms | tok/sec 1123703.8479
for step 12924 | loss 3.247231 | norm 0.3315 | time 465.9457 ms | tok/sec 1125212.6090
for step 12925 | loss 3.288217 | norm 0.2951 | time 465.4119 ms | tok/sec 1126503.2079
for step 12926 | loss 3.252862 | norm 0.3137 | time 465.6467 ms | tok/sec 1125935.0720
for step 12927 | loss 3.251164 | norm 0.3033 | time 465.6339 ms | tok/sec 1125966.2037
for step 12928 | loss 3.199573 | norm 0.3133 | time 465.3497 ms | tok/sec 1126653.8456
for step 12929 | loss 3.205530 | norm 0.3140 | time 465.1108 ms | tok/sec 1127232.5302
for step 12930 | loss 3.173439 | norm 0.2888 | time 465.9557 ms | tok/sec 1125188.4277
for step 12931 | loss 3.240852 | norm 0.2834 | time 465.2672 ms | tok/sec 1126853.6036
for step 12932 | loss 3.247896 | norm 0.2999 | time 464.8972 ms | tok/sec 1127750.5003
for step 12933 | loss 3.237021 | norm 0.3014 | time 464.9496 ms | tok/sec 1127623.2760
for step 12934 | loss 3.232073 | norm 0.3100 | time 466.6007 ms | tok/sec 1123633.2240
for step 12935 | loss 3.166844 | norm 0.3192 | time 465.3356 ms | tok/sec 1126687.9033
for step 12936 | loss 3.240117 | norm 0.2865 | time 466.4259 ms | tok/sec 1124054.2278
for step 12937 | loss 3.218462 | norm 0.3164 | time 466.2056 ms | tok/sec 1124585.3835
for step 12938 | loss 3.265614 | norm 0.2684 | time 464.9498 ms | tok/sec 1127622.6978
for step 12939 | loss 3.270462 | norm 0.3039 | time 465.2836 ms | tok/sec 1126813.7618
for step 12940 | loss 3.257765 | norm 0.2862 | time 465.4346 ms | tok/sec 1126448.3881
for step 12941 | loss 3.276484 | norm 0.3173 | time 465.5416 ms | tok/sec 1126189.3645
for step 12942 | loss 3.248914 | norm 0.3192 | time 465.5755 ms | tok/sec 1126107.4710
for step 12943 | loss 3.319556 | norm 0.3643 | time 465.4331 ms | tok/sec 1126451.8502
for step 12944 | loss 3.300981 | norm 0.3384 | time 465.7538 ms | tok/sec 1125676.2845
for step 12945 | loss 3.315223 | norm 0.3509 | time 466.5420 ms | tok/sec 1123774.4807
for step 12946 | loss 3.234602 | norm 0.3604 | time 465.9748 ms | tok/sec 1125142.3710
for step 12947 | loss 3.278105 | norm 0.3343 | time 464.1831 ms | tok/sec 1129485.3451
for step 12948 | loss 3.279600 | norm 0.3928 | time 466.0654 ms | tok/sec 1124923.6531
for step 12949 | loss 3.288276 | norm 0.3264 | time 466.2623 ms | tok/sec 1124448.5227
for step 12950 | loss 3.332415 | norm 0.3730 | time 465.7588 ms | tok/sec 1125664.1838
for step 12951 | loss 3.249984 | norm 0.3149 | time 465.7009 ms | tok/sec 1125804.2225
for step 12952 | loss 3.283547 | norm 0.3418 | time 466.0530 ms | tok/sec 1124953.5779
Will loading at 0 from edu_fineweb10B/edufineweb_train_000069.npy
for step 12953 | loss 3.314265 | norm 0.3286 | time 2563.4315 ms | tok/sec 204525.8474
for step 12954 | loss 3.328866 | norm 0.3168 | time 478.9050 ms | tok/sec 1094764.1829
for step 12955 | loss 3.217701 | norm 0.3597 | time 464.9043 ms | tok/sec 1127733.1499
for step 12956 | loss 3.285969 | norm 0.3026 | time 464.4194 ms | tok/sec 1128910.7208
for step 12957 | loss 3.263557 | norm 0.3166 | time 464.6230 ms | tok/sec 1128416.0037
for step 12958 | loss 3.231095 | norm 0.3022 | time 465.8823 ms | tok/sec 1125365.7812
for step 12959 | loss 3.243524 | norm 0.3157 | time 464.6182 ms | tok/sec 1128427.5846
for step 12960 | loss 3.275513 | norm 0.3116 | time 463.9435 ms | tok/sec 1130068.6850
for step 12961 | loss 3.263414 | norm 0.3162 | time 464.5774 ms | tok/sec 1128526.6112
for step 12962 | loss 3.236690 | norm 0.3104 | time 465.2712 ms | tok/sec 1126843.7873
for step 12963 | loss 3.321427 | norm 0.3949 | time 465.9438 ms | tok/sec 1125217.2151
for step 12964 | loss 3.283877 | norm 0.3748 | time 464.3953 ms | tok/sec 1128969.2581
for step 12965 | loss 3.224013 | norm 0.3330 | time 464.5309 ms | tok/sec 1128639.5575
for step 12966 | loss 3.231839 | norm 0.3332 | time 464.8709 ms | tok/sec 1127814.1232
for step 12967 | loss 3.201681 | norm 0.3203 | time 465.2770 ms | tok/sec 1126829.9292
for step 12968 | loss 3.210840 | norm 0.3634 | time 464.6466 ms | tok/sec 1128358.6817
for step 12969 | loss 3.259829 | norm 0.3278 | time 465.4157 ms | tok/sec 1126493.9747
for step 12970 | loss 3.279447 | norm 0.3326 | time 465.7800 ms | tok/sec 1125612.9026
for step 12971 | loss 3.153046 | norm 0.3082 | time 466.1784 ms | tok/sec 1124650.9505
for step 12972 | loss 3.204271 | norm 0.3271 | time 466.4607 ms | tok/sec 1123970.3465
for step 12973 | loss 3.199820 | norm 0.2997 | time 465.1582 ms | tok/sec 1127117.5545
for step 12974 | loss 3.244871 | norm 0.3271 | time 466.4412 ms | tok/sec 1124017.4564
for step 12975 | loss 3.221392 | norm 0.3185 | time 470.1540 ms | tok/sec 1115140.8848
for step 12976 | loss 3.205076 | norm 0.3254 | time 465.3549 ms | tok/sec 1126641.1466
for step 12977 | loss 3.272164 | norm 0.3332 | time 465.0815 ms | tok/sec 1127303.6073
for step 12978 | loss 3.221859 | norm 0.3142 | time 465.1155 ms | tok/sec 1127220.9738
for step 12979 | loss 3.274457 | norm 0.3531 | time 466.3832 ms | tok/sec 1124157.0856
for step 12980 | loss 3.294729 | norm 0.3146 | time 464.9873 ms | tok/sec 1127531.9236
for step 12981 | loss 3.247633 | norm 0.3130 | time 466.4750 ms | tok/sec 1123935.8783
for step 12982 | loss 3.288649 | norm 0.2925 | time 466.3873 ms | tok/sec 1124147.3162
for step 12983 | loss 3.296801 | norm 0.2912 | time 466.7277 ms | tok/sec 1123327.2896
for step 12984 | loss 3.368626 | norm 0.3329 | time 465.2793 ms | tok/sec 1126824.1551
for step 12985 | loss 3.290955 | norm 0.3519 | time 465.8279 ms | tok/sec 1125497.1049
for step 12986 | loss 3.253359 | norm 0.3260 | time 466.1489 ms | tok/sec 1124722.2776
for step 12987 | loss 3.346297 | norm 0.3506 | time 465.2615 ms | tok/sec 1126867.4623
for step 12988 | loss 3.306074 | norm 0.3122 | time 464.9379 ms | tok/sec 1127651.6099
for step 12989 | loss 3.259326 | norm 0.3655 | time 465.2035 ms | tok/sec 1127007.8006
for step 12990 | loss 3.232100 | norm 0.2824 | time 465.3897 ms | tok/sec 1126556.8788
for step 12991 | loss 3.243114 | norm 0.3322 | time 465.4121 ms | tok/sec 1126502.6308
for step 12992 | loss 3.260803 | norm 0.3088 | time 465.1296 ms | tok/sec 1127186.8837
for step 12993 | loss 3.248796 | norm 0.3538 | time 464.5474 ms | tok/sec 1128599.5893
for step 12994 | loss 3.255463 | norm 0.3118 | time 466.1629 ms | tok/sec 1124688.3386
for step 12995 | loss 3.308392 | norm 0.3750 | time 464.8695 ms | tok/sec 1127817.5937
for step 12996 | loss 3.294546 | norm 0.3110 | time 464.9453 ms | tok/sec 1127633.6842
for step 12997 | loss 3.264602 | norm 0.3434 | time 465.4007 ms | tok/sec 1126530.3312
for step 12998 | loss 3.295716 | norm 0.2981 | time 465.3125 ms | tok/sec 1126743.9011
for step 12999 | loss 3.282000 | norm 0.3325 | time 465.3661 ms | tok/sec 1126614.0179
validation loss 3.2715
HellaSwag accuracy: 2791/10042=0.2779
> Hello, I'm a language model, and this is a free software library dedicated exclusively to modeling languages used in the design of embedded systems in the world.

> Hello, I'm a language model, not an expert one. I'm just as good at doing things - I like playing tag games - and I'm good
> Hello, I'm a language model, I wish that we could try to explain the concept of "code", which is a set of functions to perform specific tasks
> Hello, I'm a language model, and I'm an English grammar course.I'm a high school English learner, since I've been in business for
> Hello, I'm a language model, and I can't figure out which word came first. I've not already figured out a set of rules in the previous
> > Hello, I'm a language model, and I love to teach others. I never go into any of the problems. No one would stop talking about it,
Hello, I'm a language model, I do not know the language. Now, if what he says is to say that "Hello" (the noun),
> Hello, I'm a language model, anyway. We're a professional development assistant and I get job opportunities like this. Let me show you why.
If> 
Hello, I'm a language model, so I hope to read through the sections and read through the entire post.
You may be wondering, what's called
> Hello, I'm a language model, and I am constantly seeking new opportunities for learning. How can you explain an idea in a language model? I think that> > 
Hello, I'm a language model, and for the last few years, I have the experience of dealing with large and tiny files that I just can't cope
Hello, I'm a language model, so I want to build one. I like the thing that you're talking about. What language do you call the language
> > Hello, I'm a language model, no problem. I don't know at all why I'm doing it. I'm going to go back to the point
> Hello, I'm a language model, and i am looking for it! And I am also going to be posting this.
I am going from the language
> > Hello, I'm a language model, thanks!
Last modified: 2012-07-31 by robaul (c). 2010.
To learn more,
> Hello, I'm a language model, we will be using that approach to the world. If I want to understand language, I'm going to be using your
Hello, I'm a language model, I use the model, and I am always looking for a way to build a better model. I try to think about
> Hello, I'm a language model, how to connect the word 'waxboard' to the concept of its meaning.
- How does it work?
> > Hello, I'm a language model, but some of you know this: The syntax of a language depends on the way your brain processes it.
It is
> Hello, I'm a language model, you guys love it, I have a simple syntax for it, and there's no reason to make a grammar mistake.
Hello, I'm a language model, just write one code or do another!
And I'm doing three languages for free, and I'd use the

> Hello, I'm a language model, so I can write my own program but please do so.
So please tell us more. The main difference between these
> > > Hello, I'm a language model, and it won a lot of work just fine when you break from it with the example of a web application. When we
> Hello, I'm a language model, I can read a page from an XML document in XML. The goal is to create a single XML document which describes a
Hello, I'm a language model, and I'm not. But, of course I think language models are complex if they only use patterns in order to communicate
Hello, I'm a language model, and you know that our language is pretty simple to understand.
This is very easy; it's just a little different
Hello, I'm a language model, I think every language model has a specific element of truth and a certain element of truth: grammar - I'll help you
> > > Hello, I'm a language model, and we use a very similar framework. If you want to learn about the program in the past, watch video today.
Hello, I'm a language model, and this isn't a nice book to read, but, I think, it's a good book to read with,
Hello, I'm a language model, why are you doing this?
I am a programmer in the English language as well! I worked with a lot of
> Hello, I'm a language model, and you've noticed things like all the steps have to be completed before the end of the process. I've created this
> Hello, I'm a language model, and they're pretty good at doing things, right? So I know you're writing software as a single entity, but
for step 13000 | loss 3.206084 | norm 0.2965 | time 12429.0628 ms | tok/sec 42182.4241
for step 13001 | loss 3.218026 | norm 0.3141 | time 462.8508 ms | tok/sec 1132736.4881
for step 13002 | loss 3.200255 | norm 0.2851 | time 463.6681 ms | tok/sec 1130739.8352
for step 13003 | loss 3.285572 | norm 0.3231 | time 463.9735 ms | tok/sec 1129995.5168
for step 13004 | loss 3.281261 | norm 0.2910 | time 464.7970 ms | tok/sec 1127993.4627
for step 13005 | loss 3.208485 | norm 0.3236 | time 464.2136 ms | tok/sec 1129411.0923
for step 13006 | loss 3.215259 | norm 0.3059 | time 464.7968 ms | tok/sec 1127994.0413
for step 13007 | loss 3.239779 | norm 0.2729 | time 463.9246 ms | tok/sec 1130114.5651
for step 13008 | loss 3.214819 | norm 0.3260 | time 465.1351 ms | tok/sec 1127173.5950
for step 13009 | loss 3.248161 | norm 0.2853 | time 463.6343 ms | tok/sec 1130822.4039
for step 13010 | loss 3.239645 | norm 0.2983 | time 463.2843 ms | tok/sec 1131676.7090
for step 13011 | loss 3.281424 | norm 0.3090 | time 463.7077 ms | tok/sec 1130643.3264
for step 13012 | loss 3.284952 | norm 0.3497 | time 464.4146 ms | tok/sec 1128922.3118
for step 13013 | loss 3.229017 | norm 0.3426 | time 464.0257 ms | tok/sec 1129868.3661
for step 13014 | loss 3.209452 | norm 0.3356 | time 463.6197 ms | tok/sec 1130857.8773
for step 13015 | loss 3.401620 | norm 0.4039 | time 464.0107 ms | tok/sec 1129904.9408
for step 13016 | loss 3.226839 | norm 0.4439 | time 464.6201 ms | tok/sec 1128422.9522
for step 13017 | loss 3.302760 | norm 0.3547 | time 464.4606 ms | tok/sec 1128810.4679
for step 13018 | loss 3.276167 | norm 0.3921 | time 464.3877 ms | tok/sec 1128987.8059
for step 13019 | loss 3.249900 | norm 0.3320 | time 465.1628 ms | tok/sec 1127106.5781
for step 13020 | loss 3.264887 | norm 0.3823 | time 464.9661 ms | tok/sec 1127583.3798
for step 13021 | loss 3.216057 | norm 0.3253 | time 466.6290 ms | tok/sec 1123564.9053
for step 13022 | loss 3.360878 | norm 0.3535 | time 465.1265 ms | tok/sec 1127194.3949
for step 13023 | loss 3.288039 | norm 0.3101 | time 465.6968 ms | tok/sec 1125814.0207
for step 13024 | loss 3.278844 | norm 0.3484 | time 465.5712 ms | tok/sec 1126117.8512
for step 13025 | loss 3.286573 | norm 0.3050 | time 465.1976 ms | tok/sec 1127022.2407
for step 13026 | loss 3.240545 | norm 0.3737 | time 465.1170 ms | tok/sec 1127217.5069
for step 13027 | loss 3.265868 | norm 0.3152 | time 465.3270 ms | tok/sec 1126708.6854
for step 13028 | loss 3.237737 | norm 0.2976 | time 464.9842 ms | tok/sec 1127539.4394
for step 13029 | loss 3.303565 | norm 0.3205 | time 465.3008 ms | tok/sec 1126772.1908
for step 13030 | loss 3.287056 | norm 0.3015 | time 464.4420 ms | tok/sec 1128855.6664
for step 13031 | loss 3.222435 | norm 0.2782 | time 464.7899 ms | tok/sec 1128010.8212
for step 13032 | loss 3.280435 | norm 0.3013 | time 465.0686 ms | tok/sec 1127334.8147
for step 13033 | loss 3.289253 | norm 0.2921 | time 469.1892 ms | tok/sec 1117434.1542
for step 13034 | loss 3.262746 | norm 0.2939 | time 464.9181 ms | tok/sec 1127699.6072
for step 13035 | loss 3.235816 | norm 0.3032 | time 465.1108 ms | tok/sec 1127232.5302
for step 13036 | loss 3.242912 | norm 0.2820 | time 464.9084 ms | tok/sec 1127723.3182
for step 13037 | loss 3.194855 | norm 0.2959 | time 465.0495 ms | tok/sec 1127381.0510
for step 13038 | loss 3.220653 | norm 0.2719 | time 465.2972 ms | tok/sec 1126780.8512
for step 13039 | loss 3.238147 | norm 0.2868 | time 465.6725 ms | tok/sec 1125872.8138
for step 13040 | loss 3.216532 | norm 0.2638 | time 465.3277 ms | tok/sec 1126706.9535
for step 13041 | loss 3.205746 | norm 0.2863 | time 465.3473 ms | tok/sec 1126659.6179
for step 13042 | loss 3.264284 | norm 0.2821 | time 465.5859 ms | tok/sec 1126082.0979
for step 13043 | loss 3.228609 | norm 0.3008 | time 465.9932 ms | tok/sec 1125098.0450
for step 13044 | loss 3.243843 | norm 0.3020 | time 464.3350 ms | tok/sec 1129115.9180
for step 13045 | loss 3.237927 | norm 0.3148 | time 466.0306 ms | tok/sec 1125007.6768
for step 13046 | loss 3.227561 | norm 0.2985 | time 465.7774 ms | tok/sec 1125619.2404
for step 13047 | loss 3.213679 | norm 0.3290 | time 465.5292 ms | tok/sec 1126219.3567
for step 13048 | loss 3.277158 | norm 0.3357 | time 465.4233 ms | tok/sec 1126475.5088
for step 13049 | loss 3.303780 | norm 0.3259 | time 465.5011 ms | tok/sec 1126287.4218
for step 13050 | loss 3.205407 | norm 0.3561 | time 464.9124 ms | tok/sec 1127713.4867
for step 13051 | loss 3.302883 | norm 0.3178 | time 465.4448 ms | tok/sec 1126423.5767
for step 13052 | loss 3.239274 | norm 0.3227 | time 464.2849 ms | tok/sec 1129237.6805
for step 13053 | loss 3.235860 | norm 0.3350 | time 465.2920 ms | tok/sec 1126793.5533
for step 13054 | loss 3.318549 | norm 0.3233 | time 466.0099 ms | tok/sec 1125057.7516
for step 13055 | loss 3.263635 | norm 0.3012 | time 465.2717 ms | tok/sec 1126842.6324
for step 13056 | loss 3.233843 | norm 0.3141 | time 465.4236 ms | tok/sec 1126474.9317
for step 13057 | loss 3.275125 | norm 0.3581 | time 465.0261 ms | tok/sec 1127437.6958
for step 13058 | loss 3.324806 | norm 0.3445 | time 465.6732 ms | tok/sec 1125871.0845
for step 13059 | loss 3.260806 | norm 0.3927 | time 465.5197 ms | tok/sec 1126242.4287
for step 13060 | loss 3.232920 | norm 0.3354 | time 465.0638 ms | tok/sec 1127346.3734
for step 13061 | loss 3.258689 | norm 0.3636 | time 465.3141 ms | tok/sec 1126739.8599
for step 13062 | loss 3.235791 | norm 0.3553 | time 464.6809 ms | tok/sec 1128275.3145
for step 13063 | loss 3.245628 | norm 0.3307 | time 465.3387 ms | tok/sec 1126680.3989
for step 13064 | loss 3.224033 | norm 0.3272 | time 465.8706 ms | tok/sec 1125394.0017
for step 13065 | loss 3.276095 | norm 0.3208 | time 464.7980 ms | tok/sec 1127991.1483
for step 13066 | loss 3.245511 | norm 0.3594 | time 465.2803 ms | tok/sec 1126821.8454
for step 13067 | loss 3.263515 | norm 0.3204 | time 465.1663 ms | tok/sec 1127097.9127
for step 13068 | loss 3.270036 | norm 0.3165 | time 465.4727 ms | tok/sec 1126356.0720
for step 13069 | loss 3.248507 | norm 0.3361 | time 465.4081 ms | tok/sec 1126512.4412
for step 13070 | loss 3.283349 | norm 0.3319 | time 466.0454 ms | tok/sec 1124971.9939
for step 13071 | loss 3.176208 | norm 0.3262 | time 465.1217 ms | tok/sec 1127205.9508
for step 13072 | loss 3.262538 | norm 0.3006 | time 465.3201 ms | tok/sec 1126725.4270
for step 13073 | loss 3.256435 | norm 0.3651 | time 464.9115 ms | tok/sec 1127715.7999
for step 13074 | loss 3.231299 | norm 0.2993 | time 465.5824 ms | tok/sec 1126090.7477
for step 13075 | loss 3.208345 | norm 0.3082 | time 465.3764 ms | tok/sec 1126589.1992
for step 13076 | loss 3.233962 | norm 0.2775 | time 465.2867 ms | tok/sec 1126806.2557
for step 13077 | loss 3.253321 | norm 0.3046 | time 465.2021 ms | tok/sec 1127011.2662
for step 13078 | loss 3.214342 | norm 0.2601 | time 465.5621 ms | tok/sec 1126139.7656
for step 13079 | loss 3.234191 | norm 0.2667 | time 464.3383 ms | tok/sec 1129107.8015
for step 13080 | loss 3.197463 | norm 0.2780 | time 465.3680 ms | tok/sec 1126609.4004
for step 13081 | loss 3.216462 | norm 0.2642 | time 465.2505 ms | tok/sec 1126894.0257
for step 13082 | loss 3.285258 | norm 0.3332 | time 465.1325 ms | tok/sec 1127179.9504
for step 13083 | loss 3.280070 | norm 0.3564 | time 465.2760 ms | tok/sec 1126832.2388
for step 13084 | loss 3.294022 | norm 0.3267 | time 465.2438 ms | tok/sec 1126910.1954
for step 13085 | loss 3.232791 | norm 0.3515 | time 465.1878 ms | tok/sec 1127045.9232
for step 13086 | loss 3.260614 | norm 0.3133 | time 465.8980 ms | tok/sec 1125327.7722
for step 13087 | loss 3.337772 | norm 0.3617 | time 464.7238 ms | tok/sec 1128171.1229
for step 13088 | loss 3.230399 | norm 0.3583 | time 465.1947 ms | tok/sec 1127029.1720
for step 13089 | loss 3.228581 | norm 0.3327 | time 466.2273 ms | tok/sec 1124533.0504
for step 13090 | loss 3.220143 | norm 0.3777 | time 465.4727 ms | tok/sec 1126356.0720
for step 13091 | loss 3.264518 | norm 0.3513 | time 464.8623 ms | tok/sec 1127834.9468
for step 13092 | loss 3.277523 | norm 0.3725 | time 466.3467 ms | tok/sec 1124245.0182
for step 13093 | loss 3.251996 | norm 0.3703 | time 465.3587 ms | tok/sec 1126631.9111
for step 13094 | loss 3.241023 | norm 0.3257 | time 465.1015 ms | tok/sec 1127255.0658
for step 13095 | loss 3.268190 | norm 0.3763 | time 465.1592 ms | tok/sec 1127115.2437
for step 13096 | loss 3.255946 | norm 0.3404 | time 465.2679 ms | tok/sec 1126851.8713
for step 13097 | loss 3.320453 | norm 0.3695 | time 465.6992 ms | tok/sec 1125808.2570
for step 13098 | loss 3.286417 | norm 0.3399 | time 464.7732 ms | tok/sec 1128051.3263
for step 13099 | loss 3.228134 | norm 0.3121 | time 465.5428 ms | tok/sec 1126186.4808
for step 13100 | loss 3.297297 | norm 0.3413 | time 466.3548 ms | tok/sec 1124225.4764
for step 13101 | loss 3.225364 | norm 0.3038 | time 465.3449 ms | tok/sec 1126665.3903
for step 13102 | loss 3.191146 | norm 0.3377 | time 465.9016 ms | tok/sec 1125319.1341
for step 13103 | loss 3.318535 | norm 0.3909 | time 465.8110 ms | tok/sec 1125538.0059
for step 13104 | loss 3.267413 | norm 0.3153 | time 465.5983 ms | tok/sec 1126052.1131
for step 13105 | loss 3.239082 | norm 0.3470 | time 465.9994 ms | tok/sec 1125083.0785
for step 13106 | loss 3.266145 | norm 0.3185 | time 465.8549 ms | tok/sec 1125432.0152
for step 13107 | loss 3.223567 | norm 0.3166 | time 465.2209 ms | tok/sec 1126965.6377
for step 13108 | loss 3.192368 | norm 0.3195 | time 466.0811 ms | tok/sec 1124885.6739
for step 13109 | loss 3.251934 | norm 0.3420 | time 465.3759 ms | tok/sec 1126590.3535
for step 13110 | loss 3.260092 | norm 0.3133 | time 464.9088 ms | tok/sec 1127722.1615
for step 13111 | loss 3.240886 | norm 0.3347 | time 466.0535 ms | tok/sec 1124952.4269
for step 13112 | loss 3.234787 | norm 0.3124 | time 466.3517 ms | tok/sec 1124232.9482
for step 13113 | loss 3.268372 | norm 0.3243 | time 465.5204 ms | tok/sec 1126240.6983
for step 13114 | loss 3.239628 | norm 0.3167 | time 466.2583 ms | tok/sec 1124458.2973
for step 13115 | loss 3.188783 | norm 0.3339 | time 466.0275 ms | tok/sec 1125015.1589
for step 13116 | loss 3.229632 | norm 0.3142 | time 465.9038 ms | tok/sec 1125313.9514
for step 13117 | loss 3.274668 | norm 0.3349 | time 465.7483 ms | tok/sec 1125689.5380
for step 13118 | loss 3.262875 | norm 0.3344 | time 465.5704 ms | tok/sec 1126119.5812
for step 13119 | loss 3.318999 | norm 0.3572 | time 465.3656 ms | tok/sec 1126615.1723
for step 13120 | loss 3.249689 | norm 0.3568 | time 465.6284 ms | tok/sec 1125979.4640
for step 13121 | loss 3.252364 | norm 0.3493 | time 465.4846 ms | tok/sec 1126327.2264
for step 13122 | loss 3.273568 | norm 0.3684 | time 465.6565 ms | tok/sec 1125911.4362
for step 13123 | loss 3.343949 | norm 0.4131 | time 465.4675 ms | tok/sec 1126368.7646
for step 13124 | loss 3.227510 | norm 0.3625 | time 465.6477 ms | tok/sec 1125932.7661
for step 13125 | loss 3.228327 | norm 0.3584 | time 465.6546 ms | tok/sec 1125916.0480
for step 13126 | loss 3.242272 | norm 0.3535 | time 465.5697 ms | tok/sec 1126121.3113
for step 13127 | loss 3.294697 | norm 0.3302 | time 466.1760 ms | tok/sec 1124656.7023
for step 13128 | loss 3.263966 | norm 0.3567 | time 465.6863 ms | tok/sec 1125839.3817
for step 13129 | loss 3.306074 | norm 0.3118 | time 465.3049 ms | tok/sec 1126762.3758
for step 13130 | loss 3.268565 | norm 0.3496 | time 465.5375 ms | tok/sec 1126199.1695
for step 13131 | loss 3.241981 | norm 0.3188 | time 466.2867 ms | tok/sec 1124389.8782
for step 13132 | loss 3.251728 | norm 0.3330 | time 465.2109 ms | tok/sec 1126989.8954
for step 13133 | loss 3.280784 | norm 0.3339 | time 466.1484 ms | tok/sec 1124723.4281
for step 13134 | loss 3.205758 | norm 0.2997 | time 465.3184 ms | tok/sec 1126729.4682
for step 13135 | loss 3.278995 | norm 0.3305 | time 465.1003 ms | tok/sec 1127257.9551
for step 13136 | loss 3.276580 | norm 0.3335 | time 465.0683 ms | tok/sec 1127335.3926
for step 13137 | loss 3.271185 | norm 0.3164 | time 464.9081 ms | tok/sec 1127723.8965
for step 13138 | loss 3.278363 | norm 0.3340 | time 465.6124 ms | tok/sec 1126018.0937
for step 13139 | loss 3.259867 | norm 0.2921 | time 465.7359 ms | tok/sec 1125719.5035
for step 13140 | loss 3.217369 | norm 0.3310 | time 466.2979 ms | tok/sec 1124362.8578
for step 13141 | loss 3.254955 | norm 0.3008 | time 466.0387 ms | tok/sec 1124988.1085
for step 13142 | loss 3.276944 | norm 0.3435 | time 465.0452 ms | tok/sec 1127391.4548
for step 13143 | loss 3.126265 | norm 0.2928 | time 464.2980 ms | tok/sec 1129205.7878
Will loading at 0 from edu_fineweb10B/edufineweb_train_000070.npy
for step 13144 | loss 3.212944 | norm 0.3403 | time 2539.0456 ms | tok/sec 206490.1890
for step 13145 | loss 3.227302 | norm 0.3080 | time 465.1997 ms | tok/sec 1127017.0422
for step 13146 | loss 3.232666 | norm 0.3043 | time 463.9144 ms | tok/sec 1130139.5394
for step 13147 | loss 3.262123 | norm 0.3454 | time 465.4496 ms | tok/sec 1126412.0368
for step 13148 | loss 3.211314 | norm 0.3050 | time 464.2990 ms | tok/sec 1129203.4684
for step 13149 | loss 3.200905 | norm 0.3175 | time 465.4191 ms | tok/sec 1126485.8958
for step 13150 | loss 3.228663 | norm 0.2774 | time 463.5854 ms | tok/sec 1130941.6265
for step 13151 | loss 3.233695 | norm 0.3176 | time 464.3998 ms | tok/sec 1128958.2457
for step 13152 | loss 3.255951 | norm 0.2885 | time 465.2743 ms | tok/sec 1126836.2807
for step 13153 | loss 3.179044 | norm 0.3173 | time 464.3991 ms | tok/sec 1128959.9845
for step 13154 | loss 3.323372 | norm 0.3110 | time 465.2967 ms | tok/sec 1126782.0059
for step 13155 | loss 3.226454 | norm 0.3177 | time 465.7333 ms | tok/sec 1125725.8426
for step 13156 | loss 3.282229 | norm 0.3018 | time 465.6641 ms | tok/sec 1125892.9894
for step 13157 | loss 3.269394 | norm 0.3266 | time 465.3440 ms | tok/sec 1126667.6993
for step 13158 | loss 3.283238 | norm 0.3172 | time 465.4920 ms | tok/sec 1126309.3428
for step 13159 | loss 3.199937 | norm 0.3026 | time 465.0643 ms | tok/sec 1127345.2175
for step 13160 | loss 3.285217 | norm 0.3209 | time 464.4532 ms | tok/sec 1128828.4310
for step 13161 | loss 3.321651 | norm 0.3001 | time 466.1651 ms | tok/sec 1124683.1616
for step 13162 | loss 3.275145 | norm 0.3272 | time 465.7073 ms | tok/sec 1125788.6609
for step 13163 | loss 3.317156 | norm 0.3371 | time 465.0297 ms | tok/sec 1127429.0253
for step 13164 | loss 3.245077 | norm 0.2909 | time 465.5030 ms | tok/sec 1126282.8070
for step 13165 | loss 3.264969 | norm 0.3289 | time 465.2588 ms | tok/sec 1126873.8143
for step 13166 | loss 3.265019 | norm 0.3199 | time 465.5225 ms | tok/sec 1126235.5070
for step 13167 | loss 3.230809 | norm 0.3403 | time 465.9595 ms | tok/sec 1125179.2161
for step 13168 | loss 3.263620 | norm 0.3262 | time 465.8191 ms | tok/sec 1125518.4191
for step 13169 | loss 3.259708 | norm 0.3128 | time 465.7867 ms | tok/sec 1125596.7701
for step 13170 | loss 3.238202 | norm 0.3132 | time 465.4899 ms | tok/sec 1126314.5348
for step 13171 | loss 3.259356 | norm 0.3250 | time 465.6634 ms | tok/sec 1125894.7187
for step 13172 | loss 3.234737 | norm 0.3131 | time 465.2798 ms | tok/sec 1126823.0002
for step 13173 | loss 3.285753 | norm 0.3102 | time 465.4438 ms | tok/sec 1126425.8847
for step 13174 | loss 3.270412 | norm 0.2853 | time 464.4094 ms | tok/sec 1128935.0623
for step 13175 | loss 3.266122 | norm 0.3203 | time 465.9412 ms | tok/sec 1125223.5485
for step 13176 | loss 3.348007 | norm 0.3135 | time 465.2553 ms | tok/sec 1126882.4762
for step 13177 | loss 3.270349 | norm 0.3013 | time 464.6330 ms | tok/sec 1128391.6846
for step 13178 | loss 3.234493 | norm 0.3285 | time 464.3247 ms | tok/sec 1129140.8482
for step 13179 | loss 3.261140 | norm 0.2973 | time 464.3846 ms | tok/sec 1128995.3411
for step 13180 | loss 3.218845 | norm 0.3113 | time 465.5602 ms | tok/sec 1126144.3793
for step 13181 | loss 3.215103 | norm 0.3140 | time 465.6351 ms | tok/sec 1125963.3211
for step 13182 | loss 3.288915 | norm 0.2839 | time 464.7610 ms | tok/sec 1128080.8391
for step 13183 | loss 3.187982 | norm 0.3300 | time 464.8826 ms | tok/sec 1127785.7812
for step 13184 | loss 3.188582 | norm 0.2966 | time 464.7651 ms | tok/sec 1128071.0013
for step 13185 | loss 3.237902 | norm 0.2931 | time 465.4639 ms | tok/sec 1126377.4188
for step 13186 | loss 3.242659 | norm 0.2855 | time 465.0643 ms | tok/sec 1127345.2175
for step 13187 | loss 3.245980 | norm 0.3552 | time 464.8283 ms | tok/sec 1127917.6703
for step 13188 | loss 3.271509 | norm 0.2998 | time 465.2748 ms | tok/sec 1126835.1259
for step 13189 | loss 3.161444 | norm 0.2986 | time 465.4081 ms | tok/sec 1126512.4412
for step 13190 | loss 3.233234 | norm 0.3424 | time 465.4067 ms | tok/sec 1126515.9038
for step 13191 | loss 3.280809 | norm 0.3295 | time 465.0207 ms | tok/sec 1127450.9908
for step 13192 | loss 3.310375 | norm 0.3214 | time 465.8444 ms | tok/sec 1125457.3590
for step 13193 | loss 3.240603 | norm 0.3100 | time 466.0637 ms | tok/sec 1124927.6813
for step 13194 | loss 3.309121 | norm 0.3120 | time 465.0083 ms | tok/sec 1127481.0502
for step 13195 | loss 3.317501 | norm 0.3035 | time 464.8890 ms | tok/sec 1127770.1648
for step 13196 | loss 3.230065 | norm 0.3142 | time 465.7485 ms | tok/sec 1125688.9617
for step 13197 | loss 3.273346 | norm 0.3287 | time 464.7624 ms | tok/sec 1128077.3669
for step 13198 | loss 3.255942 | norm 0.2978 | time 465.9972 ms | tok/sec 1125088.2592
for step 13199 | loss 3.292479 | norm 0.3340 | time 464.8452 ms | tok/sec 1127876.5963
for step 13200 | loss 3.325241 | norm 0.2974 | time 464.7765 ms | tok/sec 1128043.2251
for step 13201 | loss 3.252211 | norm 0.3524 | time 465.7273 ms | tok/sec 1125740.2499
for step 13202 | loss 3.203378 | norm 0.3479 | time 464.7698 ms | tok/sec 1128059.4277
for step 13203 | loss 3.260511 | norm 0.3390 | time 465.1971 ms | tok/sec 1127023.3959
for step 13204 | loss 3.248181 | norm 0.3309 | time 465.5566 ms | tok/sec 1126153.0300
for step 13205 | loss 3.236081 | norm 0.3080 | time 464.5267 ms | tok/sec 1128649.9844
for step 13206 | loss 3.237402 | norm 0.3157 | time 464.1471 ms | tok/sec 1129572.9528
for step 13207 | loss 3.305376 | norm 0.2983 | time 464.7403 ms | tok/sec 1128131.1879
for step 13208 | loss 3.271697 | norm 0.3462 | time 464.6389 ms | tok/sec 1128377.2094
for step 13209 | loss 3.236421 | norm 0.2975 | time 465.7352 ms | tok/sec 1125721.2324
for step 13210 | loss 3.301499 | norm 0.3676 | time 464.5555 ms | tok/sec 1128579.8959
for step 13211 | loss 3.290176 | norm 0.3305 | time 464.7081 ms | tok/sec 1128209.3243
for step 13212 | loss 3.270787 | norm 0.3403 | time 465.4839 ms | tok/sec 1126328.9571
for step 13213 | loss 3.292635 | norm 0.3048 | time 465.3361 ms | tok/sec 1126686.7488
for step 13214 | loss 3.248334 | norm 0.3154 | time 464.7777 ms | tok/sec 1128040.3318
for step 13215 | loss 3.208974 | norm 0.3077 | time 464.8876 ms | tok/sec 1127773.6351
for step 13216 | loss 3.179608 | norm 0.3176 | time 465.3394 ms | tok/sec 1126678.6671
for step 13217 | loss 3.249627 | norm 0.2924 | time 468.6751 ms | tok/sec 1118659.7263
for step 13218 | loss 3.188985 | norm 0.3047 | time 465.5969 ms | tok/sec 1126055.5728
for step 13219 | loss 3.218472 | norm 0.3042 | time 465.7845 ms | tok/sec 1125601.9555
for step 13220 | loss 3.266947 | norm 0.3048 | time 465.2712 ms | tok/sec 1126843.7873
for step 13221 | loss 3.226046 | norm 0.3229 | time 464.6156 ms | tok/sec 1128433.9542
for step 13222 | loss 3.218133 | norm 0.3155 | time 465.0228 ms | tok/sec 1127445.7884
for step 13223 | loss 3.195246 | norm 0.3352 | time 464.5751 ms | tok/sec 1128532.4028
for step 13224 | loss 3.223577 | norm 0.3473 | time 465.2646 ms | tok/sec 1126859.9555
for step 13225 | loss 3.228745 | norm 0.3102 | time 466.4481 ms | tok/sec 1124000.7951
for step 13226 | loss 3.248725 | norm 0.3474 | time 465.0695 ms | tok/sec 1127332.5030
for step 13227 | loss 3.331197 | norm 0.3347 | time 465.3356 ms | tok/sec 1126687.9033
for step 13228 | loss 3.286925 | norm 0.3287 | time 465.3678 ms | tok/sec 1126609.9776
for step 13229 | loss 3.264247 | norm 0.3262 | time 465.8732 ms | tok/sec 1125387.6664
for step 13230 | loss 3.314066 | norm 0.3194 | time 465.8275 ms | tok/sec 1125498.2570
for step 13231 | loss 3.257100 | norm 0.3206 | time 465.2472 ms | tok/sec 1126902.1105
for step 13232 | loss 3.274149 | norm 0.3082 | time 465.4982 ms | tok/sec 1126294.3441
for step 13233 | loss 3.245421 | norm 0.3164 | time 465.8318 ms | tok/sec 1125487.8882
for step 13234 | loss 3.307786 | norm 0.3482 | time 465.2174 ms | tok/sec 1126974.3010
for step 13235 | loss 3.286278 | norm 0.3111 | time 465.4634 ms | tok/sec 1126378.5727
for step 13236 | loss 3.360570 | norm 0.3204 | time 465.3828 ms | tok/sec 1126573.6159
for step 13237 | loss 3.271823 | norm 0.3795 | time 465.3358 ms | tok/sec 1126687.3261
for step 13238 | loss 3.245676 | norm 0.3413 | time 465.3935 ms | tok/sec 1126547.6447
for step 13239 | loss 3.260056 | norm 0.3200 | time 465.7991 ms | tok/sec 1125566.8111
for step 13240 | loss 3.212740 | norm 0.3127 | time 464.8662 ms | tok/sec 1127825.6918
for step 13241 | loss 3.241395 | norm 0.3299 | time 465.8275 ms | tok/sec 1125498.2570
for step 13242 | loss 3.254879 | norm 0.3096 | time 465.2824 ms | tok/sec 1126816.6488
for step 13243 | loss 3.243969 | norm 0.3197 | time 465.0180 ms | tok/sec 1127457.3494
for step 13244 | loss 3.253852 | norm 0.3071 | time 464.8294 ms | tok/sec 1127914.7777
for step 13245 | loss 3.216633 | norm 0.2860 | time 465.3192 ms | tok/sec 1126727.7362
for step 13246 | loss 3.223930 | norm 0.3282 | time 465.2321 ms | tok/sec 1126938.4934
for step 13247 | loss 3.279953 | norm 0.3069 | time 465.3945 ms | tok/sec 1126545.3362
for step 13248 | loss 3.252486 | norm 0.3456 | time 465.0433 ms | tok/sec 1127396.0787
for step 13249 | loss 3.249414 | norm 0.2852 | time 466.1615 ms | tok/sec 1124691.7899
validation loss 3.2688
HellaSwag accuracy: 2791/10042=0.2779
> Hello, I'm a language model, and the difference between my native language I've created is an assumption.
"I think that if you are a language
> Hello, I'm a language model, a model, and a model, because I'm going along with modeling. This model lets me keep my mind in the
> Hello, I'm a language model, I work at the University of California, San Diego, USA..<|endoftext|>In the past, many scientists had argued the fundamental
> Hello, I'm a language model, and I'm looking at two things at cross-linguistic time - The time point? Time is the most important
> Hello, I'm a language model, and I can't do all my jobs except it's great! This doesn't mean that I need any more work or
> Hello, I'm a language model, so when I'm using a program to create an English grammar, I could do it by asking the input.
If
> Hello, I'm a language model, so I really don't mind saying anything. My code is like an algorithm, and it's a programming language, so
> Hello, I'm a language model, and here's how I got to working on C++.
It's got eight parts so you can use the

> Hello, I'm a language model, and I know that this was a challenge after I first created the model; I really liked it. So, I am
> > Hello, I'm a language model, and we are looking at language in two dimensions: the length of the code and the behavior of the language:
So
Hello, I'm a language model, there are some things to look for you.
Hello on this page, I wish you a great weekend and happy day
> Hello, I'm a language model, what I've always had to do about, and are doing this with language.
I am trying to learn a language> 
Hello, I'm a language model, and I've heard one of the important contributions of J.D. Williams.
I'm a language model of my
> Hello, I'm a language model, so I'll give you a short note about this.
You may start off with something that isn't really 'object> > > 
Hello, I'm a language model, right?
I can. I already got all the steps in the project, I can. Thanks
I understand how
Hello, I'm a language model, like, I'm interested in this.
I can use a search function (probe function) to search for items
> Hello, I'm a language model, so I will be going to tell you. I will be talking about languages and I'll cover vocabulary with you. I
> > > Hello, I'm a language model, and I'm not the type I prefer.
The next paragraph in my last blog I did of it was a great
> Hello, I'm a language model, I'll send you an example of how you can translate your sentences into a different language for more support and practice, because
Hello, I'm a language model, and I like to talk to each other often, sometimes I'm not as good as my English as my own. I
Hello, I'm a language model, I like it because it's a very convenient tool, and I really enjoyed it and I'm happy to have done my
> Hello, I'm a language model, isn't it if?
"Is 'hello', sorry, isn't it
"Is 'hello', this,
> > Hello, I'm a language model, and the best way to talk about it is to go through the steps of the model and you'll find the details in
> Hello, I'm a language model, I can explain how to write my own language models. I've found the steps to using a language model with Java,
Hello, I'm a language model, just want to be specific in a specific way about the context in which I teach (e.g., we are teaching
> Hello, I'm a language model, but...
To understand English, a person needs to understand their culture, language, and culture. These are two of
> > Hello, I'm a language model, and it works quite well. What I found from the beginning is that the syntax we use when modelling the model doesn't
> Hello, I'm a language model, and I like the term "syntactic" because it's simple. But, in certain situations, when we encounter
Hello, I'm a language model, and I was born an English master English teacher and a bilingual teacher (at a private level). I've been teaching since
> Hello, I'm a language model, and you didn't learn a language during that class!" To create the map of the world we can see the names,
> Hello, I'm a language model, and one of the most interesting concepts I have been taught on the course was the need to find the roots of the language
Hello, I'm a language model, where we build and model for a language, then the code is assembled and created (with a template). I've always
for step 13250 | loss 3.233549 | norm 0.3379 | time 12430.8586 ms | tok/sec 42176.3304
for step 13251 | loss 3.223233 | norm 0.3082 | time 462.1415 ms | tok/sec 1134475.0132
for step 13252 | loss 3.168092 | norm 0.3122 | time 463.2175 ms | tok/sec 1131839.8021
for step 13253 | loss 3.182681 | norm 0.2958 | time 464.1025 ms | tok/sec 1129681.4658
for step 13254 | loss 3.173822 | norm 0.3312 | time 463.0992 ms | tok/sec 1132128.8253
for step 13255 | loss 3.226770 | norm 0.2920 | time 462.8236 ms | tok/sec 1132803.0090
for step 13256 | loss 3.204513 | norm 0.2947 | time 463.4845 ms | tok/sec 1131187.7117
for step 13257 | loss 3.198480 | norm 0.3332 | time 463.3701 ms | tok/sec 1131467.0870
for step 13258 | loss 3.163908 | norm 0.3088 | time 464.3126 ms | tok/sec 1129170.4180
for step 13259 | loss 3.206774 | norm 0.3083 | time 463.9471 ms | tok/sec 1130059.9740
for step 13260 | loss 3.252420 | norm 0.3459 | time 463.4647 ms | tok/sec 1131236.0105
for step 13261 | loss 3.202050 | norm 0.2852 | time 463.4743 ms | tok/sec 1131212.7334
for step 13262 | loss 3.226548 | norm 0.3201 | time 464.2756 ms | tok/sec 1129260.2964
for step 13263 | loss 3.256756 | norm 0.3097 | time 464.0372 ms | tok/sec 1129840.5013
for step 13264 | loss 3.216037 | norm 0.2985 | time 464.9122 ms | tok/sec 1127714.0650
for step 13265 | loss 3.306278 | norm 0.3118 | time 464.7205 ms | tok/sec 1128179.2260
for step 13266 | loss 3.259323 | norm 0.3059 | time 464.9253 ms | tok/sec 1127682.2583
for step 13267 | loss 3.233487 | norm 0.3002 | time 464.5257 ms | tok/sec 1128652.3015
for step 13268 | loss 3.237933 | norm 0.3075 | time 463.6140 ms | tok/sec 1130871.8346
for step 13269 | loss 3.226733 | norm 0.3042 | time 465.2615 ms | tok/sec 1126867.4623
for step 13270 | loss 3.309255 | norm 0.2909 | time 465.7171 ms | tok/sec 1125765.0312
for step 13271 | loss 3.286808 | norm 0.3303 | time 465.0519 ms | tok/sec 1127375.2713
for step 13272 | loss 3.344009 | norm 0.3006 | time 464.5159 ms | tok/sec 1128676.0526
for step 13273 | loss 3.261210 | norm 0.3121 | time 465.1079 ms | tok/sec 1127239.4641
for step 13274 | loss 3.318316 | norm 0.3356 | time 464.5050 ms | tok/sec 1128702.7014
for step 13275 | loss 3.303360 | norm 0.3337 | time 465.0605 ms | tok/sec 1127354.4647
for step 13276 | loss 3.273108 | norm 0.3030 | time 464.5538 ms | tok/sec 1128583.9503
for step 13277 | loss 3.251942 | norm 0.2966 | time 464.4158 ms | tok/sec 1128919.4141
for step 13278 | loss 3.360118 | norm 0.3412 | time 465.6918 ms | tok/sec 1125826.1247
for step 13279 | loss 3.237107 | norm 0.3188 | time 465.9793 ms | tok/sec 1125131.4331
for step 13280 | loss 3.213850 | norm 0.3268 | time 465.5395 ms | tok/sec 1126194.5554
for step 13281 | loss 3.241601 | norm 0.3146 | time 465.4977 ms | tok/sec 1126295.4979
for step 13282 | loss 3.307423 | norm 0.3208 | time 465.7099 ms | tok/sec 1125782.3211
for step 13283 | loss 3.291333 | norm 0.3067 | time 465.5709 ms | tok/sec 1126118.4279
for step 13284 | loss 3.260126 | norm 0.3213 | time 465.1320 ms | tok/sec 1127181.1060
for step 13285 | loss 3.192261 | norm 0.2932 | time 466.2974 ms | tok/sec 1124364.0076
for step 13286 | loss 3.261160 | norm 0.2999 | time 465.6215 ms | tok/sec 1125996.1840
for step 13287 | loss 3.226918 | norm 0.2957 | time 465.6229 ms | tok/sec 1125992.7247
for step 13288 | loss 3.146200 | norm 0.2678 | time 466.5678 ms | tok/sec 1123712.4612
for step 13289 | loss 3.194988 | norm 0.2696 | time 465.0638 ms | tok/sec 1127346.3734
for step 13290 | loss 3.206912 | norm 0.3137 | time 465.0309 ms | tok/sec 1127426.1352
for step 13291 | loss 3.262139 | norm 0.2882 | time 464.9041 ms | tok/sec 1127733.7282
for step 13292 | loss 3.219232 | norm 0.2903 | time 464.4883 ms | tok/sec 1128743.2563
for step 13293 | loss 3.227429 | norm 0.2811 | time 465.2748 ms | tok/sec 1126835.1259
for step 13294 | loss 3.238827 | norm 0.2911 | time 465.4474 ms | tok/sec 1126417.2297
for step 13295 | loss 3.255616 | norm 0.2984 | time 465.1520 ms | tok/sec 1127132.5751
for step 13296 | loss 3.237973 | norm 0.3022 | time 464.1883 ms | tok/sec 1129472.5822
for step 13297 | loss 3.263680 | norm 0.3215 | time 464.8187 ms | tok/sec 1127940.8120
for step 13298 | loss 3.204072 | norm 0.3239 | time 464.3717 ms | tok/sec 1129026.6423
for step 13299 | loss 3.273951 | norm 0.2969 | time 465.3316 ms | tok/sec 1126697.7170
for step 13300 | loss 3.238262 | norm 0.3281 | time 465.9057 ms | tok/sec 1125309.3445
for step 13301 | loss 3.293281 | norm 0.3272 | time 465.8446 ms | tok/sec 1125456.7830
for step 13302 | loss 3.264069 | norm 0.3885 | time 465.7445 ms | tok/sec 1125698.7580
for step 13303 | loss 3.316012 | norm 0.3436 | time 465.1761 ms | tok/sec 1127074.2280
for step 13304 | loss 3.273658 | norm 0.3524 | time 465.3676 ms | tok/sec 1126610.5548
for step 13305 | loss 3.243619 | norm 0.3073 | time 465.0106 ms | tok/sec 1127475.2694
for step 13306 | loss 3.267848 | norm 0.2970 | time 464.9181 ms | tok/sec 1127699.6072
for step 13307 | loss 3.290830 | norm 0.3436 | time 464.8168 ms | tok/sec 1127945.4404
for step 13308 | loss 3.305811 | norm 0.3091 | time 465.1225 ms | tok/sec 1127204.2174
for step 13309 | loss 3.260484 | norm 0.3620 | time 464.6852 ms | tok/sec 1128264.8945
for step 13310 | loss 3.274997 | norm 0.3248 | time 465.3580 ms | tok/sec 1126633.6428
for step 13311 | loss 3.222847 | norm 0.3538 | time 464.9751 ms | tok/sec 1127561.4092
for step 13312 | loss 3.277993 | norm 0.3146 | time 465.0276 ms | tok/sec 1127434.2276
for step 13313 | loss 3.249914 | norm 0.3314 | time 465.6017 ms | tok/sec 1126044.0405
for step 13314 | loss 3.257146 | norm 0.2869 | time 465.2183 ms | tok/sec 1126971.9908
for step 13315 | loss 3.265129 | norm 0.3283 | time 465.2135 ms | tok/sec 1126983.5421
for step 13316 | loss 3.352453 | norm 0.3479 | time 465.3776 ms | tok/sec 1126586.3134
for step 13317 | loss 3.262702 | norm 0.3203 | time 464.9906 ms | tok/sec 1127523.8298
for step 13318 | loss 3.260789 | norm 0.3052 | time 465.1878 ms | tok/sec 1127045.9232
for step 13319 | loss 3.228639 | norm 0.3316 | time 466.0213 ms | tok/sec 1125030.1236
for step 13320 | loss 3.221533 | norm 0.3015 | time 465.4784 ms | tok/sec 1126342.2259
for step 13321 | loss 3.298000 | norm 0.3528 | time 464.6358 ms | tok/sec 1128384.7364
for step 13322 | loss 3.221920 | norm 0.3903 | time 464.7181 ms | tok/sec 1128185.0140
for step 13323 | loss 3.188946 | norm 0.3090 | time 464.7663 ms | tok/sec 1128068.1079
for step 13324 | loss 3.193213 | norm 0.3467 | time 465.0826 ms | tok/sec 1127300.7178
for step 13325 | loss 3.160060 | norm 0.3075 | time 465.8182 ms | tok/sec 1125520.7234
for step 13326 | loss 3.226851 | norm 0.3297 | time 464.9179 ms | tok/sec 1127700.1855
for step 13327 | loss 3.208277 | norm 0.3272 | time 464.9985 ms | tok/sec 1127504.7520
for step 13328 | loss 3.204981 | norm 0.3168 | time 465.3163 ms | tok/sec 1126734.6640
for step 13329 | loss 3.199770 | norm 0.2987 | time 465.2424 ms | tok/sec 1126913.6603
for step 13330 | loss 3.171219 | norm 0.3033 | time 466.1677 ms | tok/sec 1124676.8343
for step 13331 | loss 3.231705 | norm 0.3188 | time 468.4308 ms | tok/sec 1119243.3280
for step 13332 | loss 3.225621 | norm 0.3239 | time 465.6863 ms | tok/sec 1125839.3817
for step 13333 | loss 3.324942 | norm 0.3067 | time 465.6723 ms | tok/sec 1125873.3903
Will loading at 0 from edu_fineweb10B/edufineweb_train_000071.npy
for step 13334 | loss 3.231772 | norm 0.3333 | time 2532.2831 ms | tok/sec 207041.6245
for step 13335 | loss 3.273544 | norm 0.3321 | time 474.8669 ms | tok/sec 1104073.6601
for step 13336 | loss 3.238331 | norm 0.3066 | time 464.5052 ms | tok/sec 1128702.1221
for step 13337 | loss 3.301250 | norm 0.3167 | time 465.6181 ms | tok/sec 1126004.2559
for step 13338 | loss 3.238337 | norm 0.2985 | time 464.8297 ms | tok/sec 1127914.1992
for step 13339 | loss 3.212817 | norm 0.3296 | time 465.4837 ms | tok/sec 1126329.5340
for step 13340 | loss 3.271807 | norm 0.3129 | time 465.0884 ms | tok/sec 1127286.8484
for step 13341 | loss 3.239957 | norm 0.3269 | time 463.9695 ms | tok/sec 1130005.3882
for step 13342 | loss 3.278061 | norm 0.2972 | time 465.1313 ms | tok/sec 1127182.8393
for step 13343 | loss 3.286251 | norm 0.3249 | time 464.7644 ms | tok/sec 1128072.7374
for step 13344 | loss 3.272553 | norm 0.3070 | time 464.7191 ms | tok/sec 1128182.6988
for step 13345 | loss 3.259056 | norm 0.3257 | time 465.0738 ms | tok/sec 1127322.1003
for step 13346 | loss 3.255562 | norm 0.3033 | time 464.9136 ms | tok/sec 1127710.5951
for step 13347 | loss 3.252675 | norm 0.3203 | time 465.0669 ms | tok/sec 1127338.8602
for step 13348 | loss 3.245936 | norm 0.3189 | time 464.9575 ms | tok/sec 1127604.1949
for step 13349 | loss 3.255706 | norm 0.3181 | time 465.2765 ms | tok/sec 1126831.0840
for step 13350 | loss 3.271768 | norm 0.3169 | time 465.0517 ms | tok/sec 1127375.8493
for step 13351 | loss 3.268154 | norm 0.3360 | time 465.4343 ms | tok/sec 1126448.9651
for step 13352 | loss 3.243088 | norm 0.2860 | time 465.5304 ms | tok/sec 1126216.4728
for step 13353 | loss 3.219061 | norm 0.3234 | time 465.7338 ms | tok/sec 1125724.6900
for step 13354 | loss 3.234098 | norm 0.3349 | time 465.7722 ms | tok/sec 1125631.9164
for step 13355 | loss 3.309183 | norm 0.3244 | time 465.2908 ms | tok/sec 1126796.4402
for step 13356 | loss 3.257804 | norm 0.3363 | time 465.2045 ms | tok/sec 1127005.4902
for step 13357 | loss 3.238582 | norm 0.2865 | time 465.5476 ms | tok/sec 1126174.9458
for step 13358 | loss 3.281156 | norm 0.3680 | time 466.0375 ms | tok/sec 1124990.9861
for step 13359 | loss 3.215851 | norm 0.3258 | time 465.1606 ms | tok/sec 1127111.7774
for step 13360 | loss 3.255035 | norm 0.2881 | time 465.0581 ms | tok/sec 1127360.2442
for step 13361 | loss 3.238620 | norm 0.2894 | time 464.7858 ms | tok/sec 1128020.6579
for step 13362 | loss 3.232682 | norm 0.3161 | time 465.1999 ms | tok/sec 1127016.4646
for step 13363 | loss 3.225868 | norm 0.3139 | time 465.7643 ms | tok/sec 1125650.9309
for step 13364 | loss 3.253781 | norm 0.3442 | time 464.9980 ms | tok/sec 1127505.9082
for step 13365 | loss 3.233364 | norm 0.3481 | time 465.3280 ms | tok/sec 1126706.3762
for step 13366 | loss 3.275783 | norm 0.3160 | time 464.7558 ms | tok/sec 1128093.5705
for step 13367 | loss 3.226150 | norm 0.3041 | time 465.0438 ms | tok/sec 1127394.9227
for step 13368 | loss 3.263137 | norm 0.3373 | time 465.7261 ms | tok/sec 1125743.1314
for step 13369 | loss 3.277638 | norm 0.3173 | time 464.7009 ms | tok/sec 1128226.6894
for step 13370 | loss 3.224205 | norm 0.3401 | time 465.5275 ms | tok/sec 1126223.3942
for step 13371 | loss 3.290542 | norm 0.3248 | time 464.8063 ms | tok/sec 1127970.8975
for step 13372 | loss 3.271863 | norm 0.3228 | time 465.2715 ms | tok/sec 1126843.2098
for step 13373 | loss 3.271015 | norm 0.2954 | time 465.2717 ms | tok/sec 1126842.6324
for step 13374 | loss 3.277157 | norm 0.3338 | time 464.1190 ms | tok/sec 1129641.4238
for step 13375 | loss 3.250694 | norm 0.2980 | time 464.8492 ms | tok/sec 1127866.7621
for step 13376 | loss 3.267423 | norm 0.3331 | time 464.1743 ms | tok/sec 1129506.8106
for step 13377 | loss 3.271977 | norm 0.3088 | time 465.9359 ms | tok/sec 1125236.2155
for step 13378 | loss 3.246503 | norm 0.3265 | time 464.7846 ms | tok/sec 1128023.5511
for step 13379 | loss 3.243055 | norm 0.3080 | time 465.6482 ms | tok/sec 1125931.6131
for step 13380 | loss 3.228026 | norm 0.3027 | time 465.0989 ms | tok/sec 1127261.4222
for step 13381 | loss 3.287515 | norm 0.3269 | time 465.0509 ms | tok/sec 1127377.5832
for step 13382 | loss 3.216725 | norm 0.3218 | time 465.0040 ms | tok/sec 1127491.4557
for step 13383 | loss 3.254278 | norm 0.3002 | time 465.0776 ms | tok/sec 1127312.8537
for step 13384 | loss 3.225098 | norm 0.3100 | time 464.9010 ms | tok/sec 1127741.2467
for step 13385 | loss 3.239439 | norm 0.3091 | time 465.8771 ms | tok/sec 1125378.4515
for step 13386 | loss 3.207568 | norm 0.3357 | time 464.4542 ms | tok/sec 1128826.1131
for step 13387 | loss 3.236175 | norm 0.3046 | time 464.5517 ms | tok/sec 1128589.1633
for step 13388 | loss 3.224746 | norm 0.3537 | time 465.0490 ms | tok/sec 1127382.2070
for step 13389 | loss 3.104260 | norm 0.3287 | time 464.8707 ms | tok/sec 1127814.7016
for step 13390 | loss 3.236177 | norm 0.3210 | time 466.0113 ms | tok/sec 1125054.2980
for step 13391 | loss 3.189296 | norm 0.2929 | time 465.5855 ms | tok/sec 1126083.2512
for step 13392 | loss 3.206174 | norm 0.3201 | time 464.1340 ms | tok/sec 1129604.8662
for step 13393 | loss 3.225027 | norm 0.2921 | time 464.8488 ms | tok/sec 1127867.9191
for step 13394 | loss 3.243773 | norm 0.3320 | time 465.3332 ms | tok/sec 1126693.6761
for step 13395 | loss 3.174744 | norm 0.2774 | time 465.4915 ms | tok/sec 1126310.4966
for step 13396 | loss 3.253461 | norm 0.3141 | time 464.7524 ms | tok/sec 1128101.6725
for step 13397 | loss 3.191489 | norm 0.3062 | time 466.4505 ms | tok/sec 1123995.0500
for step 13398 | loss 3.257583 | norm 0.2880 | time 464.9730 ms | tok/sec 1127566.6127
for step 13399 | loss 3.186213 | norm 0.2884 | time 464.6981 ms | tok/sec 1128233.6356
for step 13400 | loss 3.208140 | norm 0.3198 | time 465.0013 ms | tok/sec 1127497.8148
for step 13401 | loss 3.180758 | norm 0.3021 | time 465.4317 ms | tok/sec 1126455.3124
for step 13402 | loss 3.238012 | norm 0.3385 | time 466.7323 ms | tok/sec 1123316.3870
for step 13403 | loss 3.239414 | norm 0.2941 | time 466.1717 ms | tok/sec 1124667.0558
for step 13404 | loss 3.274493 | norm 0.3577 | time 465.1489 ms | tok/sec 1127140.0856
for step 13405 | loss 3.201648 | norm 0.3099 | time 465.2233 ms | tok/sec 1126959.8622
for step 13406 | loss 3.291140 | norm 0.3071 | time 465.2112 ms | tok/sec 1126989.3178
for step 13407 | loss 3.337193 | norm 0.3025 | time 464.8514 ms | tok/sec 1127861.5558
for step 13408 | loss 3.299502 | norm 0.3360 | time 465.5454 ms | tok/sec 1126180.1365
for step 13409 | loss 3.220191 | norm 0.3087 | time 464.2000 ms | tok/sec 1129444.1568
for step 13410 | loss 3.282386 | norm 0.3339 | time 465.3234 ms | tok/sec 1126717.3448
for step 13411 | loss 3.301689 | norm 0.3252 | time 465.2240 ms | tok/sec 1126958.1295
for step 13412 | loss 3.322089 | norm 0.3279 | time 465.4000 ms | tok/sec 1126532.0626
for step 13413 | loss 3.265470 | norm 0.3453 | time 465.4224 ms | tok/sec 1126477.8170
for step 13414 | loss 3.255929 | norm 0.3334 | time 465.6301 ms | tok/sec 1125975.4283
for step 13415 | loss 3.229821 | norm 0.3536 | time 465.7540 ms | tok/sec 1125675.7083
for step 13416 | loss 3.237273 | norm 0.3514 | time 467.4568 ms | tok/sec 1121575.2562
for step 13417 | loss 3.274028 | norm 0.3890 | time 464.9408 ms | tok/sec 1127644.6708
for step 13418 | loss 3.365495 | norm 0.3176 | time 465.8587 ms | tok/sec 1125422.7996
for step 13419 | loss 3.239238 | norm 0.3305 | time 466.0211 ms | tok/sec 1125030.6991
for step 13420 | loss 3.255931 | norm 0.3526 | time 466.1186 ms | tok/sec 1124795.3398
for step 13421 | loss 3.244555 | norm 0.3041 | time 465.5836 ms | tok/sec 1126087.8644
for step 13422 | loss 3.249422 | norm 0.4125 | time 464.9174 ms | tok/sec 1127701.3421
for step 13423 | loss 3.267014 | norm 0.3226 | time 466.1112 ms | tok/sec 1124813.1753
for step 13424 | loss 3.260029 | norm 0.3928 | time 464.8993 ms | tok/sec 1127745.2951
for step 13425 | loss 3.289866 | norm 0.3213 | time 465.4279 ms | tok/sec 1126464.5449
for step 13426 | loss 3.188884 | norm 0.3641 | time 464.7150 ms | tok/sec 1128192.5385
for step 13427 | loss 3.232125 | norm 0.3133 | time 465.8122 ms | tok/sec 1125535.1254
for step 13428 | loss 3.222074 | norm 0.3251 | time 465.5092 ms | tok/sec 1126267.8090
for step 13429 | loss 3.154761 | norm 0.3434 | time 466.0485 ms | tok/sec 1124964.5124
for step 13430 | loss 3.239351 | norm 0.3099 | time 464.5734 ms | tok/sec 1128536.4569
for step 13431 | loss 3.210836 | norm 0.3376 | time 465.9259 ms | tok/sec 1125260.3989
for step 13432 | loss 3.233849 | norm 0.3161 | time 464.2785 ms | tok/sec 1129253.3376
for step 13433 | loss 3.191387 | norm 0.3416 | time 465.2636 ms | tok/sec 1126862.2652
for step 13434 | loss 3.205461 | norm 0.3249 | time 465.4717 ms | tok/sec 1126358.3798
for step 13435 | loss 3.206836 | norm 0.3255 | time 465.2076 ms | tok/sec 1126997.9815
for step 13436 | loss 3.221763 | norm 0.3194 | time 465.9593 ms | tok/sec 1125179.7918
for step 13437 | loss 3.250869 | norm 0.3124 | time 465.1504 ms | tok/sec 1127136.6192
for step 13438 | loss 3.197716 | norm 0.3443 | time 464.8540 ms | tok/sec 1127855.1927
for step 13439 | loss 3.292364 | norm 0.3227 | time 465.6897 ms | tok/sec 1125831.3122
for step 13440 | loss 3.264884 | norm 0.3119 | time 465.6093 ms | tok/sec 1126025.5893
for step 13441 | loss 3.346399 | norm 0.3148 | time 465.3573 ms | tok/sec 1126635.3744
for step 13442 | loss 3.338013 | norm 0.3438 | time 465.0989 ms | tok/sec 1127261.4222
for step 13443 | loss 3.312936 | norm 0.3163 | time 464.9253 ms | tok/sec 1127682.2583
for step 13444 | loss 3.319720 | norm 0.3285 | time 465.8186 ms | tok/sec 1125519.5713
for step 13445 | loss 3.303230 | norm 0.3273 | time 464.9463 ms | tok/sec 1127631.3713
for step 13446 | loss 3.309994 | norm 0.3356 | time 465.4970 ms | tok/sec 1126297.2285
for step 13447 | loss 3.294864 | norm 0.3402 | time 466.2411 ms | tok/sec 1124499.6978
for step 13448 | loss 3.310264 | norm 0.3213 | time 466.1863 ms | tok/sec 1124631.9698
for step 13449 | loss 3.201710 | norm 0.3469 | time 466.0113 ms | tok/sec 1125054.2980
for step 13450 | loss 3.264508 | norm 0.3685 | time 466.1541 ms | tok/sec 1124709.6222
for step 13451 | loss 3.270837 | norm 0.3481 | time 464.6649 ms | tok/sec 1128314.1019
for step 13452 | loss 3.260185 | norm 0.3525 | time 464.9434 ms | tok/sec 1127638.3101
for step 13453 | loss 3.323631 | norm 0.3275 | time 465.2374 ms | tok/sec 1126925.7880
for step 13454 | loss 3.332549 | norm 0.3769 | time 465.8911 ms | tok/sec 1125344.4728
for step 13455 | loss 3.205504 | norm 0.3478 | time 464.7980 ms | tok/sec 1127991.1483
for step 13456 | loss 3.264296 | norm 0.3224 | time 465.6730 ms | tok/sec 1125871.6610
for step 13457 | loss 3.271607 | norm 0.3314 | time 465.1921 ms | tok/sec 1127035.5259
for step 13458 | loss 3.259321 | norm 0.3477 | time 465.5581 ms | tok/sec 1126149.5697
for step 13459 | loss 3.222922 | norm 0.3070 | time 465.3070 ms | tok/sec 1126757.1798
for step 13460 | loss 3.271089 | norm 0.3454 | time 465.4744 ms | tok/sec 1126352.0336
for step 13461 | loss 3.241477 | norm 0.3125 | time 465.4884 ms | tok/sec 1126317.9961
for step 13462 | loss 3.271430 | norm 0.3248 | time 465.7350 ms | tok/sec 1125721.8086
for step 13463 | loss 3.180560 | norm 0.3307 | time 465.7114 ms | tok/sec 1125778.8631
for step 13464 | loss 3.235076 | norm 0.3189 | time 465.3699 ms | tok/sec 1126604.7829
for step 13465 | loss 3.202830 | norm 0.3156 | time 465.3234 ms | tok/sec 1126717.3448
for step 13466 | loss 3.219192 | norm 0.2956 | time 466.5718 ms | tok/sec 1123702.6995
for step 13467 | loss 3.268322 | norm 0.3305 | time 465.1427 ms | tok/sec 1127155.1068
for step 13468 | loss 3.183113 | norm 0.3267 | time 465.8232 ms | tok/sec 1125508.6260
for step 13469 | loss 3.258948 | norm 0.3079 | time 466.1028 ms | tok/sec 1124833.3129
for step 13470 | loss 3.184041 | norm 0.3248 | time 464.7923 ms | tok/sec 1128005.0350
for step 13471 | loss 3.186941 | norm 0.3036 | time 465.3897 ms | tok/sec 1126556.8788
for step 13472 | loss 3.172107 | norm 0.2978 | time 466.3248 ms | tok/sec 1124297.8993
for step 13473 | loss 3.191870 | norm 0.3232 | time 465.5192 ms | tok/sec 1126243.5823
for step 13474 | loss 3.213307 | norm 0.3060 | time 465.7252 ms | tok/sec 1125745.4366
for step 13475 | loss 3.245154 | norm 0.3986 | time 465.7204 ms | tok/sec 1125756.9627
for step 13476 | loss 3.198522 | norm 0.4084 | time 465.6799 ms | tok/sec 1125854.9447
for step 13477 | loss 3.252308 | norm 0.3426 | time 465.3692 ms | tok/sec 1126606.5144
for step 13478 | loss 3.230447 | norm 0.3185 | time 465.7173 ms | tok/sec 1125764.4549
for step 13479 | loss 3.233418 | norm 0.3038 | time 464.7343 ms | tok/sec 1128145.6568
for step 13480 | loss 3.271028 | norm 0.3521 | time 464.6280 ms | tok/sec 1128403.8440
for step 13481 | loss 3.333731 | norm 0.3399 | time 464.8502 ms | tok/sec 1127864.4482
for step 13482 | loss 3.229591 | norm 0.3238 | time 464.8414 ms | tok/sec 1127885.8522
for step 13483 | loss 3.269688 | norm 0.3385 | time 465.8155 ms | tok/sec 1125527.0602
for step 13484 | loss 3.258408 | norm 0.3180 | time 464.9673 ms | tok/sec 1127580.4889
for step 13485 | loss 3.257263 | norm 0.3699 | time 464.9615 ms | tok/sec 1127594.3654
for step 13486 | loss 3.255605 | norm 0.3295 | time 464.7045 ms | tok/sec 1128218.0067
for step 13487 | loss 3.279139 | norm 0.3783 | time 465.0471 ms | tok/sec 1127386.8309
for step 13488 | loss 3.306059 | norm 0.3437 | time 464.9220 ms | tok/sec 1127690.3544
for step 13489 | loss 3.256413 | norm 0.3660 | time 466.4476 ms | tok/sec 1124001.9441
for step 13490 | loss 3.287963 | norm 0.3493 | time 464.8855 ms | tok/sec 1127778.8405
for step 13491 | loss 3.258785 | norm 0.3221 | time 465.4651 ms | tok/sec 1126374.5340
for step 13492 | loss 3.253290 | norm 0.3506 | time 465.2910 ms | tok/sec 1126795.8628
for step 13493 | loss 3.202620 | norm 0.3106 | time 465.7140 ms | tok/sec 1125772.5234
for step 13494 | loss 3.248894 | norm 0.3321 | time 464.8676 ms | tok/sec 1127822.2212
for step 13495 | loss 3.292682 | norm 0.3252 | time 465.6386 ms | tok/sec 1125954.6733
for step 13496 | loss 3.230694 | norm 0.3321 | time 465.6501 ms | tok/sec 1125927.0011
for step 13497 | loss 3.189881 | norm 0.2943 | time 465.2350 ms | tok/sec 1126931.5631
for step 13498 | loss 3.227541 | norm 0.3108 | time 464.9801 ms | tok/sec 1127549.2679
for step 13499 | loss 3.225367 | norm 0.3149 | time 465.7326 ms | tok/sec 1125727.5715
validation loss 3.2638
HellaSwag accuracy: 2781/10042=0.2769
> Hello, I'm a language model, and you're not the only one from whom I have read in this book.
It's hard to know what you
> Hello, I'm a language model, which means I understand the language to you faster. The program is running on top of many new web pages, and I
> Hello, I'm a language model, so first you know what I'm talking about.
- This post will focus on the "trend" that language
> Hello, I'm a language model, and I'm interested to you. This document is very technical. When I went to read this document, I felt like
> Hello, I'm a language model, and I'll be using them as much as I can. I mean, the language is as much about a language in
> Hello, I'm a language model, so i'm not trying to write more about what exactly that is. :)
- What is the use of the word
> Hello, I'm a language model, I'm thinking about using languages to create a visual model of an image. I'm not a language model, but like
> Hello, I'm a language model, and like I've never thought of myself, don't you?
So what am I missing from this post? Because
> Hello, I'm a language model, and I love to read a lot of songs. My favorite is "Whoa!", which is for my students.

> Hello, I'm a language model, has 1-3 letters. That does not mean that 'I' is a set, and that doesn't mean we
> Hello, I'm a language model, and I've read countless articles and tutorials, and people ask me how we can help. I'm trying to keep things
> > Hello, I'm a language model, I'm the one that will be able to work in a language. What can I achieve over the course of my life
Hello, I'm a language model, well-designed and highly useful, if you know how to do that.
- You can read and write on computer
> Hello, I'm a language model, here we are about 2 languages of India. There is Hindi, a subgroup of British English, with the �
> Hello, I'm a language model, but with a strong foundation. I can understand myself better, as well as I learn how to read texts. When you
> > Hello, I'm a language model, and you love language. So I said 'what’s a lexical and what’s a lexical
> Hello, I'm a language model, and this post doesn't sound very appealing to you. It's a fairly easy read, but it's the one you
> Hello, I'm a language model, and I'm a real life model. When you go to the website now, the interface and the navigation is in two
> Hello, I'm a language model, that makes it possible to read different parts of a complex system and build it with its own variables. I've never experiencedHello, I'm a language model, like I speak and write, and am completely fluent. I have an internal grammar, and I can learn a lot without
> 
Hello, I'm a language model, and that is why I teach languages.
Let's take a closer look at the word "dictionary" of "
> > > Hello, I'm a language model, I'm an object-oriented programmer at Fortran, and I have one. The problem with these systems is that we
Hello, I'm a language model, not a program. In the real world my first time learning a language. I know it is my first choice. I
Hello, I'm a language model, and it works so well. How do you say 'hello'?
- It's a great opportunity for you that,
> > > > Hello, I'm a language model, so we write a model of a language by describing and representing all the different structures we want an end-to-end
Hello, I'm a language model, so I'll be trying it on my phone...
I'll show you two models, one of which is
-
Hello, I'm a language model, so let me explain.
Let's explain what happens in the code, and why it's not relevant for people in
Hello, I'm a language model, and some of these problems are familiar to you, like a certain amount of 'slack' in someone's language,
> > Hello, I'm a language model, and I want you to be very careful when it comes to the programming language you're writing."
With more than 70
Hello, I'm a language model, doesn't really understand you very well. So yes, it uses only a dictionary for your words. What happens is that
> Hello, I'm a language model, and it says "my" here."
What's the difference between a real and a real language?
The Real
> Hello, I'm a language model, isn't it ?
Let's say I am going to do something like this on your own without being asked to do
for step 13500 | loss 3.255369 | norm 0.3477 | time 12486.6381 ms | tok/sec 41987.9232
for step 13501 | loss 3.162414 | norm 0.3218 | time 462.3835 ms | tok/sec 1133881.2699
for step 13502 | loss 3.197341 | norm 0.3134 | time 464.2467 ms | tok/sec 1129330.4695
for step 13503 | loss 3.249020 | norm 0.3155 | time 462.7693 ms | tok/sec 1132936.0743
for step 13504 | loss 3.222630 | norm 0.3304 | time 463.6781 ms | tok/sec 1130715.4157
for step 13505 | loss 3.224041 | norm 0.3393 | time 462.6002 ms | tok/sec 1133350.0605
for step 13506 | loss 3.179919 | norm 0.3042 | time 464.4041 ms | tok/sec 1128947.8131
for step 13507 | loss 3.247302 | norm 0.2966 | time 464.2291 ms | tok/sec 1129373.3896
for step 13508 | loss 3.301297 | norm 0.3149 | time 464.0110 ms | tok/sec 1129904.3602
for step 13509 | loss 3.192132 | norm 0.3105 | time 464.1733 ms | tok/sec 1129509.1313
for step 13510 | loss 3.239966 | norm 0.2880 | time 463.5510 ms | tok/sec 1131025.3882
for step 13511 | loss 3.254447 | norm 0.3053 | time 464.0374 ms | tok/sec 1129839.9208
for step 13512 | loss 3.243278 | norm 0.3099 | time 464.3154 ms | tok/sec 1129163.4602
for step 13513 | loss 3.251832 | norm 0.3125 | time 464.8619 ms | tok/sec 1127836.1037
for step 13514 | loss 3.210064 | norm 0.2942 | time 465.1790 ms | tok/sec 1127067.2961
for step 13515 | loss 3.404748 | norm 0.3636 | time 465.1787 ms | tok/sec 1127067.8738
for step 13516 | loss 3.273435 | norm 0.2959 | time 464.6020 ms | tok/sec 1128466.9615
for step 13517 | loss 3.194966 | norm 0.3194 | time 465.0121 ms | tok/sec 1127471.8010
for step 13518 | loss 3.220067 | norm 0.3379 | time 465.4465 ms | tok/sec 1126419.5377
for step 13519 | loss 3.274733 | norm 0.3081 | time 465.0648 ms | tok/sec 1127344.0616
for step 13520 | loss 3.243834 | norm 0.3418 | time 465.1699 ms | tok/sec 1127089.2475
for step 13521 | loss 3.229415 | norm 0.3225 | time 465.8127 ms | tok/sec 1125533.9732
for step 13522 | loss 3.242920 | norm 0.3152 | time 465.2131 ms | tok/sec 1126984.6972
for step 13523 | loss 3.280312 | norm 0.3279 | time 465.5745 ms | tok/sec 1126109.7777
for step 13524 | loss 3.245086 | norm 0.3057 | time 465.2717 ms | tok/sec 1126842.6324
Will loading at 0 from edu_fineweb10B/edufineweb_train_000072.npy
for step 13525 | loss 3.297285 | norm 0.3140 | time 2680.0299 ms | tok/sec 195627.6704
for step 13526 | loss 3.279360 | norm 0.2914 | time 464.4558 ms | tok/sec 1128822.0569
for step 13527 | loss 3.265757 | norm 0.3283 | time 464.5700 ms | tok/sec 1128544.5653
for step 13528 | loss 3.243889 | norm 0.2969 | time 464.2441 ms | tok/sec 1129336.8493
for step 13529 | loss 3.279616 | norm 0.3138 | time 465.1608 ms | tok/sec 1127111.1997
for step 13530 | loss 3.274082 | norm 0.3010 | time 465.4312 ms | tok/sec 1126456.4665
for step 13531 | loss 3.213152 | norm 0.2966 | time 465.2984 ms | tok/sec 1126777.9643
for step 13532 | loss 3.187752 | norm 0.2832 | time 464.9100 ms | tok/sec 1127719.2699
for step 13533 | loss 3.246653 | norm 0.3263 | time 465.4953 ms | tok/sec 1126301.2666
for step 13534 | loss 3.180461 | norm 0.2696 | time 464.8614 ms | tok/sec 1127837.2606
for step 13535 | loss 3.246435 | norm 0.3095 | time 464.4890 ms | tok/sec 1128741.5181
for step 13536 | loss 3.187065 | norm 0.3006 | time 464.3211 ms | tok/sec 1129149.5450
for step 13537 | loss 3.213137 | norm 0.2789 | time 465.0755 ms | tok/sec 1127318.0549
for step 13538 | loss 3.202096 | norm 0.3307 | time 465.3199 ms | tok/sec 1126726.0043
for step 13539 | loss 3.220203 | norm 0.2878 | time 465.3535 ms | tok/sec 1126644.6099
for step 13540 | loss 3.306475 | norm 0.3028 | time 465.5349 ms | tok/sec 1126205.5140
for step 13541 | loss 3.227545 | norm 0.3023 | time 466.2707 ms | tok/sec 1124428.3989
for step 13542 | loss 3.219027 | norm 0.2949 | time 464.5596 ms | tok/sec 1128570.0494
for step 13543 | loss 3.301505 | norm 0.3841 | time 465.4689 ms | tok/sec 1126365.3030
for step 13544 | loss 3.181146 | norm 0.3755 | time 465.0156 ms | tok/sec 1127463.1300
for step 13545 | loss 3.275457 | norm 0.3661 | time 466.7497 ms | tok/sec 1123274.4998
for step 13546 | loss 3.259545 | norm 0.3750 | time 465.9791 ms | tok/sec 1125132.0087
for step 13547 | loss 3.216421 | norm 0.3590 | time 464.8197 ms | tok/sec 1127938.4978
for step 13548 | loss 3.244132 | norm 0.3186 | time 465.9469 ms | tok/sec 1125209.7302
for step 13549 | loss 3.264842 | norm 0.3634 | time 465.9431 ms | tok/sec 1125218.9423
for step 13550 | loss 3.257524 | norm 0.3150 | time 465.8837 ms | tok/sec 1125362.3258
for step 13551 | loss 3.239596 | norm 0.3092 | time 465.8408 ms | tok/sec 1125465.9992
for step 13552 | loss 3.252624 | norm 0.3489 | time 465.9121 ms | tok/sec 1125293.7966
for step 13553 | loss 3.286071 | norm 0.3442 | time 465.6978 ms | tok/sec 1125811.7153
for step 13554 | loss 3.286408 | norm 0.3268 | time 465.4157 ms | tok/sec 1126493.9747
for step 13555 | loss 3.263929 | norm 0.3332 | time 465.5147 ms | tok/sec 1126254.5419
for step 13556 | loss 3.230709 | norm 0.3381 | time 465.8918 ms | tok/sec 1125342.7451
for step 13557 | loss 3.260184 | norm 0.3354 | time 466.0707 ms | tok/sec 1124910.9931
for step 13558 | loss 3.244197 | norm 0.3247 | time 465.9188 ms | tok/sec 1125277.6733
for step 13559 | loss 3.266689 | norm 0.3037 | time 464.7501 ms | tok/sec 1128107.4597
for step 13560 | loss 3.258572 | norm 0.3220 | time 465.5657 ms | tok/sec 1126131.1151
for step 13561 | loss 3.261577 | norm 0.3038 | time 465.3103 ms | tok/sec 1126749.0971
for step 13562 | loss 3.226088 | norm 0.3142 | time 464.7102 ms | tok/sec 1128204.1148
for step 13563 | loss 3.279474 | norm 0.3283 | time 465.4586 ms | tok/sec 1126390.1118
for step 13564 | loss 3.262063 | norm 0.3201 | time 465.6897 ms | tok/sec 1125831.3122
for step 13565 | loss 3.255101 | norm 0.3280 | time 465.3769 ms | tok/sec 1126588.0448
for step 13566 | loss 3.221750 | norm 0.3186 | time 465.8225 ms | tok/sec 1125510.3542
for step 13567 | loss 3.212005 | norm 0.3210 | time 465.0838 ms | tok/sec 1127297.8283
for step 13568 | loss 3.171373 | norm 0.3064 | time 465.4500 ms | tok/sec 1126410.8829
for step 13569 | loss 3.169284 | norm 0.3575 | time 465.2419 ms | tok/sec 1126914.8153
for step 13570 | loss 3.178469 | norm 0.3338 | time 464.7999 ms | tok/sec 1127986.5195
for step 13571 | loss 3.222565 | norm 0.2979 | time 465.4799 ms | tok/sec 1126338.7645
for step 13572 | loss 3.182707 | norm 0.3306 | time 465.6200 ms | tok/sec 1125999.6434
for step 13573 | loss 3.277033 | norm 0.2865 | time 466.6080 ms | tok/sec 1123615.4259
for step 13574 | loss 3.230084 | norm 0.3174 | time 465.1170 ms | tok/sec 1127217.5069
for step 13575 | loss 3.210507 | norm 0.2958 | time 465.6587 ms | tok/sec 1125906.2479
for step 13576 | loss 3.304561 | norm 0.4315 | time 465.5297 ms | tok/sec 1126218.2031
for step 13577 | loss 3.176514 | norm 0.3651 | time 465.9483 ms | tok/sec 1125206.2757
for step 13578 | loss 3.283149 | norm 0.3871 | time 466.2035 ms | tok/sec 1124590.5595
for step 13579 | loss 3.287965 | norm 0.3470 | time 465.5194 ms | tok/sec 1126243.0055
for step 13580 | loss 3.265660 | norm 0.3985 | time 465.1308 ms | tok/sec 1127183.9948
for step 13581 | loss 3.216369 | norm 0.3576 | time 466.0418 ms | tok/sec 1124980.6267
for step 13582 | loss 3.281554 | norm 0.3740 | time 464.8404 ms | tok/sec 1127888.1661
for step 13583 | loss 3.247406 | norm 0.3641 | time 464.9158 ms | tok/sec 1127705.3902
for step 13584 | loss 3.325308 | norm 0.3259 | time 465.3940 ms | tok/sec 1126546.4905
for step 13585 | loss 3.282126 | norm 0.3504 | time 464.6599 ms | tok/sec 1128326.2596
for step 13586 | loss 3.257801 | norm 0.3501 | time 465.7409 ms | tok/sec 1125707.4019
for step 13587 | loss 3.211737 | norm 0.3097 | time 465.3711 ms | tok/sec 1126601.8970
for step 13588 | loss 3.194153 | norm 0.3372 | time 466.3064 ms | tok/sec 1124342.1623
for step 13589 | loss 3.241747 | norm 0.3021 | time 465.2381 ms | tok/sec 1126924.0554
for step 13590 | loss 3.253119 | norm 0.3264 | time 465.7269 ms | tok/sec 1125741.4025
for step 13591 | loss 3.211476 | norm 0.3037 | time 465.9591 ms | tok/sec 1125180.3675
for step 13592 | loss 3.251100 | norm 0.3447 | time 465.9510 ms | tok/sec 1125199.9425
for step 13593 | loss 3.328912 | norm 0.3354 | time 464.8316 ms | tok/sec 1127909.5710
for step 13594 | loss 3.234826 | norm 0.3244 | time 465.5082 ms | tok/sec 1126270.1163
for step 13595 | loss 3.271941 | norm 0.3364 | time 465.9886 ms | tok/sec 1125108.9822
for step 13596 | loss 3.277849 | norm 0.3340 | time 465.7340 ms | tok/sec 1125724.1138
for step 13597 | loss 3.316927 | norm 0.3347 | time 465.8320 ms | tok/sec 1125487.3122
for step 13598 | loss 3.240969 | norm 0.3349 | time 465.3003 ms | tok/sec 1126773.3455
for step 13599 | loss 3.282677 | norm 0.3470 | time 465.6513 ms | tok/sec 1125924.1187
for step 13600 | loss 3.228539 | norm 0.3162 | time 465.7233 ms | tok/sec 1125750.0470
for step 13601 | loss 3.258318 | norm 0.3064 | time 465.6692 ms | tok/sec 1125880.8840
for step 13602 | loss 3.201976 | norm 0.3164 | time 465.8592 ms | tok/sec 1125421.6477
for step 13603 | loss 3.188864 | norm 0.3078 | time 465.6103 ms | tok/sec 1126023.2830
for step 13604 | loss 3.187664 | norm 0.3003 | time 468.1494 ms | tok/sec 1119915.9362
for step 13605 | loss 3.300185 | norm 0.3667 | time 465.0199 ms | tok/sec 1127452.7249
for step 13606 | loss 3.242611 | norm 0.4025 | time 463.9013 ms | tok/sec 1130171.4849
for step 13607 | loss 3.244433 | norm 0.3145 | time 465.6255 ms | tok/sec 1125986.3826
for step 13608 | loss 3.197883 | norm 0.3285 | time 466.0790 ms | tok/sec 1124890.8528
for step 13609 | loss 3.182726 | norm 0.3379 | time 465.1237 ms | tok/sec 1127201.3284
for step 13610 | loss 3.258245 | norm 0.3091 | time 464.9453 ms | tok/sec 1127633.6842
for step 13611 | loss 3.245207 | norm 0.3664 | time 465.0669 ms | tok/sec 1127338.8602
for step 13612 | loss 3.157656 | norm 0.2989 | time 466.0664 ms | tok/sec 1124921.3513
for step 13613 | loss 3.214631 | norm 0.3209 | time 465.2426 ms | tok/sec 1126913.0828
for step 13614 | loss 3.266813 | norm 0.3339 | time 464.7377 ms | tok/sec 1128137.5541
for step 13615 | loss 3.206514 | norm 0.3158 | time 464.7734 ms | tok/sec 1128050.7477
for step 13616 | loss 3.229058 | norm 0.3105 | time 464.5455 ms | tok/sec 1128604.2231
for step 13617 | loss 3.331731 | norm 0.3256 | time 465.6742 ms | tok/sec 1125868.7788
for step 13618 | loss 3.209522 | norm 0.3198 | time 464.8604 ms | tok/sec 1127839.5744
for step 13619 | loss 3.310304 | norm 0.3031 | time 464.7698 ms | tok/sec 1128059.4277
for step 13620 | loss 3.280728 | norm 0.3507 | time 466.7196 ms | tok/sec 1123346.8002
for step 13621 | loss 3.204746 | norm 0.2982 | time 465.7133 ms | tok/sec 1125774.2524
for step 13622 | loss 3.233454 | norm 0.3197 | time 465.3146 ms | tok/sec 1126738.7052
for step 13623 | loss 3.254087 | norm 0.3012 | time 464.7737 ms | tok/sec 1128050.1690
for step 13624 | loss 3.259327 | norm 0.3462 | time 465.3988 ms | tok/sec 1126534.9481
for step 13625 | loss 3.288274 | norm 0.3031 | time 466.5358 ms | tok/sec 1123789.4123
for step 13626 | loss 3.213224 | norm 0.3520 | time 465.1501 ms | tok/sec 1127137.1969
for step 13627 | loss 3.381319 | norm 0.4614 | time 465.1282 ms | tok/sec 1127190.3504
for step 13628 | loss 3.274251 | norm 0.3419 | time 466.0244 ms | tok/sec 1125022.6412
for step 13629 | loss 3.274073 | norm 0.3283 | time 465.9698 ms | tok/sec 1125154.4605
for step 13630 | loss 3.227810 | norm 0.3597 | time 465.4241 ms | tok/sec 1126473.7776
for step 13631 | loss 3.240558 | norm 0.3183 | time 465.6396 ms | tok/sec 1125952.3672
for step 13632 | loss 3.234367 | norm 0.3212 | time 465.3454 ms | tok/sec 1126664.2359
for step 13633 | loss 3.274776 | norm 0.3103 | time 465.7888 ms | tok/sec 1125591.5848
for step 13634 | loss 3.234081 | norm 0.3083 | time 465.1511 ms | tok/sec 1127134.8860
for step 13635 | loss 3.259796 | norm 0.3215 | time 466.0666 ms | tok/sec 1124920.7758
for step 13636 | loss 3.235226 | norm 0.3297 | time 463.6869 ms | tok/sec 1130693.9043
for step 13637 | loss 3.168655 | norm 0.3251 | time 466.3889 ms | tok/sec 1124143.2936
for step 13638 | loss 3.203344 | norm 0.2900 | time 465.1098 ms | tok/sec 1127234.8415
for step 13639 | loss 3.232036 | norm 0.3427 | time 465.0934 ms | tok/sec 1127274.7130
for step 13640 | loss 3.238500 | norm 0.3032 | time 465.4884 ms | tok/sec 1126317.9961
for step 13641 | loss 3.144446 | norm 0.2985 | time 464.9751 ms | tok/sec 1127561.4092
for step 13642 | loss 3.198213 | norm 0.2977 | time 465.2724 ms | tok/sec 1126840.9001
for step 13643 | loss 3.236456 | norm 0.3076 | time 466.4006 ms | tok/sec 1124115.1358
for step 13644 | loss 3.173968 | norm 0.2934 | time 465.5945 ms | tok/sec 1126061.3390
for step 13645 | loss 3.178516 | norm 0.2904 | time 465.0991 ms | tok/sec 1127260.8444
for step 13646 | loss 3.188062 | norm 0.3033 | time 465.7037 ms | tok/sec 1125797.3062
for step 13647 | loss 3.210448 | norm 0.2988 | time 465.5187 ms | tok/sec 1126244.7360
for step 13648 | loss 3.266670 | norm 0.2903 | time 465.4775 ms | tok/sec 1126344.5336
for step 13649 | loss 3.235167 | norm 0.2887 | time 465.3132 ms | tok/sec 1126742.1691
for step 13650 | loss 3.190201 | norm 0.3011 | time 466.2547 ms | tok/sec 1124466.9222
for step 13651 | loss 3.310605 | norm 0.3401 | time 465.3678 ms | tok/sec 1126609.9776
for step 13652 | loss 3.261046 | norm 0.2883 | time 465.5378 ms | tok/sec 1126198.5927
for step 13653 | loss 3.217701 | norm 0.3471 | time 465.5309 ms | tok/sec 1126215.3192
for step 13654 | loss 3.269909 | norm 0.3008 | time 465.6556 ms | tok/sec 1125913.7421
for step 13655 | loss 3.271073 | norm 0.3297 | time 465.0502 ms | tok/sec 1127379.3171
for step 13656 | loss 3.212727 | norm 0.3058 | time 465.0114 ms | tok/sec 1127473.5352
for step 13657 | loss 3.214561 | norm 0.3151 | time 464.6816 ms | tok/sec 1128273.5778
for step 13658 | loss 3.246273 | norm 0.3160 | time 466.0802 ms | tok/sec 1124887.9756
for step 13659 | loss 3.236978 | norm 0.2997 | time 465.1756 ms | tok/sec 1127075.3834
for step 13660 | loss 3.230636 | norm 0.3010 | time 465.7710 ms | tok/sec 1125634.7973
for step 13661 | loss 3.145385 | norm 0.2909 | time 464.6780 ms | tok/sec 1128282.2613
for step 13662 | loss 3.264701 | norm 0.3071 | time 465.2214 ms | tok/sec 1126964.4826
for step 13663 | loss 3.267013 | norm 0.3174 | time 464.6854 ms | tok/sec 1128264.3156
for step 13664 | loss 3.238062 | norm 0.2919 | time 465.6355 ms | tok/sec 1125962.1680
for step 13665 | loss 3.288293 | norm 0.3243 | time 466.0478 ms | tok/sec 1124966.2389
for step 13666 | loss 3.216750 | norm 0.2819 | time 465.9874 ms | tok/sec 1125111.8605
for step 13667 | loss 3.274130 | norm 0.3205 | time 465.5788 ms | tok/sec 1126099.3976
for step 13668 | loss 3.232377 | norm 0.2738 | time 465.5423 ms | tok/sec 1126187.6343
for step 13669 | loss 3.199370 | norm 0.2964 | time 465.8928 ms | tok/sec 1125340.4416
for step 13670 | loss 3.250275 | norm 0.2893 | time 465.7087 ms | tok/sec 1125785.2028
for step 13671 | loss 3.282144 | norm 0.2973 | time 465.8878 ms | tok/sec 1125352.5354
for step 13672 | loss 3.241509 | norm 0.2875 | time 465.3726 ms | tok/sec 1126598.4339
for step 13673 | loss 3.237026 | norm 0.2810 | time 465.4086 ms | tok/sec 1126511.2870
for step 13674 | loss 3.228968 | norm 0.3165 | time 465.5616 ms | tok/sec 1126140.9190
for step 13675 | loss 3.188029 | norm 0.3007 | time 465.9617 ms | tok/sec 1125174.0346
for step 13676 | loss 3.146522 | norm 0.3201 | time 466.2836 ms | tok/sec 1124397.3522
for step 13677 | loss 3.184855 | norm 0.3029 | time 467.4942 ms | tok/sec 1121485.4530
for step 13678 | loss 3.201165 | norm 0.3134 | time 466.4021 ms | tok/sec 1124111.6880
for step 13679 | loss 3.158332 | norm 0.3277 | time 466.1455 ms | tok/sec 1124730.3313
for step 13680 | loss 3.215617 | norm 0.3221 | time 465.5774 ms | tok/sec 1126102.8576
for step 13681 | loss 3.190107 | norm 0.3147 | time 465.4262 ms | tok/sec 1126468.5842
for step 13682 | loss 3.154220 | norm 0.3273 | time 464.9251 ms | tok/sec 1127682.8366
for step 13683 | loss 3.290527 | norm 0.3259 | time 465.9460 ms | tok/sec 1125212.0332
for step 13684 | loss 3.279848 | norm 0.2998 | time 465.9245 ms | tok/sec 1125263.8537
for step 13685 | loss 3.241737 | norm 0.3254 | time 465.7145 ms | tok/sec 1125771.3708
for step 13686 | loss 3.245363 | norm 0.3263 | time 466.0807 ms | tok/sec 1124886.8248
for step 13687 | loss 3.278126 | norm 0.3108 | time 465.9305 ms | tok/sec 1125249.4586
for step 13688 | loss 3.268885 | norm 0.3208 | time 466.0454 ms | tok/sec 1124971.9939
for step 13689 | loss 3.205690 | norm 0.3280 | time 465.7564 ms | tok/sec 1125669.9460
for step 13690 | loss 3.257754 | norm 0.3285 | time 465.6630 ms | tok/sec 1125895.8716
for step 13691 | loss 3.205837 | norm 0.2918 | time 465.5740 ms | tok/sec 1126110.9310
for step 13692 | loss 3.261004 | norm 0.3717 | time 464.8097 ms | tok/sec 1127962.7974
for step 13693 | loss 3.327112 | norm 0.3366 | time 465.5550 ms | tok/sec 1126157.0671
for step 13694 | loss 3.266297 | norm 0.3619 | time 465.6985 ms | tok/sec 1125809.9861
for step 13695 | loss 3.264086 | norm 0.3149 | time 465.3864 ms | tok/sec 1126564.9587
for step 13696 | loss 3.211787 | norm 0.3296 | time 465.6792 ms | tok/sec 1125856.6739
for step 13697 | loss 3.299007 | norm 0.3316 | time 465.1272 ms | tok/sec 1127192.6616
for step 13698 | loss 3.271539 | norm 0.3107 | time 465.2414 ms | tok/sec 1126915.9703
for step 13699 | loss 3.248801 | norm 0.3471 | time 465.2603 ms | tok/sec 1126870.3496
for step 13700 | loss 3.263430 | norm 0.2952 | time 466.0361 ms | tok/sec 1124994.4393
for step 13701 | loss 3.174572 | norm 0.3066 | time 465.2843 ms | tok/sec 1126812.0296
for step 13702 | loss 3.298418 | norm 0.3131 | time 466.1038 ms | tok/sec 1124831.0114
for step 13703 | loss 3.299827 | norm 0.3453 | time 464.9615 ms | tok/sec 1127594.3654
for step 13704 | loss 3.209543 | norm 0.2883 | time 466.5606 ms | tok/sec 1123729.6882
for step 13705 | loss 3.228999 | norm 0.3235 | time 465.6959 ms | tok/sec 1125816.3262
for step 13706 | loss 3.256943 | norm 0.3256 | time 464.3745 ms | tok/sec 1129019.6863
for step 13707 | loss 3.187866 | norm 0.3099 | time 464.9081 ms | tok/sec 1127723.8965
for step 13708 | loss 3.191167 | norm 0.3032 | time 464.7138 ms | tok/sec 1128195.4326
for step 13709 | loss 3.208202 | norm 0.2991 | time 465.4548 ms | tok/sec 1126399.3433
for step 13710 | loss 3.219326 | norm 0.2902 | time 464.7000 ms | tok/sec 1128229.0047
for step 13711 | loss 3.192244 | norm 0.2721 | time 465.1892 ms | tok/sec 1127042.4574
for step 13712 | loss 3.213641 | norm 0.2870 | time 465.9214 ms | tok/sec 1125271.3393
for step 13713 | loss 3.220931 | norm 0.3246 | time 465.4856 ms | tok/sec 1126324.9188
for step 13714 | loss 3.218257 | norm 0.3087 | time 464.2630 ms | tok/sec 1129291.0323
Will loading at 0 from edu_fineweb10B/edufineweb_train_000073.npy
for step 13715 | loss 3.189651 | norm 0.3052 | time 2622.3345 ms | tok/sec 199931.7798
for step 13716 | loss 3.231846 | norm 0.2882 | time 477.6917 ms | tok/sec 1097544.8274
for step 13717 | loss 3.236241 | norm 0.3297 | time 463.3934 ms | tok/sec 1131410.0366
for step 13718 | loss 3.190639 | norm 0.3172 | time 463.4852 ms | tok/sec 1131185.9660
for step 13719 | loss 3.278734 | norm 0.3283 | time 464.0243 ms | tok/sec 1129871.8493
for step 13720 | loss 3.266558 | norm 0.3082 | time 464.7503 ms | tok/sec 1128106.8810
for step 13721 | loss 3.275661 | norm 0.3064 | time 463.6548 ms | tok/sec 1130772.3961
for step 13722 | loss 3.305952 | norm 0.3124 | time 464.7152 ms | tok/sec 1128191.9597
for step 13723 | loss 3.274975 | norm 0.3304 | time 465.1120 ms | tok/sec 1127229.6410
for step 13724 | loss 3.273408 | norm 0.3268 | time 464.3989 ms | tok/sec 1128960.5641
for step 13725 | loss 3.243457 | norm 0.3339 | time 465.4200 ms | tok/sec 1126483.5876
for step 13726 | loss 3.217914 | norm 0.2974 | time 465.4498 ms | tok/sec 1126411.4598
for step 13727 | loss 3.287796 | norm 0.3282 | time 465.7688 ms | tok/sec 1125639.9831
for step 13728 | loss 3.233214 | norm 0.3318 | time 464.8399 ms | tok/sec 1127889.3231
for step 13729 | loss 3.250396 | norm 0.3109 | time 464.8907 ms | tok/sec 1127766.1162
for step 13730 | loss 3.324139 | norm 0.3289 | time 466.0182 ms | tok/sec 1125037.6060
for step 13731 | loss 3.227423 | norm 0.2858 | time 465.0307 ms | tok/sec 1127426.7132
for step 13732 | loss 3.221342 | norm 0.2942 | time 465.1954 ms | tok/sec 1127027.4392
for step 13733 | loss 3.211810 | norm 0.2736 | time 465.1647 ms | tok/sec 1127101.9566
for step 13734 | loss 3.236205 | norm 0.3115 | time 465.8380 ms | tok/sec 1125472.9114
for step 13735 | loss 3.251686 | norm 0.2895 | time 465.6653 ms | tok/sec 1125890.1071
for step 13736 | loss 3.222182 | norm 0.2964 | time 465.5321 ms | tok/sec 1126212.4353
for step 13737 | loss 3.237037 | norm 0.2751 | time 465.6286 ms | tok/sec 1125978.8875
for step 13738 | loss 3.211292 | norm 0.2704 | time 465.3039 ms | tok/sec 1126764.6852
for step 13739 | loss 3.260833 | norm 0.2947 | time 466.2366 ms | tok/sec 1124510.6234
for step 13740 | loss 3.274752 | norm 0.2730 | time 464.8724 ms | tok/sec 1127810.6527
for step 13741 | loss 3.247168 | norm 0.3448 | time 465.1439 ms | tok/sec 1127152.2180
for step 13742 | loss 3.295535 | norm 0.3227 | time 465.4298 ms | tok/sec 1126459.9286
for step 13743 | loss 3.195794 | norm 0.3529 | time 464.8700 ms | tok/sec 1127816.4369
for step 13744 | loss 3.200230 | norm 0.2941 | time 465.4431 ms | tok/sec 1126427.6157
for step 13745 | loss 3.203638 | norm 0.3401 | time 465.7304 ms | tok/sec 1125732.7580
for step 13746 | loss 3.212540 | norm 0.3315 | time 464.7081 ms | tok/sec 1128209.3243
for step 13747 | loss 3.185764 | norm 0.2966 | time 464.8674 ms | tok/sec 1127822.7996
for step 13748 | loss 3.260248 | norm 0.3190 | time 466.0275 ms | tok/sec 1125015.1589
for step 13749 | loss 3.214235 | norm 0.3199 | time 465.0657 ms | tok/sec 1127341.7499
validation loss 3.2623
HellaSwag accuracy: 2800/10042=0.2788
> Hello, I'm a language model, and there is no better way to see it. The more words you use, the less time will be spent on the
> Hello, I'm a language model, a model language, and a language language enthusiast. I know that everyone is in a big hurry and that's why I
> Hello, I'm a language model, I take my work and develop it into a language. What would it take to become a language? How would we transform
> Hello, I'm a language model, and I'm really excited to meet someone going to the real world. If those people meet something in the real world,
> Hello, I'm a language model, and I can't wait to be accepted there.
Hi, Thanks for posting this question. You're welcome to write
> Hello, I'm a language model, so please comment below if you find it helpful, that doesn't help:
I hope you found this tutorial helpful.
> > > Hello, I'm a language model, so I do understand every little thing in the room, and every single thing in the room, and I'm a bilingual
Hello, I'm a language model, and i am teaching and writing, at least.
So what is the best way to teach English? Is it really
Hello, I'm a language model, so I think that you need to understand each of them.
I want to work for that blog! What are your
> > > > Hello, I'm a language model, and I'll be able to explain your theory in the course.
It's great to watch them in a new way
Hello, I'm a language model, and what I'm trying to say is, these are the top 4, so this is basically a problem. So a
> Hello, I'm a language model, learning a language from scratch. I really got it taught me a lot for a while, but now that's all you
Hello, I'm a language model, now it's sort of a very powerful machine, it thinks, all this time we want the same thing to represent something
> > Hello, I'm a language model, what am I supposed to do with the data? The first step is to use the data in the model, and make>> > Hello, I'm a language model, and I'm not really trying to create a language for all my friends too. So my name is S.<|endoftext|>If
Hello, I'm a language model, language model is a model that shows only the relations between subjects, that is, to a set of classes and classes (

 Hello, I'm a language model, I think it's quite interesting. It tells me where I'm now and how I'm going. I've read allHello, I'm a language model, so I've got to do the analysis a second time. This means this is not my actual research, but is that
Hello, I'm a language model, but really, most of it is just for free.
In the following video, I'm discussing our ideas behind how
> > > 
> > Hello, I'm a language model, and this blog is about those ideas and I've had to write about them.
I'm trying to follow those two
Hello, I'm a language model, and I am looking for a word search program, called an open-source language.
I'm working with two other
Hello, I'm a language model, and I think that we're interested in our topic in this blog. And I'm going to give a lot of thought
> Hello, I'm a language model, and if I'm going to be teaching students, please don't worry, I'm a software teacher, I'll use
Hello, I'm a language model, and it could change you to a "Coding Code Network" in 6 weeks.
The main idea is to use
> > > Hello, I'm a language model, is learning something that interests you! If you could be interested in languages and literature. If not, check what language is
Hello, I'm a language model, and you have to master the first two functions with any of the other two functions.
In languages, you only need
Hello, I'm a language model, to understand that I've started learning more about AI. I've learned lots of interesting things about writing and programming, though
Hello, I'm a language model, and my goal is to create and test language, and I am learning a language for beginners. I plan to have my
> > > Hello, I'm a language model, so you have a similar model for other languages:
- Hindi – You need Hindi code to write the language. In
Hello, I'm a language model, and in this course, you'll learn the syntax for describing the structure of a language. You'll even get to know
Hello, I'm a language model, and I think it's important to understand that, because I'm only a tool, I shouldn't change any of the
> Hello, I'm a language model, this is really for you.
I am a professor, I am a human driver of a research team.
Let
for step 13750 | loss 3.173339 | norm 0.3231 | time 12456.1779 ms | tok/sec 42090.5997
for step 13751 | loss 3.178948 | norm 0.3001 | time 461.8523 ms | tok/sec 1135185.3965
for step 13752 | loss 3.173269 | norm 0.3317 | time 462.2829 ms | tok/sec 1134128.0512
for step 13753 | loss 3.198321 | norm 0.2932 | time 463.4356 ms | tok/sec 1131307.0113
for step 13754 | loss 3.194256 | norm 0.3319 | time 462.5802 ms | tok/sec 1133399.1283
for step 13755 | loss 3.316367 | norm 0.3460 | time 463.1555 ms | tok/sec 1131991.2878
for step 13756 | loss 3.270721 | norm 0.3160 | time 463.4621 ms | tok/sec 1131242.4118
for step 13757 | loss 3.181508 | norm 0.3148 | time 463.3014 ms | tok/sec 1131634.7784
for step 13758 | loss 3.233325 | norm 0.3531 | time 463.7282 ms | tok/sec 1130593.3344
for step 13759 | loss 3.192647 | norm 0.2982 | time 464.2484 ms | tok/sec 1129326.4097
for step 13760 | loss 3.234161 | norm 0.3783 | time 464.3908 ms | tok/sec 1128980.2708
for step 13761 | loss 3.264632 | norm 0.3148 | time 464.5083 ms | tok/sec 1128694.5908
for step 13762 | loss 3.240220 | norm 0.3138 | time 464.3569 ms | tok/sec 1129062.5828
for step 13763 | loss 3.297515 | norm 0.3245 | time 463.5851 ms | tok/sec 1130942.2082
for step 13764 | loss 3.237833 | norm 0.3140 | time 464.2346 ms | tok/sec 1129360.0492
for step 13765 | loss 3.253143 | norm 0.3354 | time 464.0005 ms | tok/sec 1129929.9058
for step 13766 | loss 3.235890 | norm 0.3504 | time 465.2488 ms | tok/sec 1126898.0681
for step 13767 | loss 3.255097 | norm 0.3386 | time 464.0951 ms | tok/sec 1129699.4567
for step 13768 | loss 3.279258 | norm 0.3319 | time 464.8647 ms | tok/sec 1127829.1624
for step 13769 | loss 3.252804 | norm 0.3418 | time 464.7946 ms | tok/sec 1127999.2488
for step 13770 | loss 3.212911 | norm 0.3123 | time 465.8113 ms | tok/sec 1125537.4298
for step 13771 | loss 3.298862 | norm 0.4126 | time 468.3902 ms | tok/sec 1119340.1794
for step 13772 | loss 3.234385 | norm 0.4182 | time 465.1649 ms | tok/sec 1127101.3789
for step 13773 | loss 3.282156 | norm 0.3612 | time 465.2863 ms | tok/sec 1126807.4105
for step 13774 | loss 3.264269 | norm 0.3686 | time 465.3151 ms | tok/sec 1126737.5506
for step 13775 | loss 3.255771 | norm 0.3460 | time 465.4644 ms | tok/sec 1126376.2649
for step 13776 | loss 3.232407 | norm 0.3783 | time 465.6048 ms | tok/sec 1126036.5446
for step 13777 | loss 3.278524 | norm 0.3365 | time 465.7419 ms | tok/sec 1125705.0968
for step 13778 | loss 3.178061 | norm 0.3107 | time 464.8199 ms | tok/sec 1127937.9192
for step 13779 | loss 3.220056 | norm 0.4059 | time 465.7271 ms | tok/sec 1125740.8262
for step 13780 | loss 3.165454 | norm 0.3137 | time 464.7202 ms | tok/sec 1128179.8048
for step 13781 | loss 3.202430 | norm 0.3691 | time 465.7676 ms | tok/sec 1125642.8640
for step 13782 | loss 3.226537 | norm 0.3685 | time 465.6947 ms | tok/sec 1125819.2081
for step 13783 | loss 3.211463 | norm 0.3725 | time 465.3542 ms | tok/sec 1126642.8782
for step 13784 | loss 3.309141 | norm 0.3631 | time 466.0912 ms | tok/sec 1124861.5067
for step 13785 | loss 3.177191 | norm 0.3657 | time 466.4094 ms | tok/sec 1124093.8747
for step 13786 | loss 3.244570 | norm 0.2997 | time 465.4207 ms | tok/sec 1126481.8564
for step 13787 | loss 3.215758 | norm 0.3472 | time 465.3695 ms | tok/sec 1126605.9373
for step 13788 | loss 3.222770 | norm 0.3327 | time 465.3921 ms | tok/sec 1126551.1075
for step 13789 | loss 3.235134 | norm 0.3301 | time 465.6582 ms | tok/sec 1125907.4009
for step 13790 | loss 3.238562 | norm 0.3626 | time 465.1785 ms | tok/sec 1127068.4514
for step 13791 | loss 3.298839 | norm 0.3199 | time 465.8034 ms | tok/sec 1125556.4410
for step 13792 | loss 3.264464 | norm 0.3669 | time 466.2962 ms | tok/sec 1124366.8821
for step 13793 | loss 3.223864 | norm 0.2936 | time 465.6732 ms | tok/sec 1125871.0845
for step 13794 | loss 3.274455 | norm 0.3607 | time 465.4109 ms | tok/sec 1126505.5162
for step 13795 | loss 3.275600 | norm 0.3527 | time 465.1902 ms | tok/sec 1127040.1469
for step 13796 | loss 3.179083 | norm 0.3797 | time 464.8974 ms | tok/sec 1127749.9219
for step 13797 | loss 3.271171 | norm 0.3216 | time 465.7848 ms | tok/sec 1125601.3793
for step 13798 | loss 3.281506 | norm 0.3147 | time 465.8027 ms | tok/sec 1125558.1694
for step 13799 | loss 3.260651 | norm 0.3394 | time 465.4176 ms | tok/sec 1126489.3582
for step 13800 | loss 3.248758 | norm 0.3140 | time 465.1041 ms | tok/sec 1127248.7095
for step 13801 | loss 3.290439 | norm 0.3526 | time 465.1988 ms | tok/sec 1127019.3526
for step 13802 | loss 3.197963 | norm 0.3384 | time 465.7514 ms | tok/sec 1125682.0468
for step 13803 | loss 3.236969 | norm 0.3059 | time 465.4562 ms | tok/sec 1126395.8815
for step 13804 | loss 3.254343 | norm 0.3356 | time 465.5151 ms | tok/sec 1126253.3882
for step 13805 | loss 3.243575 | norm 0.3941 | time 465.2531 ms | tok/sec 1126887.6735
for step 13806 | loss 3.268876 | norm 0.3337 | time 465.4856 ms | tok/sec 1126324.9188
for step 13807 | loss 3.285406 | norm 0.3289 | time 464.8712 ms | tok/sec 1127813.5448
for step 13808 | loss 3.241502 | norm 0.3074 | time 464.7791 ms | tok/sec 1128036.8599
for step 13809 | loss 3.264128 | norm 0.3295 | time 466.3999 ms | tok/sec 1124116.8597
for step 13810 | loss 3.234683 | norm 0.3202 | time 465.6124 ms | tok/sec 1126018.0937
for step 13811 | loss 3.184021 | norm 0.3076 | time 465.3919 ms | tok/sec 1126551.6846
for step 13812 | loss 3.212895 | norm 0.3473 | time 465.3542 ms | tok/sec 1126642.8782
for step 13813 | loss 3.214439 | norm 0.3082 | time 465.6296 ms | tok/sec 1125976.5813
for step 13814 | loss 3.179141 | norm 0.3366 | time 464.8345 ms | tok/sec 1127902.6288
for step 13815 | loss 3.223292 | norm 0.2930 | time 465.2324 ms | tok/sec 1126937.9158
for step 13816 | loss 3.194583 | norm 0.3102 | time 465.4560 ms | tok/sec 1126396.4584
for step 13817 | loss 3.189949 | norm 0.3064 | time 465.3618 ms | tok/sec 1126624.4075
for step 13818 | loss 3.144101 | norm 0.3156 | time 465.6370 ms | tok/sec 1125958.7089
for step 13819 | loss 3.244348 | norm 0.3239 | time 465.1198 ms | tok/sec 1127210.5732
for step 13820 | loss 3.208391 | norm 0.2897 | time 465.4295 ms | tok/sec 1126460.5057
for step 13821 | loss 3.222507 | norm 0.3106 | time 465.0805 ms | tok/sec 1127305.9189
for step 13822 | loss 3.190706 | norm 0.2878 | time 464.6153 ms | tok/sec 1128434.5333
for step 13823 | loss 3.199009 | norm 0.3115 | time 465.5919 ms | tok/sec 1126067.6819
for step 13824 | loss 3.164268 | norm 0.3046 | time 465.9331 ms | tok/sec 1125243.1249
for step 13825 | loss 3.161981 | norm 0.2988 | time 465.6441 ms | tok/sec 1125941.4135
for step 13826 | loss 3.302429 | norm 0.3347 | time 465.7753 ms | tok/sec 1125624.4260
for step 13827 | loss 3.215723 | norm 0.3465 | time 465.4181 ms | tok/sec 1126488.2040
for step 13828 | loss 3.279045 | norm 0.3280 | time 465.7204 ms | tok/sec 1125756.9627
for step 13829 | loss 3.228966 | norm 0.3237 | time 465.7536 ms | tok/sec 1125676.8607
for step 13830 | loss 3.253267 | norm 0.3582 | time 465.0822 ms | tok/sec 1127301.8736
for step 13831 | loss 3.238372 | norm 0.3214 | time 465.5354 ms | tok/sec 1126204.3604
for step 13832 | loss 3.239152 | norm 0.3410 | time 464.5145 ms | tok/sec 1128679.5285
for step 13833 | loss 3.236846 | norm 0.3094 | time 465.0404 ms | tok/sec 1127403.0147
for step 13834 | loss 3.234979 | norm 0.3239 | time 464.5891 ms | tok/sec 1128498.2334
for step 13835 | loss 3.267498 | norm 0.3258 | time 465.0543 ms | tok/sec 1127369.4916
for step 13836 | loss 3.256246 | norm 0.3206 | time 465.0297 ms | tok/sec 1127429.0253
for step 13837 | loss 3.207174 | norm 0.3398 | time 465.4868 ms | tok/sec 1126322.0343
for step 13838 | loss 3.259042 | norm 0.2908 | time 464.9386 ms | tok/sec 1127649.8751
for step 13839 | loss 3.255033 | norm 0.2919 | time 464.7822 ms | tok/sec 1128029.3375
for step 13840 | loss 3.261860 | norm 0.3116 | time 466.1877 ms | tok/sec 1124628.5188
for step 13841 | loss 3.217786 | norm 0.3067 | time 464.9580 ms | tok/sec 1127603.0385
for step 13842 | loss 3.228957 | norm 0.2875 | time 465.7254 ms | tok/sec 1125744.8603
for step 13843 | loss 3.251370 | norm 0.3263 | time 465.3020 ms | tok/sec 1126769.3040
for step 13844 | loss 3.217735 | norm 0.2851 | time 465.3969 ms | tok/sec 1126539.5650
for step 13845 | loss 3.216794 | norm 0.3337 | time 465.7385 ms | tok/sec 1125713.1645
for step 13846 | loss 3.213511 | norm 0.3116 | time 466.2681 ms | tok/sec 1124434.7234
for step 13847 | loss 3.256158 | norm 0.2938 | time 466.1341 ms | tok/sec 1124757.9446
for step 13848 | loss 3.183156 | norm 0.3014 | time 465.7829 ms | tok/sec 1125605.9886
for step 13849 | loss 3.223201 | norm 0.3171 | time 465.4739 ms | tok/sec 1126353.1874
for step 13850 | loss 3.201715 | norm 0.2747 | time 465.1568 ms | tok/sec 1127121.0207
for step 13851 | loss 3.222432 | norm 0.2857 | time 464.7176 ms | tok/sec 1128186.1716
for step 13852 | loss 3.171719 | norm 0.3100 | time 466.1787 ms | tok/sec 1124650.3753
for step 13853 | loss 3.184076 | norm 0.2820 | time 465.3018 ms | tok/sec 1126769.8814
for step 13854 | loss 3.234123 | norm 0.3158 | time 465.2836 ms | tok/sec 1126813.7618
for step 13855 | loss 3.203254 | norm 0.3049 | time 465.6162 ms | tok/sec 1126008.8685
for step 13856 | loss 3.213039 | norm 0.3048 | time 465.7016 ms | tok/sec 1125802.4934
for step 13857 | loss 3.190650 | norm 0.2987 | time 464.5286 ms | tok/sec 1128645.3502
for step 13858 | loss 3.237095 | norm 0.3132 | time 465.9212 ms | tok/sec 1125271.9151
for step 13859 | loss 3.221113 | norm 0.2878 | time 465.4975 ms | tok/sec 1126296.0747
for step 13860 | loss 3.244080 | norm 0.3253 | time 465.3950 ms | tok/sec 1126544.1820
for step 13861 | loss 3.336235 | norm 0.3159 | time 465.9002 ms | tok/sec 1125322.5893
for step 13862 | loss 3.252533 | norm 0.3102 | time 466.2447 ms | tok/sec 1124491.0725
for step 13863 | loss 3.341265 | norm 0.3278 | time 465.5092 ms | tok/sec 1126267.8090
for step 13864 | loss 3.247355 | norm 0.3200 | time 465.5635 ms | tok/sec 1126136.3054
for step 13865 | loss 3.348955 | norm 0.3187 | time 465.3504 ms | tok/sec 1126652.1139
for step 13866 | loss 3.290035 | norm 0.3275 | time 465.8797 ms | tok/sec 1125372.1163
for step 13867 | loss 3.226159 | norm 0.3210 | time 465.5817 ms | tok/sec 1126092.4777
for step 13868 | loss 3.237499 | norm 0.3579 | time 465.6346 ms | tok/sec 1125964.4741
for step 13869 | loss 3.254632 | norm 0.3117 | time 465.4303 ms | tok/sec 1126458.7746
for step 13870 | loss 3.242067 | norm 0.3425 | time 465.7583 ms | tok/sec 1125665.3362
for step 13871 | loss 3.271022 | norm 0.3259 | time 464.4430 ms | tok/sec 1128853.3484
for step 13872 | loss 3.186644 | norm 0.3394 | time 466.6853 ms | tok/sec 1123429.4406
for step 13873 | loss 3.230175 | norm 0.3148 | time 465.4579 ms | tok/sec 1126391.8427
for step 13874 | loss 3.241842 | norm 0.3028 | time 465.2970 ms | tok/sec 1126781.4285
for step 13875 | loss 3.255845 | norm 0.3305 | time 465.7168 ms | tok/sec 1125765.6075
for step 13876 | loss 3.262364 | norm 0.3246 | time 466.0857 ms | tok/sec 1124874.7410
for step 13877 | loss 3.277395 | norm 0.3063 | time 464.8428 ms | tok/sec 1127882.3812
for step 13878 | loss 3.242359 | norm 0.3145 | time 465.4396 ms | tok/sec 1126436.2707
for step 13879 | loss 3.227562 | norm 0.3052 | time 465.8761 ms | tok/sec 1125380.7552
for step 13880 | loss 3.262282 | norm 0.2993 | time 465.8043 ms | tok/sec 1125554.1366
for step 13881 | loss 3.225010 | norm 0.3168 | time 465.6670 ms | tok/sec 1125886.0720
for step 13882 | loss 3.219167 | norm 0.2957 | time 465.7836 ms | tok/sec 1125604.2601
for step 13883 | loss 3.196551 | norm 0.2942 | time 465.4067 ms | tok/sec 1126515.9038
for step 13884 | loss 3.264817 | norm 0.3069 | time 465.3373 ms | tok/sec 1126683.8625
for step 13885 | loss 3.174906 | norm 0.2838 | time 466.5861 ms | tok/sec 1123668.2478
for step 13886 | loss 3.173019 | norm 0.3067 | time 465.7843 ms | tok/sec 1125602.5317
for step 13887 | loss 3.219204 | norm 0.2881 | time 465.8725 ms | tok/sec 1125389.3942
for step 13888 | loss 3.249885 | norm 0.3034 | time 465.1394 ms | tok/sec 1127163.1953
for step 13889 | loss 3.174560 | norm 0.2866 | time 465.5187 ms | tok/sec 1126244.7360
for step 13890 | loss 3.162527 | norm 0.2805 | time 465.2534 ms | tok/sec 1126887.0960
for step 13891 | loss 3.160093 | norm 0.2999 | time 465.2765 ms | tok/sec 1126831.0840
for step 13892 | loss 3.249652 | norm 0.2771 | time 465.3976 ms | tok/sec 1126537.8337
for step 13893 | loss 3.183725 | norm 0.2765 | time 465.6229 ms | tok/sec 1125992.7247
for step 13894 | loss 3.233996 | norm 0.2743 | time 464.3474 ms | tok/sec 1129085.7714
for step 13895 | loss 3.262071 | norm 0.3090 | time 465.7094 ms | tok/sec 1125783.4738
for step 13896 | loss 3.224232 | norm 0.2946 | time 466.1763 ms | tok/sec 1124656.1271
for step 13897 | loss 3.218473 | norm 0.3011 | time 464.8945 ms | tok/sec 1127756.8623
for step 13898 | loss 3.304280 | norm 0.3063 | time 465.2705 ms | tok/sec 1126845.5195
for step 13899 | loss 3.232788 | norm 0.2838 | time 465.5852 ms | tok/sec 1126083.8279
for step 13900 | loss 3.243349 | norm 0.2849 | time 465.5144 ms | tok/sec 1126255.1187
for step 13901 | loss 3.265865 | norm 0.2963 | time 465.3311 ms | tok/sec 1126698.8715
for step 13902 | loss 3.289413 | norm 0.3125 | time 465.7640 ms | tok/sec 1125651.5071
for step 13903 | loss 3.252589 | norm 0.3061 | time 464.8387 ms | tok/sec 1127892.2157
for step 13904 | loss 3.244568 | norm 0.3027 | time 465.2259 ms | tok/sec 1126953.5092
for step 13905 | loss 3.265212 | norm 0.3221 | time 464.6666 ms | tok/sec 1128310.0493
Will loading at 0 from edu_fineweb10B/edufineweb_train_000074.npy
for step 13906 | loss 3.220933 | norm 0.3136 | time 2597.9161 ms | tok/sec 201810.9802
for step 13907 | loss 3.305421 | norm 0.3848 | time 462.6627 ms | tok/sec 1133197.0431
for step 13908 | loss 3.255743 | norm 0.3369 | time 464.5691 ms | tok/sec 1128546.8820
for step 13909 | loss 3.230402 | norm 0.3412 | time 464.6614 ms | tok/sec 1128322.7860
for step 13910 | loss 3.285135 | norm 0.3102 | time 464.9680 ms | tok/sec 1127578.7543
for step 13911 | loss 3.263093 | norm 0.3841 | time 464.9365 ms | tok/sec 1127655.0794
for step 13912 | loss 3.215859 | norm 0.3281 | time 464.3075 ms | tok/sec 1129182.5942
for step 13913 | loss 3.238458 | norm 0.3395 | time 463.1615 ms | tok/sec 1131976.7201
for step 13914 | loss 3.225957 | norm 0.3149 | time 464.0791 ms | tok/sec 1129738.3420
for step 13915 | loss 3.227414 | norm 0.3514 | time 463.4256 ms | tok/sec 1131331.4563
for step 13916 | loss 3.238130 | norm 0.3003 | time 464.4587 ms | tok/sec 1128815.1035
for step 13917 | loss 3.213301 | norm 0.3698 | time 465.5585 ms | tok/sec 1126148.4163
for step 13918 | loss 3.193473 | norm 0.3062 | time 465.5719 ms | tok/sec 1126116.1211
for step 13919 | loss 3.184294 | norm 0.3557 | time 464.3562 ms | tok/sec 1129064.3219
for step 13920 | loss 3.222028 | norm 0.3221 | time 466.0046 ms | tok/sec 1125070.4149
for step 13921 | loss 3.193656 | norm 0.3505 | time 465.4648 ms | tok/sec 1126375.1110
for step 13922 | loss 3.186471 | norm 0.3356 | time 465.3714 ms | tok/sec 1126601.3198
for step 13923 | loss 3.176108 | norm 0.3309 | time 465.8942 ms | tok/sec 1125336.9863
for step 13924 | loss 3.224197 | norm 0.3072 | time 465.3480 ms | tok/sec 1126657.8862
for step 13925 | loss 3.223823 | norm 0.3552 | time 464.6940 ms | tok/sec 1128243.4761
for step 13926 | loss 3.179680 | norm 0.3169 | time 465.2939 ms | tok/sec 1126788.9343
for step 13927 | loss 3.179338 | norm 0.3327 | time 465.3075 ms | tok/sec 1126756.0251
for step 13928 | loss 3.265087 | norm 0.3373 | time 465.0092 ms | tok/sec 1127478.7379
for step 13929 | loss 3.209522 | norm 0.3136 | time 465.6069 ms | tok/sec 1126031.3552
for step 13930 | loss 3.298610 | norm 0.3903 | time 465.8844 ms | tok/sec 1125360.5980
for step 13931 | loss 3.258530 | norm 0.3294 | time 465.5147 ms | tok/sec 1126254.5419
for step 13932 | loss 3.429714 | norm 0.4264 | time 466.9759 ms | tok/sec 1122730.2507
for step 13933 | loss 3.296185 | norm 0.3881 | time 466.2764 ms | tok/sec 1124414.6001
for step 13934 | loss 3.294292 | norm 0.3615 | time 466.7292 ms | tok/sec 1123323.8467
for step 13935 | loss 3.250058 | norm 0.3692 | time 466.6886 ms | tok/sec 1123421.4056
for step 13936 | loss 3.240818 | norm 0.3307 | time 465.4641 ms | tok/sec 1126376.8418
for step 13937 | loss 3.282346 | norm 0.3644 | time 466.0673 ms | tok/sec 1124919.0494
for step 13938 | loss 3.298651 | norm 0.3406 | time 466.1753 ms | tok/sec 1124658.4279
for step 13939 | loss 3.256319 | norm 0.3414 | time 465.6262 ms | tok/sec 1125984.6530
for step 13940 | loss 3.270170 | norm 0.3134 | time 466.1911 ms | tok/sec 1124620.4666
for step 13941 | loss 3.238805 | norm 0.3125 | time 464.8404 ms | tok/sec 1127888.1661
for step 13942 | loss 3.254626 | norm 0.3351 | time 465.3897 ms | tok/sec 1126556.8788
for step 13943 | loss 3.208382 | norm 0.2889 | time 465.8537 ms | tok/sec 1125434.8952
for step 13944 | loss 3.281392 | norm 0.3254 | time 465.3838 ms | tok/sec 1126571.3073
for step 13945 | loss 3.263235 | norm 0.3248 | time 465.0643 ms | tok/sec 1127345.2175
for step 13946 | loss 3.240526 | norm 0.3228 | time 464.5212 ms | tok/sec 1128663.3080
for step 13947 | loss 3.213926 | norm 0.3268 | time 464.7856 ms | tok/sec 1128021.2365
for step 13948 | loss 3.274425 | norm 0.3022 | time 466.0811 ms | tok/sec 1124885.6739
for step 13949 | loss 3.283365 | norm 0.3399 | time 465.4830 ms | tok/sec 1126331.2647
for step 13950 | loss 3.287374 | norm 0.3039 | time 465.1060 ms | tok/sec 1127244.0868
for step 13951 | loss 3.256684 | norm 0.3259 | time 466.1760 ms | tok/sec 1124656.7023
for step 13952 | loss 3.265449 | norm 0.3153 | time 464.9093 ms | tok/sec 1127721.0049
for step 13953 | loss 3.143364 | norm 0.2997 | time 464.8862 ms | tok/sec 1127777.1054
for step 13954 | loss 3.206073 | norm 0.3255 | time 465.0273 ms | tok/sec 1127434.8056
for step 13955 | loss 3.253280 | norm 0.2861 | time 465.6744 ms | tok/sec 1125868.2024
for step 13956 | loss 3.168744 | norm 0.3074 | time 465.0688 ms | tok/sec 1127334.2367
for step 13957 | loss 3.202256 | norm 0.2868 | time 465.0779 ms | tok/sec 1127312.2758
for step 13958 | loss 3.266692 | norm 0.2895 | time 465.5545 ms | tok/sec 1126158.2205
for step 13959 | loss 3.169036 | norm 0.3341 | time 465.6947 ms | tok/sec 1125819.2081
for step 13960 | loss 3.134976 | norm 0.3028 | time 465.3068 ms | tok/sec 1126757.7571
for step 13961 | loss 3.208664 | norm 0.3154 | time 466.3970 ms | tok/sec 1124123.7553
for step 13962 | loss 3.210936 | norm 0.3025 | time 466.0177 ms | tok/sec 1125038.7572
for step 13963 | loss 3.221529 | norm 0.3208 | time 464.6835 ms | tok/sec 1128268.9467
for step 13964 | loss 3.209591 | norm 0.2766 | time 465.2364 ms | tok/sec 1126928.0980
for step 13965 | loss 3.253425 | norm 0.3304 | time 465.1809 ms | tok/sec 1127062.6749
for step 13966 | loss 3.233656 | norm 0.3083 | time 465.4381 ms | tok/sec 1126439.7328
for step 13967 | loss 3.263244 | norm 0.3166 | time 465.1871 ms | tok/sec 1127047.6561
for step 13968 | loss 3.226776 | norm 0.3173 | time 465.4617 ms | tok/sec 1126382.6114
for step 13969 | loss 3.246129 | norm 0.3043 | time 465.6019 ms | tok/sec 1126043.4639
for step 13970 | loss 3.277419 | norm 0.3307 | time 464.4084 ms | tok/sec 1128937.3806
for step 13971 | loss 3.233367 | norm 0.2862 | time 465.5018 ms | tok/sec 1126285.6912
for step 13972 | loss 3.268975 | norm 0.3246 | time 464.5617 ms | tok/sec 1128564.8366
for step 13973 | loss 3.287747 | norm 0.3300 | time 465.2441 ms | tok/sec 1126909.6179
for step 13974 | loss 3.314674 | norm 0.3349 | time 465.3990 ms | tok/sec 1126534.3710
for step 13975 | loss 3.225578 | norm 0.3051 | time 466.1474 ms | tok/sec 1124725.7292
for step 13976 | loss 3.272400 | norm 0.3135 | time 466.3367 ms | tok/sec 1124269.1589
for step 13977 | loss 3.183539 | norm 0.3251 | time 465.3363 ms | tok/sec 1126686.1715
for step 13978 | loss 3.251595 | norm 0.3159 | time 465.6146 ms | tok/sec 1126012.9045
for step 13979 | loss 3.272539 | norm 0.3272 | time 464.8643 ms | tok/sec 1127830.3193
for step 13980 | loss 3.235407 | norm 0.3145 | time 465.5480 ms | tok/sec 1126173.7923
for step 13981 | loss 3.209508 | norm 0.3182 | time 465.4653 ms | tok/sec 1126373.9571
for step 13982 | loss 3.207126 | norm 0.2825 | time 464.6077 ms | tok/sec 1128453.0635
for step 13983 | loss 3.246129 | norm 0.3221 | time 466.1708 ms | tok/sec 1124669.3566
for step 13984 | loss 3.261834 | norm 0.2810 | time 465.9634 ms | tok/sec 1125170.0046
for step 13985 | loss 3.235871 | norm 0.3008 | time 464.8685 ms | tok/sec 1127819.9075
for step 13986 | loss 3.233124 | norm 0.2774 | time 465.4448 ms | tok/sec 1126423.5767
for step 13987 | loss 3.236858 | norm 0.3014 | time 465.4412 ms | tok/sec 1126432.2317
for step 13988 | loss 3.169203 | norm 0.2787 | time 465.3230 ms | tok/sec 1126718.4994
for step 13989 | loss 3.162983 | norm 0.2863 | time 465.1251 ms | tok/sec 1127197.8617
for step 13990 | loss 3.164463 | norm 0.2961 | time 465.9996 ms | tok/sec 1125082.5029
for step 13991 | loss 3.164159 | norm 0.2723 | time 464.3960 ms | tok/sec 1128967.5193
for step 13992 | loss 3.231899 | norm 0.3120 | time 465.6928 ms | tok/sec 1125823.8192
for step 13993 | loss 3.222231 | norm 0.2772 | time 464.3731 ms | tok/sec 1129023.1643
for step 13994 | loss 3.222746 | norm 0.3285 | time 465.7741 ms | tok/sec 1125627.3069
for step 13995 | loss 3.159792 | norm 0.3008 | time 464.8013 ms | tok/sec 1127983.0479
for step 13996 | loss 3.233317 | norm 0.2830 | time 465.2741 ms | tok/sec 1126836.8582
for step 13997 | loss 3.213244 | norm 0.2978 | time 464.9787 ms | tok/sec 1127552.7368
for step 13998 | loss 3.199148 | norm 0.2987 | time 464.8225 ms | tok/sec 1127931.5552
for step 13999 | loss 3.115010 | norm 0.3097 | time 464.8871 ms | tok/sec 1127774.7918
validation loss 3.2577
HellaSwag accuracy: 2835/10042=0.2823
> Hello, I'm a language model, and this is the way it's coming out for the user
- I'm a system analyst in the industry, and
> Hello, I'm a language model, which means I understand the language model's definitions and concepts like "sentence and utterance", "word and sentence",
> Hello, I'm a language model, but no one ever really knows how to do it. It looks very complicated.
I have the problem that we are
> Hello, I'm a language model, and I'm in a program that lets run the program properly. To be running, of course, you need to be
> Hello, I'm a language model, and I love to write articles on any topic...
After the post with your post and why that post was written:
> Hello, I'm a language model, so i'm not sure how you look at this article.. I think you've seen a lot of information here, so> 
Hello, I'm a language model, and I know that it will be a perfect fit. But a lot easier, or worse, maybe it will be able
> >>Hello, I'm a language model, so I guess that's just a normal way of saying that there was a lot of variation in the way that I went
 Hello, I'm a language model, having not the ability to talk anything outside of the loop's scope. I'm thinking about a set of "Hello," Hello, I'm a language model, so I just want to give you some code for this.
I hope you enjoyed what you have learnt here.
> > 

Hello, I'm a language model, and i am really interested in learning it, and I think I'm gonna get this very soon.
And I can
Hello, I'm a language model, and since I'm a professional model, you think I'm a bad guy! Here's how I model it, along
> > > Hello, I'm a language model, and I am actually getting some idea of where the first and last parts of the language come from.
So that sounds
>>Hello, I'm a language model, there are many algorithms. I would ask what do these AI algorithms do...I think, what do all the names of
Hello, I'm a language model, like a native model.
I love our language models, so you get to learn them from a different angle. It
 Hello, I'm a language model, this article is about creating such an interactive web page to share your views and views across the globe.
The first element Hello, I'm a language model, so you should be able to model one language or two languages in one program. You don't need to be a real> > 

Hello, I'm a language model, I hate reading people and just want to learn a language. And it's really how to approach my world. In my
Hello, I'm a language model, so I am trying to understand it with simple examples.
Here I need help in a little explanation of how to get
> > > > Hello, I'm a language model, and I don't care what's going the other way around. I know that the language models I'm teaching are somewhat
Hello, I'm a language model, and I'm a human model, so I can write it! I write I am my name, so I'm human
Hello, I'm a language model, and it lets users write a program just using it all. When I enter the program, the computer is running (in
Hello, I'm a language model, and so let's talk about the first letter of every word (and the only letter in French)? I mean the number
> > Hello, I'm a language model, and we have two sets of rules (we use the "rules" function on the other side of the program which we
>>Hello, I'm a language model, and this one is a powerful representation of how a language model works, and I'm not sure where to start." (
 Hello, I'm a language model, but this isn't my first time. Here are my top 10 ideas about the best language models and how you can create Hello, I'm a language model, and we use the English language, and I can't think of a better term to refer to the style I am at> 

Hello, I'm a language model, and you've used, um, the basic languages, like German, Spanish, Czech, Romanian, Russian, Slovak
> Hello, I'm a language model, have 2 buttons. They say what your input refers to and they'll give the code you need to read out.
> 
Hello, I'm a language model, and by the way, the basic concepts are explained and implemented. So now, when I write a complex program, I
> Hello, I'm a language model, so I'll just introduce you to something that is not a language model, so you already know what I'm doing right
> Hello, I'm a language model, learning the right tools for English learners.
(Please read the attached article here at Language Modeling Tools)<|endoftext|>When
for step 14000 | loss 3.202415 | norm 0.3484 | time 12536.7632 ms | tok/sec 41820.0449
for step 14001 | loss 3.269616 | norm 0.2989 | time 462.5080 ms | tok/sec 1133576.1581
for step 14002 | loss 3.210334 | norm 0.3441 | time 462.8875 ms | tok/sec 1132646.6389
for step 14003 | loss 3.276202 | norm 0.3209 | time 464.5398 ms | tok/sec 1128618.1249
for step 14004 | loss 3.214863 | norm 0.2886 | time 463.7556 ms | tok/sec 1130526.4915
for step 14005 | loss 3.256012 | norm 0.3568 | time 462.9843 ms | tok/sec 1132409.8320
for step 14006 | loss 3.237634 | norm 0.3250 | time 463.8629 ms | tok/sec 1130265.0084
for step 14007 | loss 3.213780 | norm 0.3272 | time 464.5975 ms | tok/sec 1128477.9644
for step 14008 | loss 3.299871 | norm 0.3505 | time 463.8214 ms | tok/sec 1130366.1009
for step 14009 | loss 3.294752 | norm 0.3669 | time 464.0813 ms | tok/sec 1129733.1184
for step 14010 | loss 3.255261 | norm 0.3633 | time 463.2936 ms | tok/sec 1131653.9962
for step 14011 | loss 3.218543 | norm 0.3408 | time 463.6383 ms | tok/sec 1130812.5183
for step 14012 | loss 3.207492 | norm 0.3474 | time 464.4926 ms | tok/sec 1128732.8276
for step 14013 | loss 3.209352 | norm 0.3588 | time 464.8049 ms | tok/sec 1127974.3690
for step 14014 | loss 3.267485 | norm 0.3339 | time 464.4115 ms | tok/sec 1128929.8462
for step 14015 | loss 3.240913 | norm 0.3217 | time 464.4716 ms | tok/sec 1128783.8140
for step 14016 | loss 3.210275 | norm 0.3329 | time 465.6198 ms | tok/sec 1126000.2200
for step 14017 | loss 3.295069 | norm 0.3363 | time 465.2042 ms | tok/sec 1127006.0678
for step 14018 | loss 3.230418 | norm 0.3290 | time 465.4894 ms | tok/sec 1126315.6885
for step 14019 | loss 3.282044 | norm 0.3556 | time 464.9258 ms | tok/sec 1127681.1017
for step 14020 | loss 3.275709 | norm 0.3600 | time 465.0769 ms | tok/sec 1127314.5874
for step 14021 | loss 3.261733 | norm 0.3194 | time 464.4563 ms | tok/sec 1128820.8980
for step 14022 | loss 3.241290 | norm 0.3532 | time 464.2513 ms | tok/sec 1129319.4500
for step 14023 | loss 3.204014 | norm 0.3054 | time 465.0712 ms | tok/sec 1127328.4575
for step 14024 | loss 3.211061 | norm 0.3187 | time 464.7322 ms | tok/sec 1128150.8657
for step 14025 | loss 3.136092 | norm 0.3377 | time 465.4610 ms | tok/sec 1126384.3422
for step 14026 | loss 3.201489 | norm 0.3189 | time 465.6262 ms | tok/sec 1125984.6530
for step 14027 | loss 3.201131 | norm 0.3203 | time 464.8869 ms | tok/sec 1127775.3702
for step 14028 | loss 3.191137 | norm 0.3146 | time 464.7071 ms | tok/sec 1128211.6396
for step 14029 | loss 3.227936 | norm 0.3128 | time 465.5573 ms | tok/sec 1126151.2998
for step 14030 | loss 3.205215 | norm 0.3525 | time 465.0898 ms | tok/sec 1127283.3812
for step 14031 | loss 3.179321 | norm 0.2957 | time 465.5898 ms | tok/sec 1126072.8716
for step 14032 | loss 3.188773 | norm 0.3391 | time 465.9302 ms | tok/sec 1125250.0344
for step 14033 | loss 3.186557 | norm 0.3430 | time 465.5917 ms | tok/sec 1126068.2586
for step 14034 | loss 3.175128 | norm 0.3738 | time 465.0412 ms | tok/sec 1127401.2807
for step 14035 | loss 3.196347 | norm 0.3330 | time 465.8115 ms | tok/sec 1125536.8537
for step 14036 | loss 3.253408 | norm 0.3567 | time 465.4160 ms | tok/sec 1126493.3976
for step 14037 | loss 3.284714 | norm 0.3226 | time 464.9866 ms | tok/sec 1127533.6580
for step 14038 | loss 3.322417 | norm 0.3485 | time 466.1160 ms | tok/sec 1124801.6685
for step 14039 | loss 3.377061 | norm 0.3775 | time 465.2319 ms | tok/sec 1126939.0709
for step 14040 | loss 3.183854 | norm 0.3320 | time 465.8716 ms | tok/sec 1125391.6980
for step 14041 | loss 3.236743 | norm 0.3520 | time 465.4119 ms | tok/sec 1126503.2079
for step 14042 | loss 3.193747 | norm 0.3250 | time 465.3027 ms | tok/sec 1126767.5720
for step 14043 | loss 3.251783 | norm 0.3191 | time 466.2452 ms | tok/sec 1124489.9224
for step 14044 | loss 3.261668 | norm 0.3283 | time 465.4171 ms | tok/sec 1126490.5123
for step 14045 | loss 3.275388 | norm 0.3390 | time 466.0885 ms | tok/sec 1124867.8361
for step 14046 | loss 3.266820 | norm 0.3263 | time 465.5051 ms | tok/sec 1126277.6153
for step 14047 | loss 3.237944 | norm 0.3143 | time 466.4159 ms | tok/sec 1124078.3603
for step 14048 | loss 3.230394 | norm 0.3182 | time 464.7951 ms | tok/sec 1127998.0916
for step 14049 | loss 3.261440 | norm 0.3358 | time 465.0345 ms | tok/sec 1127417.4649
for step 14050 | loss 3.214533 | norm 0.3224 | time 466.5139 ms | tok/sec 1123842.2506
for step 14051 | loss 3.248545 | norm 0.3162 | time 465.0815 ms | tok/sec 1127303.6073
for step 14052 | loss 3.280040 | norm 0.3313 | time 464.9706 ms | tok/sec 1127572.3944
for step 14053 | loss 3.223961 | norm 0.3080 | time 464.9940 ms | tok/sec 1127515.7361
for step 14054 | loss 3.240368 | norm 0.3057 | time 466.2545 ms | tok/sec 1124467.4972
for step 14055 | loss 3.258857 | norm 0.3031 | time 465.1983 ms | tok/sec 1127020.5078
for step 14056 | loss 3.245296 | norm 0.3063 | time 465.0514 ms | tok/sec 1127376.4272
for step 14057 | loss 3.205872 | norm 0.3328 | time 465.2047 ms | tok/sec 1127004.9126
for step 14058 | loss 3.208388 | norm 0.2973 | time 465.3149 ms | tok/sec 1126738.1279
for step 14059 | loss 3.259517 | norm 0.3161 | time 465.6746 ms | tok/sec 1125867.6260
for step 14060 | loss 3.248114 | norm 0.2957 | time 465.0865 ms | tok/sec 1127291.4715
for step 14061 | loss 3.228501 | norm 0.2948 | time 466.2864 ms | tok/sec 1124390.4531
for step 14062 | loss 3.198797 | norm 0.3213 | time 464.8945 ms | tok/sec 1127756.8623
for step 14063 | loss 3.207228 | norm 0.2908 | time 464.9909 ms | tok/sec 1127523.2517
for step 14064 | loss 3.192801 | norm 0.3164 | time 465.7214 ms | tok/sec 1125754.6575
for step 14065 | loss 3.237253 | norm 0.3253 | time 465.3292 ms | tok/sec 1126703.4898
for step 14066 | loss 3.230946 | norm 0.3502 | time 464.6802 ms | tok/sec 1128277.0512
for step 14067 | loss 3.182295 | norm 0.2998 | time 465.3144 ms | tok/sec 1126739.2825
for step 14068 | loss 3.178281 | norm 0.3153 | time 465.5142 ms | tok/sec 1126255.6955
for step 14069 | loss 3.202627 | norm 0.3174 | time 465.2536 ms | tok/sec 1126886.5185
for step 14070 | loss 3.180382 | norm 0.3259 | time 465.2305 ms | tok/sec 1126942.5360
for step 14071 | loss 3.199758 | norm 0.3324 | time 466.1958 ms | tok/sec 1124608.9637
for step 14072 | loss 3.282553 | norm 0.3439 | time 465.5125 ms | tok/sec 1126259.7333
for step 14073 | loss 3.262660 | norm 0.3403 | time 465.9665 ms | tok/sec 1125162.5203
for step 14074 | loss 3.256470 | norm 0.3492 | time 464.9637 ms | tok/sec 1127589.1617
for step 14075 | loss 3.251139 | norm 0.3305 | time 465.5786 ms | tok/sec 1126099.9743
for step 14076 | loss 3.174439 | norm 0.3160 | time 465.0514 ms | tok/sec 1127376.4272
for step 14077 | loss 3.278318 | norm 0.3435 | time 466.7635 ms | tok/sec 1123241.2218
for step 14078 | loss 3.291433 | norm 0.3281 | time 464.9148 ms | tok/sec 1127707.7035
for step 14079 | loss 3.251212 | norm 0.3676 | time 464.2744 ms | tok/sec 1129263.1959
for step 14080 | loss 3.231642 | norm 0.3729 | time 467.2787 ms | tok/sec 1122002.7336
for step 14081 | loss 3.253807 | norm 0.3324 | time 466.0237 ms | tok/sec 1125024.3679
for step 14082 | loss 3.323499 | norm 0.3801 | time 464.7880 ms | tok/sec 1128015.4502
for step 14083 | loss 3.270802 | norm 0.3189 | time 466.1608 ms | tok/sec 1124693.5156
for step 14084 | loss 3.328275 | norm 0.4149 | time 465.8675 ms | tok/sec 1125401.4890
for step 14085 | loss 3.266825 | norm 0.3753 | time 465.9195 ms | tok/sec 1125275.9458
for step 14086 | loss 3.247083 | norm 0.3344 | time 465.7705 ms | tok/sec 1125635.9497
for step 14087 | loss 3.308185 | norm 0.3728 | time 465.8978 ms | tok/sec 1125328.3481
for step 14088 | loss 3.250414 | norm 0.3401 | time 464.7691 ms | tok/sec 1128061.1637
for step 14089 | loss 3.212835 | norm 0.3349 | time 466.0504 ms | tok/sec 1124959.9083
for step 14090 | loss 3.239563 | norm 0.3252 | time 465.9524 ms | tok/sec 1125196.4880
for step 14091 | loss 3.199685 | norm 0.3458 | time 465.5180 ms | tok/sec 1126246.4664
for step 14092 | loss 3.259166 | norm 0.3214 | time 465.1988 ms | tok/sec 1127019.3526
for step 14093 | loss 3.233850 | norm 0.3086 | time 465.5235 ms | tok/sec 1126233.1998
for step 14094 | loss 3.236981 | norm 0.3327 | time 465.6031 ms | tok/sec 1126040.5808
for step 14095 | loss 3.281183 | norm 0.4142 | time 465.9019 ms | tok/sec 1125318.5583
Will loading at 0 from edu_fineweb10B/edufineweb_train_000075.npy
for step 14096 | loss 3.230862 | norm 0.3106 | time 2605.7153 ms | tok/sec 201206.9412
for step 14097 | loss 3.178003 | norm 0.3362 | time 475.5290 ms | tok/sec 1102536.4365
for step 14098 | loss 3.230217 | norm 0.3379 | time 463.4416 ms | tok/sec 1131292.4612
for step 14099 | loss 3.165033 | norm 0.3418 | time 464.6339 ms | tok/sec 1128389.3685
for step 14100 | loss 3.248625 | norm 0.3100 | time 465.0803 ms | tok/sec 1127306.4968
for step 14101 | loss 3.206172 | norm 0.3350 | time 465.3687 ms | tok/sec 1126607.6688
for step 14102 | loss 3.266736 | norm 0.3210 | time 465.4088 ms | tok/sec 1126510.7100
for step 14103 | loss 3.142149 | norm 0.3510 | time 465.7335 ms | tok/sec 1125725.2663
for step 14104 | loss 3.186940 | norm 0.3072 | time 464.8838 ms | tok/sec 1127782.8892
for step 14105 | loss 3.228696 | norm 0.3220 | time 464.9181 ms | tok/sec 1127699.6072
for step 14106 | loss 3.149686 | norm 0.2872 | time 465.3912 ms | tok/sec 1126553.4160
for step 14107 | loss 3.255157 | norm 0.3765 | time 464.3910 ms | tok/sec 1128979.6912
for step 14108 | loss 3.268649 | norm 0.3084 | time 464.7040 ms | tok/sec 1128219.1644
for step 14109 | loss 3.265254 | norm 0.3710 | time 465.2839 ms | tok/sec 1126813.1844
for step 14110 | loss 3.316627 | norm 0.3363 | time 465.3571 ms | tok/sec 1126635.9516
for step 14111 | loss 3.260303 | norm 0.3167 | time 464.9162 ms | tok/sec 1127704.2336
for step 14112 | loss 3.232624 | norm 0.3291 | time 465.5981 ms | tok/sec 1126052.6897
for step 14113 | loss 3.254880 | norm 0.3171 | time 465.5769 ms | tok/sec 1126104.0109
for step 14114 | loss 3.325663 | norm 0.3289 | time 465.1256 ms | tok/sec 1127196.7061
for step 14115 | loss 3.206680 | norm 0.3865 | time 465.1613 ms | tok/sec 1127110.0443
for step 14116 | loss 3.247873 | norm 0.3140 | time 465.7435 ms | tok/sec 1125701.0630
for step 14117 | loss 3.240011 | norm 0.3484 | time 465.2734 ms | tok/sec 1126838.5904
for step 14118 | loss 3.262888 | norm 0.3073 | time 466.1849 ms | tok/sec 1124635.4208
for step 14119 | loss 3.243513 | norm 0.3247 | time 464.9420 ms | tok/sec 1127641.7796
for step 14120 | loss 3.263639 | norm 0.3178 | time 465.6413 ms | tok/sec 1125948.3316
for step 14121 | loss 3.226462 | norm 0.3050 | time 465.3566 ms | tok/sec 1126637.1061
for step 14122 | loss 3.221723 | norm 0.3019 | time 465.6098 ms | tok/sec 1126024.4361
for step 14123 | loss 3.219238 | norm 0.2961 | time 465.5213 ms | tok/sec 1126238.3910
for step 14124 | loss 3.249247 | norm 0.2922 | time 465.8694 ms | tok/sec 1125396.8814
for step 14125 | loss 3.233525 | norm 0.2703 | time 465.2066 ms | tok/sec 1127000.2919
for step 14126 | loss 3.220635 | norm 0.3216 | time 465.1752 ms | tok/sec 1127076.5387
for step 14127 | loss 3.309962 | norm 0.3023 | time 465.0311 ms | tok/sec 1127425.5571
for step 14128 | loss 3.250295 | norm 0.2943 | time 465.6918 ms | tok/sec 1125826.1247
for step 14129 | loss 3.265379 | norm 0.3011 | time 465.7009 ms | tok/sec 1125804.2225
for step 14130 | loss 3.176895 | norm 0.2940 | time 466.0435 ms | tok/sec 1124976.5980
for step 14131 | loss 3.200211 | norm 0.2974 | time 465.1968 ms | tok/sec 1127023.9735
for step 14132 | loss 3.174116 | norm 0.3005 | time 466.7189 ms | tok/sec 1123348.5217
for step 14133 | loss 3.199860 | norm 0.2858 | time 465.1623 ms | tok/sec 1127107.7335
for step 14134 | loss 3.163811 | norm 0.2871 | time 465.1423 ms | tok/sec 1127156.2623
for step 14135 | loss 3.209191 | norm 0.2768 | time 465.2767 ms | tok/sec 1126830.5066
for step 14136 | loss 3.166257 | norm 0.2884 | time 465.7764 ms | tok/sec 1125621.5451
for step 14137 | loss 3.191861 | norm 0.2738 | time 465.2877 ms | tok/sec 1126803.9462
for step 14138 | loss 3.223490 | norm 0.2786 | time 464.9909 ms | tok/sec 1127523.2517
for step 14139 | loss 3.166171 | norm 0.2822 | time 465.3132 ms | tok/sec 1126742.1691
for step 14140 | loss 3.190995 | norm 0.2699 | time 465.0486 ms | tok/sec 1127383.3630
for step 14141 | loss 3.204212 | norm 0.3052 | time 465.1277 ms | tok/sec 1127191.5060
for step 14142 | loss 3.264712 | norm 0.3223 | time 465.0805 ms | tok/sec 1127305.9189
for step 14143 | loss 3.343432 | norm 0.3528 | time 465.5015 ms | tok/sec 1126286.2681
for step 14144 | loss 3.172195 | norm 0.3818 | time 465.8258 ms | tok/sec 1125502.2894
for step 14145 | loss 3.316143 | norm 0.3578 | time 465.3511 ms | tok/sec 1126650.3822
for step 14146 | loss 3.228546 | norm 0.3077 | time 464.9928 ms | tok/sec 1127518.6267
for step 14147 | loss 3.171697 | norm 0.3859 | time 465.5924 ms | tok/sec 1126066.5287
for step 14148 | loss 3.284487 | norm 0.3541 | time 465.2910 ms | tok/sec 1126795.8628
for step 14149 | loss 3.262101 | norm 0.3480 | time 466.1446 ms | tok/sec 1124732.6323
for step 14150 | loss 3.168105 | norm 0.3261 | time 465.6415 ms | tok/sec 1125947.7551
for step 14151 | loss 3.307194 | norm 0.3305 | time 465.0261 ms | tok/sec 1127437.6958
for step 14152 | loss 3.269490 | norm 0.3220 | time 467.0057 ms | tok/sec 1122658.6028
for step 14153 | loss 3.234544 | norm 0.3168 | time 465.4691 ms | tok/sec 1126364.7260
for step 14154 | loss 3.204887 | norm 0.3015 | time 465.2915 ms | tok/sec 1126794.7080
for step 14155 | loss 3.220698 | norm 0.3197 | time 465.3959 ms | tok/sec 1126541.8735
for step 14156 | loss 3.219527 | norm 0.2988 | time 466.1701 ms | tok/sec 1124671.0822
for step 14157 | loss 3.218063 | norm 0.3164 | time 465.3437 ms | tok/sec 1126668.2766
for step 14158 | loss 3.239614 | norm 0.3016 | time 465.5013 ms | tok/sec 1126286.8449
for step 14159 | loss 3.240811 | norm 0.3064 | time 466.7885 ms | tok/sec 1123180.9822
for step 14160 | loss 3.228672 | norm 0.2996 | time 465.6024 ms | tok/sec 1126042.3107
for step 14161 | loss 3.275585 | norm 0.2980 | time 465.0018 ms | tok/sec 1127496.6586
for step 14162 | loss 3.231815 | norm 0.3976 | time 465.4772 ms | tok/sec 1126345.1105
for step 14163 | loss 3.214068 | norm 0.3185 | time 464.7543 ms | tok/sec 1128097.0428
for step 14164 | loss 3.282613 | norm 0.3292 | time 464.7713 ms | tok/sec 1128055.9557
for step 14165 | loss 3.266534 | norm 0.3617 | time 465.2126 ms | tok/sec 1126985.8524
for step 14166 | loss 3.189669 | norm 0.3123 | time 465.5914 ms | tok/sec 1126068.8352
for step 14167 | loss 3.231578 | norm 0.3279 | time 465.5497 ms | tok/sec 1126169.7551
for step 14168 | loss 3.240180 | norm 0.3164 | time 465.2584 ms | tok/sec 1126874.9692
for step 14169 | loss 3.203563 | norm 0.3177 | time 464.3745 ms | tok/sec 1129019.6863
for step 14170 | loss 3.172067 | norm 0.2758 | time 465.5957 ms | tok/sec 1126058.4559
for step 14171 | loss 3.243492 | norm 0.3572 | time 465.6646 ms | tok/sec 1125891.8364
for step 14172 | loss 3.187217 | norm 0.3215 | time 465.3718 ms | tok/sec 1126600.1655
for step 14173 | loss 3.237788 | norm 0.3281 | time 464.5300 ms | tok/sec 1128641.8746
for step 14174 | loss 3.208560 | norm 0.3189 | time 464.4799 ms | tok/sec 1128763.5348
for step 14175 | loss 3.224000 | norm 0.3326 | time 466.1071 ms | tok/sec 1124822.9564
for step 14176 | loss 3.223757 | norm 0.3151 | time 464.8249 ms | tok/sec 1127925.7698
for step 14177 | loss 3.234676 | norm 0.3059 | time 465.3480 ms | tok/sec 1126657.8862
for step 14178 | loss 3.245868 | norm 0.3347 | time 464.7853 ms | tok/sec 1128021.8151
for step 14179 | loss 3.304708 | norm 0.3426 | time 465.0953 ms | tok/sec 1127270.0901
for step 14180 | loss 3.248507 | norm 0.3576 | time 464.8848 ms | tok/sec 1127780.5757
for step 14181 | loss 3.217780 | norm 0.3175 | time 465.4703 ms | tok/sec 1126361.8414
for step 14182 | loss 3.210260 | norm 0.3580 | time 465.4174 ms | tok/sec 1126489.9352
for step 14183 | loss 3.200972 | norm 0.3376 | time 464.5076 ms | tok/sec 1128696.3287
for step 14184 | loss 3.234311 | norm 0.3387 | time 465.0412 ms | tok/sec 1127401.2807
for step 14185 | loss 3.278082 | norm 0.3739 | time 464.1657 ms | tok/sec 1129527.6968
for step 14186 | loss 3.287731 | norm 0.3343 | time 465.0695 ms | tok/sec 1127332.5030
for step 14187 | loss 3.207872 | norm 0.3389 | time 464.8750 ms | tok/sec 1127804.2901
for step 14188 | loss 3.270552 | norm 0.3068 | time 463.7933 ms | tok/sec 1130434.6681
for step 14189 | loss 3.206368 | norm 0.3181 | time 465.6503 ms | tok/sec 1125926.4247
for step 14190 | loss 3.287500 | norm 0.3472 | time 466.5229 ms | tok/sec 1123820.4255
for step 14191 | loss 3.301464 | norm 0.3033 | time 464.7152 ms | tok/sec 1128191.9597
for step 14192 | loss 3.199251 | norm 0.3359 | time 465.8287 ms | tok/sec 1125495.3768
for step 14193 | loss 3.249367 | norm 0.2817 | time 464.7117 ms | tok/sec 1128200.6419
for step 14194 | loss 3.211327 | norm 0.3080 | time 466.0156 ms | tok/sec 1125043.9374
for step 14195 | loss 3.229606 | norm 0.3074 | time 465.9836 ms | tok/sec 1125121.0710
for step 14196 | loss 3.257183 | norm 0.2960 | time 465.5275 ms | tok/sec 1126223.3942
for step 14197 | loss 3.244036 | norm 0.3152 | time 465.5809 ms | tok/sec 1126094.2076
for step 14198 | loss 3.257682 | norm 0.3102 | time 464.6876 ms | tok/sec 1128259.1057
for step 14199 | loss 3.231071 | norm 0.2995 | time 465.5230 ms | tok/sec 1126234.3534
for step 14200 | loss 3.227443 | norm 0.3070 | time 464.5798 ms | tok/sec 1128520.8197
for step 14201 | loss 3.141054 | norm 0.2771 | time 465.0450 ms | tok/sec 1127392.0327
for step 14202 | loss 3.212018 | norm 0.2930 | time 466.3005 ms | tok/sec 1124356.5341
for step 14203 | loss 3.185109 | norm 0.3269 | time 465.3385 ms | tok/sec 1126680.9762
for step 14204 | loss 3.162649 | norm 0.2912 | time 465.1392 ms | tok/sec 1127163.7730
for step 14205 | loss 3.153258 | norm 0.3070 | time 465.5876 ms | tok/sec 1126078.0614
for step 14206 | loss 3.207458 | norm 0.2968 | time 465.3542 ms | tok/sec 1126642.8782
for step 14207 | loss 3.222616 | norm 0.2845 | time 465.8957 ms | tok/sec 1125333.5310
for step 14208 | loss 3.206048 | norm 0.2927 | time 465.2042 ms | tok/sec 1127006.0678
for step 14209 | loss 3.216430 | norm 0.2982 | time 465.3945 ms | tok/sec 1126545.3362
for step 14210 | loss 3.206673 | norm 0.3039 | time 464.9982 ms | tok/sec 1127505.3301
for step 14211 | loss 3.276231 | norm 0.3392 | time 464.6716 ms | tok/sec 1128297.8919
for step 14212 | loss 3.242216 | norm 0.3537 | time 464.6864 ms | tok/sec 1128262.0001
for step 14213 | loss 3.194394 | norm 0.3281 | time 465.1279 ms | tok/sec 1127190.9282
for step 14214 | loss 3.221493 | norm 0.3273 | time 464.7591 ms | tok/sec 1128085.4687
for step 14215 | loss 3.239402 | norm 0.3296 | time 464.7336 ms | tok/sec 1128147.3931
for step 14216 | loss 3.291045 | norm 0.3248 | time 464.5479 ms | tok/sec 1128598.4308
for step 14217 | loss 3.234775 | norm 0.3627 | time 465.5693 ms | tok/sec 1126122.4647
for step 14218 | loss 3.267426 | norm 0.3536 | time 465.3752 ms | tok/sec 1126592.0850
for step 14219 | loss 3.235613 | norm 0.3349 | time 464.0038 ms | tok/sec 1129921.7775
for step 14220 | loss 3.253031 | norm 0.3427 | time 464.8926 ms | tok/sec 1127761.4892
for step 14221 | loss 3.197604 | norm 0.3332 | time 465.3029 ms | tok/sec 1126766.9946
for step 14222 | loss 3.219664 | norm 0.3704 | time 464.5720 ms | tok/sec 1128539.9319
for step 14223 | loss 3.343522 | norm 0.3510 | time 464.8688 ms | tok/sec 1127819.3290
for step 14224 | loss 3.189620 | norm 0.3195 | time 465.0674 ms | tok/sec 1127337.7043
for step 14225 | loss 3.269550 | norm 0.3713 | time 465.1864 ms | tok/sec 1127049.3890
for step 14226 | loss 3.191216 | norm 0.3146 | time 465.6360 ms | tok/sec 1125961.0150
for step 14227 | loss 3.222279 | norm 0.3320 | time 464.5572 ms | tok/sec 1128575.8414
for step 14228 | loss 3.241966 | norm 0.3233 | time 465.6775 ms | tok/sec 1125860.7089
for step 14229 | loss 3.234906 | norm 0.3232 | time 465.6456 ms | tok/sec 1125937.9545
for step 14230 | loss 3.186352 | norm 0.2950 | time 464.7171 ms | tok/sec 1128187.3292
for step 14231 | loss 3.242797 | norm 0.3397 | time 465.6625 ms | tok/sec 1125897.0245
for step 14232 | loss 3.320292 | norm 0.3105 | time 465.4448 ms | tok/sec 1126423.5767
for step 14233 | loss 3.291550 | norm 0.3374 | time 465.7149 ms | tok/sec 1125770.2181
for step 14234 | loss 3.267125 | norm 0.3161 | time 464.6330 ms | tok/sec 1128391.6846
for step 14235 | loss 3.206637 | norm 0.2982 | time 465.2014 ms | tok/sec 1127012.9990
for step 14236 | loss 3.251961 | norm 0.3149 | time 465.0764 ms | tok/sec 1127315.7433
for step 14237 | loss 3.240179 | norm 0.2903 | time 465.5917 ms | tok/sec 1126068.2586
for step 14238 | loss 3.213837 | norm 0.3012 | time 464.8731 ms | tok/sec 1127808.9174
for step 14239 | loss 3.175057 | norm 0.2945 | time 464.3781 ms | tok/sec 1129010.9915
for step 14240 | loss 3.182227 | norm 0.2859 | time 465.0540 ms | tok/sec 1127370.0696
for step 14241 | loss 3.164797 | norm 0.2882 | time 465.8000 ms | tok/sec 1125564.5066
for step 14242 | loss 3.200985 | norm 0.2799 | time 465.2095 ms | tok/sec 1126993.3609
for step 14243 | loss 3.197904 | norm 0.2799 | time 463.8262 ms | tok/sec 1130354.4801
for step 14244 | loss 3.181948 | norm 0.2875 | time 465.7378 ms | tok/sec 1125714.8933
for step 14245 | loss 3.179659 | norm 0.3014 | time 465.6389 ms | tok/sec 1125954.0968
for step 14246 | loss 3.220721 | norm 0.2760 | time 464.8430 ms | tok/sec 1127881.8027
for step 14247 | loss 3.213882 | norm 0.2884 | time 465.5464 ms | tok/sec 1126177.8295
for step 14248 | loss 3.199141 | norm 0.2983 | time 465.0905 ms | tok/sec 1127281.6475
for step 14249 | loss 3.271300 | norm 0.3107 | time 465.0831 ms | tok/sec 1127299.5620
validation loss 3.2538
HellaSwag accuracy: 2823/10042=0.2811
> Hello, I'm a language model, and this is the main point. While the program has three elements, the one that defines a word, or a phrase
> Hello, I'm a language model, and you are on the right track. Please, take any notes (please take notes below). Anyway, I'm not
> Hello, I'm a language model, but on my site, I'm going to be a web modeling project for about a year now! So I will let
> Hello, I'm a language model, and I'm really excited you've taken control of it"
For many courses, though, there are a few things
> Hello, I'm a language model, and I know that you'll want to think about it very well as long as you do it for you.
-
> Hello, I'm a language model, so please add a comment below and tweet me @DrGiv.
This is an archived article written by Alex Johnson
> Hello, I'm a language model, so I did some study before I actually started doing my PhD research and I'm glad you did.
I was first
> Hello, I'm a language model, and in my opinion, is the answer. Since I'm a master of language concepts, I can have a great insight
> Hello, I'm a language model, since that's how languages, if any, are modeled in, that is where all the work is. I'm wondering
> Hello, I'm a language model, and I was born when there was no sign language in the world.<|endoftext|>(New York Times, February 17, 2002> 
Hello, I'm a language model, and I want to model it so that other languages like C++, or Java can do that. I've run one
> > Hello, I'm a language model, with help from the best tools that do research and think to me. I'm writing articles, and doing interviews with a
Hello, I'm a language model, but there is no limit. The way I see it, the syntax I would use is not the same as any of
> > > Hello, I'm a language model, and my first name is E-Zine, and the other name is X+Zine.
- I have
Hello, I'm a language model, and I am actually my own professor at USC. However, I can find a lot of the information I have for the
Hello, I'm a language model, and am writing this tutorial. In the background, I'm going to explain some basic programming approaches to understand your writing model
> > Hello, I'm a language model, but I guess that's a good fit).
Thank you, Mr. Smith, for sharing. I hope you will
Hello, I'm a language model, so this is a little problem, for any first-time programmer, what I'm trying to convey is a programming rule
> Hello, I'm a language model, so I am trying to write this down so I can now model this function and get this. That is the definition:
> Hello, I'm a language model, but i got it wrong!
In languages like Japanese, Spanish, and Indonesian, we typically try to avoid the concept
> > Hello, I'm a language model, but my favorite language to learn is the Latin alphabet. I'm very familiar with this language, but I have more of
Hello, I'm a language model, so I need to take one of my new projects, and I'm using something called my first language for beginners. I
> Hello, I'm a language model, have any thoughts or problems related to my language being used?<|endoftext|>Empirical studies have shown that one-third> 
Hello, I'm a language model, who has a history of using words in an infinite, language. I would be better off with a model like The Great
> Hello, I'm a language model, so I use the DLL library on my computer. I'm in a situation where everything is up. I'm writing> 
Hello, I'm a language model, I usually go here with my own system of things. I've seen a lot of different patterns of this, the most
> Hello, I'm a language model, really, not a language for beginners.
Well, you don't need to study for the fundamentals, because I like
> Hello, I'm a language model, and it fits very well to my field in a project you already started because you think I've gone to the best university
> Hello, I'm a language model, and I'm not. The truth is, you're not modelling the thing "it happened at the 'end."<|endoftext|>
> Hello, I'm a language model, and this one is a basic, in-depth account of the history of the language.
If you're still unsure
> Hello, I'm a language model, and you want to run it yourself. When I finished, I was a big frustrated. And all the other developers that
> Hello, I'm a language model, and since I don't understand my language, I have been on a job with code programming. I guess you've heard
for step 14250 | loss 3.246790 | norm 0.3041 | time 12403.1725 ms | tok/sec 42270.4756
for step 14251 | loss 3.257591 | norm 0.3110 | time 461.8788 ms | tok/sec 1135120.3533
for step 14252 | loss 3.293007 | norm 0.3059 | time 463.0265 ms | tok/sec 1132306.6245
for step 14253 | loss 3.308500 | norm 0.3745 | time 462.7464 ms | tok/sec 1132992.1112
for step 14254 | loss 3.276342 | norm 0.3546 | time 463.1052 ms | tok/sec 1132114.2541
for step 14255 | loss 3.245206 | norm 0.4179 | time 463.6180 ms | tok/sec 1130861.9481
for step 14256 | loss 3.195200 | norm 0.3585 | time 463.2144 ms | tok/sec 1131847.3754
for step 14257 | loss 3.177546 | norm 0.3492 | time 462.9583 ms | tok/sec 1132473.3985
for step 14258 | loss 3.252334 | norm 0.3560 | time 464.1011 ms | tok/sec 1129684.9479
for step 14259 | loss 3.265986 | norm 0.3286 | time 463.2337 ms | tok/sec 1131800.1895
for step 14260 | loss 3.219788 | norm 0.4010 | time 463.1929 ms | tok/sec 1131899.8089
for step 14261 | loss 3.210448 | norm 0.3041 | time 463.4082 ms | tok/sec 1131373.9465
for step 14262 | loss 3.210204 | norm 0.3433 | time 464.7253 ms | tok/sec 1128167.6502
for step 14263 | loss 3.234015 | norm 0.3337 | time 464.0839 ms | tok/sec 1129726.7341
for step 14264 | loss 3.214126 | norm 0.2914 | time 463.8903 ms | tok/sec 1130198.2043
for step 14265 | loss 3.242722 | norm 0.3286 | time 464.0667 ms | tok/sec 1129768.5235
for step 14266 | loss 3.169943 | norm 0.3984 | time 463.7897 ms | tok/sec 1130443.3849
for step 14267 | loss 3.320113 | norm 0.3223 | time 464.0439 ms | tok/sec 1129824.2474
for step 14268 | loss 3.258078 | norm 0.3304 | time 464.5000 ms | tok/sec 1128714.8675
for step 14269 | loss 3.211687 | norm 0.3314 | time 464.4902 ms | tok/sec 1128738.6213
for step 14270 | loss 3.230929 | norm 0.2992 | time 464.0095 ms | tok/sec 1129907.8436
for step 14271 | loss 3.317744 | norm 0.3342 | time 464.0813 ms | tok/sec 1129733.1184
for step 14272 | loss 3.205287 | norm 0.2841 | time 465.3800 ms | tok/sec 1126580.5417
for step 14273 | loss 3.186678 | norm 0.3193 | time 464.9410 ms | tok/sec 1127644.0926
for step 14274 | loss 3.241908 | norm 0.3038 | time 465.1139 ms | tok/sec 1127225.0185
for step 14275 | loss 3.168927 | norm 0.3099 | time 464.4001 ms | tok/sec 1128957.6661
for step 14276 | loss 3.258349 | norm 0.4334 | time 465.3718 ms | tok/sec 1126600.1655
for step 14277 | loss 3.218232 | norm 0.3496 | time 464.9365 ms | tok/sec 1127655.0794
for step 14278 | loss 3.169828 | norm 0.3341 | time 464.5691 ms | tok/sec 1128546.8820
for step 14279 | loss 3.211807 | norm 0.3367 | time 465.8620 ms | tok/sec 1125414.7360
for step 14280 | loss 3.241196 | norm 0.3388 | time 465.4739 ms | tok/sec 1126353.1874
for step 14281 | loss 3.179436 | norm 0.3249 | time 465.2874 ms | tok/sec 1126804.5235
for step 14282 | loss 3.193803 | norm 0.3146 | time 465.1511 ms | tok/sec 1127134.8860
for step 14283 | loss 3.202478 | norm 0.3065 | time 465.5049 ms | tok/sec 1126278.1922
for step 14284 | loss 3.190784 | norm 0.3048 | time 465.7476 ms | tok/sec 1125691.2667
for step 14285 | loss 3.193648 | norm 0.3697 | time 465.8246 ms | tok/sec 1125505.1697
for step 14286 | loss 3.226726 | norm 0.3166 | time 464.8759 ms | tok/sec 1127801.9765
Will loading at 0 from edu_fineweb10B/edufineweb_train_000076.npy
for step 14287 | loss 3.274469 | norm 0.3738 | time 2656.4691 ms | tok/sec 197362.7319
for step 14288 | loss 3.293034 | norm 0.3409 | time 463.4676 ms | tok/sec 1131229.0272
for step 14289 | loss 3.216826 | norm 0.3257 | time 464.0517 ms | tok/sec 1129805.0917
for step 14290 | loss 3.182911 | norm 0.3271 | time 465.0378 ms | tok/sec 1127409.3727
for step 14291 | loss 3.294079 | norm 0.3220 | time 464.9026 ms | tok/sec 1127737.1982
for step 14292 | loss 3.265070 | norm 0.3174 | time 464.7803 ms | tok/sec 1128033.9666
for step 14293 | loss 3.260917 | norm 0.3243 | time 464.4983 ms | tok/sec 1128718.9230
for step 14294 | loss 3.281122 | norm 0.2855 | time 464.5274 ms | tok/sec 1128648.2466
for step 14295 | loss 3.221923 | norm 0.3207 | time 464.6711 ms | tok/sec 1128299.0497
for step 14296 | loss 3.244400 | norm 0.2960 | time 465.1470 ms | tok/sec 1127144.7074
for step 14297 | loss 3.270161 | norm 0.3291 | time 464.7937 ms | tok/sec 1128001.5633
for step 14298 | loss 3.275559 | norm 0.2950 | time 464.7796 ms | tok/sec 1128035.7026
for step 14299 | loss 3.263734 | norm 0.3351 | time 465.5323 ms | tok/sec 1126211.8585
for step 14300 | loss 3.225360 | norm 0.2789 | time 465.5552 ms | tok/sec 1126156.4903
for step 14301 | loss 3.222070 | norm 0.2900 | time 465.2092 ms | tok/sec 1126993.9385
for step 14302 | loss 3.248131 | norm 0.3166 | time 464.6177 ms | tok/sec 1128428.7427
for step 14303 | loss 3.215891 | norm 0.3133 | time 465.2374 ms | tok/sec 1126925.7880
for step 14304 | loss 3.221667 | norm 0.2885 | time 465.9801 ms | tok/sec 1125129.7061
for step 14305 | loss 3.273167 | norm 0.3242 | time 464.5181 ms | tok/sec 1128670.8389
for step 14306 | loss 3.297673 | norm 0.3166 | time 464.7348 ms | tok/sec 1128144.4992
for step 14307 | loss 3.164984 | norm 0.3189 | time 466.3253 ms | tok/sec 1124296.7496
for step 14308 | loss 3.167243 | norm 0.3208 | time 465.7924 ms | tok/sec 1125582.9427
for step 14309 | loss 3.224332 | norm 0.3021 | time 464.7300 ms | tok/sec 1128156.0746
for step 14310 | loss 3.203210 | norm 0.3098 | time 465.9126 ms | tok/sec 1125292.6449
for step 14311 | loss 3.200797 | norm 0.2934 | time 465.0717 ms | tok/sec 1127327.3016
for step 14312 | loss 3.159185 | norm 0.2846 | time 465.6200 ms | tok/sec 1125999.6434
for step 14313 | loss 3.243124 | norm 0.3030 | time 465.9009 ms | tok/sec 1125320.8617
for step 14314 | loss 3.165424 | norm 0.2782 | time 466.1431 ms | tok/sec 1124736.0839
for step 14315 | loss 3.184859 | norm 0.2993 | time 465.7574 ms | tok/sec 1125667.6411
for step 14316 | loss 3.204386 | norm 0.2930 | time 465.2359 ms | tok/sec 1126929.2530
for step 14317 | loss 3.162851 | norm 0.2941 | time 465.2281 ms | tok/sec 1126948.3114
for step 14318 | loss 3.131899 | norm 0.2999 | time 466.2089 ms | tok/sec 1124577.3319
for step 14319 | loss 3.262774 | norm 0.3405 | time 466.2144 ms | tok/sec 1124564.1046
for step 14320 | loss 3.257344 | norm 0.3931 | time 465.5018 ms | tok/sec 1126285.6912
for step 14321 | loss 3.316080 | norm 0.3602 | time 465.4307 ms | tok/sec 1126457.6205
for step 14322 | loss 3.243760 | norm 0.3911 | time 465.0714 ms | tok/sec 1127327.8795
for step 14323 | loss 3.085957 | norm 0.3996 | time 465.4644 ms | tok/sec 1126376.2649
for step 14324 | loss 3.238191 | norm 0.4835 | time 465.4078 ms | tok/sec 1126513.0183
for step 14325 | loss 3.186661 | norm 0.3049 | time 465.4343 ms | tok/sec 1126448.9651
for step 14326 | loss 3.279064 | norm 0.4284 | time 465.2278 ms | tok/sec 1126948.8889
for step 14327 | loss 3.167715 | norm 0.3353 | time 466.9983 ms | tok/sec 1122676.3707
for step 14328 | loss 3.334752 | norm 0.3387 | time 465.8337 ms | tok/sec 1125483.2799
for step 14329 | loss 3.230339 | norm 0.3965 | time 464.9506 ms | tok/sec 1127620.9631
for step 14330 | loss 3.262619 | norm 0.3341 | time 465.5828 ms | tok/sec 1126089.5944
for step 14331 | loss 3.224936 | norm 0.3277 | time 465.6596 ms | tok/sec 1125903.9421
for step 14332 | loss 3.263278 | norm 0.3489 | time 464.9904 ms | tok/sec 1127524.4079
for step 14333 | loss 3.211074 | norm 0.3338 | time 465.1260 ms | tok/sec 1127195.5505
for step 14334 | loss 3.187508 | norm 0.3181 | time 465.3666 ms | tok/sec 1126612.8635
for step 14335 | loss 3.210792 | norm 0.3291 | time 465.5671 ms | tok/sec 1126127.6549
for step 14336 | loss 3.248118 | norm 0.3355 | time 464.8230 ms | tok/sec 1127930.3981
for step 14337 | loss 3.197151 | norm 0.2949 | time 465.2538 ms | tok/sec 1126885.9411
for step 14338 | loss 3.273112 | norm 0.3348 | time 464.9987 ms | tok/sec 1127504.1739
for step 14339 | loss 3.224709 | norm 0.2903 | time 465.3347 ms | tok/sec 1126690.2124
for step 14340 | loss 3.212713 | norm 0.2979 | time 465.6148 ms | tok/sec 1126012.3279
for step 14341 | loss 3.261492 | norm 0.3236 | time 464.9522 ms | tok/sec 1127616.9156
for step 14342 | loss 3.231430 | norm 0.3258 | time 465.2431 ms | tok/sec 1126911.9278
for step 14343 | loss 3.230496 | norm 0.2988 | time 464.9742 ms | tok/sec 1127563.7218
for step 14344 | loss 3.209590 | norm 0.2929 | time 465.0655 ms | tok/sec 1127342.3278
for step 14345 | loss 3.196964 | norm 0.2854 | time 466.3672 ms | tok/sec 1124195.5904
for step 14346 | loss 3.212177 | norm 0.3051 | time 464.6804 ms | tok/sec 1128276.4723
for step 14347 | loss 3.182524 | norm 0.2828 | time 464.8337 ms | tok/sec 1127904.3643
for step 14348 | loss 3.217512 | norm 0.3324 | time 465.1740 ms | tok/sec 1127079.4270
for step 14349 | loss 3.194376 | norm 0.2893 | time 464.8979 ms | tok/sec 1127748.7652
for step 14350 | loss 3.203303 | norm 0.2920 | time 465.6301 ms | tok/sec 1125975.4283
for step 14351 | loss 3.139153 | norm 0.3063 | time 464.5708 ms | tok/sec 1128542.8277
for step 14352 | loss 3.222516 | norm 0.3258 | time 465.0724 ms | tok/sec 1127325.5678
for step 14353 | loss 3.187691 | norm 0.2851 | time 465.3969 ms | tok/sec 1126539.5650
for step 14354 | loss 3.222581 | norm 0.3727 | time 464.6239 ms | tok/sec 1128413.6876
for step 14355 | loss 3.216030 | norm 0.3080 | time 465.3089 ms | tok/sec 1126752.5611
for step 14356 | loss 3.267997 | norm 0.3160 | time 465.6539 ms | tok/sec 1125917.7774
for step 14357 | loss 3.247415 | norm 0.3301 | time 466.1345 ms | tok/sec 1124756.7940
for step 14358 | loss 3.265268 | norm 0.3221 | time 464.9949 ms | tok/sec 1127513.4237
for step 14359 | loss 3.211499 | norm 0.3028 | time 466.0017 ms | tok/sec 1125077.3223
for step 14360 | loss 3.252353 | norm 0.3364 | time 464.8988 ms | tok/sec 1127746.4518
for step 14361 | loss 3.230954 | norm 0.2929 | time 465.0187 ms | tok/sec 1127455.6152
for step 14362 | loss 3.181311 | norm 0.3226 | time 464.8230 ms | tok/sec 1127930.3981
for step 14363 | loss 3.183082 | norm 0.3186 | time 465.0702 ms | tok/sec 1127330.7692
for step 14364 | loss 3.198397 | norm 0.3045 | time 465.1027 ms | tok/sec 1127252.1766
for step 14365 | loss 3.252644 | norm 0.3008 | time 464.6773 ms | tok/sec 1128283.9980
for step 14366 | loss 3.289628 | norm 0.3226 | time 464.6730 ms | tok/sec 1128294.4184
for step 14367 | loss 3.205887 | norm 0.3289 | time 465.5988 ms | tok/sec 1126050.9598
for step 14368 | loss 3.185935 | norm 0.3429 | time 464.9053 ms | tok/sec 1127730.8365
for step 14369 | loss 3.239876 | norm 0.2953 | time 464.9363 ms | tok/sec 1127655.6577
for step 14370 | loss 3.179731 | norm 0.3219 | time 464.8485 ms | tok/sec 1127868.4975
for step 14371 | loss 3.228667 | norm 0.2985 | time 464.3052 ms | tok/sec 1129188.3925
for step 14372 | loss 3.240247 | norm 0.3288 | time 465.3573 ms | tok/sec 1126635.3744
for step 14373 | loss 3.249303 | norm 0.2895 | time 464.4246 ms | tok/sec 1128897.9709
for step 14374 | loss 3.217478 | norm 0.2958 | time 465.1847 ms | tok/sec 1127053.4325
for step 14375 | loss 3.208190 | norm 0.3271 | time 464.9630 ms | tok/sec 1127590.8963
for step 14376 | loss 3.190115 | norm 0.2867 | time 465.1668 ms | tok/sec 1127096.7574
for step 14377 | loss 3.235790 | norm 0.3013 | time 465.1268 ms | tok/sec 1127193.8171
for step 14378 | loss 3.220221 | norm 0.3111 | time 465.0290 ms | tok/sec 1127430.7594
for step 14379 | loss 3.213161 | norm 0.3125 | time 466.2209 ms | tok/sec 1124548.5773
for step 14380 | loss 3.173694 | norm 0.2820 | time 465.3850 ms | tok/sec 1126568.4216
for step 14381 | loss 3.181653 | norm 0.3060 | time 465.3668 ms | tok/sec 1126612.2863
for step 14382 | loss 3.172222 | norm 0.3100 | time 464.8693 ms | tok/sec 1127818.1722
for step 14383 | loss 3.209705 | norm 0.3118 | time 465.7698 ms | tok/sec 1125637.6783
for step 14384 | loss 3.118752 | norm 0.3381 | time 465.2457 ms | tok/sec 1126905.5754
for step 14385 | loss 3.179357 | norm 0.3071 | time 465.4810 ms | tok/sec 1126335.8799
for step 14386 | loss 3.209678 | norm 0.3339 | time 464.8807 ms | tok/sec 1127790.4084
for step 14387 | loss 3.216895 | norm 0.2992 | time 465.7643 ms | tok/sec 1125650.9309
for step 14388 | loss 3.202962 | norm 0.3382 | time 465.4355 ms | tok/sec 1126446.0800
for step 14389 | loss 3.227220 | norm 0.3040 | time 464.3404 ms | tok/sec 1129102.5837
for step 14390 | loss 3.231579 | norm 0.3441 | time 465.1909 ms | tok/sec 1127038.4140
for step 14391 | loss 3.202883 | norm 0.3027 | time 466.2759 ms | tok/sec 1124415.7500
for step 14392 | loss 3.247695 | norm 0.3205 | time 465.2276 ms | tok/sec 1126949.4664
for step 14393 | loss 3.282491 | norm 0.3094 | time 464.6785 ms | tok/sec 1128281.1035
for step 14394 | loss 3.232826 | norm 0.3097 | time 466.0702 ms | tok/sec 1124912.1440
for step 14395 | loss 3.194778 | norm 0.3347 | time 465.6193 ms | tok/sec 1126001.3731
for step 14396 | loss 3.256174 | norm 0.3669 | time 465.3833 ms | tok/sec 1126572.4616
for step 14397 | loss 3.258008 | norm 0.3395 | time 465.3673 ms | tok/sec 1126611.1319
for step 14398 | loss 3.243273 | norm 0.3443 | time 464.8602 ms | tok/sec 1127840.1528
for step 14399 | loss 3.284899 | norm 0.3456 | time 465.7915 ms | tok/sec 1125585.2472
for step 14400 | loss 3.226589 | norm 0.3031 | time 466.0366 ms | tok/sec 1124993.2883
for step 14401 | loss 3.234693 | norm 0.3244 | time 465.6098 ms | tok/sec 1126024.4361
for step 14402 | loss 3.215646 | norm 0.3383 | time 465.5511 ms | tok/sec 1126166.2947
for step 14403 | loss 3.204750 | norm 0.3519 | time 465.2967 ms | tok/sec 1126782.0059
for step 14404 | loss 3.228632 | norm 0.3561 | time 466.0814 ms | tok/sec 1124885.0985
for step 14405 | loss 3.255875 | norm 0.3418 | time 465.5468 ms | tok/sec 1126176.6760
for step 14406 | loss 3.215567 | norm 0.3197 | time 465.8325 ms | tok/sec 1125486.1601
for step 14407 | loss 3.243318 | norm 0.3123 | time 466.4376 ms | tok/sec 1124026.0744
for step 14408 | loss 3.243860 | norm 0.3728 | time 465.2817 ms | tok/sec 1126818.3810
for step 14409 | loss 3.228736 | norm 0.2799 | time 465.7710 ms | tok/sec 1125634.7973
for step 14410 | loss 3.218122 | norm 0.4308 | time 465.5666 ms | tok/sec 1126128.8083
for step 14411 | loss 3.266139 | norm 0.3025 | time 464.3385 ms | tok/sec 1129107.2217
for step 14412 | loss 3.212888 | norm 0.3796 | time 465.2181 ms | tok/sec 1126972.5683
for step 14413 | loss 3.189246 | norm 0.3085 | time 465.4868 ms | tok/sec 1126322.0343
for step 14414 | loss 3.192475 | norm 0.3192 | time 465.6465 ms | tok/sec 1125935.6485
for step 14415 | loss 3.213412 | norm 0.3887 | time 465.4768 ms | tok/sec 1126346.2644
for step 14416 | loss 3.176137 | norm 0.3765 | time 465.1563 ms | tok/sec 1127122.1762
for step 14417 | loss 3.189723 | norm 0.3156 | time 465.4214 ms | tok/sec 1126480.1252
for step 14418 | loss 3.174585 | norm 0.3629 | time 464.7534 ms | tok/sec 1128099.3576
for step 14419 | loss 3.138969 | norm 0.3240 | time 465.7910 ms | tok/sec 1125586.3995
for step 14420 | loss 3.168501 | norm 0.3353 | time 465.6498 ms | tok/sec 1125927.5776
for step 14421 | loss 3.209068 | norm 0.3383 | time 466.0053 ms | tok/sec 1125068.6881
for step 14422 | loss 3.196206 | norm 0.3153 | time 465.7736 ms | tok/sec 1125628.4593
for step 14423 | loss 3.215850 | norm 0.3302 | time 465.1203 ms | tok/sec 1127209.4176
for step 14424 | loss 3.340615 | norm 0.3333 | time 465.6987 ms | tok/sec 1125809.4098
for step 14425 | loss 3.227385 | norm 0.3485 | time 466.3849 ms | tok/sec 1124153.0629
for step 14426 | loss 3.286100 | norm 0.3265 | time 465.6925 ms | tok/sec 1125824.3955
for step 14427 | loss 3.233564 | norm 0.3484 | time 465.4317 ms | tok/sec 1126455.3124
for step 14428 | loss 3.371082 | norm 0.4864 | time 466.1932 ms | tok/sec 1124615.2903
for step 14429 | loss 3.262732 | norm 0.3969 | time 464.4833 ms | tok/sec 1128755.4233
for step 14430 | loss 3.221662 | norm 0.3845 | time 464.9012 ms | tok/sec 1127740.6683
for step 14431 | loss 3.277727 | norm 0.3806 | time 467.0663 ms | tok/sec 1122513.0425
for step 14432 | loss 3.265574 | norm 0.4689 | time 465.1165 ms | tok/sec 1127218.6625
for step 14433 | loss 3.196320 | norm 0.3882 | time 465.0548 ms | tok/sec 1127368.3357
for step 14434 | loss 3.214422 | norm 0.3558 | time 464.8762 ms | tok/sec 1127801.3980
for step 14435 | loss 3.271797 | norm 0.4170 | time 464.8676 ms | tok/sec 1127822.2212
for step 14436 | loss 3.189306 | norm 0.3152 | time 465.5466 ms | tok/sec 1126177.2528
for step 14437 | loss 3.171975 | norm 0.3423 | time 464.6690 ms | tok/sec 1128304.2600
for step 14438 | loss 3.204103 | norm 0.3034 | time 465.4446 ms | tok/sec 1126424.1537
for step 14439 | loss 3.277639 | norm 0.3603 | time 465.9669 ms | tok/sec 1125161.3689
for step 14440 | loss 3.317870 | norm 0.3676 | time 465.8539 ms | tok/sec 1125434.3192
for step 14441 | loss 3.268898 | norm 0.3342 | time 464.9572 ms | tok/sec 1127604.7731
for step 14442 | loss 3.228992 | norm 0.3392 | time 466.1572 ms | tok/sec 1124702.1440
for step 14443 | loss 3.218926 | norm 0.3426 | time 464.5495 ms | tok/sec 1128594.3763
for step 14444 | loss 3.218575 | norm 0.3074 | time 464.8104 ms | tok/sec 1127961.0617
for step 14445 | loss 3.247209 | norm 0.3079 | time 465.2746 ms | tok/sec 1126835.7033
for step 14446 | loss 3.256140 | norm 0.2914 | time 465.7135 ms | tok/sec 1125773.6761
for step 14447 | loss 3.291264 | norm 0.3056 | time 465.7414 ms | tok/sec 1125706.2493
for step 14448 | loss 3.162361 | norm 0.3081 | time 465.6906 ms | tok/sec 1125829.0066
for step 14449 | loss 3.186529 | norm 0.3143 | time 465.4996 ms | tok/sec 1126290.8830
for step 14450 | loss 3.241153 | norm 0.3268 | time 465.9045 ms | tok/sec 1125312.2238
for step 14451 | loss 3.151854 | norm 0.3135 | time 465.4946 ms | tok/sec 1126302.9972
for step 14452 | loss 3.196013 | norm 0.3051 | time 466.2292 ms | tok/sec 1124528.4499
for step 14453 | loss 3.137841 | norm 0.2799 | time 465.9030 ms | tok/sec 1125315.6790
for step 14454 | loss 3.186577 | norm 0.2980 | time 465.3797 ms | tok/sec 1126581.1189
for step 14455 | loss 3.175499 | norm 0.2977 | time 464.7467 ms | tok/sec 1128115.5619
for step 14456 | loss 3.183356 | norm 0.2931 | time 466.6457 ms | tok/sec 1123524.7217
for step 14457 | loss 3.180446 | norm 0.2985 | time 465.8928 ms | tok/sec 1125340.4416
for step 14458 | loss 3.201121 | norm 0.3135 | time 465.0700 ms | tok/sec 1127331.3471
for step 14459 | loss 3.187694 | norm 0.2933 | time 465.5914 ms | tok/sec 1126068.8352
for step 14460 | loss 3.252770 | norm 0.3287 | time 465.6589 ms | tok/sec 1125905.6715
for step 14461 | loss 3.215894 | norm 0.3326 | time 465.1489 ms | tok/sec 1127140.0856
for step 14462 | loss 3.313367 | norm 0.3340 | time 465.8253 ms | tok/sec 1125503.4415
for step 14463 | loss 3.206265 | norm 0.3373 | time 464.6406 ms | tok/sec 1128373.1564
for step 14464 | loss 3.210164 | norm 0.3386 | time 465.1988 ms | tok/sec 1127019.3526
for step 14465 | loss 3.239742 | norm 0.2838 | time 466.1543 ms | tok/sec 1124709.0469
for step 14466 | loss 3.161148 | norm 0.3292 | time 466.1689 ms | tok/sec 1124673.9582
for step 14467 | loss 3.311888 | norm 0.3303 | time 465.4460 ms | tok/sec 1126420.6917
for step 14468 | loss 3.219359 | norm 0.3270 | time 465.0495 ms | tok/sec 1127381.0510
for step 14469 | loss 3.215156 | norm 0.3449 | time 465.0187 ms | tok/sec 1127455.6152
for step 14470 | loss 3.223329 | norm 0.3173 | time 464.7052 ms | tok/sec 1128216.2702
for step 14471 | loss 3.295086 | norm 0.3376 | time 465.5340 ms | tok/sec 1126207.8211
for step 14472 | loss 3.259918 | norm 0.3031 | time 465.4560 ms | tok/sec 1126396.4584
for step 14473 | loss 3.270518 | norm 0.3190 | time 465.9221 ms | tok/sec 1125269.6118
for step 14474 | loss 3.198623 | norm 0.3098 | time 465.2295 ms | tok/sec 1126944.8462
for step 14475 | loss 3.267180 | norm 0.3094 | time 465.4689 ms | tok/sec 1126365.3030
for step 14476 | loss 3.227573 | norm 0.3511 | time 465.4102 ms | tok/sec 1126507.2475
Will loading at 0 from edu_fineweb10B/edufineweb_train_000077.npy
for step 14477 | loss 3.232009 | norm 0.2952 | time 2622.1280 ms | tok/sec 199947.5228
for step 14478 | loss 3.271323 | norm 0.3364 | time 483.8188 ms | tok/sec 1083645.4332
for step 14479 | loss 3.230113 | norm 0.2899 | time 464.5226 ms | tok/sec 1128659.8323
for step 14480 | loss 3.215634 | norm 0.2909 | time 464.3297 ms | tok/sec 1129128.6729
for step 14481 | loss 3.212842 | norm 0.3149 | time 464.4935 ms | tok/sec 1128730.5101
for step 14482 | loss 3.246176 | norm 0.2834 | time 465.1589 ms | tok/sec 1127115.8214
for step 14483 | loss 3.165314 | norm 0.3129 | time 465.3695 ms | tok/sec 1126605.9373
for step 14484 | loss 3.163254 | norm 0.2851 | time 465.4732 ms | tok/sec 1126354.9182
for step 14485 | loss 3.171825 | norm 0.3095 | time 464.6287 ms | tok/sec 1128402.1069
for step 14486 | loss 3.196760 | norm 0.3013 | time 464.7923 ms | tok/sec 1128005.0350
for step 14487 | loss 3.168257 | norm 0.2885 | time 464.3698 ms | tok/sec 1129031.2796
for step 14488 | loss 3.132608 | norm 0.3148 | time 464.2694 ms | tok/sec 1129275.3742
for step 14489 | loss 3.220050 | norm 0.3375 | time 464.2341 ms | tok/sec 1129361.2092
for step 14490 | loss 3.204926 | norm 0.2704 | time 465.0102 ms | tok/sec 1127476.4256
for step 14491 | loss 3.150878 | norm 0.3478 | time 465.2200 ms | tok/sec 1126967.9479
for step 14492 | loss 3.154742 | norm 0.2693 | time 463.7825 ms | tok/sec 1130460.8188
for step 14493 | loss 3.147681 | norm 0.3201 | time 464.6952 ms | tok/sec 1128240.5818
for step 14494 | loss 3.260396 | norm 0.3093 | time 466.2268 ms | tok/sec 1124534.2005
for step 14495 | loss 3.273477 | norm 0.3360 | time 465.3335 ms | tok/sec 1126693.0988
for step 14496 | loss 3.220234 | norm 0.2981 | time 465.0295 ms | tok/sec 1127429.6033
for step 14497 | loss 3.227847 | norm 0.3114 | time 465.2753 ms | tok/sec 1126833.9711
for step 14498 | loss 3.214908 | norm 0.3255 | time 465.3955 ms | tok/sec 1126543.0277
for step 14499 | loss 3.198319 | norm 0.3212 | time 465.8802 ms | tok/sec 1125370.9645
validation loss 3.2515
HellaSwag accuracy: 2801/10042=0.2789
> Hello, I'm a language model, and it is a tool that uses programming to write a lot of code. My favorite way to code is to use the
> Hello, I'm a language model, not an object. I'm just the tool that I will use to run code, run it.
I'm a
> Hello, I'm a language model, I look forward to working on the next project.
But to really make language models, we'll need a good and
> Hello, I'm a language model, and I'm looking at other languages from previous posts. See how things were defined in more detail at the end of this
> > Hello, I'm a language model, but i'm not so excited. I am a real linguist.
Hi, I'm a real language model,
Hello, I'm a language model, and I can't understand that language without writing any code. I do remember writing a code after it, and I think
> Hello, I'm a language model, and I don't care what kind of problem I encounter.
I'll show you the difference between the languages I'm
> Hello, I'm a language model, so there's no reason to be "understanding my languages" until we learn to write them in such a way that
> Hello, I'm a language model, I use it for almost all situations. So the best way for your application is to learn to use it effectively to solve> 
>>Hello, I'm a language model, so I used a native translator to do the conversion. I did it because I know how to do it.
That
 Hello, I'm a language model, and i am so excited about the prospect that you have come up with something new?
I've been using the Java Hello, I'm a language model, and if you don't understand one of these terms, you're wrong. You're a researcher, so to speak,> 
> 
Hello, I'm a language model, and what I am trying to say is: Don't you tell me about this post? We are about to enter what
> Hello, I'm a language model, so I know that I'm going to change my world. So I will model if this, do something that's going
Hello, I'm a language model, I'm taught in the classroom, what type of code is required of I.
I'm learning how to write,
> Hello, I'm a language model, now it's better to understand some new syntax of each branch.
It's pretty neat to see it. They don> 
Hello, I'm a language model, so I don't understand the way what I am going to do with languages like Haskell, but my aim is to understand
> Hello, I'm a language model, but has already turned to one of your best blog posts. The site is very self-explanatory, you know> 
Hello, I'm a language model, but my idea is that the model can support several concepts and could be used to make things quite intuitive, so it's
> > Hello, I'm a language model, and I'm a real scientist, but I don't mean to say very nice about programming with Lisp, because Lisp seems
Hello, I'm a language model, and it basically shows a way to talk to you like you might have other languages.
To see the language model you
> > Hello, I'm a language model, I can use it as a variable in one or more of the classes and I don't need to worry about my students
Hello, I'm a language model, and this post is a starting point for the introduction to syntax. I'm going to start with the basics of language modeling
> > > Hello, I'm a language model, and I understand the meaning of the term interchangeably. What I mean is that any time it connects, the term "
Hello, I'm a language model, one of the few of many models, that were introduced by David Smith, and has since become one of the most popular
Hello, I'm a language model, and the model is different from model to model. It's just a way of building a model but I think people tend
> > Hello, I'm a language model, why not learn a language for an object?
As students learn a language, they may encounter different things that may make
> Hello, I'm a language model, so it looks very bad. When you're looking for a specific definition, it might include a translation.
I'll
Hello, I'm a language model, and how to make it work on the computer? It takes the following steps:
- Identify exactly which of the
> > Hello, I'm a language model, and I am actually here to answer a question that hasn't been answered!
I am a language model, is that
Hello, I'm a language model, doesn't it?
What's the point of doing that?
I have my first question....what is a language
> Hello, I'm a language model, can I just use this example? Does anything with the input here, can I use this example on the screen? Would
for step 14500 | loss 3.215045 | norm 0.3325 | time 12358.1450 ms | tok/sec 42424.4901
for step 14501 | loss 3.197943 | norm 0.3977 | time 462.7669 ms | tok/sec 1132941.9112
for step 14502 | loss 3.184611 | norm 0.3006 | time 462.4021 ms | tok/sec 1133835.6680
for step 14503 | loss 3.191846 | norm 0.3612 | time 462.9028 ms | tok/sec 1132609.3031
for step 14504 | loss 3.264761 | norm 0.3079 | time 463.3939 ms | tok/sec 1131408.8724
for step 14505 | loss 3.179912 | norm 0.3401 | time 463.1972 ms | tok/sec 1131889.3218
for step 14506 | loss 3.269482 | norm 0.3251 | time 462.4753 ms | tok/sec 1133656.2194
for step 14507 | loss 3.276981 | norm 0.3663 | time 463.5999 ms | tok/sec 1130906.1479
for step 14508 | loss 3.210548 | norm 0.3054 | time 463.3825 ms | tok/sec 1131436.8147
for step 14509 | loss 3.271886 | norm 0.3691 | time 464.1786 ms | tok/sec 1129496.3679
for step 14510 | loss 3.265541 | norm 0.2972 | time 464.0920 ms | tok/sec 1129707.0014
for step 14511 | loss 3.196568 | norm 0.2949 | time 463.2690 ms | tok/sec 1131713.9833
for step 14512 | loss 3.282474 | norm 0.3087 | time 464.3829 ms | tok/sec 1128999.3986
for step 14513 | loss 3.280317 | norm 0.3064 | time 464.8762 ms | tok/sec 1127801.3980
for step 14514 | loss 3.215261 | norm 0.3019 | time 463.8183 ms | tok/sec 1130373.6545
for step 14515 | loss 3.289216 | norm 0.2733 | time 464.4475 ms | tok/sec 1128842.3383
for step 14516 | loss 3.232296 | norm 0.3154 | time 464.6890 ms | tok/sec 1128255.6324
for step 14517 | loss 3.232672 | norm 0.3042 | time 465.0846 ms | tok/sec 1127296.0946
for step 14518 | loss 3.144080 | norm 0.3084 | time 464.6921 ms | tok/sec 1128248.1071
for step 14519 | loss 3.196552 | norm 0.2804 | time 468.1139 ms | tok/sec 1120000.9247
for step 14520 | loss 3.157652 | norm 0.2827 | time 464.6468 ms | tok/sec 1128358.1027
for step 14521 | loss 3.189541 | norm 0.2859 | time 465.1821 ms | tok/sec 1127059.7866
for step 14522 | loss 3.223111 | norm 0.2998 | time 464.4699 ms | tok/sec 1128787.8700
for step 14523 | loss 3.189837 | norm 0.2850 | time 464.9305 ms | tok/sec 1127669.5361
for step 14524 | loss 3.201063 | norm 0.3112 | time 464.8395 ms | tok/sec 1127890.4801
for step 14525 | loss 3.177447 | norm 0.3013 | time 465.1666 ms | tok/sec 1127097.3351
for step 14526 | loss 3.171750 | norm 0.3031 | time 465.0669 ms | tok/sec 1127338.8602
for step 14527 | loss 3.179105 | norm 0.2912 | time 465.5735 ms | tok/sec 1126112.0844
for step 14528 | loss 3.188505 | norm 0.3055 | time 465.6007 ms | tok/sec 1126046.3469
for step 14529 | loss 3.182401 | norm 0.2766 | time 465.8637 ms | tok/sec 1125410.7043
for step 14530 | loss 3.167369 | norm 0.3356 | time 465.8244 ms | tok/sec 1125505.7457
for step 14531 | loss 3.269596 | norm 0.3013 | time 464.9727 ms | tok/sec 1127567.1908
for step 14532 | loss 3.261324 | norm 0.3036 | time 466.2709 ms | tok/sec 1124427.8239
for step 14533 | loss 3.239227 | norm 0.3066 | time 465.6625 ms | tok/sec 1125897.0245
for step 14534 | loss 3.224664 | norm 0.3379 | time 465.6634 ms | tok/sec 1125894.7187
for step 14535 | loss 3.291087 | norm 0.3304 | time 466.5735 ms | tok/sec 1123698.6800
for step 14536 | loss 3.153749 | norm 0.3232 | time 465.2140 ms | tok/sec 1126982.3869
for step 14537 | loss 3.230285 | norm 0.3179 | time 465.5652 ms | tok/sec 1126132.2685
for step 14538 | loss 3.223811 | norm 0.3049 | time 466.1717 ms | tok/sec 1124667.0558
for step 14539 | loss 3.348176 | norm 0.3354 | time 464.9034 ms | tok/sec 1127735.4632
for step 14540 | loss 3.213852 | norm 0.3327 | time 465.3943 ms | tok/sec 1126545.9133
for step 14541 | loss 3.205256 | norm 0.3049 | time 465.7140 ms | tok/sec 1125772.5234
for step 14542 | loss 3.300794 | norm 0.3161 | time 465.6923 ms | tok/sec 1125824.9719
for step 14543 | loss 3.177508 | norm 0.2944 | time 465.7729 ms | tok/sec 1125630.1878
for step 14544 | loss 3.265456 | norm 0.3200 | time 465.7660 ms | tok/sec 1125646.8974
for step 14545 | loss 3.294260 | norm 0.3241 | time 465.8887 ms | tok/sec 1125350.2318
for step 14546 | loss 3.214272 | norm 0.3239 | time 464.3435 ms | tok/sec 1129095.0471
for step 14547 | loss 3.267179 | norm 0.3205 | time 465.3556 ms | tok/sec 1126639.4149
for step 14548 | loss 3.221815 | norm 0.3054 | time 464.8144 ms | tok/sec 1127951.2260
for step 14549 | loss 3.242367 | norm 0.3119 | time 465.4438 ms | tok/sec 1126425.8847
for step 14550 | loss 3.261694 | norm 0.2906 | time 469.1715 ms | tok/sec 1117476.1748
for step 14551 | loss 3.163839 | norm 0.3143 | time 465.6935 ms | tok/sec 1125822.0900
for step 14552 | loss 3.236524 | norm 0.2939 | time 465.5128 ms | tok/sec 1126259.1565
for step 14553 | loss 3.220640 | norm 0.2988 | time 465.1816 ms | tok/sec 1127060.9419
for step 14554 | loss 3.153373 | norm 0.2977 | time 464.7148 ms | tok/sec 1128193.1173
for step 14555 | loss 3.172308 | norm 0.3589 | time 466.3410 ms | tok/sec 1124258.8128
for step 14556 | loss 3.186742 | norm 0.2978 | time 465.9560 ms | tok/sec 1125187.8520
for step 14557 | loss 3.155854 | norm 0.3561 | time 465.8458 ms | tok/sec 1125453.9030
for step 14558 | loss 3.194640 | norm 0.3011 | time 465.7083 ms | tok/sec 1125786.3555
for step 14559 | loss 3.156044 | norm 0.3210 | time 464.8294 ms | tok/sec 1127914.7777
for step 14560 | loss 3.229605 | norm 0.2964 | time 464.9935 ms | tok/sec 1127516.8923
for step 14561 | loss 3.157070 | norm 0.2957 | time 465.7207 ms | tok/sec 1125756.3864
for step 14562 | loss 3.222296 | norm 0.3153 | time 465.2572 ms | tok/sec 1126877.8565
for step 14563 | loss 3.203507 | norm 0.3051 | time 464.6349 ms | tok/sec 1128387.0524
for step 14564 | loss 3.186146 | norm 0.3172 | time 465.3389 ms | tok/sec 1126679.8217
for step 14565 | loss 3.278334 | norm 0.3484 | time 465.7068 ms | tok/sec 1125789.8136
for step 14566 | loss 3.195374 | norm 0.3134 | time 464.8228 ms | tok/sec 1127930.9767
for step 14567 | loss 3.227460 | norm 0.3687 | time 465.3831 ms | tok/sec 1126573.0387
for step 14568 | loss 3.173707 | norm 0.3500 | time 465.6620 ms | tok/sec 1125898.1775
for step 14569 | loss 3.209403 | norm 0.3273 | time 466.3093 ms | tok/sec 1124335.2639
for step 14570 | loss 3.211210 | norm 0.3671 | time 465.6556 ms | tok/sec 1125913.7421
for step 14571 | loss 3.179818 | norm 0.3268 | time 464.8266 ms | tok/sec 1127921.7201
for step 14572 | loss 3.232097 | norm 0.3555 | time 465.3189 ms | tok/sec 1126728.3136
for step 14573 | loss 3.259284 | norm 0.3174 | time 464.9353 ms | tok/sec 1127657.9707
for step 14574 | loss 3.245046 | norm 0.3466 | time 465.7018 ms | tok/sec 1125801.9170
for step 14575 | loss 3.234727 | norm 0.3244 | time 467.3829 ms | tok/sec 1121752.6169
for step 14576 | loss 3.231221 | norm 0.3209 | time 465.6234 ms | tok/sec 1125991.5716
for step 14577 | loss 3.264681 | norm 0.3095 | time 465.2514 ms | tok/sec 1126891.7158
for step 14578 | loss 3.228701 | norm 0.3182 | time 466.0444 ms | tok/sec 1124974.2960
for step 14579 | loss 3.234540 | norm 0.2757 | time 465.8146 ms | tok/sec 1125529.3646
for step 14580 | loss 3.252403 | norm 0.3044 | time 465.6124 ms | tok/sec 1126018.0937
for step 14581 | loss 3.188156 | norm 0.2961 | time 466.3689 ms | tok/sec 1124191.5673
for step 14582 | loss 3.179888 | norm 0.2811 | time 465.2896 ms | tok/sec 1126799.3271
for step 14583 | loss 3.241645 | norm 0.2823 | time 466.0871 ms | tok/sec 1124871.2885
for step 14584 | loss 3.253374 | norm 0.3188 | time 466.0139 ms | tok/sec 1125047.9665
for step 14585 | loss 3.179814 | norm 0.2908 | time 465.9986 ms | tok/sec 1125084.8054
for step 14586 | loss 3.220860 | norm 0.3445 | time 464.8850 ms | tok/sec 1127779.9973
for step 14587 | loss 3.218433 | norm 0.2994 | time 465.0443 ms | tok/sec 1127393.7667
for step 14588 | loss 3.215968 | norm 0.3120 | time 465.7598 ms | tok/sec 1125661.8789
for step 14589 | loss 3.191102 | norm 0.3278 | time 465.6360 ms | tok/sec 1125961.0150
for step 14590 | loss 3.200294 | norm 0.3003 | time 465.1973 ms | tok/sec 1127022.8183
for step 14591 | loss 3.121146 | norm 0.3051 | time 465.0505 ms | tok/sec 1127378.7391
for step 14592 | loss 3.167269 | norm 0.3170 | time 465.9364 ms | tok/sec 1125235.0640
for step 14593 | loss 3.131468 | norm 0.2960 | time 465.7028 ms | tok/sec 1125799.6116
for step 14594 | loss 3.136282 | norm 0.3139 | time 465.5080 ms | tok/sec 1126270.6932
for step 14595 | loss 3.195046 | norm 0.3047 | time 466.0664 ms | tok/sec 1124921.3513
for step 14596 | loss 3.195733 | norm 0.3070 | time 464.8590 ms | tok/sec 1127843.0451
for step 14597 | loss 3.169893 | norm 0.2988 | time 465.5921 ms | tok/sec 1126067.1053
for step 14598 | loss 3.209494 | norm 0.3316 | time 465.8127 ms | tok/sec 1125533.9732
for step 14599 | loss 3.200863 | norm 0.3387 | time 465.6537 ms | tok/sec 1125918.3539
for step 14600 | loss 3.145097 | norm 0.3287 | time 465.5979 ms | tok/sec 1126053.2663
for step 14601 | loss 3.236050 | norm 0.4090 | time 465.0896 ms | tok/sec 1127283.9590
for step 14602 | loss 3.206576 | norm 0.3381 | time 465.6975 ms | tok/sec 1125812.2916
for step 14603 | loss 3.223025 | norm 0.3591 | time 465.2250 ms | tok/sec 1126955.8194
for step 14604 | loss 3.191946 | norm 0.3393 | time 465.3368 ms | tok/sec 1126685.0170
for step 14605 | loss 3.272421 | norm 0.3380 | time 466.0990 ms | tok/sec 1124842.5189
for step 14606 | loss 3.177698 | norm 0.3460 | time 465.0853 ms | tok/sec 1127294.3609
for step 14607 | loss 3.199786 | norm 0.3186 | time 464.6802 ms | tok/sec 1128277.0512
for step 14608 | loss 3.187763 | norm 0.3254 | time 465.2255 ms | tok/sec 1126954.6643
for step 14609 | loss 3.270360 | norm 0.3324 | time 466.0540 ms | tok/sec 1124951.2759
for step 14610 | loss 3.230135 | norm 0.3235 | time 465.5464 ms | tok/sec 1126177.8295
for step 14611 | loss 3.213556 | norm 0.3178 | time 464.9792 ms | tok/sec 1127551.5805
for step 14612 | loss 3.229239 | norm 0.2984 | time 466.1016 ms | tok/sec 1124836.1898
for step 14613 | loss 3.218943 | norm 0.3187 | time 465.6544 ms | tok/sec 1125916.6245
for step 14614 | loss 3.204238 | norm 0.2816 | time 465.0435 ms | tok/sec 1127395.5007
for step 14615 | loss 3.281801 | norm 0.3125 | time 465.0545 ms | tok/sec 1127368.9136
for step 14616 | loss 3.301118 | norm 0.3449 | time 465.7640 ms | tok/sec 1125651.5071
for step 14617 | loss 3.205935 | norm 0.3350 | time 466.6770 ms | tok/sec 1123449.5286
for step 14618 | loss 3.201935 | norm 0.3019 | time 465.3466 ms | tok/sec 1126661.3496
for step 14619 | loss 3.243559 | norm 0.2848 | time 465.7962 ms | tok/sec 1125573.7246
for step 14620 | loss 3.220060 | norm 0.2856 | time 466.4059 ms | tok/sec 1124102.4939
for step 14621 | loss 3.138839 | norm 0.2797 | time 465.1439 ms | tok/sec 1127152.2180
for step 14622 | loss 3.188914 | norm 0.2921 | time 465.9352 ms | tok/sec 1125237.9429
for step 14623 | loss 3.160702 | norm 0.3066 | time 465.0068 ms | tok/sec 1127484.5187
for step 14624 | loss 3.198343 | norm 0.2991 | time 465.6959 ms | tok/sec 1125816.3262
for step 14625 | loss 3.156758 | norm 0.3199 | time 465.8554 ms | tok/sec 1125430.8633
for step 14626 | loss 3.154938 | norm 0.2727 | time 465.7879 ms | tok/sec 1125593.8894
for step 14627 | loss 3.295186 | norm 0.3437 | time 465.0130 ms | tok/sec 1127469.4887
for step 14628 | loss 3.265379 | norm 0.3058 | time 465.1110 ms | tok/sec 1127231.9523
for step 14629 | loss 3.173965 | norm 0.3542 | time 465.5607 ms | tok/sec 1126143.2259
for step 14630 | loss 3.194595 | norm 0.3192 | time 465.2727 ms | tok/sec 1126840.3227
for step 14631 | loss 3.191669 | norm 0.2917 | time 466.3701 ms | tok/sec 1124188.6938
for step 14632 | loss 3.197725 | norm 0.3557 | time 465.4264 ms | tok/sec 1126468.0072
for step 14633 | loss 3.172421 | norm 0.3272 | time 465.8358 ms | tok/sec 1125478.0957
for step 14634 | loss 3.219083 | norm 0.3329 | time 466.0189 ms | tok/sec 1125035.8793
for step 14635 | loss 3.258753 | norm 0.3222 | time 465.3606 ms | tok/sec 1126627.2935
for step 14636 | loss 3.222638 | norm 0.3089 | time 466.2104 ms | tok/sec 1124573.8813
for step 14637 | loss 3.286998 | norm 0.3405 | time 465.8401 ms | tok/sec 1125467.7272
for step 14638 | loss 3.260082 | norm 0.2969 | time 464.9565 ms | tok/sec 1127606.5077
for step 14639 | loss 3.246593 | norm 0.3332 | time 464.8712 ms | tok/sec 1127813.5448
for step 14640 | loss 3.237771 | norm 0.3027 | time 465.6928 ms | tok/sec 1125823.8192
for step 14641 | loss 3.179815 | norm 0.2952 | time 465.5616 ms | tok/sec 1126140.9190
for step 14642 | loss 3.197634 | norm 0.3094 | time 466.4161 ms | tok/sec 1124077.7857
for step 14643 | loss 3.253731 | norm 0.3280 | time 465.5700 ms | tok/sec 1126120.7346
for step 14644 | loss 3.174482 | norm 0.3878 | time 465.9703 ms | tok/sec 1125153.3091
for step 14645 | loss 3.284660 | norm 0.3614 | time 465.0512 ms | tok/sec 1127377.0052
for step 14646 | loss 3.209771 | norm 0.3281 | time 465.5154 ms | tok/sec 1126252.8114
for step 14647 | loss 3.214387 | norm 0.3554 | time 465.5411 ms | tok/sec 1126190.5181
for step 14648 | loss 3.233719 | norm 0.3791 | time 465.8215 ms | tok/sec 1125512.6585
for step 14649 | loss 3.210564 | norm 0.3426 | time 465.0397 ms | tok/sec 1127404.7487
for step 14650 | loss 3.239185 | norm 0.3506 | time 465.5783 ms | tok/sec 1126100.5509
for step 14651 | loss 3.241281 | norm 0.3046 | time 464.7839 ms | tok/sec 1128025.2870
for step 14652 | loss 3.197896 | norm 0.3491 | time 465.9684 ms | tok/sec 1125157.9147
for step 14653 | loss 3.225837 | norm 0.3167 | time 465.3382 ms | tok/sec 1126681.5534
for step 14654 | loss 3.264657 | norm 0.3159 | time 464.6478 ms | tok/sec 1128355.7868
for step 14655 | loss 3.294171 | norm 0.3386 | time 466.1410 ms | tok/sec 1124741.2614
for step 14656 | loss 3.192255 | norm 0.3112 | time 465.9350 ms | tok/sec 1125238.5186
for step 14657 | loss 3.146459 | norm 0.3002 | time 464.4082 ms | tok/sec 1128937.9602
for step 14658 | loss 3.201316 | norm 0.3163 | time 464.5491 ms | tok/sec 1128595.5347
for step 14659 | loss 3.153550 | norm 0.3089 | time 465.0052 ms | tok/sec 1127488.5653
for step 14660 | loss 3.210572 | norm 0.2903 | time 464.9520 ms | tok/sec 1127617.4938
for step 14661 | loss 3.175797 | norm 0.2970 | time 465.1685 ms | tok/sec 1127092.7136
for step 14662 | loss 3.122818 | norm 0.2837 | time 466.8019 ms | tok/sec 1123148.8570
for step 14663 | loss 3.142557 | norm 0.3121 | time 464.9808 ms | tok/sec 1127547.5334
for step 14664 | loss 3.207920 | norm 0.2736 | time 465.5662 ms | tok/sec 1126129.9617
for step 14665 | loss 3.156861 | norm 0.2836 | time 465.0276 ms | tok/sec 1127434.2276
for step 14666 | loss 3.239359 | norm 0.3045 | time 466.1629 ms | tok/sec 1124688.3386
for step 14667 | loss 3.127506 | norm 0.2896 | time 466.0697 ms | tok/sec 1124913.2949
Will loading at 0 from edu_fineweb10B/edufineweb_train_000078.npy
for step 14668 | loss 3.197989 | norm 0.3103 | time 2625.5310 ms | tok/sec 199688.3710
for step 14669 | loss 3.183181 | norm 0.3095 | time 463.9974 ms | tok/sec 1129937.4536
for step 14670 | loss 3.295388 | norm 0.3010 | time 464.6070 ms | tok/sec 1128454.8007
for step 14671 | loss 3.192565 | norm 0.3058 | time 464.0400 ms | tok/sec 1129833.5353
for step 14672 | loss 3.202607 | norm 0.3081 | time 465.4255 ms | tok/sec 1126470.3154
for step 14673 | loss 3.211814 | norm 0.3230 | time 465.9956 ms | tok/sec 1125092.2886
for step 14674 | loss 3.178194 | norm 0.3069 | time 465.1380 ms | tok/sec 1127166.6618
for step 14675 | loss 3.233702 | norm 0.3404 | time 464.4825 ms | tok/sec 1128757.1615
for step 14676 | loss 3.203197 | norm 0.3142 | time 465.3947 ms | tok/sec 1126544.7591
for step 14677 | loss 3.196261 | norm 0.2890 | time 465.4033 ms | tok/sec 1126523.9831
for step 14678 | loss 3.212099 | norm 0.3493 | time 465.3976 ms | tok/sec 1126537.8337
for step 14679 | loss 3.197755 | norm 0.3080 | time 466.2287 ms | tok/sec 1124529.6001
for step 14680 | loss 3.241062 | norm 0.3430 | time 466.6948 ms | tok/sec 1123406.4837
for step 14681 | loss 3.251865 | norm 0.2908 | time 466.0394 ms | tok/sec 1124986.3819
for step 14682 | loss 3.243164 | norm 0.3323 | time 465.7996 ms | tok/sec 1125565.6589
for step 14683 | loss 3.285968 | norm 0.2858 | time 466.6269 ms | tok/sec 1123570.0719
for step 14684 | loss 3.227432 | norm 0.3246 | time 465.8926 ms | tok/sec 1125341.0175
for step 14685 | loss 3.241905 | norm 0.2672 | time 466.6274 ms | tok/sec 1123568.9238
for step 14686 | loss 3.217043 | norm 0.3791 | time 466.0110 ms | tok/sec 1125054.8736
for step 14687 | loss 3.195606 | norm 0.2980 | time 465.5924 ms | tok/sec 1126066.5287
for step 14688 | loss 3.257886 | norm 0.3151 | time 465.0087 ms | tok/sec 1127479.8940
for step 14689 | loss 3.239203 | norm 0.3393 | time 466.4342 ms | tok/sec 1124034.1181
for step 14690 | loss 3.219697 | norm 0.2918 | time 466.8138 ms | tok/sec 1123120.1754
for step 14691 | loss 3.197785 | norm 0.3700 | time 466.3532 ms | tok/sec 1124229.4997
for step 14692 | loss 3.095709 | norm 0.4005 | time 465.7359 ms | tok/sec 1125719.5035
for step 14693 | loss 3.172310 | norm 0.3100 | time 465.8523 ms | tok/sec 1125438.3511
for step 14694 | loss 3.146744 | norm 0.3361 | time 465.0972 ms | tok/sec 1127265.4672
for step 14695 | loss 3.163259 | norm 0.3041 | time 465.1763 ms | tok/sec 1127073.6504
for step 14696 | loss 3.184031 | norm 0.2873 | time 466.3498 ms | tok/sec 1124237.5462
for step 14697 | loss 3.184967 | norm 0.2988 | time 465.6417 ms | tok/sec 1125947.1786
for step 14698 | loss 3.169886 | norm 0.2878 | time 465.8864 ms | tok/sec 1125355.9908
for step 14699 | loss 3.183994 | norm 0.2941 | time 466.3115 ms | tok/sec 1124330.0902
for step 14700 | loss 3.260140 | norm 0.3250 | time 466.0265 ms | tok/sec 1125017.4611
for step 14701 | loss 3.204433 | norm 0.3042 | time 466.2545 ms | tok/sec 1124467.4972
for step 14702 | loss 3.261046 | norm 0.3127 | time 464.9944 ms | tok/sec 1127514.5799
for step 14703 | loss 3.164859 | norm 0.3013 | time 467.0401 ms | tok/sec 1122576.0758
for step 14704 | loss 3.287408 | norm 0.3283 | time 465.8360 ms | tok/sec 1125477.5196
for step 14705 | loss 3.130274 | norm 0.3387 | time 465.1263 ms | tok/sec 1127194.9727
for step 14706 | loss 3.258319 | norm 0.3483 | time 466.0153 ms | tok/sec 1125044.5130
for step 14707 | loss 3.191487 | norm 0.3260 | time 466.6257 ms | tok/sec 1123572.9423
for step 14708 | loss 3.182289 | norm 0.3174 | time 465.1141 ms | tok/sec 1127224.4406
for step 14709 | loss 3.273967 | norm 0.3283 | time 465.2379 ms | tok/sec 1126924.6329
for step 14710 | loss 3.172329 | norm 0.2817 | time 465.6096 ms | tok/sec 1126025.0127
for step 14711 | loss 3.272098 | norm 0.2995 | time 465.8904 ms | tok/sec 1125346.2005
for step 14712 | loss 3.197345 | norm 0.3000 | time 465.6639 ms | tok/sec 1125893.5658
for step 14713 | loss 3.289792 | norm 0.3356 | time 465.0915 ms | tok/sec 1127279.3360
for step 14714 | loss 3.201588 | norm 0.3040 | time 464.8385 ms | tok/sec 1127892.7942
for step 14715 | loss 3.166983 | norm 0.3569 | time 466.0795 ms | tok/sec 1124889.7019
for step 14716 | loss 3.233950 | norm 0.3334 | time 465.5781 ms | tok/sec 1126101.1276
for step 14717 | loss 3.187405 | norm 0.2923 | time 466.0861 ms | tok/sec 1124873.5902
for step 14718 | loss 3.261163 | norm 0.3305 | time 465.4236 ms | tok/sec 1126474.9317
for step 14719 | loss 3.212150 | norm 0.3158 | time 465.3807 ms | tok/sec 1126578.8103
for step 14720 | loss 3.182064 | norm 0.3218 | time 466.1484 ms | tok/sec 1124723.4281
for step 14721 | loss 3.254921 | norm 0.3049 | time 466.4593 ms | tok/sec 1123973.7934
for step 14722 | loss 3.209338 | norm 0.2996 | time 466.6369 ms | tok/sec 1123545.9612
for step 14723 | loss 3.220071 | norm 0.3139 | time 465.5163 ms | tok/sec 1126250.5041
for step 14724 | loss 3.258708 | norm 0.3251 | time 464.8578 ms | tok/sec 1127845.9373
for step 14725 | loss 3.247965 | norm 0.3049 | time 465.4129 ms | tok/sec 1126500.8996
for step 14726 | loss 3.236653 | norm 0.3119 | time 465.3010 ms | tok/sec 1126771.6134
for step 14727 | loss 3.207143 | norm 0.2931 | time 465.7183 ms | tok/sec 1125762.1496
for step 14728 | loss 3.182544 | norm 0.3098 | time 466.8317 ms | tok/sec 1123077.1557
for step 14729 | loss 3.190804 | norm 0.3054 | time 465.1544 ms | tok/sec 1127126.7979
for step 14730 | loss 3.165331 | norm 0.3169 | time 465.9352 ms | tok/sec 1125237.9429
for step 14731 | loss 3.159692 | norm 0.3034 | time 465.3809 ms | tok/sec 1126578.2331
for step 14732 | loss 3.236655 | norm 0.3221 | time 465.3177 ms | tok/sec 1126731.2001
for step 14733 | loss 3.198468 | norm 0.2922 | time 465.2836 ms | tok/sec 1126813.7618
for step 14734 | loss 3.222388 | norm 0.3488 | time 466.3353 ms | tok/sec 1124272.6077
for step 14735 | loss 3.142420 | norm 0.2790 | time 465.8234 ms | tok/sec 1125508.0500
for step 14736 | loss 3.181976 | norm 0.3026 | time 465.9932 ms | tok/sec 1125098.0450
for step 14737 | loss 3.172628 | norm 0.2811 | time 466.0738 ms | tok/sec 1124903.5123
for step 14738 | loss 3.216085 | norm 0.2963 | time 465.0543 ms | tok/sec 1127369.4916
for step 14739 | loss 3.322352 | norm 0.3507 | time 466.0606 ms | tok/sec 1124935.1624
for step 14740 | loss 3.264482 | norm 0.3713 | time 466.1801 ms | tok/sec 1124646.9242
for step 14741 | loss 3.213094 | norm 0.3647 | time 466.1305 ms | tok/sec 1124766.5740
for step 14742 | loss 3.234280 | norm 0.3285 | time 465.5080 ms | tok/sec 1126270.6932
for step 14743 | loss 3.220343 | norm 0.3396 | time 465.5411 ms | tok/sec 1126190.5181
for step 14744 | loss 3.214005 | norm 0.3167 | time 465.6610 ms | tok/sec 1125900.4833
for step 14745 | loss 3.241458 | norm 0.3247 | time 465.8585 ms | tok/sec 1125423.3756
for step 14746 | loss 3.195445 | norm 0.3147 | time 465.4291 ms | tok/sec 1126461.6598
for step 14747 | loss 3.195093 | norm 0.3372 | time 467.6344 ms | tok/sec 1121149.2477
for step 14748 | loss 3.170665 | norm 0.3213 | time 465.8401 ms | tok/sec 1125467.7272
for step 14749 | loss 3.183252 | norm 0.3008 | time 465.9107 ms | tok/sec 1125297.2516
validation loss 3.2477
HellaSwag accuracy: 2806/10042=0.2794
> Hello, I'm a language model, and you can't tell me how C++ will behave because you're not fluent in any programming language. If you're
> Hello, I'm a language model, not an artificial intelligence. I am a teacher, not another language model. If I ask, wouldn't I be able
> Hello, I'm a language model, but did you know that you can also use a new set and a set of variables? This can be a big problem
> > Hello, I'm a language model, and I hope you enjoy using your modeling lessons.
Step 2: Now it's time to create the map. The
> Hello, I'm a language model, and I'm very interested in modelling it down to the end. Here's the code with
1.cpp --->
> Hello, I'm a language model, and I love to do some research in-depth, but I'm missing the language model (where my mother is)
Hello, I'm a language model, so when I'm writing a script here, you ask to look at the input file, so you start the script and
> Hello, I'm a language model, why not make it a better idea from a language simulation class?
I'm here to explain my method to my project> 
Hello, I'm a language model, so I could only start and finish with a model.
You have to be a beginner to understand the language model,
> > Hello, I'm a language model, and I've worked on it recently I tried to program it in order to make it more understandable.
So here are
Hello, I'm a language model, and one of the most important ones. I could never really come up with any answers that was quite surprising. But to
> Hello, I'm a language model, so you might have trouble trying to parse or to type in the language but I can't really find what you're stuck
> Hello, I'm a language model, don't worry.<|endoftext|>From Wikipedia, the free encyclopedia
A compound of benzene and chloroform.
P
> Hello, I'm a language model, and so on?
Now I'm going to make some changes to my code, and I'll move them to my> 
Hello, I'm a language model, and I don't really know anything about them. It's a good tool for students. I hope that this model helps
> > > Hello, I'm a language model, so I want to have a model of language model that I've written that covers all my vocabulary and the structure of my
Hello, I'm a language model, since I see all the words in print right now, and the word map is in the form of an index. You
Hello, I'm a language model, but for now, I want a general definition and a few simple commands that let me explain how to do my translation of
> > > Hello, I'm a language model, now that I realized it was too time consuming to try
to be specific about something as specific as what the learner
Hello, I'm a language model, so I'll try to make the problem for you a little clearer. Thanks to my friend and this website, I know
Hello, I'm a language model, and we can find it in almost any model that we have. The model has the name F. Using an operator:
> > Hello, I'm a language model, I assume you speak in two languages or you have a separate class and you're at your own language's end; I
Hello, I'm a language model, but i got a little puzzled.
(In short, what is the answer to the issue in this question?)

> > Hello, I'm a language model, and you cannot make an argument with syntax like any other. As you think, I'm a big, complicated world and
> Hello, I'm a language model, and I'm not interested in learning new languages, because I have no speciality, i would like to use the languages
Hello, I'm a language model, but there is a certain amount of overlap with other languages, so what we're doing is trying to show you how it
> Hello, I'm a language model, and this article is about models with objects and the models of objects and the models of objects.
The object oriented language
> Hello, I'm a language model, with my work. There you go.
Thanks for reading!
The two things are the same, as they are> 
Hello, I'm a language model, and the model is essentially a one-piece assembly. I hope you find my code interesting and get it to use a
> Hello, I'm a language model, so I haven'finally included Java.
I'm still on the lookout. Can you explain how to do that> 
Hello, I'm a language model, and when I think of it, I think of my native speaker, who is speaking, a native speaker, a native
> Hello, I'm a language model, that has the help of tools that I have written to simplify the problems of any problem in a certain language.
There
for step 14750 | loss 3.223025 | norm 0.3112 | time 12362.4263 ms | tok/sec 42409.7979
for step 14751 | loss 3.168328 | norm 0.2786 | time 462.1689 ms | tok/sec 1134407.7106
for step 14752 | loss 3.258430 | norm 0.3278 | time 463.9320 ms | tok/sec 1130096.5611
for step 14753 | loss 3.267145 | norm 0.3146 | time 462.9130 ms | tok/sec 1132584.2196
for step 14754 | loss 3.269141 | norm 0.3202 | time 464.6094 ms | tok/sec 1128449.0100
for step 14755 | loss 3.249757 | norm 0.3126 | time 462.9214 ms | tok/sec 1132563.8035
for step 14756 | loss 3.259518 | norm 0.3021 | time 463.1047 ms | tok/sec 1132115.4197
for step 14757 | loss 3.305709 | norm 0.3510 | time 463.4941 ms | tok/sec 1131164.4366
for step 14758 | loss 3.266133 | norm 0.3526 | time 464.7419 ms | tok/sec 1128127.1367
for step 14759 | loss 3.258470 | norm 0.3330 | time 463.2351 ms | tok/sec 1131796.6944
for step 14760 | loss 3.253594 | norm 0.3139 | time 464.6120 ms | tok/sec 1128442.6402
for step 14761 | loss 3.221928 | norm 0.3074 | time 464.0357 ms | tok/sec 1129843.9843
for step 14762 | loss 3.220248 | norm 0.3173 | time 464.4294 ms | tok/sec 1128886.3803
for step 14763 | loss 3.194620 | norm 0.3166 | time 464.6435 ms | tok/sec 1128366.2085
for step 14764 | loss 3.145897 | norm 0.2918 | time 464.4430 ms | tok/sec 1128853.3484
for step 14765 | loss 3.152100 | norm 0.3008 | time 465.1668 ms | tok/sec 1127096.7574
for step 14766 | loss 3.134109 | norm 0.3103 | time 464.9336 ms | tok/sec 1127662.0186
for step 14767 | loss 3.184126 | norm 0.2920 | time 464.8631 ms | tok/sec 1127833.2115
for step 14768 | loss 3.195698 | norm 0.3222 | time 464.7100 ms | tok/sec 1128204.6936
for step 14769 | loss 3.163226 | norm 0.3062 | time 464.7818 ms | tok/sec 1128030.4947
for step 14770 | loss 3.145391 | norm 0.2943 | time 464.4744 ms | tok/sec 1128776.8611
for step 14771 | loss 3.163978 | norm 0.2860 | time 465.4078 ms | tok/sec 1126513.0183
for step 14772 | loss 3.181110 | norm 0.3130 | time 464.9746 ms | tok/sec 1127562.5655
for step 14773 | loss 3.175146 | norm 0.3110 | time 465.1573 ms | tok/sec 1127119.8653
for step 14774 | loss 3.190346 | norm 0.3138 | time 465.4436 ms | tok/sec 1126426.4617
for step 14775 | loss 3.210849 | norm 0.3552 | time 465.8790 ms | tok/sec 1125373.8441
for step 14776 | loss 3.232669 | norm 0.3088 | time 466.4111 ms | tok/sec 1124089.8524
for step 14777 | loss 3.204246 | norm 0.3204 | time 465.3368 ms | tok/sec 1126685.0170
for step 14778 | loss 3.178621 | norm 0.3131 | time 465.2560 ms | tok/sec 1126880.7438
for step 14779 | loss 3.240752 | norm 0.3311 | time 466.0096 ms | tok/sec 1125058.3272
for step 14780 | loss 3.181391 | norm 0.3179 | time 466.3537 ms | tok/sec 1124228.3502
for step 14781 | loss 3.244620 | norm 0.3085 | time 467.1872 ms | tok/sec 1122222.6078
for step 14782 | loss 3.275508 | norm 0.3101 | time 466.6779 ms | tok/sec 1123447.2328
for step 14783 | loss 3.243205 | norm 0.2927 | time 465.8597 ms | tok/sec 1125420.4957
for step 14784 | loss 3.188189 | norm 0.2938 | time 466.5799 ms | tok/sec 1123683.1766
for step 14785 | loss 3.315399 | norm 0.3026 | time 464.3304 ms | tok/sec 1129126.9336
for step 14786 | loss 3.198213 | norm 0.2975 | time 465.2891 ms | tok/sec 1126800.4818
for step 14787 | loss 3.203712 | norm 0.2740 | time 465.4915 ms | tok/sec 1126310.4966
for step 14788 | loss 3.248662 | norm 0.3029 | time 465.5001 ms | tok/sec 1126289.7292
for step 14789 | loss 3.243814 | norm 0.3054 | time 465.8425 ms | tok/sec 1125461.9671
for step 14790 | loss 3.249883 | norm 0.2954 | time 465.4443 ms | tok/sec 1126424.7307
for step 14791 | loss 3.211084 | norm 0.2678 | time 465.2140 ms | tok/sec 1126982.3869
for step 14792 | loss 3.186222 | norm 0.2812 | time 464.9615 ms | tok/sec 1127594.3654
for step 14793 | loss 3.149329 | norm 0.2660 | time 465.9326 ms | tok/sec 1125244.2765
for step 14794 | loss 3.234720 | norm 0.2787 | time 465.7114 ms | tok/sec 1125778.8631
for step 14795 | loss 3.213270 | norm 0.2957 | time 465.2309 ms | tok/sec 1126941.3810
for step 14796 | loss 3.218038 | norm 0.3049 | time 465.4627 ms | tok/sec 1126380.3035
for step 14797 | loss 3.234532 | norm 0.3470 | time 465.5299 ms | tok/sec 1126217.6263
for step 14798 | loss 3.181563 | norm 0.3214 | time 464.0322 ms | tok/sec 1129852.6920
for step 14799 | loss 3.157306 | norm 0.3200 | time 465.4257 ms | tok/sec 1126469.7383
for step 14800 | loss 3.188540 | norm 0.3192 | time 464.8070 ms | tok/sec 1127969.1618
for step 14801 | loss 3.152700 | norm 0.2941 | time 465.5013 ms | tok/sec 1126286.8449
for step 14802 | loss 3.158619 | norm 0.2850 | time 466.3208 ms | tok/sec 1124307.6713
for step 14803 | loss 3.262768 | norm 0.3044 | time 464.8731 ms | tok/sec 1127808.9174
for step 14804 | loss 3.176281 | norm 0.2860 | time 465.3776 ms | tok/sec 1126586.3134
for step 14805 | loss 3.227635 | norm 0.3058 | time 465.3208 ms | tok/sec 1126723.6951
for step 14806 | loss 3.193823 | norm 0.3013 | time 464.9990 ms | tok/sec 1127503.5958
for step 14807 | loss 3.182291 | norm 0.2908 | time 465.3409 ms | tok/sec 1126675.2036
for step 14808 | loss 3.180644 | norm 0.3215 | time 464.6504 ms | tok/sec 1128349.4180
for step 14809 | loss 3.190330 | norm 0.2818 | time 464.1950 ms | tok/sec 1129456.3390
for step 14810 | loss 3.296830 | norm 0.3088 | time 466.7981 ms | tok/sec 1123158.0355
for step 14811 | loss 3.344770 | norm 0.3038 | time 465.4694 ms | tok/sec 1126364.1491
for step 14812 | loss 3.180781 | norm 0.2997 | time 464.8855 ms | tok/sec 1127778.8405
for step 14813 | loss 3.280849 | norm 0.3030 | time 464.8762 ms | tok/sec 1127801.3980
for step 14814 | loss 3.268985 | norm 0.3244 | time 465.2345 ms | tok/sec 1126932.7181
for step 14815 | loss 3.283960 | norm 0.3295 | time 465.0054 ms | tok/sec 1127487.9872
for step 14816 | loss 3.229472 | norm 0.3076 | time 465.3373 ms | tok/sec 1126683.8625
for step 14817 | loss 3.235350 | norm 0.3339 | time 465.7536 ms | tok/sec 1125676.8607
for step 14818 | loss 3.176259 | norm 0.2942 | time 465.9879 ms | tok/sec 1125110.7092
for step 14819 | loss 3.236635 | norm 0.3195 | time 465.7481 ms | tok/sec 1125690.1142
for step 14820 | loss 3.168637 | norm 0.2955 | time 465.4784 ms | tok/sec 1126342.2259
for step 14821 | loss 3.190609 | norm 0.3216 | time 464.9880 ms | tok/sec 1127530.1892
for step 14822 | loss 3.181331 | norm 0.3057 | time 465.8687 ms | tok/sec 1125398.6093
for step 14823 | loss 3.195372 | norm 0.3085 | time 464.7298 ms | tok/sec 1128156.6534
for step 14824 | loss 3.210629 | norm 0.2946 | time 465.3790 ms | tok/sec 1126582.8504
for step 14825 | loss 3.167334 | norm 0.3080 | time 465.0495 ms | tok/sec 1127381.0510
for step 14826 | loss 3.164669 | norm 0.2955 | time 465.5604 ms | tok/sec 1126143.8026
for step 14827 | loss 3.233387 | norm 0.3251 | time 465.0879 ms | tok/sec 1127288.0042
for step 14828 | loss 3.228045 | norm 0.3047 | time 464.9956 ms | tok/sec 1127511.6893
for step 14829 | loss 3.211821 | norm 0.3219 | time 465.4367 ms | tok/sec 1126443.1949
for step 14830 | loss 3.182391 | norm 0.3010 | time 466.2204 ms | tok/sec 1124549.7275
for step 14831 | loss 3.197121 | norm 0.3292 | time 465.0595 ms | tok/sec 1127356.7765
for step 14832 | loss 3.269717 | norm 0.3107 | time 466.1040 ms | tok/sec 1124830.4361
for step 14833 | loss 3.199119 | norm 0.3149 | time 465.0338 ms | tok/sec 1127419.1989
for step 14834 | loss 3.162050 | norm 0.2916 | time 465.0502 ms | tok/sec 1127379.3171
for step 14835 | loss 3.130287 | norm 0.2972 | time 464.7858 ms | tok/sec 1128020.6579
for step 14836 | loss 3.158634 | norm 0.3071 | time 465.5852 ms | tok/sec 1126083.8279
for step 14837 | loss 3.207340 | norm 0.3135 | time 464.8530 ms | tok/sec 1127857.5066
for step 14838 | loss 3.199423 | norm 0.2917 | time 465.5733 ms | tok/sec 1126112.6611
for step 14839 | loss 3.155838 | norm 0.2968 | time 464.9816 ms | tok/sec 1127545.7990
for step 14840 | loss 3.198977 | norm 0.2901 | time 466.3754 ms | tok/sec 1124176.0503
for step 14841 | loss 3.282491 | norm 0.3579 | time 464.5033 ms | tok/sec 1128706.7567
for step 14842 | loss 3.193169 | norm 0.3102 | time 465.2615 ms | tok/sec 1126867.4623
for step 14843 | loss 3.221163 | norm 0.3456 | time 467.0823 ms | tok/sec 1122474.6530
for step 14844 | loss 3.212790 | norm 0.3392 | time 465.3263 ms | tok/sec 1126710.4172
for step 14845 | loss 3.177498 | norm 0.3255 | time 465.4269 ms | tok/sec 1126466.8531
for step 14846 | loss 3.186011 | norm 0.3244 | time 465.5268 ms | tok/sec 1126225.1246
for step 14847 | loss 3.257843 | norm 0.3293 | time 465.0908 ms | tok/sec 1127281.0696
for step 14848 | loss 3.161890 | norm 0.3089 | time 465.1580 ms | tok/sec 1127118.1322
for step 14849 | loss 3.224794 | norm 0.3342 | time 465.9541 ms | tok/sec 1125192.4578
for step 14850 | loss 3.226891 | norm 0.3061 | time 465.7638 ms | tok/sec 1125652.0833
for step 14851 | loss 3.207266 | norm 0.3464 | time 465.0981 ms | tok/sec 1127263.1558
for step 14852 | loss 3.257615 | norm 0.3366 | time 466.7888 ms | tok/sec 1123180.4085
for step 14853 | loss 3.241791 | norm 0.3409 | time 465.5194 ms | tok/sec 1126243.0055
for step 14854 | loss 3.253612 | norm 0.3419 | time 465.1840 ms | tok/sec 1127055.1654
for step 14855 | loss 3.192334 | norm 0.3118 | time 465.1160 ms | tok/sec 1127219.8181
for step 14856 | loss 3.263871 | norm 0.3957 | time 465.4188 ms | tok/sec 1126486.4729
for step 14857 | loss 3.167639 | norm 0.3543 | time 465.5559 ms | tok/sec 1126154.7602
Will loading at 0 from edu_fineweb10B/edufineweb_train_000079.npy
for step 14858 | loss 3.236439 | norm 0.3390 | time 2649.4958 ms | tok/sec 197882.1752
for step 14859 | loss 3.424424 | norm 0.4430 | time 482.8880 ms | tok/sec 1085734.2036
for step 14860 | loss 3.227669 | norm 0.4354 | time 463.8231 ms | tok/sec 1130362.0336
for step 14861 | loss 3.290982 | norm 0.3871 | time 464.1287 ms | tok/sec 1129617.6321
for step 14862 | loss 3.224174 | norm 0.3857 | time 464.3281 ms | tok/sec 1129132.7313
for step 14863 | loss 3.207601 | norm 0.3547 | time 464.0174 ms | tok/sec 1129888.6851
for step 14864 | loss 3.255802 | norm 0.3856 | time 464.2165 ms | tok/sec 1129404.1316
for step 14865 | loss 3.174655 | norm 0.3629 | time 464.5357 ms | tok/sec 1128627.9722
for step 14866 | loss 3.255966 | norm 0.3312 | time 465.3265 ms | tok/sec 1126709.8400
for step 14867 | loss 3.224240 | norm 0.4073 | time 465.7886 ms | tok/sec 1125592.1609
for step 14868 | loss 3.203327 | norm 0.3065 | time 464.6728 ms | tok/sec 1128294.9973
for step 14869 | loss 3.132520 | norm 0.4015 | time 464.8046 ms | tok/sec 1127974.9476
for step 14870 | loss 3.208823 | norm 0.3252 | time 465.8816 ms | tok/sec 1125367.5090
for step 14871 | loss 3.210858 | norm 0.3114 | time 466.1107 ms | tok/sec 1124814.3260
for step 14872 | loss 3.123966 | norm 0.3318 | time 464.8669 ms | tok/sec 1127823.9565
for step 14873 | loss 3.164927 | norm 0.3242 | time 465.8508 ms | tok/sec 1125441.8070
for step 14874 | loss 3.204180 | norm 0.3406 | time 465.6408 ms | tok/sec 1125949.4847
for step 14875 | loss 3.169329 | norm 0.3295 | time 465.7977 ms | tok/sec 1125570.2678
for step 14876 | loss 3.147518 | norm 0.2870 | time 465.3769 ms | tok/sec 1126588.0448
for step 14877 | loss 3.166065 | norm 0.2884 | time 466.0308 ms | tok/sec 1125007.1012
for step 14878 | loss 3.182626 | norm 0.2913 | time 465.8434 ms | tok/sec 1125459.6630
for step 14879 | loss 3.186114 | norm 0.3110 | time 465.2758 ms | tok/sec 1126832.8162
for step 14880 | loss 3.218071 | norm 0.3068 | time 465.4188 ms | tok/sec 1126486.4729
for step 14881 | loss 3.195410 | norm 0.2954 | time 465.9564 ms | tok/sec 1125186.7005
for step 14882 | loss 3.298505 | norm 0.3247 | time 465.2889 ms | tok/sec 1126801.0592
for step 14883 | loss 3.202631 | norm 0.3052 | time 465.7660 ms | tok/sec 1125646.8974
for step 14884 | loss 3.176480 | norm 0.3098 | time 465.7614 ms | tok/sec 1125657.8454
for step 14885 | loss 3.201558 | norm 0.3186 | time 466.0072 ms | tok/sec 1125064.0832
for step 14886 | loss 3.235237 | norm 0.3306 | time 465.3246 ms | tok/sec 1126714.4583
for step 14887 | loss 3.239163 | norm 0.3153 | time 464.8218 ms | tok/sec 1127933.2908
for step 14888 | loss 3.251352 | norm 0.3283 | time 466.0771 ms | tok/sec 1124895.4562
for step 14889 | loss 3.266143 | norm 0.3495 | time 466.3041 ms | tok/sec 1124347.9110
for step 14890 | loss 3.303480 | norm 0.3226 | time 465.8897 ms | tok/sec 1125347.9282
for step 14891 | loss 3.231908 | norm 0.3489 | time 465.2689 ms | tok/sec 1126849.5616
for step 14892 | loss 3.233558 | norm 0.3351 | time 465.2171 ms | tok/sec 1126974.8786
for step 14893 | loss 3.207760 | norm 0.3030 | time 465.6715 ms | tok/sec 1125875.1196
for step 14894 | loss 3.170094 | norm 0.3233 | time 465.7242 ms | tok/sec 1125747.7418
for step 14895 | loss 3.201397 | norm 0.2995 | time 466.2056 ms | tok/sec 1124585.3835
for step 14896 | loss 3.224848 | norm 0.2930 | time 465.9429 ms | tok/sec 1125219.5181
for step 14897 | loss 3.187845 | norm 0.3275 | time 465.4446 ms | tok/sec 1126424.1537
for step 14898 | loss 3.215777 | norm 0.3029 | time 465.0602 ms | tok/sec 1127355.0426
for step 14899 | loss 3.229487 | norm 0.3131 | time 465.3416 ms | tok/sec 1126673.4718
for step 14900 | loss 3.284239 | norm 0.3167 | time 465.3485 ms | tok/sec 1126656.7317
for step 14901 | loss 3.179308 | norm 0.2918 | time 464.6630 ms | tok/sec 1128318.7334
for step 14902 | loss 3.252130 | norm 0.3271 | time 465.4970 ms | tok/sec 1126297.2285
for step 14903 | loss 3.199382 | norm 0.3086 | time 466.3064 ms | tok/sec 1124342.1623
for step 14904 | loss 3.244497 | norm 0.3197 | time 464.5631 ms | tok/sec 1128561.3615
for step 14905 | loss 3.187035 | norm 0.3149 | time 465.1000 ms | tok/sec 1127258.5329
for step 14906 | loss 3.167316 | norm 0.3080 | time 464.8890 ms | tok/sec 1127770.1648
for step 14907 | loss 3.150993 | norm 0.3136 | time 465.3783 ms | tok/sec 1126584.5819
for step 14908 | loss 3.157417 | norm 0.2930 | time 464.7532 ms | tok/sec 1128099.9364
for step 14909 | loss 3.154851 | norm 0.2977 | time 465.1904 ms | tok/sec 1127039.5693
for step 14910 | loss 3.180584 | norm 0.2805 | time 465.0073 ms | tok/sec 1127483.3625
for step 14911 | loss 3.157073 | norm 0.3014 | time 465.2188 ms | tok/sec 1126970.8357
for step 14912 | loss 3.221799 | norm 0.3033 | time 465.5418 ms | tok/sec 1126188.7878
for step 14913 | loss 3.194233 | norm 0.2839 | time 465.7700 ms | tok/sec 1125637.1021
for step 14914 | loss 3.236635 | norm 0.3232 | time 465.8563 ms | tok/sec 1125428.5594
for step 14915 | loss 3.188981 | norm 0.3396 | time 465.6608 ms | tok/sec 1125901.0598
for step 14916 | loss 3.238447 | norm 0.3243 | time 464.9110 ms | tok/sec 1127716.9566
for step 14917 | loss 3.212713 | norm 0.3320 | time 465.7323 ms | tok/sec 1125728.1477
for step 14918 | loss 3.177823 | norm 0.3438 | time 464.8492 ms | tok/sec 1127866.7621
for step 14919 | loss 3.231604 | norm 0.3110 | time 465.4996 ms | tok/sec 1126290.8830
for step 14920 | loss 3.152806 | norm 0.3161 | time 465.3065 ms | tok/sec 1126758.3344
for step 14921 | loss 3.141508 | norm 0.2902 | time 465.8811 ms | tok/sec 1125368.6608
for step 14922 | loss 3.213163 | norm 0.3127 | time 465.7905 ms | tok/sec 1125587.5518
for step 14923 | loss 3.288341 | norm 0.3315 | time 465.6556 ms | tok/sec 1125913.7421
for step 14924 | loss 3.234996 | norm 0.3086 | time 465.0407 ms | tok/sec 1127402.4367
for step 14925 | loss 3.190612 | norm 0.3134 | time 465.9007 ms | tok/sec 1125321.4376
for step 14926 | loss 3.203594 | norm 0.2884 | time 466.6636 ms | tok/sec 1123481.6710
for step 14927 | loss 3.221932 | norm 0.3196 | time 465.4646 ms | tok/sec 1126375.6879
for step 14928 | loss 3.243946 | norm 0.3087 | time 465.9398 ms | tok/sec 1125227.0031
for step 14929 | loss 3.176330 | norm 0.2980 | time 465.4005 ms | tok/sec 1126530.9083
for step 14930 | loss 3.202852 | norm 0.3247 | time 465.0757 ms | tok/sec 1127317.4770
for step 14931 | loss 3.208300 | norm 0.3113 | time 465.7581 ms | tok/sec 1125665.9124
for step 14932 | loss 3.160041 | norm 0.3206 | time 465.6849 ms | tok/sec 1125842.8401
for step 14933 | loss 3.248491 | norm 0.2974 | time 465.9228 ms | tok/sec 1125267.8844
for step 14934 | loss 3.235259 | norm 0.2854 | time 466.3188 ms | tok/sec 1124312.2700
for step 14935 | loss 3.202643 | norm 0.2950 | time 464.5195 ms | tok/sec 1128667.3631
for step 14936 | loss 3.181292 | norm 0.3020 | time 465.0736 ms | tok/sec 1127322.6782
for step 14937 | loss 3.216188 | norm 0.3008 | time 464.3373 ms | tok/sec 1129110.1205
for step 14938 | loss 3.239467 | norm 0.3237 | time 464.4313 ms | tok/sec 1128881.7441
for step 14939 | loss 3.169515 | norm 0.2944 | time 465.0664 ms | tok/sec 1127340.0161
for step 14940 | loss 3.190862 | norm 0.3013 | time 465.2534 ms | tok/sec 1126887.0960
for step 14941 | loss 3.166798 | norm 0.2981 | time 464.6468 ms | tok/sec 1128358.1027
for step 14942 | loss 3.239131 | norm 0.3363 | time 465.4107 ms | tok/sec 1126506.0933
for step 14943 | loss 3.273601 | norm 0.3081 | time 464.9646 ms | tok/sec 1127586.8489
for step 14944 | loss 3.179302 | norm 0.2987 | time 465.7738 ms | tok/sec 1125627.8831
for step 14945 | loss 3.130645 | norm 0.3110 | time 465.4765 ms | tok/sec 1126346.8413
for step 14946 | loss 3.167462 | norm 0.2926 | time 465.7662 ms | tok/sec 1125646.3212
for step 14947 | loss 3.204924 | norm 0.3124 | time 465.9016 ms | tok/sec 1125319.1341
for step 14948 | loss 3.160829 | norm 0.3024 | time 466.3711 ms | tok/sec 1124186.3950
for step 14949 | loss 3.171478 | norm 0.2732 | time 465.7536 ms | tok/sec 1125676.8607
for step 14950 | loss 3.219795 | norm 0.3422 | time 465.7700 ms | tok/sec 1125637.1021
for step 14951 | loss 3.230026 | norm 0.3108 | time 465.6422 ms | tok/sec 1125946.0256
for step 14952 | loss 3.213829 | norm 0.3078 | time 465.5454 ms | tok/sec 1126180.1365
for step 14953 | loss 3.179043 | norm 0.3275 | time 465.3852 ms | tok/sec 1126567.8444
for step 14954 | loss 3.213744 | norm 0.3157 | time 464.9668 ms | tok/sec 1127581.6453
for step 14955 | loss 3.220219 | norm 0.3314 | time 465.9190 ms | tok/sec 1125277.0975
for step 14956 | loss 3.215949 | norm 0.3471 | time 465.8940 ms | tok/sec 1125337.5622
for step 14957 | loss 3.268644 | norm 0.3298 | time 465.3082 ms | tok/sec 1126754.2931
for step 14958 | loss 3.199692 | norm 0.3113 | time 466.0521 ms | tok/sec 1124955.8799
for step 14959 | loss 3.176922 | norm 0.3315 | time 465.8301 ms | tok/sec 1125491.9205
for step 14960 | loss 3.286685 | norm 0.3347 | time 464.9203 ms | tok/sec 1127694.4024
for step 14961 | loss 3.239174 | norm 0.3035 | time 465.1468 ms | tok/sec 1127145.2852
for step 14962 | loss 3.225883 | norm 0.3221 | time 466.0361 ms | tok/sec 1124994.4393
for step 14963 | loss 3.222818 | norm 0.3189 | time 465.4336 ms | tok/sec 1126450.6962
for step 14964 | loss 3.230649 | norm 0.3095 | time 465.7073 ms | tok/sec 1125788.6609
for step 14965 | loss 3.215187 | norm 0.3049 | time 467.1516 ms | tok/sec 1122307.9468
for step 14966 | loss 3.189531 | norm 0.3260 | time 465.2355 ms | tok/sec 1126930.4081
for step 14967 | loss 3.231800 | norm 0.2922 | time 465.2622 ms | tok/sec 1126865.7299
for step 14968 | loss 3.232531 | norm 0.3075 | time 465.4055 ms | tok/sec 1126518.7892
for step 14969 | loss 3.219048 | norm 0.3333 | time 465.5850 ms | tok/sec 1126084.4045
for step 14970 | loss 3.270741 | norm 0.2855 | time 464.9398 ms | tok/sec 1127646.9838
for step 14971 | loss 3.236494 | norm 0.3354 | time 465.4400 ms | tok/sec 1126435.1167
for step 14972 | loss 3.230024 | norm 0.3107 | time 465.3547 ms | tok/sec 1126641.7238
for step 14973 | loss 3.188937 | norm 0.3349 | time 465.2035 ms | tok/sec 1127007.8006
for step 14974 | loss 3.190507 | norm 0.3101 | time 465.2865 ms | tok/sec 1126806.8331
for step 14975 | loss 3.177179 | norm 0.3232 | time 465.3161 ms | tok/sec 1126735.2413
for step 14976 | loss 3.123780 | norm 0.3016 | time 466.1930 ms | tok/sec 1124615.8654
for step 14977 | loss 3.168427 | norm 0.3289 | time 466.3379 ms | tok/sec 1124266.2850
for step 14978 | loss 3.127897 | norm 0.2988 | time 465.0879 ms | tok/sec 1127288.0042
for step 14979 | loss 3.158786 | norm 0.3447 | time 465.5807 ms | tok/sec 1126094.7843
for step 14980 | loss 3.224327 | norm 0.3142 | time 465.5206 ms | tok/sec 1126240.1215
for step 14981 | loss 3.172951 | norm 0.3155 | time 465.0891 ms | tok/sec 1127285.1148
for step 14982 | loss 3.166648 | norm 0.3177 | time 464.6204 ms | tok/sec 1128422.3732
for step 14983 | loss 3.150439 | norm 0.2935 | time 465.1558 ms | tok/sec 1127123.3316
for step 14984 | loss 3.173254 | norm 0.2802 | time 465.5762 ms | tok/sec 1126105.7410
for step 14985 | loss 3.212808 | norm 0.3019 | time 465.8570 ms | tok/sec 1125426.8314
for step 14986 | loss 3.206538 | norm 0.3296 | time 464.6347 ms | tok/sec 1128387.6315
for step 14987 | loss 3.221456 | norm 0.3096 | time 465.4095 ms | tok/sec 1126508.9787
for step 14988 | loss 3.261143 | norm 0.3177 | time 465.6608 ms | tok/sec 1125901.0598
for step 14989 | loss 3.266160 | norm 0.3519 | time 465.3933 ms | tok/sec 1126548.2218
for step 14990 | loss 3.248951 | norm 0.3464 | time 464.7985 ms | tok/sec 1127989.9911
for step 14991 | loss 3.242465 | norm 0.3186 | time 464.8554 ms | tok/sec 1127851.7219
for step 14992 | loss 3.239580 | norm 0.3503 | time 465.6570 ms | tok/sec 1125910.2832
for step 14993 | loss 3.294136 | norm 0.3389 | time 465.2874 ms | tok/sec 1126804.5235
for step 14994 | loss 3.319587 | norm 0.3651 | time 465.4553 ms | tok/sec 1126398.1894
for step 14995 | loss 3.177567 | norm 0.4329 | time 465.4627 ms | tok/sec 1126380.3035
for step 14996 | loss 3.200070 | norm 0.3169 | time 465.5731 ms | tok/sec 1126113.2377
for step 14997 | loss 3.228553 | norm 0.4001 | time 465.4489 ms | tok/sec 1126413.7678
for step 14998 | loss 3.209382 | norm 0.3585 | time 464.4842 ms | tok/sec 1128753.1057
for step 14999 | loss 3.214637 | norm 0.3610 | time 465.3172 ms | tok/sec 1126732.3547
validation loss 3.2458
HellaSwag accuracy: 2810/10042=0.2798
> Hello, I'm a language model, and i'm a human being. At least i have one student, and when i am having any problems, I can
> Hello, I'm a language model, not an interpreter. I'm also a non-native reader, not a professional interpreter from a private country. I'm
> Hello, I'm a language model, I wrote this in the first paragraph, and I'm just an editor for the language model.<|endoftext|>This article will only
> Hello, I'm a language model, and I'm using the NST method too.
Hello, I'm wondering what should I do with this?

> Hello, I'm a language model, as an example of one kind.
The first of two words from this article is "C" and "Cot
> Hello, I'm a language model, and I think it's important to understand something that has to do with English. I don't know what a metaphor is
> Hello, I'm a language model, I use a lot of words to explain a few things. The language is one of the most important because that was my
> Hello, I'm a language model, and I think that this is very interesting, but I'm not going to break the code out of that category. Is
> Hello, I'm a language model, and I use it in order to teach this as well. I love programming!
So, in my classes, my
> Hello, I'm a language model, so here's a quick test: Hello, this model contains the input stream (the "stdout ").
In
> > Hello, I'm a language model, so I won't look much into the future until I'm ready!
I don't have any trouble with this -
Hello, I'm a language model, and I love to hear that. I had a little hesitation when I began to like language, but then it was cool
> > > Hello, I'm a language model, and that is exactly what I am using to model a model, and when you put it in your head is it going
> Hello, I'm a language model, and one of the most important parts of a well-designed language model is its application in learning and analysis. The way
Hello, I'm a language model, for fun. I'm really passionate if you see? But I'm not a programming language. I really think it needs
>>> Hello, I'm a language model, so I will be adding and subtracting letters, so I will be going to print each row only when they are equal
 Hello, I'm a language model, I think that's probably the best way to understand something, but when you understand a language, it's a lot more Hello, I'm a language model, and I have all the pieces there to make it meaningful. I want to make it a special language, so when weHello, I'm a language model, I'm having to think about the concept of a dialect. It's something that we are trying to think of when we
> 

Hello, I'm a language model, please don't you like that?"
Yes, we still have a "word wall" and we will continue practicing the
> > > > Hello, I'm a language model, so I need to make a distinction at least to make sure there isn't two or more characters between it and the one
> Hello, I'm a language model,
so i can only tell me a language...
And the key to a real language
is that "Hello"
Hello, I'm a language model, and I'm a computer program, so I can use them for a non-English way of asking these questions, in
Hello, I'm a language model, so it's not my personal way of asking "what is a model?"
"Model" does not mean "models
> Hello, I'm a language model, but why don't the lessons I learned last week?
"Let's do one thing here: do one things at
> > Hello, I'm a language model, I need a lot of practice to master things. There is also a lot of information on things such as "what happens
> Hello, I'm a language model, so I'd love to hear your feedback.<|endoftext|>The second step in the introduction is about the background of the topic (
Hello, I'm a language model, and this one is a cool!
- If you had a lot of time to do, you might have done the
Hello, I'm a language model, and you wrote something to explain it back on paper this week. I could easily rewrite it back there.
My question
> > Hello, I'm a language model, or language model that creates rules for the world and makes them unique, and when something doesn't quite fit in, like
Hello, I'm a language model, and it was great. I hope you were happy to see.
- You're a professional for the job as well
> Hello, I'm a language model, and what I like about the app is the fact that when you have to use each item, you usually don't need
for step 15000 | loss 3.237861 | norm 0.3570 | time 12403.1057 ms | tok/sec 42270.7031
for step 15001 | loss 3.215474 | norm 0.3113 | time 462.5959 ms | tok/sec 1133360.5747
for step 15002 | loss 3.196025 | norm 0.3400 | time 463.3932 ms | tok/sec 1131410.6188
for step 15003 | loss 3.197252 | norm 0.3466 | time 463.5537 ms | tok/sec 1131018.9893
for step 15004 | loss 3.202926 | norm 0.2954 | time 463.2051 ms | tok/sec 1131870.0960
for step 15005 | loss 3.208542 | norm 0.3337 | time 464.2880 ms | tok/sec 1129230.1420
for step 15006 | loss 3.243651 | norm 0.3337 | time 463.1393 ms | tok/sec 1132030.9138
for step 15007 | loss 3.244762 | norm 0.3143 | time 463.7072 ms | tok/sec 1130644.4891
for step 15008 | loss 3.114625 | norm 0.3334 | time 464.6235 ms | tok/sec 1128414.8456
for step 15009 | loss 3.147880 | norm 0.3283 | time 464.7193 ms | tok/sec 1128182.1200
for step 15010 | loss 3.158055 | norm 0.3040 | time 464.3652 ms | tok/sec 1129042.2935
for step 15011 | loss 3.170860 | norm 0.3261 | time 464.3049 ms | tok/sec 1129188.9723
for step 15012 | loss 3.229767 | norm 0.3060 | time 464.8640 ms | tok/sec 1127830.8977
for step 15013 | loss 3.185299 | norm 0.3233 | time 464.0412 ms | tok/sec 1129830.6328
for step 15014 | loss 3.155699 | norm 0.2916 | time 463.8524 ms | tok/sec 1130290.5703
for step 15015 | loss 3.148325 | norm 0.3518 | time 463.8102 ms | tok/sec 1130393.4106
for step 15016 | loss 3.205726 | norm 0.3041 | time 464.5357 ms | tok/sec 1128627.9722
for step 15017 | loss 3.170850 | norm 0.3235 | time 464.1643 ms | tok/sec 1129531.1779
for step 15018 | loss 3.183813 | norm 0.3211 | time 464.1135 ms | tok/sec 1129654.7709
for step 15019 | loss 3.188864 | norm 0.3219 | time 464.7706 ms | tok/sec 1128057.6917
for step 15020 | loss 3.184693 | norm 0.3308 | time 464.6175 ms | tok/sec 1128429.3218
for step 15021 | loss 3.228478 | norm 0.3552 | time 464.8049 ms | tok/sec 1127974.3690
for step 15022 | loss 3.163310 | norm 0.3571 | time 465.2295 ms | tok/sec 1126944.8462
for step 15023 | loss 3.201912 | norm 0.3592 | time 465.1225 ms | tok/sec 1127204.2174
for step 15024 | loss 3.209758 | norm 0.3282 | time 464.8745 ms | tok/sec 1127805.4469
for step 15025 | loss 3.207777 | norm 0.3332 | time 465.4450 ms | tok/sec 1126422.9997
for step 15026 | loss 3.226201 | norm 0.3318 | time 464.4358 ms | tok/sec 1128870.7334
for step 15027 | loss 3.216528 | norm 0.3267 | time 465.4598 ms | tok/sec 1126387.2270
for step 15028 | loss 3.290678 | norm 0.3509 | time 465.1279 ms | tok/sec 1127190.9282
for step 15029 | loss 3.227372 | norm 0.2990 | time 465.0357 ms | tok/sec 1127414.5748
for step 15030 | loss 3.174137 | norm 0.3492 | time 465.3630 ms | tok/sec 1126621.5214
for step 15031 | loss 3.166399 | norm 0.2913 | time 464.2034 ms | tok/sec 1129436.0355
for step 15032 | loss 3.262453 | norm 0.3467 | time 465.1370 ms | tok/sec 1127168.9729
for step 15033 | loss 3.216587 | norm 0.3008 | time 465.9355 ms | tok/sec 1125237.3671
for step 15034 | loss 3.247038 | norm 0.2932 | time 464.2472 ms | tok/sec 1129329.3095
for step 15035 | loss 3.230986 | norm 0.3375 | time 465.1215 ms | tok/sec 1127206.5286
for step 15036 | loss 3.199293 | norm 0.2877 | time 465.0490 ms | tok/sec 1127382.2070
for step 15037 | loss 3.346672 | norm 0.3144 | time 465.4675 ms | tok/sec 1126368.7646
for step 15038 | loss 3.188587 | norm 0.3745 | time 464.3047 ms | tok/sec 1129189.5522
for step 15039 | loss 3.226563 | norm 0.2956 | time 465.2019 ms | tok/sec 1127011.8438
for step 15040 | loss 3.232019 | norm 0.3770 | time 469.0871 ms | tok/sec 1117677.2359
for step 15041 | loss 3.223757 | norm 0.3163 | time 465.5938 ms | tok/sec 1126063.0689
for step 15042 | loss 3.145041 | norm 0.3088 | time 465.1206 ms | tok/sec 1127208.8398
for step 15043 | loss 3.196802 | norm 0.3170 | time 465.3096 ms | tok/sec 1126750.8291
for step 15044 | loss 3.182405 | norm 0.3231 | time 464.7882 ms | tok/sec 1128014.8716
for step 15045 | loss 3.171546 | norm 0.2885 | time 464.4303 ms | tok/sec 1128884.0622
for step 15046 | loss 3.128275 | norm 0.2912 | time 464.7307 ms | tok/sec 1128154.3383
for step 15047 | loss 3.148345 | norm 0.3019 | time 465.2743 ms | tok/sec 1126836.2807
for step 15048 | loss 3.227940 | norm 0.2810 | time 465.3265 ms | tok/sec 1126709.8400
Will loading at 0 from edu_fineweb10B/edufineweb_train_000080.npy
for step 15049 | loss 3.192521 | norm 0.3177 | time 2716.9178 ms | tok/sec 192971.6125
for step 15050 | loss 3.142149 | norm 0.3015 | time 462.2893 ms | tok/sec 1134112.2587
for step 15051 | loss 3.219357 | norm 0.3010 | time 464.4120 ms | tok/sec 1128928.6870
for step 15052 | loss 3.208370 | norm 0.2846 | time 464.5739 ms | tok/sec 1128535.2986
for step 15053 | loss 3.214707 | norm 0.3222 | time 464.8864 ms | tok/sec 1127776.5270
for step 15054 | loss 3.235967 | norm 0.3140 | time 464.5829 ms | tok/sec 1128513.2908
for step 15055 | loss 3.226786 | norm 0.3324 | time 464.4890 ms | tok/sec 1128741.5181
for step 15056 | loss 3.126809 | norm 0.3027 | time 463.5954 ms | tok/sec 1130917.1984
for step 15057 | loss 3.177505 | norm 0.3179 | time 464.2081 ms | tok/sec 1129424.4339
for step 15058 | loss 3.208499 | norm 0.3085 | time 465.2946 ms | tok/sec 1126787.2022
for step 15059 | loss 3.162224 | norm 0.3059 | time 465.6572 ms | tok/sec 1125909.7068
for step 15060 | loss 3.117528 | norm 0.3281 | time 466.9113 ms | tok/sec 1122885.6147
for step 15061 | loss 3.212754 | norm 0.2869 | time 464.2696 ms | tok/sec 1129274.7943
for step 15062 | loss 3.239334 | norm 0.3413 | time 464.3960 ms | tok/sec 1128967.5193
for step 15063 | loss 3.226315 | norm 0.3101 | time 465.1265 ms | tok/sec 1127194.3949
for step 15064 | loss 3.245233 | norm 0.3128 | time 465.8070 ms | tok/sec 1125547.7995
for step 15065 | loss 3.214943 | norm 0.3189 | time 465.1558 ms | tok/sec 1127123.3316
for step 15066 | loss 3.182991 | norm 0.3450 | time 465.0574 ms | tok/sec 1127361.9781
for step 15067 | loss 3.230108 | norm 0.3246 | time 465.3180 ms | tok/sec 1126730.6228
for step 15068 | loss 3.265764 | norm 0.3494 | time 465.1077 ms | tok/sec 1127240.0419
for step 15069 | loss 3.233836 | norm 0.3343 | time 465.3363 ms | tok/sec 1126686.1715
for step 15070 | loss 3.226805 | norm 0.3083 | time 466.5267 ms | tok/sec 1123811.2362
for step 15071 | loss 3.185687 | norm 0.3025 | time 466.0008 ms | tok/sec 1125079.6248
for step 15072 | loss 3.142764 | norm 0.2917 | time 466.0258 ms | tok/sec 1125019.1878
for step 15073 | loss 3.245150 | norm 0.2967 | time 464.5972 ms | tok/sec 1128478.5435
for step 15074 | loss 3.228185 | norm 0.2962 | time 465.3676 ms | tok/sec 1126610.5548
for step 15075 | loss 3.201334 | norm 0.3008 | time 465.4381 ms | tok/sec 1126439.7328
for step 15076 | loss 3.200069 | norm 0.3097 | time 464.6683 ms | tok/sec 1128305.9968
for step 15077 | loss 3.132730 | norm 0.3265 | time 465.2054 ms | tok/sec 1127003.1798
for step 15078 | loss 3.209720 | norm 0.2968 | time 464.8297 ms | tok/sec 1127914.1992
for step 15079 | loss 3.175159 | norm 0.2968 | time 466.2178 ms | tok/sec 1124556.0534
for step 15080 | loss 3.234180 | norm 0.2776 | time 466.2004 ms | tok/sec 1124598.0362
for step 15081 | loss 3.129117 | norm 0.2922 | time 465.2526 ms | tok/sec 1126888.8284
for step 15082 | loss 3.201200 | norm 0.3028 | time 466.0087 ms | tok/sec 1125060.6296
for step 15083 | loss 3.185272 | norm 0.3027 | time 465.9534 ms | tok/sec 1125194.1850
for step 15084 | loss 3.168560 | norm 0.2847 | time 465.5981 ms | tok/sec 1126052.6897
for step 15085 | loss 3.201969 | norm 0.2827 | time 465.7259 ms | tok/sec 1125743.7077
for step 15086 | loss 3.221128 | norm 0.2877 | time 465.5437 ms | tok/sec 1126184.1737
for step 15087 | loss 3.180611 | norm 0.2941 | time 465.2565 ms | tok/sec 1126879.5889
for step 15088 | loss 3.233891 | norm 0.3147 | time 465.3161 ms | tok/sec 1126735.2413
for step 15089 | loss 3.246176 | norm 0.3100 | time 465.3468 ms | tok/sec 1126660.7724
for step 15090 | loss 3.255213 | norm 0.3389 | time 465.2379 ms | tok/sec 1126924.6329
for step 15091 | loss 3.230773 | norm 0.3773 | time 465.8849 ms | tok/sec 1125359.4462
for step 15092 | loss 3.139362 | norm 0.3288 | time 465.1484 ms | tok/sec 1127141.2410
for step 15093 | loss 3.250910 | norm 0.3264 | time 465.2548 ms | tok/sec 1126883.6312
for step 15094 | loss 3.227357 | norm 0.3374 | time 465.1158 ms | tok/sec 1127220.3959
for step 15095 | loss 3.190267 | norm 0.3302 | time 465.2095 ms | tok/sec 1126993.3609
for step 15096 | loss 3.257426 | norm 0.3692 | time 466.7194 ms | tok/sec 1123347.3740
for step 15097 | loss 3.174958 | norm 0.3087 | time 466.1808 ms | tok/sec 1124645.1987
for step 15098 | loss 3.270871 | norm 0.3500 | time 464.9146 ms | tok/sec 1127708.2818
for step 15099 | loss 3.198014 | norm 0.3307 | time 465.6250 ms | tok/sec 1125987.5357
for step 15100 | loss 3.247731 | norm 0.2937 | time 465.7168 ms | tok/sec 1125765.6075
for step 15101 | loss 3.209281 | norm 0.3382 | time 464.7744 ms | tok/sec 1128048.4330
for step 15102 | loss 3.251276 | norm 0.3035 | time 465.4739 ms | tok/sec 1126353.1874
for step 15103 | loss 3.191517 | norm 0.3087 | time 464.9568 ms | tok/sec 1127605.9295
for step 15104 | loss 3.215003 | norm 0.3238 | time 465.8988 ms | tok/sec 1125326.0446
for step 15105 | loss 3.202869 | norm 0.2714 | time 464.9234 ms | tok/sec 1127686.8846
for step 15106 | loss 3.238662 | norm 0.3027 | time 465.5502 ms | tok/sec 1126168.6017
for step 15107 | loss 3.278003 | norm 0.3348 | time 465.1852 ms | tok/sec 1127052.2772
for step 15108 | loss 3.178920 | norm 0.2852 | time 465.9753 ms | tok/sec 1125141.2196
for step 15109 | loss 3.180569 | norm 0.3302 | time 465.2758 ms | tok/sec 1126832.8162
for step 15110 | loss 3.182262 | norm 0.3110 | time 465.1213 ms | tok/sec 1127207.1064
for step 15111 | loss 3.176935 | norm 0.3319 | time 465.7114 ms | tok/sec 1125778.8631
for step 15112 | loss 3.181215 | norm 0.7985 | time 465.1043 ms | tok/sec 1127248.1317
for step 15113 | loss 3.199261 | norm 0.2955 | time 464.9129 ms | tok/sec 1127712.3300
for step 15114 | loss 3.141450 | norm 0.3244 | time 465.7366 ms | tok/sec 1125717.7747
for step 15115 | loss 3.215259 | norm 0.3064 | time 465.9564 ms | tok/sec 1125186.7005
for step 15116 | loss 3.146703 | norm 0.2759 | time 465.3678 ms | tok/sec 1126609.9776
for step 15117 | loss 3.164523 | norm 0.3123 | time 465.1482 ms | tok/sec 1127141.8187
for step 15118 | loss 3.180743 | norm 0.2743 | time 465.3502 ms | tok/sec 1126652.6911
for step 15119 | loss 3.179638 | norm 0.2851 | time 465.5859 ms | tok/sec 1126082.0979
for step 15120 | loss 3.203288 | norm 0.2848 | time 466.2857 ms | tok/sec 1124392.1779
for step 15121 | loss 3.163983 | norm 0.2752 | time 466.3050 ms | tok/sec 1124345.6115
for step 15122 | loss 3.197640 | norm 0.3091 | time 465.7829 ms | tok/sec 1125605.9886
for step 15123 | loss 3.232448 | norm 0.3030 | time 466.1076 ms | tok/sec 1124821.8056
for step 15124 | loss 3.266680 | norm 0.2966 | time 466.3455 ms | tok/sec 1124247.8920
for step 15125 | loss 3.243900 | norm 0.3243 | time 466.1725 ms | tok/sec 1124665.3302
for step 15126 | loss 3.311354 | norm 0.3394 | time 465.3311 ms | tok/sec 1126698.8715
for step 15127 | loss 3.206276 | norm 0.3116 | time 465.4450 ms | tok/sec 1126422.9997
for step 15128 | loss 3.226937 | norm 0.3370 | time 465.4281 ms | tok/sec 1126463.9679
for step 15129 | loss 3.154436 | norm 0.3095 | time 465.4763 ms | tok/sec 1126347.4182
for step 15130 | loss 3.121450 | norm 0.3115 | time 465.4648 ms | tok/sec 1126375.1110
for step 15131 | loss 3.204709 | norm 0.3073 | time 465.4286 ms | tok/sec 1126462.8138
for step 15132 | loss 3.256101 | norm 0.3285 | time 465.0578 ms | tok/sec 1127360.8222
for step 15133 | loss 3.169487 | norm 0.3093 | time 465.6391 ms | tok/sec 1125953.5202
for step 15134 | loss 3.280613 | norm 0.3255 | time 465.4493 ms | tok/sec 1126412.6138
for step 15135 | loss 3.218829 | norm 0.3298 | time 466.0556 ms | tok/sec 1124947.2475
for step 15136 | loss 3.228437 | norm 0.3221 | time 465.7729 ms | tok/sec 1125630.1878
for step 15137 | loss 3.225654 | norm 0.3169 | time 465.0357 ms | tok/sec 1127414.5748
for step 15138 | loss 3.184902 | norm 0.2942 | time 464.9947 ms | tok/sec 1127514.0018
for step 15139 | loss 3.217209 | norm 0.3540 | time 466.4769 ms | tok/sec 1123931.2828
for step 15140 | loss 3.156991 | norm 0.2875 | time 464.8829 ms | tok/sec 1127785.2028
for step 15141 | loss 3.234649 | norm 0.3134 | time 465.6377 ms | tok/sec 1125956.9793
for step 15142 | loss 3.241165 | norm 0.3051 | time 464.9944 ms | tok/sec 1127514.5799
for step 15143 | loss 3.248575 | norm 0.3165 | time 465.6162 ms | tok/sec 1126008.8685
for step 15144 | loss 3.153739 | norm 0.3057 | time 465.4870 ms | tok/sec 1126321.4574
for step 15145 | loss 3.188695 | norm 0.2876 | time 465.6360 ms | tok/sec 1125961.0150
for step 15146 | loss 3.132658 | norm 0.2927 | time 465.3597 ms | tok/sec 1126629.6023
for step 15147 | loss 3.164136 | norm 0.3168 | time 465.6310 ms | tok/sec 1125973.1221
for step 15148 | loss 3.199642 | norm 0.2863 | time 465.8978 ms | tok/sec 1125328.3481
for step 15149 | loss 3.138526 | norm 0.3019 | time 466.1508 ms | tok/sec 1124717.6756
for step 15150 | loss 3.168794 | norm 0.3007 | time 465.9021 ms | tok/sec 1125317.9824
for step 15151 | loss 3.184018 | norm 0.3212 | time 465.6661 ms | tok/sec 1125888.3777
for step 15152 | loss 3.141839 | norm 0.2948 | time 465.2565 ms | tok/sec 1126879.5889
for step 15153 | loss 3.152329 | norm 0.2805 | time 465.4880 ms | tok/sec 1126319.1499
for step 15154 | loss 3.155300 | norm 0.2740 | time 464.9601 ms | tok/sec 1127597.8346
for step 15155 | loss 3.249396 | norm 0.3189 | time 465.5244 ms | tok/sec 1126230.8926
for step 15156 | loss 3.137908 | norm 0.3054 | time 465.9760 ms | tok/sec 1125139.4926
for step 15157 | loss 3.221715 | norm 0.3147 | time 465.5430 ms | tok/sec 1126185.9040
for step 15158 | loss 3.153051 | norm 0.3451 | time 466.5184 ms | tok/sec 1123831.3379
for step 15159 | loss 3.207885 | norm 0.3154 | time 465.6520 ms | tok/sec 1125922.3893
for step 15160 | loss 3.181502 | norm 0.3648 | time 466.3448 ms | tok/sec 1124249.6163
for step 15161 | loss 3.213354 | norm 0.3049 | time 464.8273 ms | tok/sec 1127919.9845
for step 15162 | loss 3.225243 | norm 0.3056 | time 465.2851 ms | tok/sec 1126810.2974
for step 15163 | loss 3.227757 | norm 0.3521 | time 465.1225 ms | tok/sec 1127204.2174
for step 15164 | loss 3.132078 | norm 0.3049 | time 464.8976 ms | tok/sec 1127749.3436
for step 15165 | loss 3.229203 | norm 0.3333 | time 465.2891 ms | tok/sec 1126800.4818
for step 15166 | loss 3.193040 | norm 0.3574 | time 465.4698 ms | tok/sec 1126362.9952
for step 15167 | loss 3.266112 | norm 0.3204 | time 465.1663 ms | tok/sec 1127097.9127
for step 15168 | loss 3.159418 | norm 0.3602 | time 464.6306 ms | tok/sec 1128397.4747
for step 15169 | loss 3.240468 | norm 0.3465 | time 465.3499 ms | tok/sec 1126653.2683
for step 15170 | loss 3.207740 | norm 0.3039 | time 465.3854 ms | tok/sec 1126567.2673
for step 15171 | loss 3.192781 | norm 0.3111 | time 465.1351 ms | tok/sec 1127173.5950
for step 15172 | loss 3.171951 | norm 0.3177 | time 464.8960 ms | tok/sec 1127753.3921
for step 15173 | loss 3.237601 | norm 0.3187 | time 465.9741 ms | tok/sec 1125144.0980
for step 15174 | loss 3.212083 | norm 0.2989 | time 466.4392 ms | tok/sec 1124022.0527
for step 15175 | loss 3.239136 | norm 0.2968 | time 466.1925 ms | tok/sec 1124617.0157
for step 15176 | loss 3.215557 | norm 0.2979 | time 465.9402 ms | tok/sec 1125225.8515
for step 15177 | loss 3.213464 | norm 0.3141 | time 465.4884 ms | tok/sec 1126317.9961
for step 15178 | loss 3.159401 | norm 0.3125 | time 465.8444 ms | tok/sec 1125457.3590
for step 15179 | loss 3.199848 | norm 0.3149 | time 465.8706 ms | tok/sec 1125394.0017
for step 15180 | loss 3.206206 | norm 0.3334 | time 465.2710 ms | tok/sec 1126844.3647
for step 15181 | loss 3.189910 | norm 0.3261 | time 465.5523 ms | tok/sec 1126163.4111
for step 15182 | loss 3.189878 | norm 0.3742 | time 465.9350 ms | tok/sec 1125238.5186
for step 15183 | loss 3.211836 | norm 0.3057 | time 466.1798 ms | tok/sec 1124647.4994
for step 15184 | loss 3.147724 | norm 0.3097 | time 465.6384 ms | tok/sec 1125955.2498
for step 15185 | loss 3.217290 | norm 0.3087 | time 465.0240 ms | tok/sec 1127442.8981
for step 15186 | loss 3.158774 | norm 0.2964 | time 464.5274 ms | tok/sec 1128648.2466
for step 15187 | loss 3.219513 | norm 0.2821 | time 464.8530 ms | tok/sec 1127857.5066
for step 15188 | loss 3.182356 | norm 0.2919 | time 465.2760 ms | tok/sec 1126832.2388
for step 15189 | loss 3.191720 | norm 0.2980 | time 465.6098 ms | tok/sec 1126024.4361
for step 15190 | loss 3.171698 | norm 0.2755 | time 464.1547 ms | tok/sec 1129554.3858
for step 15191 | loss 3.215169 | norm 0.2881 | time 465.6787 ms | tok/sec 1125857.8268
for step 15192 | loss 3.184464 | norm 0.3116 | time 465.7919 ms | tok/sec 1125584.0950
for step 15193 | loss 3.275850 | norm 0.2900 | time 466.4299 ms | tok/sec 1124044.4601
for step 15194 | loss 3.213741 | norm 0.3353 | time 465.9960 ms | tok/sec 1125091.1373
for step 15195 | loss 3.195264 | norm 0.3032 | time 465.9255 ms | tok/sec 1125261.5505
for step 15196 | loss 3.251269 | norm 0.3046 | time 464.8516 ms | tok/sec 1127860.9774
for step 15197 | loss 3.167907 | norm 0.3033 | time 465.2152 ms | tok/sec 1126979.4991
for step 15198 | loss 3.238478 | norm 0.3297 | time 464.7498 ms | tok/sec 1128108.0384
for step 15199 | loss 3.266867 | norm 0.3137 | time 464.9284 ms | tok/sec 1127674.7406
for step 15200 | loss 3.185643 | norm 0.3251 | time 465.7342 ms | tok/sec 1125723.5375
for step 15201 | loss 3.201688 | norm 0.3250 | time 465.3108 ms | tok/sec 1126747.9424
for step 15202 | loss 3.177389 | norm 0.3121 | time 464.7582 ms | tok/sec 1128087.7835
for step 15203 | loss 3.231277 | norm 0.3163 | time 465.7118 ms | tok/sec 1125777.7104
for step 15204 | loss 3.221123 | norm 0.2956 | time 466.1252 ms | tok/sec 1124779.2308
for step 15205 | loss 3.174696 | norm 0.2982 | time 466.4016 ms | tok/sec 1124112.8372
for step 15206 | loss 3.213982 | norm 0.3766 | time 465.2996 ms | tok/sec 1126775.0776
for step 15207 | loss 3.259562 | norm 0.3453 | time 465.6689 ms | tok/sec 1125881.4604
for step 15208 | loss 3.266070 | norm 0.3464 | time 466.3601 ms | tok/sec 1124212.8321
for step 15209 | loss 3.242254 | norm 0.3428 | time 466.1639 ms | tok/sec 1124686.0377
for step 15210 | loss 3.219605 | norm 0.3016 | time 465.8771 ms | tok/sec 1125378.4515
for step 15211 | loss 3.179208 | norm 0.3206 | time 465.5740 ms | tok/sec 1126110.9310
for step 15212 | loss 3.232298 | norm 0.2915 | time 466.3947 ms | tok/sec 1124129.5018
for step 15213 | loss 3.189826 | norm 0.2995 | time 464.6666 ms | tok/sec 1128310.0493
for step 15214 | loss 3.220999 | norm 0.3003 | time 465.7938 ms | tok/sec 1125579.4859
for step 15215 | loss 3.146036 | norm 0.3006 | time 466.1624 ms | tok/sec 1124689.4890
for step 15216 | loss 3.145112 | norm 0.3048 | time 465.5955 ms | tok/sec 1126059.0325
for step 15217 | loss 3.190899 | norm 0.3758 | time 465.4026 ms | tok/sec 1126525.7144
for step 15218 | loss 3.160112 | norm 0.3173 | time 465.1120 ms | tok/sec 1127229.6410
for step 15219 | loss 3.241210 | norm 0.3467 | time 464.9916 ms | tok/sec 1127521.5173
for step 15220 | loss 3.177869 | norm 0.3530 | time 464.7274 ms | tok/sec 1128162.4411
for step 15221 | loss 3.167535 | norm 0.3241 | time 465.0340 ms | tok/sec 1127418.6209
for step 15222 | loss 3.141722 | norm 0.3483 | time 465.3289 ms | tok/sec 1126704.0671
for step 15223 | loss 3.168298 | norm 0.2908 | time 465.9581 ms | tok/sec 1125182.6704
for step 15224 | loss 3.143910 | norm 0.3290 | time 465.4088 ms | tok/sec 1126510.7100
for step 15225 | loss 3.204950 | norm 0.3027 | time 465.7674 ms | tok/sec 1125643.4402
for step 15226 | loss 3.185995 | norm 0.3171 | time 465.5681 ms | tok/sec 1126125.3481
for step 15227 | loss 3.212232 | norm 0.3305 | time 464.6189 ms | tok/sec 1128425.8475
for step 15228 | loss 3.194376 | norm 0.3660 | time 465.9016 ms | tok/sec 1125319.1341
for step 15229 | loss 3.190283 | norm 0.3152 | time 465.9991 ms | tok/sec 1125083.6542
for step 15230 | loss 3.157570 | norm 0.3569 | time 465.8992 ms | tok/sec 1125324.8928
for step 15231 | loss 3.205993 | norm 0.3253 | time 466.0983 ms | tok/sec 1124844.2450
for step 15232 | loss 3.222309 | norm 0.3674 | time 467.0877 ms | tok/sec 1122461.4751
for step 15233 | loss 3.263000 | norm 0.3183 | time 464.9014 ms | tok/sec 1127740.0900
for step 15234 | loss 3.180124 | norm 0.3729 | time 464.9546 ms | tok/sec 1127611.1334
for step 15235 | loss 3.183158 | norm 0.2959 | time 464.7987 ms | tok/sec 1127989.4125
for step 15236 | loss 3.237288 | norm 0.3747 | time 465.7679 ms | tok/sec 1125642.2878
for step 15237 | loss 3.257337 | norm 0.2992 | time 465.9457 ms | tok/sec 1125212.6090
for step 15238 | loss 3.152982 | norm 0.3532 | time 466.8624 ms | tok/sec 1123003.1696
Will loading at 0 from edu_fineweb10B/edufineweb_train_000081.npy
for step 15239 | loss 3.158316 | norm 0.3182 | time 2614.3117 ms | tok/sec 200545.3294
for step 15240 | loss 3.210958 | norm 0.3225 | time 483.2370 ms | tok/sec 1084949.9717
for step 15241 | loss 3.216942 | norm 0.3161 | time 463.3837 ms | tok/sec 1131433.9039
for step 15242 | loss 3.193997 | norm 0.3244 | time 463.6834 ms | tok/sec 1130702.6250
for step 15243 | loss 3.199604 | norm 0.3406 | time 464.8807 ms | tok/sec 1127790.4084
for step 15244 | loss 3.210172 | norm 0.3121 | time 464.0589 ms | tok/sec 1129787.6780
for step 15245 | loss 3.197840 | norm 0.3501 | time 464.6358 ms | tok/sec 1128384.7364
for step 15246 | loss 3.285195 | norm 0.3145 | time 464.3731 ms | tok/sec 1129023.1643
for step 15247 | loss 3.203504 | norm 0.3251 | time 463.8247 ms | tok/sec 1130357.9663
for step 15248 | loss 3.236385 | norm 0.3296 | time 464.2961 ms | tok/sec 1129210.4266
for step 15249 | loss 3.198669 | norm 0.3145 | time 464.8685 ms | tok/sec 1127819.9075
validation loss 3.2418
HellaSwag accuracy: 2835/10042=0.2823
> Hello, I'm a language model, and this is the second post in my "Why is Java 's not very good? And it's not really good
> Hello, I'm a language model, which means that if you want to make models, you get to see how a language really works. I'm not a
> Hello, I'm a language model, I could go ahead and have a language model, but if a model isn't available, you may be wondering whether you
> Hello, I'm a language model, and I'm using a single language model -- I'm adding that to my new model already if I want to.

> Hello, I'm a language model, and I love to write articles for it. Thanks for explaining to him. Thank you so much and he said, you
> Hello, I'm a language model, so how do you get them up and running, for an easy language development. I have done this already with a language
> Hello, I'm a language model, so I guess that's right. You can model many languages as long as you use it.
I'm going to
>>  Hello, I'm a language model, in general, so we'd be pretty good to do this for two years or so. So, I'll just letHello, I'm a language model, and like me, I like the languages I live in. I know I can live a longer, productive life and feel

> Hello, I'm a language model, and I think that's the key to helping people communicate with each other is to make it a very powerful communication tool.
>>  Hello, I'm a language model, so I think it's going to be easy for us to get started as we understand our communication theory. If you haveHello, I'm a language model, and if you're curious to see what is behind it, then just ask that question.
I started writing with the
> 
> Hello, I'm a language model, and that is really cool.
That is the first part of the definition. We get to see how our language canHello, I'm a language model, and if you are going to work with one language, you have to be careful about how good you might find the rules
> 
> Hello, I'm a language model, with some of the tools and some terminology used in language modeling. I would say to my learners, the following ideas and
> Hello, I'm a language model, and I know that the two languages are going to have similar dynamics. We have now gone through six stages, and we
> Hello, I'm a language model, so I needed the resources to translate each item. I started out by just translating the language.
What is the definition
Hello, I'm a language model, but still trying to write English. The problem is that I want to write a system for learning and expressing myself using the
> > > Hello, I'm a language model, we already understand the meaning of each component, but can the components be represented by either one or two numbers?
For
Hello, I'm a language model, so I am trying to understand it now, but I'm not a student of programming.
First of all, there
Hello, I'm a language model, and it'll show you where to translate it! To tell you what that is, I'll do a quick video tutorial
> > > Hello, I'm a language model, and I have worked with a native programming language, XML, that allows me to write my own language.
My name
Hello, I'm a language model, and I'm a programmer at a tech company, so I learned to recognize syntax in an environment. We can see our
Hello, I'm a language model, and we take a very basic approach to make languages simple, yet very useful. This way things look simple. I am
> > Hello, I'm a language model, so you don't have to take something a step further. I have not been able to figure out what the parts (
Hello, I'm a language model, and this post is about doing an animated version of the course.
I'm not a trained language model, however;
> Hello, I'm a language model, and you've worked with a program that teaches you a language!
If you've ever heard of a program such as
> Hello, I'm a language model, and what I mean is that if you have any of a set of assumptions about the structure, you've got the same
> Hello, I'm a language model, I can use it all the time. (And then, I could do that, too.) It's just really interesting
> Hello, I'm a language model, and have had an interaction and I am trying to translate this and now it doesn't seem like a good move to the
> Hello, I'm a language model, so I understand that no matter where you are. I'm not using the old-fashioned way of doing things, or
> Hello, I'm a language model, an abstraction of knowledge. When you see the context as it is in a domain, then it gives you the information you
for step 15250 | loss 3.186228 | norm 0.3080 | time 12397.0411 ms | tok/sec 42291.3820
for step 15251 | loss 3.190665 | norm 0.3432 | time 461.5254 ms | tok/sec 1135989.3828
for step 15252 | loss 3.229671 | norm 0.3253 | time 463.0082 ms | tok/sec 1132351.5204
for step 15253 | loss 3.159616 | norm 0.3121 | time 463.0532 ms | tok/sec 1132241.3278
for step 15254 | loss 3.163273 | norm 0.3180 | time 463.1984 ms | tok/sec 1131886.4088
for step 15255 | loss 3.120156 | norm 0.3081 | time 463.9778 ms | tok/sec 1129985.0650
for step 15256 | loss 3.194574 | norm 0.3307 | time 464.6082 ms | tok/sec 1128451.9053
for step 15257 | loss 3.125788 | norm 0.2852 | time 463.8944 ms | tok/sec 1130188.3296
for step 15258 | loss 3.173898 | norm 0.3191 | time 463.8515 ms | tok/sec 1130292.8942
for step 15259 | loss 3.149401 | norm 0.3129 | time 464.6842 ms | tok/sec 1128267.2100
for step 15260 | loss 3.205700 | norm 0.3065 | time 463.8770 ms | tok/sec 1130230.7340
for step 15261 | loss 3.115716 | norm 0.3061 | time 463.9442 ms | tok/sec 1130066.9428
for step 15262 | loss 3.188300 | norm 0.3308 | time 464.4177 ms | tok/sec 1128914.7776
for step 15263 | loss 3.163602 | norm 0.3227 | time 466.9216 ms | tok/sec 1122860.9600
for step 15264 | loss 3.184887 | norm 0.3128 | time 465.0886 ms | tok/sec 1127286.2706
for step 15265 | loss 3.207958 | norm 0.3454 | time 464.6509 ms | tok/sec 1128348.2601
for step 15266 | loss 3.212340 | norm 0.3156 | time 465.3161 ms | tok/sec 1126735.2413
for step 15267 | loss 3.184287 | norm 0.3227 | time 465.0013 ms | tok/sec 1127497.8148
for step 15268 | loss 3.208587 | norm 0.3042 | time 464.1671 ms | tok/sec 1129524.2157
for step 15269 | loss 3.179387 | norm 0.3253 | time 465.5066 ms | tok/sec 1126274.1542
for step 15270 | loss 3.208624 | norm 0.3010 | time 464.1299 ms | tok/sec 1129614.7307
for step 15271 | loss 3.177899 | norm 0.3412 | time 465.0786 ms | tok/sec 1127310.5421
for step 15272 | loss 3.154604 | norm 0.3305 | time 465.0891 ms | tok/sec 1127285.1148
for step 15273 | loss 3.250858 | norm 0.3148 | time 465.0962 ms | tok/sec 1127267.7787
for step 15274 | loss 3.222776 | norm 0.3132 | time 465.9455 ms | tok/sec 1125213.1847
for step 15275 | loss 3.218896 | norm 0.2943 | time 470.2990 ms | tok/sec 1114797.1692
for step 15276 | loss 3.201548 | norm 0.3355 | time 465.4179 ms | tok/sec 1126488.7811
for step 15277 | loss 3.221848 | norm 0.2956 | time 465.9631 ms | tok/sec 1125170.5803
for step 15278 | loss 3.199232 | norm 0.3075 | time 465.5106 ms | tok/sec 1126264.3480
for step 15279 | loss 3.205987 | norm 0.3151 | time 465.7602 ms | tok/sec 1125660.7265
for step 15280 | loss 3.209500 | norm 0.3053 | time 465.6577 ms | tok/sec 1125908.5538
for step 15281 | loss 3.187554 | norm 0.2981 | time 466.7656 ms | tok/sec 1123236.0582
for step 15282 | loss 3.242783 | norm 0.3021 | time 465.9798 ms | tok/sec 1125130.2817
for step 15283 | loss 3.218394 | norm 0.3046 | time 467.4277 ms | tok/sec 1121645.0495
for step 15284 | loss 3.227314 | norm 0.2844 | time 464.9832 ms | tok/sec 1127541.7520
for step 15285 | loss 3.201340 | norm 0.2966 | time 465.5347 ms | tok/sec 1126206.0907
for step 15286 | loss 3.146051 | norm 0.3018 | time 465.2970 ms | tok/sec 1126781.4285
for step 15287 | loss 3.185202 | norm 0.2871 | time 465.9834 ms | tok/sec 1125121.6467
for step 15288 | loss 3.140778 | norm 0.2883 | time 466.0084 ms | tok/sec 1125061.2052
for step 15289 | loss 3.138381 | norm 0.2844 | time 466.0852 ms | tok/sec 1124875.8918
for step 15290 | loss 3.148427 | norm 0.2895 | time 465.4403 ms | tok/sec 1126434.5397
for step 15291 | loss 3.251940 | norm 0.3098 | time 466.0580 ms | tok/sec 1124941.4927
for step 15292 | loss 3.168488 | norm 0.3057 | time 466.0399 ms | tok/sec 1124985.2309
for step 15293 | loss 3.201711 | norm 0.3148 | time 465.8942 ms | tok/sec 1125336.9863
for step 15294 | loss 3.189132 | norm 0.2833 | time 465.4744 ms | tok/sec 1126352.0336
for step 15295 | loss 3.215484 | norm 0.2975 | time 465.4946 ms | tok/sec 1126302.9972
for step 15296 | loss 3.132949 | norm 0.3267 | time 466.4478 ms | tok/sec 1124001.3696
for step 15297 | loss 3.189126 | norm 0.2966 | time 465.4934 ms | tok/sec 1126305.8815
for step 15298 | loss 3.181120 | norm 0.3105 | time 465.9796 ms | tok/sec 1125130.8574
for step 15299 | loss 3.244120 | norm 0.3242 | time 465.2021 ms | tok/sec 1127011.2662
for step 15300 | loss 3.251253 | norm 0.3337 | time 464.8135 ms | tok/sec 1127953.5403
for step 15301 | loss 3.285979 | norm 0.3117 | time 464.6380 ms | tok/sec 1128379.5254
for step 15302 | loss 3.197917 | norm 0.3208 | time 465.9123 ms | tok/sec 1125293.2208
for step 15303 | loss 3.194319 | norm 0.3161 | time 466.0282 ms | tok/sec 1125013.4323
for step 15304 | loss 3.211413 | norm 0.3062 | time 465.5628 ms | tok/sec 1126138.0355
for step 15305 | loss 3.156947 | norm 0.3251 | time 466.0168 ms | tok/sec 1125041.0595
for step 15306 | loss 3.199972 | norm 0.3095 | time 465.1458 ms | tok/sec 1127147.5961
for step 15307 | loss 3.201557 | norm 0.3472 | time 465.7025 ms | tok/sec 1125800.1880
for step 15308 | loss 3.205325 | norm 0.3239 | time 464.8712 ms | tok/sec 1127813.5448
for step 15309 | loss 3.217461 | norm 0.3435 | time 465.6043 ms | tok/sec 1126037.6978
for step 15310 | loss 3.229108 | norm 0.3336 | time 465.0126 ms | tok/sec 1127470.6448
for step 15311 | loss 3.194928 | norm 0.3202 | time 465.4837 ms | tok/sec 1126329.5340
for step 15312 | loss 3.237453 | norm 0.3431 | time 465.1527 ms | tok/sec 1127130.8419
for step 15313 | loss 3.216551 | norm 0.2941 | time 464.6742 ms | tok/sec 1128291.5238
for step 15314 | loss 3.199145 | norm 0.3319 | time 465.5530 ms | tok/sec 1126161.6809
for step 15315 | loss 3.208915 | norm 0.3155 | time 464.8490 ms | tok/sec 1127867.3406
for step 15316 | loss 3.232671 | norm 0.3090 | time 466.0823 ms | tok/sec 1124882.7968
for step 15317 | loss 3.209036 | norm 0.2889 | time 465.4200 ms | tok/sec 1126483.5876
for step 15318 | loss 3.250115 | norm 0.3209 | time 464.9565 ms | tok/sec 1127606.5077
for step 15319 | loss 3.235397 | norm 0.2908 | time 464.4914 ms | tok/sec 1128735.7244
for step 15320 | loss 3.179930 | norm 0.3104 | time 465.0905 ms | tok/sec 1127281.6475
for step 15321 | loss 3.173106 | norm 0.2885 | time 466.4736 ms | tok/sec 1123939.3251
for step 15322 | loss 3.150081 | norm 0.3117 | time 465.4329 ms | tok/sec 1126452.4273
for step 15323 | loss 3.171633 | norm 0.3052 | time 465.4987 ms | tok/sec 1126293.1904
for step 15324 | loss 3.172503 | norm 0.3361 | time 465.2166 ms | tok/sec 1126976.0337
for step 15325 | loss 3.165320 | norm 0.3007 | time 465.0624 ms | tok/sec 1127349.8411
for step 15326 | loss 3.220995 | norm 0.3109 | time 465.2181 ms | tok/sec 1126972.5683
for step 15327 | loss 3.160689 | norm 0.3104 | time 465.0345 ms | tok/sec 1127417.4649
for step 15328 | loss 3.150520 | norm 0.2808 | time 464.5951 ms | tok/sec 1128483.7555
for step 15329 | loss 3.211813 | norm 0.2991 | time 464.4628 ms | tok/sec 1128805.2529
for step 15330 | loss 3.174427 | norm 0.2943 | time 465.0431 ms | tok/sec 1127396.6567
for step 15331 | loss 3.148854 | norm 0.3064 | time 465.2894 ms | tok/sec 1126799.9045
for step 15332 | loss 3.224743 | norm 0.3319 | time 465.5104 ms | tok/sec 1126264.9248
for step 15333 | loss 3.207900 | norm 0.3000 | time 464.7939 ms | tok/sec 1128000.9846
for step 15334 | loss 3.222119 | norm 0.3593 | time 465.7800 ms | tok/sec 1125612.9026
for step 15335 | loss 3.297405 | norm 0.3409 | time 464.9971 ms | tok/sec 1127508.2206
for step 15336 | loss 3.090770 | norm 0.3575 | time 465.7216 ms | tok/sec 1125754.0812
for step 15337 | loss 3.235962 | norm 0.2968 | time 465.5507 ms | tok/sec 1126167.4482
for step 15338 | loss 3.197286 | norm 0.3275 | time 464.5352 ms | tok/sec 1128629.1307
for step 15339 | loss 3.253833 | norm 0.3084 | time 465.6591 ms | tok/sec 1125905.0950
for step 15340 | loss 3.229732 | norm 0.3237 | time 465.0135 ms | tok/sec 1127468.3325
for step 15341 | loss 3.210251 | norm 0.3314 | time 465.3025 ms | tok/sec 1126768.1493
for step 15342 | loss 3.206987 | norm 0.3367 | time 465.8546 ms | tok/sec 1125432.5912
for step 15343 | loss 3.224165 | norm 0.3031 | time 464.3748 ms | tok/sec 1129019.1066
for step 15344 | loss 3.196649 | norm 0.3276 | time 464.7057 ms | tok/sec 1128215.1126
for step 15345 | loss 3.235362 | norm 0.2840 | time 465.4768 ms | tok/sec 1126346.2644
for step 15346 | loss 3.250551 | norm 0.3372 | time 464.8604 ms | tok/sec 1127839.5744
for step 15347 | loss 3.241717 | norm 0.2907 | time 465.5499 ms | tok/sec 1126169.1784
for step 15348 | loss 3.170929 | norm 0.3462 | time 465.4770 ms | tok/sec 1126345.6874
for step 15349 | loss 3.186944 | norm 0.2830 | time 465.6413 ms | tok/sec 1125948.3316
for step 15350 | loss 3.232011 | norm 0.3064 | time 464.8306 ms | tok/sec 1127911.8851
for step 15351 | loss 3.221600 | norm 0.3007 | time 465.6689 ms | tok/sec 1125881.4604
for step 15352 | loss 3.178744 | norm 0.2964 | time 466.0199 ms | tok/sec 1125033.5770
for step 15353 | loss 3.164375 | norm 0.3142 | time 465.1268 ms | tok/sec 1127193.8171
for step 15354 | loss 3.194382 | norm 0.2900 | time 465.6370 ms | tok/sec 1125958.7089
for step 15355 | loss 3.196419 | norm 0.3085 | time 465.5693 ms | tok/sec 1126122.4647
for step 15356 | loss 3.186985 | norm 0.2892 | time 465.5428 ms | tok/sec 1126186.4808
for step 15357 | loss 3.185841 | norm 0.3086 | time 464.7851 ms | tok/sec 1128022.3938
for step 15358 | loss 3.166445 | norm 0.2787 | time 464.8638 ms | tok/sec 1127831.4761
for step 15359 | loss 3.110882 | norm 0.2943 | time 465.3947 ms | tok/sec 1126544.7591
for step 15360 | loss 3.207244 | norm 0.2937 | time 465.4608 ms | tok/sec 1126384.9192
for step 15361 | loss 3.160253 | norm 0.2999 | time 465.7784 ms | tok/sec 1125616.9357
for step 15362 | loss 3.184448 | norm 0.2857 | time 465.6997 ms | tok/sec 1125807.1043
for step 15363 | loss 3.161163 | norm 0.2831 | time 465.9517 ms | tok/sec 1125198.2152
for step 15364 | loss 3.201828 | norm 0.3014 | time 464.7336 ms | tok/sec 1128147.3931
for step 15365 | loss 3.176204 | norm 0.3144 | time 466.0990 ms | tok/sec 1124842.5189
for step 15366 | loss 3.211751 | norm 0.2940 | time 465.5747 ms | tok/sec 1126109.2010
for step 15367 | loss 3.256106 | norm 0.3590 | time 464.4821 ms | tok/sec 1128758.3202
for step 15368 | loss 3.226882 | norm 0.3130 | time 465.3616 ms | tok/sec 1126624.9847
for step 15369 | loss 3.170655 | norm 0.3443 | time 465.6360 ms | tok/sec 1125961.0150
for step 15370 | loss 3.121615 | norm 0.3183 | time 465.9283 ms | tok/sec 1125254.6408
for step 15371 | loss 3.187630 | norm 0.3043 | time 466.1229 ms | tok/sec 1124784.9840
for step 15372 | loss 3.321153 | norm 0.3293 | time 465.8279 ms | tok/sec 1125497.1049
for step 15373 | loss 3.269359 | norm 0.3339 | time 465.0838 ms | tok/sec 1127297.8283
for step 15374 | loss 3.201867 | norm 0.3095 | time 465.3676 ms | tok/sec 1126610.5548
for step 15375 | loss 3.150443 | norm 0.3251 | time 466.4125 ms | tok/sec 1124086.4048
for step 15376 | loss 3.217987 | norm 0.3204 | time 465.6577 ms | tok/sec 1125908.5538
for step 15377 | loss 3.212734 | norm 0.3215 | time 465.6556 ms | tok/sec 1125913.7421
for step 15378 | loss 3.230639 | norm 0.3492 | time 465.5571 ms | tok/sec 1126151.8766
for step 15379 | loss 3.168576 | norm 0.3639 | time 467.0024 ms | tok/sec 1122666.6270
for step 15380 | loss 3.231972 | norm 0.3370 | time 466.0642 ms | tok/sec 1124926.5304
for step 15381 | loss 3.136944 | norm 0.3114 | time 465.4446 ms | tok/sec 1126424.1537
for step 15382 | loss 3.283442 | norm 0.3561 | time 465.7021 ms | tok/sec 1125801.3407
for step 15383 | loss 3.188239 | norm 0.3069 | time 465.2681 ms | tok/sec 1126851.2939
for step 15384 | loss 3.207484 | norm 0.3277 | time 465.0054 ms | tok/sec 1127487.9872
for step 15385 | loss 3.167454 | norm 0.2938 | time 465.2054 ms | tok/sec 1127003.1798
for step 15386 | loss 3.221648 | norm 0.3218 | time 465.3382 ms | tok/sec 1126681.5534
for step 15387 | loss 3.214131 | norm 0.2919 | time 465.5614 ms | tok/sec 1126141.4957
for step 15388 | loss 3.195726 | norm 0.3002 | time 465.0083 ms | tok/sec 1127481.0502
for step 15389 | loss 3.215823 | norm 0.3189 | time 464.9463 ms | tok/sec 1127631.3713
for step 15390 | loss 3.166751 | norm 0.2864 | time 465.7228 ms | tok/sec 1125751.1996
for step 15391 | loss 3.180725 | norm 0.3125 | time 464.9718 ms | tok/sec 1127569.5035
for step 15392 | loss 3.158136 | norm 0.2949 | time 465.4655 ms | tok/sec 1126373.3801
for step 15393 | loss 3.152239 | norm 0.3259 | time 465.7288 ms | tok/sec 1125736.7921
for step 15394 | loss 3.169550 | norm 0.3076 | time 465.6992 ms | tok/sec 1125808.2570
for step 15395 | loss 3.198110 | norm 0.3019 | time 464.9932 ms | tok/sec 1127517.4705
for step 15396 | loss 3.135635 | norm 0.3110 | time 465.3034 ms | tok/sec 1126765.8399
for step 15397 | loss 3.137956 | norm 0.2997 | time 465.0309 ms | tok/sec 1127426.1352
for step 15398 | loss 3.136070 | norm 0.3062 | time 465.2376 ms | tok/sec 1126925.2104
for step 15399 | loss 3.165974 | norm 0.2961 | time 465.3697 ms | tok/sec 1126605.3601
for step 15400 | loss 3.177279 | norm 0.2980 | time 465.5843 ms | tok/sec 1126086.1345
for step 15401 | loss 3.188317 | norm 0.2876 | time 466.3908 ms | tok/sec 1124138.6963
for step 15402 | loss 3.178532 | norm 0.3119 | time 465.0416 ms | tok/sec 1127400.1247
for step 15403 | loss 3.263048 | norm 0.2976 | time 465.3907 ms | tok/sec 1126554.5702
for step 15404 | loss 3.161421 | norm 0.2805 | time 465.1089 ms | tok/sec 1127237.1528
for step 15405 | loss 3.276510 | norm 0.3507 | time 465.6603 ms | tok/sec 1125902.2127
for step 15406 | loss 3.314355 | norm 0.3538 | time 464.9935 ms | tok/sec 1127516.8923
for step 15407 | loss 3.189818 | norm 0.3546 | time 465.3745 ms | tok/sec 1126593.8165
for step 15408 | loss 3.234359 | norm 0.3333 | time 465.3869 ms | tok/sec 1126563.8044
for step 15409 | loss 3.241122 | norm 0.3344 | time 465.3709 ms | tok/sec 1126602.4742
for step 15410 | loss 3.214891 | norm 0.3311 | time 464.1368 ms | tok/sec 1129597.9031
for step 15411 | loss 3.153659 | norm 0.3514 | time 465.5623 ms | tok/sec 1126139.1889
for step 15412 | loss 3.263632 | norm 0.3087 | time 465.2617 ms | tok/sec 1126866.8848
for step 15413 | loss 3.276187 | norm 0.3561 | time 466.1982 ms | tok/sec 1124603.2123
for step 15414 | loss 3.219642 | norm 0.3195 | time 464.5641 ms | tok/sec 1128559.0447
for step 15415 | loss 3.206615 | norm 0.3305 | time 465.8215 ms | tok/sec 1125512.6585
for step 15416 | loss 3.183369 | norm 0.3222 | time 465.1427 ms | tok/sec 1127155.1068
for step 15417 | loss 3.230493 | norm 0.3192 | time 465.6811 ms | tok/sec 1125852.0626
for step 15418 | loss 3.179338 | norm 0.3076 | time 464.6375 ms | tok/sec 1128380.6834
for step 15419 | loss 3.254396 | norm 0.3310 | time 465.6601 ms | tok/sec 1125902.7892
for step 15420 | loss 3.184966 | norm 0.2898 | time 465.4970 ms | tok/sec 1126297.2285
for step 15421 | loss 3.241152 | norm 0.3045 | time 466.1400 ms | tok/sec 1124743.5625
for step 15422 | loss 3.218750 | norm 0.3026 | time 464.2045 ms | tok/sec 1129433.1351
for step 15423 | loss 3.240402 | norm 0.2781 | time 465.7712 ms | tok/sec 1125634.2211
for step 15424 | loss 3.255671 | norm 0.3079 | time 465.3788 ms | tok/sec 1126583.4275
for step 15425 | loss 3.258266 | norm 0.3035 | time 465.2822 ms | tok/sec 1126817.2262
for step 15426 | loss 3.225861 | norm 0.3119 | time 465.8117 ms | tok/sec 1125536.2776
for step 15427 | loss 3.224868 | norm 0.3117 | time 464.9239 ms | tok/sec 1127685.7280
for step 15428 | loss 3.161118 | norm 0.3124 | time 465.2128 ms | tok/sec 1126985.2748
for step 15429 | loss 3.163257 | norm 0.2684 | time 465.8263 ms | tok/sec 1125501.1373
Will loading at 0 from edu_fineweb10B/edufineweb_train_000082.npy
for step 15430 | loss 3.182237 | norm 0.2886 | time 2618.1400 ms | tok/sec 200252.0887
for step 15431 | loss 3.141289 | norm 0.2967 | time 463.9466 ms | tok/sec 1130061.1355
for step 15432 | loss 3.216738 | norm 0.2863 | time 464.2956 ms | tok/sec 1129211.5863
for step 15433 | loss 3.154885 | norm 0.2970 | time 465.1916 ms | tok/sec 1127036.6811
for step 15434 | loss 3.162057 | norm 0.2679 | time 465.0295 ms | tok/sec 1127429.6033
for step 15435 | loss 3.166239 | norm 0.2874 | time 464.4129 ms | tok/sec 1128926.3688
for step 15436 | loss 3.194153 | norm 0.2800 | time 464.3149 ms | tok/sec 1129164.6199
for step 15437 | loss 3.185034 | norm 0.2968 | time 464.1013 ms | tok/sec 1129684.3675
for step 15438 | loss 3.180579 | norm 0.2853 | time 465.1060 ms | tok/sec 1127244.0868
for step 15439 | loss 3.315204 | norm 0.3359 | time 464.7515 ms | tok/sec 1128103.9874
for step 15440 | loss 3.180411 | norm 0.3137 | time 465.7803 ms | tok/sec 1125612.3264
for step 15441 | loss 3.249795 | norm 0.3557 | time 464.7989 ms | tok/sec 1127988.8339
for step 15442 | loss 3.204906 | norm 0.3109 | time 464.7038 ms | tok/sec 1128219.7433
for step 15443 | loss 3.179425 | norm 0.3167 | time 464.6583 ms | tok/sec 1128330.3123
for step 15444 | loss 3.203906 | norm 0.3207 | time 465.4200 ms | tok/sec 1126483.5876
for step 15445 | loss 3.183573 | norm 0.3166 | time 464.3042 ms | tok/sec 1129190.7119
for step 15446 | loss 3.229120 | norm 0.3005 | time 465.1680 ms | tok/sec 1127093.8689
for step 15447 | loss 3.214202 | norm 0.2915 | time 466.6350 ms | tok/sec 1123550.5536
for step 15448 | loss 3.218136 | norm 0.3324 | time 465.5008 ms | tok/sec 1126287.9987
for step 15449 | loss 3.242396 | norm 0.2964 | time 465.5144 ms | tok/sec 1126255.1187
for step 15450 | loss 3.227758 | norm 0.3093 | time 465.1377 ms | tok/sec 1127167.2396
for step 15451 | loss 3.172242 | norm 0.3300 | time 465.5223 ms | tok/sec 1126236.0838
for step 15452 | loss 3.304595 | norm 0.4186 | time 465.6746 ms | tok/sec 1125867.6260
for step 15453 | loss 3.180563 | norm 0.3632 | time 466.1283 ms | tok/sec 1124771.7518
for step 15454 | loss 3.183680 | norm 0.3218 | time 465.6973 ms | tok/sec 1125812.8680
for step 15455 | loss 3.194078 | norm 0.3283 | time 465.6906 ms | tok/sec 1125829.0066
for step 15456 | loss 3.175936 | norm 0.3220 | time 465.5030 ms | tok/sec 1126282.8070
for step 15457 | loss 3.245133 | norm 0.3147 | time 465.8222 ms | tok/sec 1125510.9303
for step 15458 | loss 3.246384 | norm 0.3070 | time 465.4024 ms | tok/sec 1126526.2915
for step 15459 | loss 3.269967 | norm 0.3425 | time 465.4436 ms | tok/sec 1126426.4617
for step 15460 | loss 3.224879 | norm 0.3352 | time 465.4181 ms | tok/sec 1126488.2040
for step 15461 | loss 3.226048 | norm 0.3230 | time 466.0563 ms | tok/sec 1124945.5211
for step 15462 | loss 3.158245 | norm 0.3076 | time 465.8000 ms | tok/sec 1125564.5066
for step 15463 | loss 3.188151 | norm 0.3583 | time 465.2011 ms | tok/sec 1127013.5766
for step 15464 | loss 3.174018 | norm 0.2761 | time 466.1458 ms | tok/sec 1124729.7560
for step 15465 | loss 3.205552 | norm 0.3341 | time 465.8208 ms | tok/sec 1125514.3867
for step 15466 | loss 3.240289 | norm 0.3400 | time 465.5905 ms | tok/sec 1126071.1417
for step 15467 | loss 3.205492 | norm 0.3230 | time 465.2796 ms | tok/sec 1126823.5777
for step 15468 | loss 3.187553 | norm 0.3337 | time 466.1508 ms | tok/sec 1124717.6756
for step 15469 | loss 3.192638 | norm 0.3311 | time 464.6957 ms | tok/sec 1128239.4241
for step 15470 | loss 3.182126 | norm 0.3491 | time 464.7241 ms | tok/sec 1128170.5441
for step 15471 | loss 3.203837 | norm 0.3303 | time 464.5300 ms | tok/sec 1128641.8746
for step 15472 | loss 3.115069 | norm 0.3128 | time 465.0354 ms | tok/sec 1127415.1528
for step 15473 | loss 3.210853 | norm 0.3566 | time 464.8881 ms | tok/sec 1127772.4783
for step 15474 | loss 3.199469 | norm 0.3142 | time 464.8194 ms | tok/sec 1127939.0763
for step 15475 | loss 3.248535 | norm 0.3106 | time 465.2090 ms | tok/sec 1126994.5160
for step 15476 | loss 3.225963 | norm 0.3340 | time 465.4989 ms | tok/sec 1126292.6135
for step 15477 | loss 3.249967 | norm 0.3034 | time 465.8365 ms | tok/sec 1125476.3676
for step 15478 | loss 3.225596 | norm 0.2942 | time 465.1594 ms | tok/sec 1127114.6659
for step 15479 | loss 3.317527 | norm 0.4360 | time 465.6789 ms | tok/sec 1125857.2504
for step 15480 | loss 3.278764 | norm 0.3481 | time 465.4336 ms | tok/sec 1126450.6962
for step 15481 | loss 3.179971 | norm 0.3363 | time 469.6236 ms | tok/sec 1116400.5351
for step 15482 | loss 3.197815 | norm 0.3431 | time 465.6982 ms | tok/sec 1125810.5625
for step 15483 | loss 3.273027 | norm 0.3196 | time 465.1711 ms | tok/sec 1127086.3591
for step 15484 | loss 3.353956 | norm 0.3766 | time 466.4729 ms | tok/sec 1123941.0484
for step 15485 | loss 3.273704 | norm 0.3340 | time 464.8104 ms | tok/sec 1127961.0617
for step 15486 | loss 3.224710 | norm 0.3170 | time 465.4710 ms | tok/sec 1126360.1106
for step 15487 | loss 3.201749 | norm 0.3342 | time 465.5657 ms | tok/sec 1126131.1151
for step 15488 | loss 3.211726 | norm 0.3265 | time 465.8170 ms | tok/sec 1125523.6038
for step 15489 | loss 3.212300 | norm 0.2869 | time 465.3568 ms | tok/sec 1126636.5288
for step 15490 | loss 3.258023 | norm 0.3155 | time 464.7622 ms | tok/sec 1128077.9456
for step 15491 | loss 3.235320 | norm 0.3178 | time 464.8881 ms | tok/sec 1127772.4783
for step 15492 | loss 3.248289 | norm 0.3073 | time 465.7757 ms | tok/sec 1125623.2737
for step 15493 | loss 3.226001 | norm 0.2951 | time 465.7726 ms | tok/sec 1125630.7640
for step 15494 | loss 3.156629 | norm 0.2871 | time 465.6274 ms | tok/sec 1125981.7702
for step 15495 | loss 3.210384 | norm 0.2907 | time 466.4667 ms | tok/sec 1123955.9845
for step 15496 | loss 3.231477 | norm 0.2812 | time 465.1845 ms | tok/sec 1127054.0101
for step 15497 | loss 3.167233 | norm 0.3103 | time 465.1775 ms | tok/sec 1127070.7620
for step 15498 | loss 3.181847 | norm 0.2971 | time 468.9307 ms | tok/sec 1118050.0151
for step 15499 | loss 3.206503 | norm 0.3059 | time 465.2090 ms | tok/sec 1126994.5160
validation loss 3.2396
HellaSwag accuracy: 2831/10042=0.2819
> Hello, I'm a language model, and I think that's a good guide. That means I might have a slightly different idea about that. That's because
> Hello, I'm a language model, which means that when you're talking about things, you will want to explain something quickly—sometimes much quicker than you can
> Hello, I'm a language model, I got this:
I'm a language model, I didn't get this:
I've got it's kind
> Hello, I'm a language model, and I'm interested to read the blog because I'm talking about grammar as well as doing more advanced grammar. This is
> Hello, I'm a language model, and I can't figure out a good programming language for me, if I am not sure how to write one. It
> Hello, I'm a language model, so when I'm teaching a class on the language program what is a simple model, it's called language model.
> 
Hello, I'm a language model, and I understand that you want to model exactly what I mean. I need to try to describe any aspect that I understand
> > > > Hello, I'm a language model, so I will be creating a new model (in my case, a set of classes to generate any classes with that name
Hello, I'm a language model, so I hope this inspires you to work with our target audience as we're learning more and more about the language. To
Hello, I'm a language model, and am trying to make an educated guess about the language of your language.
MySQL is the most-used programming
Hello, I'm a language model, there are some things to be remembered regarding some of the problems that I have encountered for teaching languages. This will give students
> > > > > Hello, I'm a language model, but if you're just adding variables to a model or using variables to represent objects, you'll be able to use data
Hello, I'm a language model, then what I actually wanted to find is what exactly it looks like. Now let's do that. First thing I'm
Hello, I'm a language model, and can't help you.
Now that you know how to implement it and let me get back to you and use
Hello, I'm a language model, I'm like to make things come easy?
After I have some data, I can see that it is pretty straight
Hello, I'm a language model, and I am always good at my translation and rendering strategies.
What we're trying to do is learn how our models
> > > > > > Hello, I'm a language model, and I've been talking to lots of children (or adults) about using the model for language learners, I will soon
Hello, I'm a language model, but I'm stuck, you know how it works.
You are only trying to find the exact correct answer because you
Hello, I'm a language model, and I'm a translator! I really am not fluent in the language syntax it has so many variations here, so we
Hello, I'm a language model, I can do it without any kind of assistance, however I'm working at a level where I know how to understand what
Hello, I'm a language model, so I've been thinking about this over and over again. What do you need to know? The main thing is that
Hello, I'm a language model, so I will be trying to be pretty complicated as I'm supposed to.
The process of a compiler is defined as
> > > > > Hello, I'm a language model, I like to model in my classes. I think I'm not that bad in my classes, but I would prefer that
Hello, I'm a language model, and it allows all people to communicate clearly without using complex technical systems such as HTML, XML or CSS, and you don
Hello, I'm a language model, and this post is about translation (e.g. from English).
- I'm a language model, the other
Hello, I'm a language model, doesn't need the background for an article or web page to show HTML?...not a website.
That is,
Hello, I'm a language model, but that hasn't always worked. I wanted to model the process of writing (which I use quite frequently, and have
> > > Hello, I'm a language model, and I've never had to work on it, but I'm already working. I have that, 'Okay, do
Hello, I'm a language model, and you've got already spoken an English for my friend, that's why I say "English"). I'm working with
Hello, I'm a language model, so I would say C# and F# would be the same thing. I don't know what the difference is.
> Hello, I'm a language model, and most of what I do is modeling the way the data are processed and the programming the way the Python program is executed> 
Hello, I'm a language model, why don't you mention you?
- How would the syntax model work
- The language does not have to be
for step 15500 | loss 3.192227 | norm 0.2883 | time 12648.7339 ms | tok/sec 41449.8404
for step 15501 | loss 3.170650 | norm 0.3213 | time 465.3978 ms | tok/sec 1126537.2566
for step 15502 | loss 3.271527 | norm 0.3295 | time 462.8983 ms | tok/sec 1132620.3869
for step 15503 | loss 3.179523 | norm 0.3193 | time 462.8992 ms | tok/sec 1132618.0535
for step 15504 | loss 3.147704 | norm 0.3138 | time 462.7242 ms | tok/sec 1133046.4021
for step 15505 | loss 3.152967 | norm 0.3200 | time 462.7132 ms | tok/sec 1133073.2577
for step 15506 | loss 3.196957 | norm 0.2967 | time 462.8477 ms | tok/sec 1132744.0734
for step 15507 | loss 3.167483 | norm 0.2940 | time 463.1770 ms | tok/sec 1131938.8459
for step 15508 | loss 3.157615 | norm 0.2696 | time 463.4693 ms | tok/sec 1131224.9537
for step 15509 | loss 3.195898 | norm 0.2965 | time 462.7151 ms | tok/sec 1133068.5871
for step 15510 | loss 3.220086 | norm 0.2988 | time 463.6729 ms | tok/sec 1130728.2067
for step 15511 | loss 3.206939 | norm 0.2946 | time 463.9103 ms | tok/sec 1130149.4133
for step 15512 | loss 3.227186 | norm 0.3172 | time 463.0589 ms | tok/sec 1132227.3367
for step 15513 | loss 3.236276 | norm 0.2976 | time 463.7995 ms | tok/sec 1130419.5594
for step 15514 | loss 3.208149 | norm 0.3548 | time 464.3953 ms | tok/sec 1128969.2581
for step 15515 | loss 3.226257 | norm 0.3082 | time 464.9625 ms | tok/sec 1127592.0527
for step 15516 | loss 3.258555 | norm 0.3132 | time 463.8889 ms | tok/sec 1130201.6896
for step 15517 | loss 3.181781 | norm 0.2991 | time 465.0400 ms | tok/sec 1127404.1707
for step 15518 | loss 3.244868 | norm 0.3428 | time 464.4518 ms | tok/sec 1128831.9077
for step 15519 | loss 3.236036 | norm 0.3418 | time 465.2286 ms | tok/sec 1126947.1563
for step 15520 | loss 3.169383 | norm 0.3282 | time 465.6250 ms | tok/sec 1125987.5357
for step 15521 | loss 3.221407 | norm 0.3305 | time 464.5400 ms | tok/sec 1128617.5457
for step 15522 | loss 3.163124 | norm 0.3041 | time 464.1435 ms | tok/sec 1129581.6563
for step 15523 | loss 3.183328 | norm 0.3457 | time 464.7028 ms | tok/sec 1128222.0586
for step 15524 | loss 3.231625 | norm 0.3186 | time 464.5760 ms | tok/sec 1128530.0861
for step 15525 | loss 3.191728 | norm 0.2952 | time 465.0040 ms | tok/sec 1127491.4557
for step 15526 | loss 3.172566 | norm 0.2965 | time 465.2226 ms | tok/sec 1126961.5948
for step 15527 | loss 3.231848 | norm 0.3170 | time 464.9096 ms | tok/sec 1127720.4265
for step 15528 | loss 3.208957 | norm 0.2794 | time 464.3118 ms | tok/sec 1129172.1574
for step 15529 | loss 3.245121 | norm 0.2921 | time 464.5095 ms | tok/sec 1128691.6941
for step 15530 | loss 3.226476 | norm 0.3059 | time 464.1793 ms | tok/sec 1129494.6274
for step 15531 | loss 3.159609 | norm 0.2779 | time 467.0873 ms | tok/sec 1122462.6210
for step 15532 | loss 3.230751 | norm 0.2852 | time 465.7280 ms | tok/sec 1125738.5210
for step 15533 | loss 3.156264 | norm 0.2745 | time 464.5662 ms | tok/sec 1128553.8321
for step 15534 | loss 3.202516 | norm 0.2929 | time 465.3478 ms | tok/sec 1126658.4635
for step 15535 | loss 3.159021 | norm 0.2850 | time 464.9007 ms | tok/sec 1127741.8250
for step 15536 | loss 3.192117 | norm 0.2874 | time 466.1078 ms | tok/sec 1124821.2303
for step 15537 | loss 3.172205 | norm 0.3036 | time 464.9794 ms | tok/sec 1127551.0023
for step 15538 | loss 3.248830 | norm 0.2953 | time 465.2314 ms | tok/sec 1126940.2259
for step 15539 | loss 3.202326 | norm 0.2989 | time 465.9424 ms | tok/sec 1125220.6696
for step 15540 | loss 3.153569 | norm 0.2966 | time 465.0159 ms | tok/sec 1127462.5519
for step 15541 | loss 3.166314 | norm 0.2950 | time 465.2979 ms | tok/sec 1126779.1191
for step 15542 | loss 3.228189 | norm 0.3304 | time 464.8101 ms | tok/sec 1127961.6402
for step 15543 | loss 3.167189 | norm 0.2961 | time 466.2030 ms | tok/sec 1124591.7098
for step 15544 | loss 3.166723 | norm 0.3291 | time 466.5873 ms | tok/sec 1123665.3769
for step 15545 | loss 3.234808 | norm 0.3080 | time 465.2097 ms | tok/sec 1126992.7833
for step 15546 | loss 3.189383 | norm 0.3222 | time 464.9129 ms | tok/sec 1127712.3300
for step 15547 | loss 3.232399 | norm 0.3262 | time 465.1570 ms | tok/sec 1127120.4430
for step 15548 | loss 3.234868 | norm 0.3133 | time 465.5228 ms | tok/sec 1126234.9302
for step 15549 | loss 3.230905 | norm 0.2934 | time 465.6408 ms | tok/sec 1125949.4847
for step 15550 | loss 3.207367 | norm 0.2933 | time 465.9486 ms | tok/sec 1125205.6999
for step 15551 | loss 3.164437 | norm 0.3200 | time 465.0512 ms | tok/sec 1127377.0052
for step 15552 | loss 3.202640 | norm 0.3118 | time 464.4179 ms | tok/sec 1128914.1981
for step 15553 | loss 3.238672 | norm 0.2859 | time 465.5430 ms | tok/sec 1126185.9040
for step 15554 | loss 3.171543 | norm 0.3205 | time 465.6410 ms | tok/sec 1125948.9081
for step 15555 | loss 3.166615 | norm 0.2988 | time 465.3463 ms | tok/sec 1126661.9269
for step 15556 | loss 3.162604 | norm 0.2917 | time 466.5430 ms | tok/sec 1123772.1836
for step 15557 | loss 3.210661 | norm 0.3051 | time 465.9314 ms | tok/sec 1125247.1555
for step 15558 | loss 3.205606 | norm 0.3107 | time 465.5774 ms | tok/sec 1126102.8576
for step 15559 | loss 3.218387 | norm 0.3137 | time 464.4771 ms | tok/sec 1128770.4876
for step 15560 | loss 3.193502 | norm 0.2817 | time 465.5576 ms | tok/sec 1126150.7231
for step 15561 | loss 3.211455 | norm 0.2995 | time 465.4264 ms | tok/sec 1126468.0072
for step 15562 | loss 3.324504 | norm 0.4179 | time 465.7574 ms | tok/sec 1125667.6411
for step 15563 | loss 3.264012 | norm 0.3180 | time 465.9240 ms | tok/sec 1125265.0053
for step 15564 | loss 3.188911 | norm 0.3377 | time 465.1234 ms | tok/sec 1127201.9062
for step 15565 | loss 3.246503 | norm 0.3194 | time 465.9004 ms | tok/sec 1125322.0135
for step 15566 | loss 3.203997 | norm 0.3090 | time 465.3060 ms | tok/sec 1126759.4891
for step 15567 | loss 3.146193 | norm 0.3051 | time 464.8185 ms | tok/sec 1127941.3905
for step 15568 | loss 3.200723 | norm 0.2902 | time 465.7526 ms | tok/sec 1125679.1657
for step 15569 | loss 3.190890 | norm 0.2944 | time 466.1553 ms | tok/sec 1124706.7459
for step 15570 | loss 3.272232 | norm 0.3341 | time 464.0458 ms | tok/sec 1129819.6036
for step 15571 | loss 3.177311 | norm 0.3100 | time 466.2952 ms | tok/sec 1124369.1816
for step 15572 | loss 3.206168 | norm 0.2997 | time 465.1015 ms | tok/sec 1127255.0658
for step 15573 | loss 3.130620 | norm 0.2916 | time 465.6835 ms | tok/sec 1125846.2985
for step 15574 | loss 3.191236 | norm 0.3011 | time 465.8134 ms | tok/sec 1125532.2450
for step 15575 | loss 3.173801 | norm 0.3098 | time 465.5299 ms | tok/sec 1126217.6263
for step 15576 | loss 3.230962 | norm 0.3096 | time 464.4856 ms | tok/sec 1128749.6294
for step 15577 | loss 3.225366 | norm 0.3089 | time 465.5366 ms | tok/sec 1126201.4766
for step 15578 | loss 3.175879 | norm 0.2964 | time 465.1561 ms | tok/sec 1127122.7539
for step 15579 | loss 3.192528 | norm 0.3028 | time 465.1074 ms | tok/sec 1127240.6198
for step 15580 | loss 3.264605 | norm 0.3211 | time 465.1108 ms | tok/sec 1127232.5302
for step 15581 | loss 3.264103 | norm 0.3190 | time 466.1856 ms | tok/sec 1124633.6953
for step 15582 | loss 3.279025 | norm 0.3450 | time 465.8651 ms | tok/sec 1125407.2486
for step 15583 | loss 3.209330 | norm 0.3182 | time 465.3666 ms | tok/sec 1126612.8635
for step 15584 | loss 3.280322 | norm 0.3549 | time 465.5719 ms | tok/sec 1126116.1211
for step 15585 | loss 3.275145 | norm 0.3162 | time 464.7052 ms | tok/sec 1128216.2702
for step 15586 | loss 3.246284 | norm 0.3653 | time 465.3385 ms | tok/sec 1126680.9762
for step 15587 | loss 3.268757 | norm 0.3441 | time 465.9784 ms | tok/sec 1125133.7358
for step 15588 | loss 3.261878 | norm 0.3307 | time 465.0195 ms | tok/sec 1127453.8810
for step 15589 | loss 3.231767 | norm 0.3535 | time 464.7133 ms | tok/sec 1128196.5902
for step 15590 | loss 3.245127 | norm 0.3359 | time 465.2207 ms | tok/sec 1126966.2152
for step 15591 | loss 3.166963 | norm 0.3936 | time 464.8385 ms | tok/sec 1127892.7942
for step 15592 | loss 3.175657 | norm 0.3067 | time 465.6765 ms | tok/sec 1125863.0146
for step 15593 | loss 3.221720 | norm 0.3137 | time 464.7169 ms | tok/sec 1128187.9080
for step 15594 | loss 3.185229 | norm 0.2957 | time 464.7655 ms | tok/sec 1128069.8439
for step 15595 | loss 3.281174 | norm 0.3290 | time 465.5473 ms | tok/sec 1126175.5225
for step 15596 | loss 3.196453 | norm 0.2967 | time 464.4988 ms | tok/sec 1128717.7643
for step 15597 | loss 3.210615 | norm 0.3156 | time 465.2395 ms | tok/sec 1126920.5904
for step 15598 | loss 3.205504 | norm 0.2847 | time 466.1672 ms | tok/sec 1124677.9847
for step 15599 | loss 3.211085 | norm 0.3032 | time 466.6853 ms | tok/sec 1123429.4406
for step 15600 | loss 3.225494 | norm 0.3095 | time 465.0238 ms | tok/sec 1127443.4762
for step 15601 | loss 3.182557 | norm 0.2870 | time 464.7057 ms | tok/sec 1128215.1126
for step 15602 | loss 3.158988 | norm 0.2867 | time 465.0555 ms | tok/sec 1127366.6018
for step 15603 | loss 3.149858 | norm 0.2850 | time 464.6444 ms | tok/sec 1128363.8925
for step 15604 | loss 3.224611 | norm 0.2913 | time 465.3633 ms | tok/sec 1126620.9442
for step 15605 | loss 3.316111 | norm 0.3749 | time 465.5521 ms | tok/sec 1126163.9878
for step 15606 | loss 3.194866 | norm 0.3239 | time 465.1344 ms | tok/sec 1127175.3283
for step 15607 | loss 3.206649 | norm 0.3233 | time 465.8885 ms | tok/sec 1125350.8077
for step 15608 | loss 3.193601 | norm 0.3083 | time 465.2455 ms | tok/sec 1126906.1529
for step 15609 | loss 3.212843 | norm 0.3134 | time 465.4014 ms | tok/sec 1126528.5999
for step 15610 | loss 3.170737 | norm 0.2906 | time 465.3041 ms | tok/sec 1126764.1079
for step 15611 | loss 3.183704 | norm 0.3060 | time 465.7907 ms | tok/sec 1125586.9757
for step 15612 | loss 3.183671 | norm 0.3013 | time 466.1777 ms | tok/sec 1124652.6760
for step 15613 | loss 3.121117 | norm 0.3247 | time 465.9233 ms | tok/sec 1125266.7328
for step 15614 | loss 3.136976 | norm 0.2999 | time 466.6619 ms | tok/sec 1123485.6889
for step 15615 | loss 3.222236 | norm 0.3668 | time 466.2802 ms | tok/sec 1124405.4012
for step 15616 | loss 3.231951 | norm 0.3166 | time 465.8535 ms | tok/sec 1125435.4711
for step 15617 | loss 3.242725 | norm 0.3624 | time 465.4737 ms | tok/sec 1126353.7643
for step 15618 | loss 3.286749 | norm 0.3311 | time 466.1169 ms | tok/sec 1124799.3671
for step 15619 | loss 3.255108 | norm 0.3306 | time 466.2869 ms | tok/sec 1124389.3033
Will loading at 0 from edu_fineweb10B/edufineweb_train_000083.npy
for step 15620 | loss 3.247304 | norm 0.3591 | time 2622.4856 ms | tok/sec 199920.2560
for step 15621 | loss 3.229864 | norm 0.3098 | time 483.5303 ms | tok/sec 1084291.9636
for step 15622 | loss 3.242255 | norm 0.3455 | time 463.6018 ms | tok/sec 1130901.4951
for step 15623 | loss 3.270842 | norm 0.3193 | time 463.1300 ms | tok/sec 1132053.6418
for step 15624 | loss 3.228191 | norm 0.3442 | time 463.8016 ms | tok/sec 1130414.3295
for step 15625 | loss 3.242633 | norm 0.3406 | time 464.0360 ms | tok/sec 1129843.4038
for step 15626 | loss 3.231841 | norm 0.3314 | time 464.8852 ms | tok/sec 1127779.4189
for step 15627 | loss 3.160286 | norm 0.3075 | time 464.1705 ms | tok/sec 1129516.0933
for step 15628 | loss 3.228575 | norm 0.3649 | time 464.0946 ms | tok/sec 1129700.6174
for step 15629 | loss 3.228948 | norm 0.3240 | time 464.7555 ms | tok/sec 1128094.1492
for step 15630 | loss 3.205400 | norm 0.3150 | time 464.6790 ms | tok/sec 1128279.9457
for step 15631 | loss 3.227239 | norm 0.3240 | time 465.3981 ms | tok/sec 1126536.6794
for step 15632 | loss 3.156505 | norm 0.3293 | time 466.4516 ms | tok/sec 1123992.1774
for step 15633 | loss 3.230681 | norm 0.3023 | time 464.8876 ms | tok/sec 1127773.6351
for step 15634 | loss 3.218037 | norm 0.3587 | time 466.0976 ms | tok/sec 1124845.9712
for step 15635 | loss 3.249448 | norm 0.3020 | time 465.0183 ms | tok/sec 1127456.7713
for step 15636 | loss 3.219414 | norm 0.3365 | time 465.6119 ms | tok/sec 1126019.2469
for step 15637 | loss 3.169111 | norm 0.3028 | time 465.9467 ms | tok/sec 1125210.3060
for step 15638 | loss 3.206878 | norm 0.3221 | time 466.8982 ms | tok/sec 1122917.1514
for step 15639 | loss 3.212490 | norm 0.3099 | time 465.0595 ms | tok/sec 1127356.7765
for step 15640 | loss 3.156761 | norm 0.2993 | time 466.4416 ms | tok/sec 1124016.3073
for step 15641 | loss 3.160227 | norm 0.3069 | time 466.2654 ms | tok/sec 1124441.0480
for step 15642 | loss 3.222734 | norm 0.2973 | time 465.8806 ms | tok/sec 1125369.8126
for step 15643 | loss 3.163542 | norm 0.2916 | time 466.2340 ms | tok/sec 1124516.9489
for step 15644 | loss 3.201864 | norm 0.3234 | time 466.0976 ms | tok/sec 1124845.9712
for step 15645 | loss 3.212029 | norm 0.2925 | time 465.9905 ms | tok/sec 1125104.3770
for step 15646 | loss 3.198992 | norm 0.3013 | time 465.2107 ms | tok/sec 1126990.4730
for step 15647 | loss 3.152871 | norm 0.3014 | time 466.0976 ms | tok/sec 1124845.9712
for step 15648 | loss 3.176224 | norm 0.2789 | time 465.4477 ms | tok/sec 1126416.6527
for step 15649 | loss 3.352439 | norm 0.3778 | time 466.0902 ms | tok/sec 1124863.8083
for step 15650 | loss 3.169773 | norm 0.2993 | time 466.6831 ms | tok/sec 1123434.6060
for step 15651 | loss 3.225958 | norm 0.3449 | time 465.7426 ms | tok/sec 1125703.3680
for step 15652 | loss 3.219523 | norm 0.3777 | time 466.4307 ms | tok/sec 1124042.7365
for step 15653 | loss 3.291016 | norm 0.3568 | time 465.0524 ms | tok/sec 1127374.1153
for step 15654 | loss 3.257501 | norm 0.3129 | time 465.6429 ms | tok/sec 1125944.2961
for step 15655 | loss 3.258495 | norm 0.3638 | time 466.5427 ms | tok/sec 1123772.7578
for step 15656 | loss 3.234351 | norm 0.3377 | time 465.9245 ms | tok/sec 1125263.8537
for step 15657 | loss 3.230131 | norm 0.3506 | time 466.2578 ms | tok/sec 1124459.4473
for step 15658 | loss 3.160273 | norm 0.3219 | time 465.9162 ms | tok/sec 1125284.0074
for step 15659 | loss 3.278630 | norm 0.3548 | time 465.8785 ms | tok/sec 1125374.9959
for step 15660 | loss 3.189057 | norm 0.3187 | time 465.6885 ms | tok/sec 1125834.1942
for step 15661 | loss 3.208205 | norm 0.3414 | time 465.5368 ms | tok/sec 1126200.8998
for step 15662 | loss 3.214816 | norm 0.3170 | time 465.5430 ms | tok/sec 1126185.9040
for step 15663 | loss 3.195825 | norm 0.3073 | time 465.7829 ms | tok/sec 1125605.9886
for step 15664 | loss 3.208148 | norm 0.3195 | time 465.9278 ms | tok/sec 1125255.7924
for step 15665 | loss 3.224069 | norm 0.3068 | time 465.9123 ms | tok/sec 1125293.2208
for step 15666 | loss 3.183959 | norm 0.2961 | time 465.8086 ms | tok/sec 1125543.7668
for step 15667 | loss 3.230523 | norm 0.3112 | time 466.9499 ms | tok/sec 1122792.7351
for step 15668 | loss 3.213144 | norm 0.3083 | time 464.7696 ms | tok/sec 1128060.0064
for step 15669 | loss 3.215250 | norm 0.3003 | time 465.1656 ms | tok/sec 1127099.6458
for step 15670 | loss 3.191560 | norm 0.2939 | time 465.3711 ms | tok/sec 1126601.8970
for step 15671 | loss 3.206326 | norm 0.3011 | time 466.1093 ms | tok/sec 1124817.7781
for step 15672 | loss 3.186385 | norm 0.3069 | time 466.1198 ms | tok/sec 1124792.4632
for step 15673 | loss 3.183712 | norm 0.2824 | time 464.7264 ms | tok/sec 1128164.7563
for step 15674 | loss 3.167706 | norm 0.3294 | time 465.0877 ms | tok/sec 1127288.5821
for step 15675 | loss 3.195625 | norm 0.3035 | time 464.5829 ms | tok/sec 1128513.2908
for step 15676 | loss 3.141187 | norm 0.3313 | time 465.2267 ms | tok/sec 1126951.7766
for step 15677 | loss 3.218184 | norm 0.2782 | time 464.8359 ms | tok/sec 1127899.1577
for step 15678 | loss 3.221476 | norm 0.3507 | time 464.9835 ms | tok/sec 1127541.1738
for step 15679 | loss 3.148413 | norm 0.3081 | time 465.8782 ms | tok/sec 1125375.5719
for step 15680 | loss 3.191953 | norm 0.3375 | time 465.9901 ms | tok/sec 1125105.5283
for step 15681 | loss 3.165541 | norm 0.3206 | time 465.5848 ms | tok/sec 1126084.9812
for step 15682 | loss 3.163728 | norm 0.3011 | time 464.8135 ms | tok/sec 1127953.5403
for step 15683 | loss 3.195114 | norm 0.3683 | time 465.2588 ms | tok/sec 1126873.8143
for step 15684 | loss 3.228494 | norm 0.3236 | time 465.1091 ms | tok/sec 1127236.5750
for step 15685 | loss 3.197030 | norm 0.3208 | time 465.2054 ms | tok/sec 1127003.1798
for step 15686 | loss 3.231626 | norm 0.3087 | time 465.3726 ms | tok/sec 1126598.4339
for step 15687 | loss 3.243222 | norm 0.3227 | time 464.7334 ms | tok/sec 1128147.9718
for step 15688 | loss 3.276748 | norm 0.3327 | time 465.1415 ms | tok/sec 1127157.9955
for step 15689 | loss 3.236449 | norm 0.3299 | time 465.3897 ms | tok/sec 1126556.8788
for step 15690 | loss 3.233541 | norm 0.3545 | time 465.0319 ms | tok/sec 1127423.8231
for step 15691 | loss 3.218570 | norm 0.3310 | time 465.0950 ms | tok/sec 1127270.6680
for step 15692 | loss 3.246140 | norm 0.3394 | time 465.4210 ms | tok/sec 1126481.2793
for step 15693 | loss 3.221984 | norm 0.3120 | time 464.8798 ms | tok/sec 1127792.7220
for step 15694 | loss 3.175640 | norm 0.3395 | time 465.5316 ms | tok/sec 1126213.5889
for step 15695 | loss 3.219160 | norm 0.3274 | time 465.7619 ms | tok/sec 1125656.6930
for step 15696 | loss 3.210238 | norm 0.3021 | time 465.9579 ms | tok/sec 1125183.2461
for step 15697 | loss 3.184719 | norm 0.2953 | time 466.1596 ms | tok/sec 1124696.3917
for step 15698 | loss 3.172741 | norm 0.3210 | time 465.8709 ms | tok/sec 1125393.4258
for step 15699 | loss 3.179404 | norm 0.3412 | time 465.5678 ms | tok/sec 1126125.9248
for step 15700 | loss 3.268842 | norm 0.3263 | time 465.4710 ms | tok/sec 1126360.1106
for step 15701 | loss 3.255451 | norm 0.3276 | time 465.3854 ms | tok/sec 1126567.2673
for step 15702 | loss 3.205311 | norm 0.3358 | time 465.1699 ms | tok/sec 1127089.2475
for step 15703 | loss 3.210747 | norm 0.2982 | time 466.0027 ms | tok/sec 1125075.0198
for step 15704 | loss 3.234578 | norm 0.2911 | time 466.1276 ms | tok/sec 1124773.4777
for step 15705 | loss 3.146224 | norm 0.2844 | time 465.3368 ms | tok/sec 1126685.0170
for step 15706 | loss 3.182238 | norm 0.2960 | time 464.8616 ms | tok/sec 1127836.6821
for step 15707 | loss 3.173726 | norm 0.3192 | time 465.7319 ms | tok/sec 1125729.3003
for step 15708 | loss 3.201924 | norm 0.3036 | time 465.6386 ms | tok/sec 1125954.6733
for step 15709 | loss 3.194201 | norm 0.3084 | time 466.3146 ms | tok/sec 1124322.6171
for step 15710 | loss 3.170257 | norm 0.2921 | time 466.7597 ms | tok/sec 1123250.4018
for step 15711 | loss 3.144031 | norm 0.3234 | time 466.3165 ms | tok/sec 1124318.0184
for step 15712 | loss 3.167169 | norm 0.2962 | time 465.8256 ms | tok/sec 1125502.8654
for step 15713 | loss 3.126063 | norm 0.2995 | time 465.0264 ms | tok/sec 1127437.1177
for step 15714 | loss 3.145418 | norm 0.3205 | time 466.4702 ms | tok/sec 1123947.3675
for step 15715 | loss 3.186145 | norm 0.3313 | time 465.7915 ms | tok/sec 1125585.2472
for step 15716 | loss 3.210581 | norm 0.3020 | time 466.2979 ms | tok/sec 1124362.8578
for step 15717 | loss 3.173126 | norm 0.3302 | time 466.6119 ms | tok/sec 1123606.2400
for step 15718 | loss 3.181763 | norm 0.3422 | time 464.6597 ms | tok/sec 1128326.8386
for step 15719 | loss 3.301754 | norm 0.3135 | time 465.7810 ms | tok/sec 1125610.5979
for step 15720 | loss 3.233729 | norm 0.3368 | time 465.9703 ms | tok/sec 1125153.3091
for step 15721 | loss 3.184548 | norm 0.3161 | time 465.1952 ms | tok/sec 1127028.0168
for step 15722 | loss 3.178524 | norm 0.3297 | time 465.3356 ms | tok/sec 1126687.9033
for step 15723 | loss 3.266330 | norm 0.3422 | time 465.3261 ms | tok/sec 1126710.9945
for step 15724 | loss 3.229539 | norm 0.3209 | time 465.9593 ms | tok/sec 1125179.7918
for step 15725 | loss 3.154466 | norm 0.3080 | time 465.7609 ms | tok/sec 1125658.9978
for step 15726 | loss 3.346753 | norm 0.4612 | time 465.9708 ms | tok/sec 1125152.1577
for step 15727 | loss 3.208379 | norm 0.4174 | time 465.8027 ms | tok/sec 1125558.1694
for step 15728 | loss 3.276875 | norm 0.4013 | time 466.0091 ms | tok/sec 1125059.4784
for step 15729 | loss 3.209667 | norm 0.3181 | time 464.6478 ms | tok/sec 1128355.7868
for step 15730 | loss 3.209923 | norm 0.3465 | time 464.5305 ms | tok/sec 1128640.7160
for step 15731 | loss 3.164702 | norm 0.3445 | time 465.2138 ms | tok/sec 1126982.9645
for step 15732 | loss 3.162589 | norm 0.3038 | time 465.5507 ms | tok/sec 1126167.4482
for step 15733 | loss 3.218639 | norm 0.3389 | time 465.2691 ms | tok/sec 1126848.9841
for step 15734 | loss 3.215521 | norm 0.3086 | time 464.6375 ms | tok/sec 1128380.6834
for step 15735 | loss 3.206352 | norm 0.2887 | time 466.0287 ms | tok/sec 1125012.2812
for step 15736 | loss 3.215412 | norm 0.3153 | time 465.4183 ms | tok/sec 1126487.6270
for step 15737 | loss 3.264746 | norm 0.3140 | time 466.2476 ms | tok/sec 1124484.1723
for step 15738 | loss 3.207591 | norm 0.2826 | time 465.2042 ms | tok/sec 1127006.0678
for step 15739 | loss 3.175768 | norm 0.3115 | time 465.4815 ms | tok/sec 1126334.7261
for step 15740 | loss 3.160722 | norm 0.3203 | time 466.1875 ms | tok/sec 1124629.0940
for step 15741 | loss 3.199070 | norm 0.3006 | time 466.6297 ms | tok/sec 1123563.1831
for step 15742 | loss 3.163260 | norm 0.3121 | time 466.0759 ms | tok/sec 1124898.3334
for step 15743 | loss 3.171427 | norm 0.3368 | time 465.5089 ms | tok/sec 1126268.3858
for step 15744 | loss 3.185615 | norm 0.3207 | time 465.1105 ms | tok/sec 1127233.1080
for step 15745 | loss 3.198223 | norm 0.3025 | time 465.3764 ms | tok/sec 1126589.1992
for step 15746 | loss 3.152823 | norm 0.3139 | time 466.0544 ms | tok/sec 1124950.1250
for step 15747 | loss 3.191839 | norm 0.2907 | time 465.5209 ms | tok/sec 1126239.5446
for step 15748 | loss 3.139120 | norm 0.2980 | time 465.0097 ms | tok/sec 1127477.5817
for step 15749 | loss 3.262697 | norm 0.3213 | time 465.8928 ms | tok/sec 1125340.4416
validation loss 3.2377
HellaSwag accuracy: 2829/10042=0.2817
> Hello, I'm a language model, and like most languages, you've not even used one. My first language works best in languages of the U.S
> Hello, I'm a language model, which means that every language has its own features and some that are defined by different principles
Cease and environment

> Hello, I'm a language model, but no two languages are alike.
I'm a computer literate and one of the first programmers of this course.
> > Hello, I'm a language model, and I'm very excited to write up Java. I guess there's plenty more to try.
I'm trying to
Hello, I'm a language model, and I can't understand you if I wasn't trying to translate and can't translate it. That was the problem when
> Hello, I'm a language model, so i'm not sure if i're going to understand your first-of-a-kind grammar question.
Do
> Hello, I'm a language model, so I do the grammar part.
I know what I have and know what I need.
I know how I
> > Hello, I'm a language model, and I understand that language as a language rather than as a unit. What I notice is that after all of the different
Hello, I'm a language model, and how I am going to do this, especially with the concept of an "official" dictionary, so I'll leave
> Hello, I'm a language model, for the sake of clarity, since they are not built over time.
I just have to work at this. And
> > Hello, I'm a language model, and I have already modeled a 3D sound model for 3D sound simulations.
The sound model is based on an
Hello, I'm a language model, so I want to have all the languages from my course. I'm interested in studying English? Because in order to learn
> Hello, I'm a language model, do I just need to convert. After creating an interpreter, I need to convert. I'm able to convert two to
> Hello, I'm a language model, we can use most of the elements in this project, if you prefer you have one. If you prefer you have two
> Hello, I'm a language model, and that means to be a model is a language model that can be an implementation tool.
What is Java?
> 
Hello, I'm a language model, I hope you read and see what is the difference between a sentence, a sentence, and a sentence or a single sentence
> Hello, I'm a language model, how do we get there?
Hi friends, when I say to write a program, I mean, a language model> 
Hello, I'm a language model, and it could possibly be called a Language Builder for instance, like English Language, or a language programming language. A good
>>>  Hello, I'm a language model, and I'm a beginner and am writing software, so I wanted to run Apache in one directory. As I mentioned this Hello, I'm a language model, learning, and learning in. I learned this language more quickly in the course than I did when I was in the processHello, I'm a language model, so I've tried to write my blog because I know how to work with a variety of language-dynamic methods like>


> Hello, I'm a language model, but this isn't my first time. A lot of the time there aren't enough people to make it. However, Hello, I'm a language model, and this post is about working through this, which is called "language modeling".
I'm trying to program languages and> > 

Hello, I'm a language model, and I think that we can make a whole lot of things.
If you are looking to design your first language,
Hello, I'm a language model, and am feeling a little overwhelmed.
When I use this article, I do not think students who have never heard of
> > > Hello, I'm a language model, isn't it? Yes, we have it from the perspective of having an engine which has the engine power supply and the
Hello, I'm a language model, and you've made what you see. Your favorite character, or your name, isn't the problem, and now that
Hello, I'm a language model, and my goal is to create my own language for the language model by making them easy to understand. Since language isn't
> > > Hello, I'm a language model, and you want the students to develop something. You can't do things all by myself -- you can't do things all
Hello, I'm a language model, and since I don't understand grammar, it's very helpful. I mean, yeah, if you find yourself needing to
Hello, I'm a language model, and I think it's important to have a clear understanding of the system. It is not because I live within a city
> Hello, I'm a language model, has two versions and is called a language. The most basic version of the language includes all of the words in a sentence
for step 15750 | loss 3.154890 | norm 0.2925 | time 12556.5932 ms | tok/sec 41754.0007
for step 15751 | loss 3.147518 | norm 0.2862 | time 462.6851 ms | tok/sec 1133142.1538
for step 15752 | loss 3.180536 | norm 0.2948 | time 465.2534 ms | tok/sec 1126887.0960
for step 15753 | loss 3.256047 | norm 0.3396 | time 463.4476 ms | tok/sec 1131277.9115
for step 15754 | loss 3.216110 | norm 0.2915 | time 463.8033 ms | tok/sec 1130410.2619
for step 15755 | loss 3.155571 | norm 0.3216 | time 462.6362 ms | tok/sec 1133261.8660
for step 15756 | loss 3.240774 | norm 0.3111 | time 463.1851 ms | tok/sec 1131919.0357
for step 15757 | loss 3.159603 | norm 0.4450 | time 464.1035 ms | tok/sec 1129679.1445
for step 15758 | loss 3.182427 | norm 0.3486 | time 464.1232 ms | tok/sec 1129630.9786
for step 15759 | loss 3.390286 | norm 0.3876 | time 463.6943 ms | tok/sec 1130675.8818
for step 15760 | loss 3.229364 | norm 0.3549 | time 465.3234 ms | tok/sec 1126717.3448
for step 15761 | loss 3.196579 | norm 0.3534 | time 464.6783 ms | tok/sec 1128281.6824
for step 15762 | loss 3.200493 | norm 0.3257 | time 464.2286 ms | tok/sec 1129374.5496
for step 15763 | loss 3.194603 | norm 0.3088 | time 464.2243 ms | tok/sec 1129384.9902
for step 15764 | loss 3.254710 | norm 0.3660 | time 465.5919 ms | tok/sec 1126067.6819
for step 15765 | loss 3.226621 | norm 0.2921 | time 464.2220 ms | tok/sec 1129390.7905
for step 15766 | loss 3.206891 | norm 0.3186 | time 464.8137 ms | tok/sec 1127952.9617
for step 15767 | loss 3.229347 | norm 0.3405 | time 463.4795 ms | tok/sec 1131199.9315
for step 15768 | loss 3.214484 | norm 0.2933 | time 464.8626 ms | tok/sec 1127834.3684
for step 15769 | loss 3.238352 | norm 0.3326 | time 465.1775 ms | tok/sec 1127070.7620
for step 15770 | loss 3.205069 | norm 0.2928 | time 465.6007 ms | tok/sec 1126046.3469
for step 15771 | loss 3.178290 | norm 0.3152 | time 466.1543 ms | tok/sec 1124709.0469
for step 15772 | loss 3.208335 | norm 0.3297 | time 465.8861 ms | tok/sec 1125356.5667
for step 15773 | loss 3.246799 | norm 0.2922 | time 465.7145 ms | tok/sec 1125771.3708
for step 15774 | loss 3.207269 | norm 0.3025 | time 465.7116 ms | tok/sec 1125778.2868
for step 15775 | loss 3.147589 | norm 0.2819 | time 464.0796 ms | tok/sec 1129737.1812
for step 15776 | loss 3.135054 | norm 0.3010 | time 465.5478 ms | tok/sec 1126174.3691
for step 15777 | loss 3.194828 | norm 0.2869 | time 465.5159 ms | tok/sec 1126251.6578
for step 15778 | loss 3.197655 | norm 0.2922 | time 464.9115 ms | tok/sec 1127715.7999
for step 15779 | loss 3.207815 | norm 0.2808 | time 464.9110 ms | tok/sec 1127716.9566
for step 15780 | loss 3.153154 | norm 0.2860 | time 465.1420 ms | tok/sec 1127156.8400
for step 15781 | loss 3.209636 | norm 0.3081 | time 465.6267 ms | tok/sec 1125983.4999
for step 15782 | loss 3.206372 | norm 0.2936 | time 465.4653 ms | tok/sec 1126373.9571
for step 15783 | loss 3.163044 | norm 0.2979 | time 465.4677 ms | tok/sec 1126368.1877
for step 15784 | loss 3.226695 | norm 0.2931 | time 465.0707 ms | tok/sec 1127329.6133
for step 15785 | loss 3.142307 | norm 0.3067 | time 465.0016 ms | tok/sec 1127497.2367
for step 15786 | loss 3.185693 | norm 0.2854 | time 464.8049 ms | tok/sec 1127974.3690
for step 15787 | loss 3.228818 | norm 0.3147 | time 464.7477 ms | tok/sec 1128113.2470
for step 15788 | loss 3.232434 | norm 0.3160 | time 465.2045 ms | tok/sec 1127005.4902
for step 15789 | loss 3.201408 | norm 0.3042 | time 465.9526 ms | tok/sec 1125195.9123
for step 15790 | loss 3.238911 | norm 0.3463 | time 465.4412 ms | tok/sec 1126432.2317
for step 15791 | loss 3.213350 | norm 0.3115 | time 464.6347 ms | tok/sec 1128387.6315
for step 15792 | loss 3.252606 | norm 0.2931 | time 465.1675 ms | tok/sec 1127095.0243
for step 15793 | loss 3.227866 | norm 0.2977 | time 464.6068 ms | tok/sec 1128455.3798
for step 15794 | loss 3.243945 | norm 0.2954 | time 466.4621 ms | tok/sec 1123966.8996
for step 15795 | loss 3.247576 | norm 0.3385 | time 464.8805 ms | tok/sec 1127790.9868
for step 15796 | loss 3.243487 | norm 0.3158 | time 464.6773 ms | tok/sec 1128283.9980
for step 15797 | loss 3.265446 | norm 0.3268 | time 465.6606 ms | tok/sec 1125901.6362
for step 15798 | loss 3.244648 | norm 0.3217 | time 465.0469 ms | tok/sec 1127387.4088
for step 15799 | loss 3.197400 | norm 0.3216 | time 466.2120 ms | tok/sec 1124569.8556
for step 15800 | loss 3.178530 | norm 0.3207 | time 464.6835 ms | tok/sec 1128268.9467
for step 15801 | loss 3.196986 | norm 0.2843 | time 465.0986 ms | tok/sec 1127262.0001
for step 15802 | loss 3.226671 | norm 0.3047 | time 465.1926 ms | tok/sec 1127034.3706
for step 15803 | loss 3.187216 | norm 0.3152 | time 465.0230 ms | tok/sec 1127445.2103
for step 15804 | loss 3.233478 | norm 0.2843 | time 465.4171 ms | tok/sec 1126490.5123
for step 15805 | loss 3.238361 | norm 0.3008 | time 465.7278 ms | tok/sec 1125739.0973
for step 15806 | loss 3.206845 | norm 0.3059 | time 465.1248 ms | tok/sec 1127198.4394
for step 15807 | loss 3.238977 | norm 0.2820 | time 465.3575 ms | tok/sec 1126634.7972
for step 15808 | loss 3.229363 | norm 0.2809 | time 464.9851 ms | tok/sec 1127537.1268
for step 15809 | loss 3.180786 | norm 0.3063 | time 465.2712 ms | tok/sec 1126843.7873
for step 15810 | loss 3.198805 | norm 0.2901 | time 464.6351 ms | tok/sec 1128386.4734
Will loading at 0 from edu_fineweb10B/edufineweb_train_000084.npy
for step 15811 | loss 3.148629 | norm 0.3089 | time 2660.2657 ms | tok/sec 197081.0672
for step 15812 | loss 3.185478 | norm 0.2995 | time 461.9112 ms | tok/sec 1135040.6709
for step 15813 | loss 3.196881 | norm 0.2835 | time 463.9058 ms | tok/sec 1130160.4490
for step 15814 | loss 3.256115 | norm 0.2908 | time 463.9974 ms | tok/sec 1129937.4536
for step 15815 | loss 3.260430 | norm 0.2793 | time 464.6599 ms | tok/sec 1128326.2596
for step 15816 | loss 3.159320 | norm 0.2932 | time 464.4115 ms | tok/sec 1128929.8462
for step 15817 | loss 3.196600 | norm 0.3269 | time 465.0185 ms | tok/sec 1127456.1933
for step 15818 | loss 3.190992 | norm 0.2872 | time 464.7565 ms | tok/sec 1128091.8344
for step 15819 | loss 3.172487 | norm 0.3177 | time 465.1275 ms | tok/sec 1127192.0838
for step 15820 | loss 3.226924 | norm 0.2944 | time 464.6959 ms | tok/sec 1128238.8453
for step 15821 | loss 3.140928 | norm 0.3345 | time 464.4983 ms | tok/sec 1128718.9230
for step 15822 | loss 3.218178 | norm 0.3077 | time 465.1423 ms | tok/sec 1127156.2623
for step 15823 | loss 3.176792 | norm 0.3000 | time 465.6496 ms | tok/sec 1125928.1541
for step 15824 | loss 3.181160 | norm 0.3269 | time 465.0469 ms | tok/sec 1127387.4088
for step 15825 | loss 3.205395 | norm 0.3135 | time 465.0981 ms | tok/sec 1127263.1558
for step 15826 | loss 3.174717 | norm 0.3164 | time 465.8303 ms | tok/sec 1125491.3445
for step 15827 | loss 3.246775 | norm 0.2975 | time 465.3869 ms | tok/sec 1126563.8044
for step 15828 | loss 3.203523 | norm 0.2972 | time 464.8640 ms | tok/sec 1127830.8977
for step 15829 | loss 3.147891 | norm 0.3041 | time 464.4101 ms | tok/sec 1128933.3236
for step 15830 | loss 3.240311 | norm 0.3139 | time 464.9258 ms | tok/sec 1127681.1017
for step 15831 | loss 3.214187 | norm 0.3197 | time 465.2650 ms | tok/sec 1126858.8006
for step 15832 | loss 3.205499 | norm 0.3307 | time 465.1182 ms | tok/sec 1127214.6178
for step 15833 | loss 3.210384 | norm 0.3309 | time 465.1716 ms | tok/sec 1127085.2037
for step 15834 | loss 3.206385 | norm 0.2996 | time 466.3761 ms | tok/sec 1124174.3262
for step 15835 | loss 3.225546 | norm 0.3204 | time 466.7277 ms | tok/sec 1123327.2896
for step 15836 | loss 3.182268 | norm 0.3011 | time 464.8671 ms | tok/sec 1127823.3780
for step 15837 | loss 3.206574 | norm 0.2860 | time 465.6074 ms | tok/sec 1126030.2020
for step 15838 | loss 3.194960 | norm 0.3034 | time 465.4186 ms | tok/sec 1126487.0499
for step 15839 | loss 3.155717 | norm 0.3442 | time 465.1709 ms | tok/sec 1127086.9368
for step 15840 | loss 3.231285 | norm 0.3131 | time 465.4245 ms | tok/sec 1126472.6235
for step 15841 | loss 3.194480 | norm 0.3125 | time 465.3621 ms | tok/sec 1126623.8303
for step 15842 | loss 3.221811 | norm 0.2907 | time 465.9121 ms | tok/sec 1125293.7966
for step 15843 | loss 3.249762 | norm 0.3166 | time 464.8643 ms | tok/sec 1127830.3193
for step 15844 | loss 3.248255 | norm 0.2921 | time 466.3026 ms | tok/sec 1124351.3602
for step 15845 | loss 3.138974 | norm 0.3138 | time 464.7155 ms | tok/sec 1128191.3809
for step 15846 | loss 3.155596 | norm 0.3176 | time 465.4617 ms | tok/sec 1126382.6114
for step 15847 | loss 3.189506 | norm 0.3192 | time 465.2505 ms | tok/sec 1126894.0257
for step 15848 | loss 3.172361 | norm 0.3211 | time 465.9405 ms | tok/sec 1125225.2758
for step 15849 | loss 3.181604 | norm 0.3040 | time 465.3423 ms | tok/sec 1126671.7401
for step 15850 | loss 3.186012 | norm 0.3305 | time 465.5755 ms | tok/sec 1126107.4710
for step 15851 | loss 3.178827 | norm 0.2857 | time 465.4474 ms | tok/sec 1126417.2297
for step 15852 | loss 3.142087 | norm 0.3046 | time 465.4667 ms | tok/sec 1126370.4954
for step 15853 | loss 3.201893 | norm 0.3077 | time 464.8683 ms | tok/sec 1127820.4859
for step 15854 | loss 3.206845 | norm 0.3023 | time 465.3866 ms | tok/sec 1126564.3816
for step 15855 | loss 3.240508 | norm 0.2842 | time 465.4543 ms | tok/sec 1126400.4972
for step 15856 | loss 3.178548 | norm 0.2872 | time 466.1074 ms | tok/sec 1124822.3810
for step 15857 | loss 3.213536 | norm 0.3042 | time 463.9571 ms | tok/sec 1130035.5840
for step 15858 | loss 3.223516 | norm 0.3161 | time 464.5174 ms | tok/sec 1128672.5768
for step 15859 | loss 3.242936 | norm 0.2894 | time 464.5286 ms | tok/sec 1128645.3502
for step 15860 | loss 3.171900 | norm 0.2901 | time 465.5929 ms | tok/sec 1126065.3754
for step 15861 | loss 3.180822 | norm 0.3262 | time 465.2650 ms | tok/sec 1126858.8006
for step 15862 | loss 3.195341 | norm 0.2922 | time 464.8087 ms | tok/sec 1127965.1117
for step 15863 | loss 3.199556 | norm 0.3182 | time 465.0962 ms | tok/sec 1127267.7787
for step 15864 | loss 3.147264 | norm 0.3073 | time 465.7736 ms | tok/sec 1125628.4593
for step 15865 | loss 3.183037 | norm 0.3496 | time 465.4143 ms | tok/sec 1126497.4371
for step 15866 | loss 3.190645 | norm 0.3111 | time 464.3869 ms | tok/sec 1128989.5448
for step 15867 | loss 3.200089 | norm 0.2992 | time 464.6738 ms | tok/sec 1128292.6816
for step 15868 | loss 3.202327 | norm 0.3266 | time 464.5143 ms | tok/sec 1128680.1078
for step 15869 | loss 3.174697 | norm 0.2876 | time 464.6077 ms | tok/sec 1128453.0635
for step 15870 | loss 3.230328 | norm 0.3025 | time 464.8330 ms | tok/sec 1127906.0999
for step 15871 | loss 3.223599 | norm 0.3143 | time 466.2251 ms | tok/sec 1124538.2260
for step 15872 | loss 3.170778 | norm 0.2911 | time 465.2665 ms | tok/sec 1126855.3359
for step 15873 | loss 3.241925 | norm 0.3061 | time 465.3287 ms | tok/sec 1126704.6444
for step 15874 | loss 3.200469 | norm 0.2901 | time 464.9255 ms | tok/sec 1127681.6800
for step 15875 | loss 3.216522 | norm 0.3118 | time 464.5581 ms | tok/sec 1128573.5246
for step 15876 | loss 3.194574 | norm 0.3066 | time 465.0631 ms | tok/sec 1127348.1072
for step 15877 | loss 3.225019 | norm 0.2731 | time 465.0598 ms | tok/sec 1127356.1985
for step 15878 | loss 3.202513 | norm 0.3124 | time 465.5397 ms | tok/sec 1126193.9786
for step 15879 | loss 3.206308 | norm 0.2862 | time 466.1114 ms | tok/sec 1124812.6000
for step 15880 | loss 3.249311 | norm 0.3455 | time 465.4958 ms | tok/sec 1126300.1128
for step 15881 | loss 3.207104 | norm 0.3201 | time 465.0497 ms | tok/sec 1127380.4731
for step 15882 | loss 3.187222 | norm 0.3215 | time 464.9966 ms | tok/sec 1127509.3769
for step 15883 | loss 3.199603 | norm 0.3003 | time 464.7298 ms | tok/sec 1128156.6534
for step 15884 | loss 3.255210 | norm 0.2898 | time 464.8845 ms | tok/sec 1127781.1541
for step 15885 | loss 3.179209 | norm 0.3321 | time 465.7199 ms | tok/sec 1125758.1154
for step 15886 | loss 3.170995 | norm 0.2854 | time 464.7303 ms | tok/sec 1128155.4958
for step 15887 | loss 3.195511 | norm 0.2972 | time 464.9317 ms | tok/sec 1127666.6447
for step 15888 | loss 3.161510 | norm 0.2978 | time 464.7567 ms | tok/sec 1128091.2557
for step 15889 | loss 3.179162 | norm 0.2738 | time 466.1045 ms | tok/sec 1124829.2853
for step 15890 | loss 3.199439 | norm 0.3217 | time 465.3845 ms | tok/sec 1126569.5758
for step 15891 | loss 3.158960 | norm 0.3083 | time 465.0505 ms | tok/sec 1127378.7391
for step 15892 | loss 3.209790 | norm 0.3087 | time 466.0096 ms | tok/sec 1125058.3272
for step 15893 | loss 3.177904 | norm 0.3177 | time 465.4901 ms | tok/sec 1126313.9579
for step 15894 | loss 3.235259 | norm 0.3362 | time 464.8011 ms | tok/sec 1127983.6265
for step 15895 | loss 3.140844 | norm 0.2869 | time 465.3785 ms | tok/sec 1126584.0047
for step 15896 | loss 3.244152 | norm 0.3172 | time 465.7719 ms | tok/sec 1125632.4926
for step 15897 | loss 3.181021 | norm 0.2845 | time 465.6148 ms | tok/sec 1126012.3279
for step 15898 | loss 3.211643 | norm 0.2882 | time 465.9657 ms | tok/sec 1125164.2475
for step 15899 | loss 3.118691 | norm 0.2907 | time 465.8527 ms | tok/sec 1125437.1991
for step 15900 | loss 3.200457 | norm 0.2962 | time 465.0517 ms | tok/sec 1127375.8493
for step 15901 | loss 3.212343 | norm 0.2934 | time 465.1718 ms | tok/sec 1127084.6261
for step 15902 | loss 3.168016 | norm 0.3297 | time 466.0273 ms | tok/sec 1125015.7345
for step 15903 | loss 3.211178 | norm 0.2919 | time 465.5876 ms | tok/sec 1126078.0614
for step 15904 | loss 3.177742 | norm 0.2972 | time 466.1455 ms | tok/sec 1124730.3313
for step 15905 | loss 3.184962 | norm 0.3062 | time 464.5503 ms | tok/sec 1128592.6386
for step 15906 | loss 3.236830 | norm 0.3030 | time 465.1504 ms | tok/sec 1127136.6192
for step 15907 | loss 3.237556 | norm 0.2809 | time 464.8695 ms | tok/sec 1127817.5937
for step 15908 | loss 3.190847 | norm 0.3269 | time 465.9665 ms | tok/sec 1125162.5203
for step 15909 | loss 3.214345 | norm 0.2901 | time 464.4330 ms | tok/sec 1128877.6875
for step 15910 | loss 3.208799 | norm 0.2922 | time 465.1415 ms | tok/sec 1127157.9955
for step 15911 | loss 3.256420 | norm 0.3185 | time 465.1604 ms | tok/sec 1127112.3551
for step 15912 | loss 3.194262 | norm 0.2978 | time 465.6126 ms | tok/sec 1126017.5171
for step 15913 | loss 3.205994 | norm 0.3051 | time 465.1618 ms | tok/sec 1127108.8889
for step 15914 | loss 3.175495 | norm 0.3062 | time 465.6820 ms | tok/sec 1125849.7570
for step 15915 | loss 3.135111 | norm 0.3139 | time 464.9889 ms | tok/sec 1127527.8767
for step 15916 | loss 3.157468 | norm 0.2852 | time 465.6115 ms | tok/sec 1126020.4000
for step 15917 | loss 3.139048 | norm 0.3053 | time 464.8802 ms | tok/sec 1127791.5652
for step 15918 | loss 3.146625 | norm 0.3167 | time 465.5318 ms | tok/sec 1126213.0121
for step 15919 | loss 3.166297 | norm 0.2861 | time 465.8213 ms | tok/sec 1125513.2345
for step 15920 | loss 3.155176 | norm 0.3018 | time 465.3060 ms | tok/sec 1126759.4891
for step 15921 | loss 3.183208 | norm 0.2954 | time 464.9980 ms | tok/sec 1127505.9082
for step 15922 | loss 3.188421 | norm 0.2953 | time 465.3316 ms | tok/sec 1126697.7170
for step 15923 | loss 3.190431 | norm 0.3290 | time 465.0168 ms | tok/sec 1127460.2397
for step 15924 | loss 3.169707 | norm 0.2953 | time 465.8177 ms | tok/sec 1125521.8756
for step 15925 | loss 3.172480 | norm 0.3050 | time 464.7715 ms | tok/sec 1128055.3770
for step 15926 | loss 3.210690 | norm 0.3587 | time 465.2245 ms | tok/sec 1126956.9745
for step 15927 | loss 3.186397 | norm 0.3059 | time 465.4737 ms | tok/sec 1126353.7643
for step 15928 | loss 3.151710 | norm 0.3533 | time 465.4531 ms | tok/sec 1126403.3821
for step 15929 | loss 3.222613 | norm 0.3287 | time 465.2390 ms | tok/sec 1126921.7454
for step 15930 | loss 3.220665 | norm 0.3056 | time 465.0843 ms | tok/sec 1127296.6725
for step 15931 | loss 3.147617 | norm 0.3140 | time 464.8159 ms | tok/sec 1127947.7546
for step 15932 | loss 3.194975 | norm 0.3226 | time 465.3964 ms | tok/sec 1126540.7193
for step 15933 | loss 3.185259 | norm 0.2916 | time 466.0649 ms | tok/sec 1124924.8040
for step 15934 | loss 3.175968 | norm 0.3203 | time 465.4827 ms | tok/sec 1126331.8416
for step 15935 | loss 3.192338 | norm 0.2956 | time 465.6222 ms | tok/sec 1125994.4543
for step 15936 | loss 3.161256 | norm 0.3063 | time 465.4989 ms | tok/sec 1126292.6135
for step 15937 | loss 3.196033 | norm 0.3066 | time 465.0176 ms | tok/sec 1127458.5055
for step 15938 | loss 3.235528 | norm 0.3051 | time 465.2610 ms | tok/sec 1126868.6172
for step 15939 | loss 3.242805 | norm 0.2929 | time 464.9673 ms | tok/sec 1127580.4889
for step 15940 | loss 3.161126 | norm 0.3089 | time 465.2798 ms | tok/sec 1126823.0002
for step 15941 | loss 3.184151 | norm 0.3037 | time 465.1439 ms | tok/sec 1127152.2180
for step 15942 | loss 3.200207 | norm 0.3002 | time 465.2193 ms | tok/sec 1126969.6806
for step 15943 | loss 3.182495 | norm 0.2756 | time 464.9081 ms | tok/sec 1127723.8965
for step 15944 | loss 3.345156 | norm 0.3226 | time 466.1591 ms | tok/sec 1124697.5422
for step 15945 | loss 3.162988 | norm 0.3146 | time 465.0538 ms | tok/sec 1127370.6475
for step 15946 | loss 3.198291 | norm 0.2971 | time 466.2850 ms | tok/sec 1124393.9026
for step 15947 | loss 3.192451 | norm 0.2971 | time 464.5171 ms | tok/sec 1128673.1561
for step 15948 | loss 3.215445 | norm 0.3032 | time 465.8337 ms | tok/sec 1125483.2799
for step 15949 | loss 3.176896 | norm 0.3184 | time 466.0385 ms | tok/sec 1124988.6840
for step 15950 | loss 3.126216 | norm 0.2874 | time 465.2829 ms | tok/sec 1126815.4940
for step 15951 | loss 3.159951 | norm 0.3046 | time 466.0337 ms | tok/sec 1125000.1947
for step 15952 | loss 3.165046 | norm 0.2787 | time 466.1338 ms | tok/sec 1124758.5199
for step 15953 | loss 3.167095 | norm 0.3152 | time 464.4024 ms | tok/sec 1128951.8702
for step 15954 | loss 3.085957 | norm 0.2941 | time 465.3430 ms | tok/sec 1126670.0083
for step 15955 | loss 3.169776 | norm 0.3095 | time 465.6336 ms | tok/sec 1125966.7803
for step 15956 | loss 3.187339 | norm 0.3083 | time 464.5290 ms | tok/sec 1128644.1917
for step 15957 | loss 3.152806 | norm 0.3061 | time 464.9367 ms | tok/sec 1127654.5012
for step 15958 | loss 3.192356 | norm 0.2799 | time 465.6217 ms | tok/sec 1125995.6075
for step 15959 | loss 3.195127 | norm 0.3129 | time 464.4902 ms | tok/sec 1128738.6213
for step 15960 | loss 3.129532 | norm 0.2960 | time 465.0443 ms | tok/sec 1127393.7667
for step 15961 | loss 3.185889 | norm 0.2887 | time 464.8709 ms | tok/sec 1127814.1232
for step 15962 | loss 3.211460 | norm 0.3290 | time 464.9310 ms | tok/sec 1127668.3795
for step 15963 | loss 3.228193 | norm 0.2908 | time 466.6688 ms | tok/sec 1123469.0434
for step 15964 | loss 3.231295 | norm 0.3283 | time 465.8012 ms | tok/sec 1125561.6260
for step 15965 | loss 3.206172 | norm 0.2906 | time 465.5833 ms | tok/sec 1126088.4411
for step 15966 | loss 3.228213 | norm 0.3016 | time 464.9925 ms | tok/sec 1127519.2048
for step 15967 | loss 3.198950 | norm 0.3392 | time 465.2512 ms | tok/sec 1126892.2933
for step 15968 | loss 3.223082 | norm 0.2826 | time 464.8473 ms | tok/sec 1127871.3899
for step 15969 | loss 3.230896 | norm 0.2932 | time 465.7955 ms | tok/sec 1125575.4530
for step 15970 | loss 3.174056 | norm 0.3068 | time 465.4624 ms | tok/sec 1126380.8805
for step 15971 | loss 3.210148 | norm 0.3020 | time 464.6130 ms | tok/sec 1128440.3239
for step 15972 | loss 3.192050 | norm 0.2967 | time 465.6556 ms | tok/sec 1125913.7421
for step 15973 | loss 3.191342 | norm 0.3610 | time 466.0327 ms | tok/sec 1125002.4969
for step 15974 | loss 3.194871 | norm 0.3208 | time 465.8988 ms | tok/sec 1125326.0446
for step 15975 | loss 3.227732 | norm 0.3087 | time 465.6169 ms | tok/sec 1126007.1388
for step 15976 | loss 3.197627 | norm 0.3067 | time 465.6625 ms | tok/sec 1125897.0245
for step 15977 | loss 3.193127 | norm 0.2876 | time 466.1624 ms | tok/sec 1124689.4890
for step 15978 | loss 3.177832 | norm 0.2772 | time 465.0476 ms | tok/sec 1127385.6749
for step 15979 | loss 3.248859 | norm 0.3115 | time 465.9114 ms | tok/sec 1125295.5241
for step 15980 | loss 3.192978 | norm 0.2887 | time 465.2741 ms | tok/sec 1126836.8582
for step 15981 | loss 3.186208 | norm 0.2940 | time 465.3022 ms | tok/sec 1126768.7267
for step 15982 | loss 3.230007 | norm 0.2941 | time 464.8032 ms | tok/sec 1127978.4191
for step 15983 | loss 3.165493 | norm 0.2897 | time 464.9484 ms | tok/sec 1127626.1672
for step 15984 | loss 3.238274 | norm 0.2941 | time 466.2712 ms | tok/sec 1124427.2490
for step 15985 | loss 3.188149 | norm 0.3205 | time 464.4704 ms | tok/sec 1128786.7111
for step 15986 | loss 3.180110 | norm 0.2943 | time 465.4272 ms | tok/sec 1126466.2761
for step 15987 | loss 3.172018 | norm 0.3493 | time 465.6756 ms | tok/sec 1125865.3203
for step 15988 | loss 3.177969 | norm 0.2872 | time 466.3811 ms | tok/sec 1124162.2578
for step 15989 | loss 3.172803 | norm 0.2965 | time 465.2793 ms | tok/sec 1126824.1551
for step 15990 | loss 3.203239 | norm 0.3250 | time 464.7791 ms | tok/sec 1128036.8599
for step 15991 | loss 3.203392 | norm 0.2996 | time 466.2197 ms | tok/sec 1124551.4527
for step 15992 | loss 3.127192 | norm 0.3240 | time 465.2953 ms | tok/sec 1126785.4701
for step 15993 | loss 3.177964 | norm 0.3011 | time 465.6432 ms | tok/sec 1125943.7196
for step 15994 | loss 3.186884 | norm 0.3317 | time 465.7474 ms | tok/sec 1125691.8430
for step 15995 | loss 3.182100 | norm 0.2971 | time 465.2510 ms | tok/sec 1126892.8707
for step 15996 | loss 3.155164 | norm 0.3747 | time 466.7931 ms | tok/sec 1123170.0824
for step 15997 | loss 3.229872 | norm 0.2945 | time 465.4698 ms | tok/sec 1126362.9952
for step 15998 | loss 3.211883 | norm 0.3241 | time 464.8986 ms | tok/sec 1127747.0302
for step 15999 | loss 3.194161 | norm 0.3287 | time 465.2019 ms | tok/sec 1127011.8438
validation loss 3.2343
HellaSwag accuracy: 2828/10042=0.2816
> Hello, I'm a language model, and the results are interesting, but with some additional caveats. One of the advantages of a language model is that it can
> Hello, I'm a language model, not an artificial language. I can be good at reading grammar, learning simple syntax and syntactic information. I'm not
> Hello, I'm a language model, but before that, I'm going to talk about the functions or classes.
I'm going to go over that you
> Hello, I'm a language model, and I'm just trying to show you to the models without coding their characters -- I already wanted to do that on my
> Hello, I'm a language model, and I can't figure out which ones actually came from the data from some sources. The easiest part about my model is
> Hello, I'm a language model, so here's a quick list of many languages, both the most basic and complex, which we can include in our course
>>  Hello, I'm a language model, so I will be writing to the language language models in the future. You can take notes if you don't know theHello, I'm a language model, so I should assume that everything is correct. Let's say my code is a
(String, int, and r
> 
> Hello, I'm a language model, and that is just a metaphor. Let me start with my own definition. I'll walk you through the world of a
> > Hello, I'm a language model, and for the purpose of this article, the 'language' is assumed to represent itself (like the name of an acronym
> Hello, I'm a language model, don't worry. We define the syntax and semantics behind each language, but that's not how we communicate with each other
Hello, I'm a language model, we can use artificial intelligence to mimic that process, we can use machine learning, the algorithm we are trained on, the
Hello, I'm a language model, how do we model and explain the function call? Try it! The next question is, why does a language model call
> > > Hello, I'm a language model, and I want to give a model to others. The model is a description of a language. I want them to understand
Hello, I'm a language model, I started looking at how different languages were written in the past as part of my life. Then I heard a bunch of
Hello, I'm a language model, so I've written this in Java that basically says what I'm trying to do and the basic problem to do is do
> > > > > > Hello, I'm a language model, and I understand that you will be better understood if you start with the correct "if" and I understand the "you
Hello, I'm a language model, and my goal is to create models that help a language model model or model the language environment.
Language-level modeling
Hello, I'm a language model, and I'm a person of color and I've always been.
|That's where my heart got a kick-
Hello, I'm a language model, but if it's been around for a while and there are various language models that you can use to describe what a language
Hello, I'm a language model, and the key for a language model is not only to ensure that you define your model, but, in the above case
Hello, I'm a language model, and i often use a bunch of variables, but they don't always work (they just say nothing at all, that
> > > > Hello, I'm a language model, my career path is to improve a bunch of languages together, and I'm going to learn them all through this journey with
Hello, I'm a language model, and my mother's speech is like a game of cards.
2. Language model is for the building the
computer
Hello, I'm a language model, and this article is about each.
The code is here. I'm going to use the language model to represent this
Hello, I'm a language model, or not, the interface model or the interface to the interface, such as web applications. I'm an author and a
> > > Hello, I'm a language model, and I have seen programming languages become like languages. As a result, if you want to understand the language model for programming
Hello, I'm a language model, and you don't have to.
And a typical example is: "If you want to turn the computer so the
Hello, I'm a language model, so you've been playing around using Java. Now, I'm an XML parser and we're going to use the

> > > Hello, I'm a language model, modeling and learning. So far, not any data.
First of all, it's not what you do with a
Hello, I'm a language model, and every language we use is considered a model, which I think makes a good generalization of the real language.

Hello, I'm a language model, that has the capabilities to transform a language model into any type of language. You use that to decide which language you want
for step 16000 | loss 3.156296 | norm 0.3301 | time 12440.6221 ms | tok/sec 42143.2301
Will loading at 0 from edu_fineweb10B/edufineweb_train_000085.npy
for step 16001 | loss 3.164524 | norm 0.2942 | time 2569.1926 ms | tok/sec 204067.2195
for step 16002 | loss 3.232176 | norm 0.3227 | time 473.2077 ms | tok/sec 1107944.7492
for step 16003 | loss 3.229301 | norm 0.3339 | time 461.2958 ms | tok/sec 1136554.7898
for step 16004 | loss 3.194137 | norm 0.3165 | time 462.7337 ms | tok/sec 1133023.0505
for step 16005 | loss 3.205184 | norm 0.3232 | time 462.9557 ms | tok/sec 1132479.8139
for step 16006 | loss 3.210549 | norm 0.2890 | time 462.6522 ms | tok/sec 1133222.7378
for step 16007 | loss 3.269080 | norm 0.3483 | time 464.0992 ms | tok/sec 1129689.5907
for step 16008 | loss 3.159272 | norm 0.2932 | time 463.5627 ms | tok/sec 1130996.8845
for step 16009 | loss 3.233782 | norm 0.3044 | time 464.2258 ms | tok/sec 1129381.5100
for step 16010 | loss 3.198452 | norm 0.2887 | time 463.7678 ms | tok/sec 1130496.8507
for step 16011 | loss 3.201132 | norm 0.3183 | time 464.8142 ms | tok/sec 1127951.8046
for step 16012 | loss 3.203636 | norm 0.3142 | time 463.9208 ms | tok/sec 1130123.8578
for step 16013 | loss 3.248323 | norm 0.3051 | time 463.7074 ms | tok/sec 1130643.9078
for step 16014 | loss 3.168561 | norm 0.3382 | time 464.5336 ms | tok/sec 1128633.1855
for step 16015 | loss 3.191226 | norm 0.2876 | time 463.9375 ms | tok/sec 1130083.2036
for step 16016 | loss 3.137488 | norm 0.3137 | time 464.3552 ms | tok/sec 1129066.6407
for step 16017 | loss 3.180056 | norm 0.3022 | time 464.9189 ms | tok/sec 1127697.8722
for step 16018 | loss 3.180633 | norm 0.3110 | time 464.4935 ms | tok/sec 1128730.5101
for step 16019 | loss 3.193602 | norm 0.3033 | time 465.0459 ms | tok/sec 1127389.7208
for step 16020 | loss 3.180250 | norm 0.2934 | time 464.7582 ms | tok/sec 1128087.7835
for step 16021 | loss 3.178113 | norm 0.3140 | time 465.7075 ms | tok/sec 1125788.0846
for step 16022 | loss 3.163828 | norm 0.2822 | time 464.6313 ms | tok/sec 1128395.7377
for step 16023 | loss 3.147865 | norm 0.2976 | time 464.9608 ms | tok/sec 1127596.1000
for step 16024 | loss 3.143460 | norm 0.3030 | time 464.5658 ms | tok/sec 1128554.9905
for step 16025 | loss 3.232468 | norm 0.2949 | time 465.1353 ms | tok/sec 1127173.0172
for step 16026 | loss 3.182149 | norm 0.2817 | time 463.8448 ms | tok/sec 1130309.1615
for step 16027 | loss 3.162548 | norm 0.2952 | time 464.8068 ms | tok/sec 1127969.7403
for step 16028 | loss 3.220421 | norm 0.2750 | time 465.1141 ms | tok/sec 1127224.4406
for step 16029 | loss 3.153960 | norm 0.2824 | time 465.5268 ms | tok/sec 1126225.1246
for step 16030 | loss 3.225576 | norm 0.3068 | time 465.6606 ms | tok/sec 1125901.6362
for step 16031 | loss 3.182020 | norm 0.2833 | time 464.5989 ms | tok/sec 1128474.4898
for step 16032 | loss 3.164388 | norm 0.3109 | time 465.4098 ms | tok/sec 1126508.4016
for step 16033 | loss 3.204829 | norm 0.2910 | time 464.6802 ms | tok/sec 1128277.0512
for step 16034 | loss 3.252816 | norm 0.3084 | time 465.5011 ms | tok/sec 1126287.4218
for step 16035 | loss 3.199357 | norm 0.2962 | time 465.3990 ms | tok/sec 1126534.3710
for step 16036 | loss 3.308517 | norm 0.3135 | time 465.4720 ms | tok/sec 1126357.8028
for step 16037 | loss 3.191979 | norm 0.2786 | time 464.8049 ms | tok/sec 1127974.3690
for step 16038 | loss 3.204468 | norm 0.3242 | time 465.5852 ms | tok/sec 1126083.8279
for step 16039 | loss 3.250430 | norm 0.2995 | time 466.5766 ms | tok/sec 1123691.2153
for step 16040 | loss 3.190060 | norm 0.2905 | time 464.4899 ms | tok/sec 1128739.2006
for step 16041 | loss 3.236500 | norm 0.2955 | time 464.6597 ms | tok/sec 1128326.8386
for step 16042 | loss 3.157584 | norm 0.2881 | time 464.9918 ms | tok/sec 1127520.9392
for step 16043 | loss 3.205938 | norm 0.3001 | time 464.1783 ms | tok/sec 1129496.9480
for step 16044 | loss 3.217587 | norm 0.2701 | time 464.1201 ms | tok/sec 1129638.5223
for step 16045 | loss 3.225668 | norm 0.2879 | time 465.6124 ms | tok/sec 1126018.0937
for step 16046 | loss 3.250781 | norm 0.3078 | time 464.6876 ms | tok/sec 1128259.1057
for step 16047 | loss 3.131059 | norm 0.3886 | time 464.9782 ms | tok/sec 1127553.8931
for step 16048 | loss 3.251966 | norm 0.3187 | time 465.7254 ms | tok/sec 1125744.8603
for step 16049 | loss 3.251579 | norm 0.2917 | time 465.6265 ms | tok/sec 1125984.0764
for step 16050 | loss 3.223277 | norm 0.2963 | time 465.1222 ms | tok/sec 1127204.7952
for step 16051 | loss 3.148121 | norm 0.3074 | time 464.6208 ms | tok/sec 1128421.2151
for step 16052 | loss 3.190962 | norm 0.2750 | time 464.9260 ms | tok/sec 1127680.5234
for step 16053 | loss 3.238741 | norm 0.3054 | time 465.4779 ms | tok/sec 1126343.3798
for step 16054 | loss 3.189927 | norm 0.2925 | time 465.1949 ms | tok/sec 1127028.5944
for step 16055 | loss 3.171714 | norm 0.3203 | time 465.3275 ms | tok/sec 1126707.5308
for step 16056 | loss 3.101187 | norm 0.3152 | time 464.4244 ms | tok/sec 1128898.5504
for step 16057 | loss 3.189138 | norm 0.2905 | time 464.5529 ms | tok/sec 1128586.2672
for step 16058 | loss 3.151156 | norm 0.3013 | time 465.6956 ms | tok/sec 1125816.9026
for step 16059 | loss 3.154889 | norm 0.2887 | time 465.0550 ms | tok/sec 1127367.7577
for step 16060 | loss 3.152877 | norm 0.3102 | time 464.8685 ms | tok/sec 1127819.9075
for step 16061 | loss 3.133743 | norm 0.2888 | time 465.1661 ms | tok/sec 1127098.4904
for step 16062 | loss 3.124110 | norm 0.2751 | time 463.8405 ms | tok/sec 1130319.6193
for step 16063 | loss 3.161403 | norm 0.3179 | time 464.4504 ms | tok/sec 1128835.3846
for step 16064 | loss 3.138196 | norm 0.2990 | time 464.9382 ms | tok/sec 1127651.0316
for step 16065 | loss 3.176965 | norm 0.3033 | time 466.7661 ms | tok/sec 1123234.9107
for step 16066 | loss 3.149811 | norm 0.2940 | time 465.0593 ms | tok/sec 1127357.3544
for step 16067 | loss 3.136999 | norm 0.3112 | time 465.7943 ms | tok/sec 1125578.3336
for step 16068 | loss 3.261539 | norm 0.3189 | time 466.3017 ms | tok/sec 1124353.6597
for step 16069 | loss 3.164955 | norm 0.3884 | time 465.0681 ms | tok/sec 1127335.9705
for step 16070 | loss 3.203309 | norm 0.3417 | time 464.4792 ms | tok/sec 1128765.2730
for step 16071 | loss 3.223147 | norm 0.3810 | time 464.6285 ms | tok/sec 1128402.6859
for step 16072 | loss 3.221345 | norm 0.3619 | time 464.6595 ms | tok/sec 1128327.4175
for step 16073 | loss 3.242755 | norm 0.3493 | time 465.8585 ms | tok/sec 1125423.3756
for step 16074 | loss 3.170262 | norm 0.2961 | time 465.4608 ms | tok/sec 1126384.9192
for step 16075 | loss 3.219245 | norm 0.3515 | time 464.7131 ms | tok/sec 1128197.1690
for step 16076 | loss 3.188205 | norm 0.3040 | time 464.9138 ms | tok/sec 1127710.0167
for step 16077 | loss 3.198045 | norm 0.3431 | time 465.1201 ms | tok/sec 1127209.9954
for step 16078 | loss 3.236455 | norm 0.3243 | time 465.8051 ms | tok/sec 1125552.4083
for step 16079 | loss 3.197638 | norm 0.3341 | time 465.3819 ms | tok/sec 1126575.9245
for step 16080 | loss 3.200188 | norm 0.2861 | time 465.5869 ms | tok/sec 1126079.7913
for step 16081 | loss 3.245407 | norm 0.3118 | time 465.6494 ms | tok/sec 1125928.7306
for step 16082 | loss 3.194084 | norm 0.3137 | time 464.8647 ms | tok/sec 1127829.1624
for step 16083 | loss 3.202029 | norm 0.3347 | time 465.0362 ms | tok/sec 1127413.4188
for step 16084 | loss 3.171215 | norm 0.2969 | time 464.4401 ms | tok/sec 1128860.3023
for step 16085 | loss 3.205208 | norm 0.3052 | time 464.9918 ms | tok/sec 1127520.9392
for step 16086 | loss 3.188254 | norm 0.3208 | time 466.5055 ms | tok/sec 1123862.3534
for step 16087 | loss 3.177954 | norm 0.2981 | time 465.2557 ms | tok/sec 1126881.3213
for step 16088 | loss 3.209387 | norm 0.2957 | time 466.0914 ms | tok/sec 1124860.9313
for step 16089 | loss 3.165586 | norm 0.3146 | time 466.1040 ms | tok/sec 1124830.4361
for step 16090 | loss 3.209791 | norm 0.3278 | time 466.4116 ms | tok/sec 1124088.7032
for step 16091 | loss 3.117132 | norm 0.2984 | time 465.1480 ms | tok/sec 1127142.3965
for step 16092 | loss 3.131828 | norm 0.3080 | time 465.9019 ms | tok/sec 1125318.5583
for step 16093 | loss 3.138640 | norm 0.2917 | time 465.3647 ms | tok/sec 1126617.4811
for step 16094 | loss 3.175128 | norm 0.3046 | time 465.0197 ms | tok/sec 1127453.3030
for step 16095 | loss 3.218957 | norm 0.2928 | time 465.6246 ms | tok/sec 1125988.6888
for step 16096 | loss 3.205521 | norm 0.2941 | time 465.3969 ms | tok/sec 1126539.5650
for step 16097 | loss 3.208488 | norm 0.5480 | time 465.1470 ms | tok/sec 1127144.7074
for step 16098 | loss 3.129712 | norm 0.2862 | time 465.2579 ms | tok/sec 1126876.1241
for step 16099 | loss 3.147144 | norm 0.2855 | time 465.5449 ms | tok/sec 1126181.2900
for step 16100 | loss 3.098252 | norm 0.2824 | time 465.5929 ms | tok/sec 1126065.3754
for step 16101 | loss 3.179727 | norm 0.2882 | time 465.7035 ms | tok/sec 1125797.8825
for step 16102 | loss 3.167257 | norm 0.3018 | time 465.0874 ms | tok/sec 1127289.1600
for step 16103 | loss 3.171640 | norm 0.3132 | time 464.9069 ms | tok/sec 1127726.7882
for step 16104 | loss 3.285070 | norm 0.3265 | time 464.7181 ms | tok/sec 1128185.0140
for step 16105 | loss 3.130761 | norm 0.3028 | time 466.2268 ms | tok/sec 1124534.2005
for step 16106 | loss 3.198301 | norm 0.3177 | time 465.2500 ms | tok/sec 1126895.1807
for step 16107 | loss 3.181184 | norm 0.3116 | time 465.8694 ms | tok/sec 1125396.8814
for step 16108 | loss 3.223714 | norm 0.3064 | time 466.0079 ms | tok/sec 1125062.3564
for step 16109 | loss 3.167964 | norm 0.3058 | time 465.2696 ms | tok/sec 1126847.8293
for step 16110 | loss 3.215357 | norm 0.3009 | time 466.3939 ms | tok/sec 1124131.2258
for step 16111 | loss 3.179016 | norm 0.2779 | time 465.6088 ms | tok/sec 1126026.7425
for step 16112 | loss 3.183113 | norm 0.3048 | time 465.0261 ms | tok/sec 1127437.6958
for step 16113 | loss 3.167825 | norm 0.3149 | time 466.1243 ms | tok/sec 1124781.5321
for step 16114 | loss 3.204954 | norm 0.2893 | time 465.2450 ms | tok/sec 1126907.3079
for step 16115 | loss 3.221740 | norm 0.2962 | time 465.2069 ms | tok/sec 1126999.7143
for step 16116 | loss 3.239998 | norm 0.2967 | time 465.5623 ms | tok/sec 1126139.1889
for step 16117 | loss 3.189710 | norm 0.3003 | time 465.0846 ms | tok/sec 1127296.0946
for step 16118 | loss 3.266754 | norm 0.3332 | time 465.5032 ms | tok/sec 1126282.2301
for step 16119 | loss 3.253438 | norm 0.3047 | time 464.5979 ms | tok/sec 1128476.8062
for step 16120 | loss 3.181696 | norm 0.3192 | time 464.9529 ms | tok/sec 1127615.1809
for step 16121 | loss 3.173579 | norm 0.2961 | time 465.3296 ms | tok/sec 1126702.3352
for step 16122 | loss 3.212788 | norm 0.3309 | time 464.7756 ms | tok/sec 1128045.5397
for step 16123 | loss 3.208512 | norm 0.3110 | time 464.9003 ms | tok/sec 1127742.9817
for step 16124 | loss 3.208290 | norm 0.2912 | time 466.0332 ms | tok/sec 1125001.3458
for step 16125 | loss 3.255774 | norm 0.2911 | time 466.1040 ms | tok/sec 1124830.4361
for step 16126 | loss 3.139983 | norm 0.2945 | time 464.5767 ms | tok/sec 1128528.3487
for step 16127 | loss 3.123317 | norm 0.2880 | time 464.9127 ms | tok/sec 1127712.9083
for step 16128 | loss 3.136050 | norm 0.2847 | time 465.8139 ms | tok/sec 1125531.0928
for step 16129 | loss 3.154782 | norm 0.3129 | time 466.6536 ms | tok/sec 1123505.7789
for step 16130 | loss 3.126286 | norm 0.2754 | time 465.1854 ms | tok/sec 1127051.6996
for step 16131 | loss 3.082584 | norm 0.2781 | time 465.2095 ms | tok/sec 1126993.3609
for step 16132 | loss 3.131804 | norm 0.2748 | time 465.6518 ms | tok/sec 1125922.9657
for step 16133 | loss 3.169603 | norm 0.3148 | time 465.7197 ms | tok/sec 1125758.6917
for step 16134 | loss 3.153326 | norm 0.2798 | time 464.8035 ms | tok/sec 1127977.8405
for step 16135 | loss 3.175732 | norm 0.2895 | time 464.9632 ms | tok/sec 1127590.3181
for step 16136 | loss 3.185708 | norm 0.2544 | time 465.6136 ms | tok/sec 1126015.2108
for step 16137 | loss 3.162555 | norm 0.3103 | time 464.4639 ms | tok/sec 1128802.3557
for step 16138 | loss 3.195499 | norm 0.3087 | time 465.3006 ms | tok/sec 1126772.7681
for step 16139 | loss 3.217813 | norm 0.2992 | time 464.0510 ms | tok/sec 1129806.8331
for step 16140 | loss 3.179371 | norm 0.2984 | time 465.4605 ms | tok/sec 1126385.4961
for step 16141 | loss 3.253678 | norm 0.3276 | time 465.3313 ms | tok/sec 1126698.2943
for step 16142 | loss 3.201951 | norm 0.2912 | time 465.0090 ms | tok/sec 1127479.3159
for step 16143 | loss 3.182999 | norm 0.3279 | time 465.6644 ms | tok/sec 1125892.4129
for step 16144 | loss 3.173730 | norm 0.3063 | time 464.9081 ms | tok/sec 1127723.8965
for step 16145 | loss 3.163859 | norm 0.3154 | time 465.1320 ms | tok/sec 1127181.1060
for step 16146 | loss 3.183787 | norm 0.3074 | time 465.1017 ms | tok/sec 1127254.4880
for step 16147 | loss 3.207839 | norm 0.3019 | time 465.4343 ms | tok/sec 1126448.9651
for step 16148 | loss 3.177146 | norm 0.3112 | time 465.0178 ms | tok/sec 1127457.9274
for step 16149 | loss 3.220072 | norm 0.3162 | time 466.1481 ms | tok/sec 1124724.0034
for step 16150 | loss 3.226340 | norm 0.2951 | time 464.5381 ms | tok/sec 1128622.1797
for step 16151 | loss 3.175352 | norm 0.3051 | time 464.6573 ms | tok/sec 1128332.6281
for step 16152 | loss 3.252610 | norm 0.3471 | time 465.9243 ms | tok/sec 1125264.4295
for step 16153 | loss 3.229972 | norm 0.3274 | time 464.9293 ms | tok/sec 1127672.4275
for step 16154 | loss 3.206857 | norm 0.3366 | time 464.7686 ms | tok/sec 1128062.3211
for step 16155 | loss 3.184646 | norm 0.3071 | time 465.4171 ms | tok/sec 1126490.5123
for step 16156 | loss 3.232680 | norm 0.3270 | time 465.1933 ms | tok/sec 1127032.6378
for step 16157 | loss 3.188710 | norm 0.3158 | time 465.3900 ms | tok/sec 1126556.3016
for step 16158 | loss 3.221418 | norm 0.3009 | time 464.1843 ms | tok/sec 1129482.4444
for step 16159 | loss 3.187168 | norm 0.3406 | time 465.1234 ms | tok/sec 1127201.9062
for step 16160 | loss 3.197471 | norm 0.3151 | time 464.7315 ms | tok/sec 1128152.6020
for step 16161 | loss 3.198809 | norm 0.3221 | time 466.0153 ms | tok/sec 1125044.5130
for step 16162 | loss 3.118140 | norm 0.3182 | time 464.9711 ms | tok/sec 1127571.2380
for step 16163 | loss 3.138132 | norm 0.3089 | time 465.1933 ms | tok/sec 1127032.6378
for step 16164 | loss 3.154477 | norm 0.3014 | time 465.7152 ms | tok/sec 1125769.6418
for step 16165 | loss 3.177271 | norm 0.3247 | time 465.1175 ms | tok/sec 1127216.3513
for step 16166 | loss 3.150111 | norm 0.3085 | time 464.1767 ms | tok/sec 1129501.0091
for step 16167 | loss 3.172831 | norm 0.2938 | time 464.3538 ms | tok/sec 1129070.1190
for step 16168 | loss 3.117870 | norm 0.3009 | time 465.6126 ms | tok/sec 1126017.5171
for step 16169 | loss 3.146889 | norm 0.3143 | time 464.7522 ms | tok/sec 1128102.2512
for step 16170 | loss 3.140431 | norm 0.3294 | time 465.2219 ms | tok/sec 1126963.3275
for step 16171 | loss 3.120829 | norm 0.2973 | time 465.1275 ms | tok/sec 1127192.0838
for step 16172 | loss 3.197689 | norm 0.3267 | time 465.2576 ms | tok/sec 1126876.7016
for step 16173 | loss 3.225337 | norm 0.3218 | time 465.5201 ms | tok/sec 1126241.2751
for step 16174 | loss 3.197353 | norm 0.3409 | time 464.8738 ms | tok/sec 1127807.1822
for step 16175 | loss 3.150313 | norm 0.3133 | time 465.5781 ms | tok/sec 1126101.1276
for step 16176 | loss 3.164585 | norm 0.3074 | time 464.6649 ms | tok/sec 1128314.1019
for step 16177 | loss 3.201707 | norm 0.3315 | time 465.0056 ms | tok/sec 1127487.4091
for step 16178 | loss 3.208934 | norm 0.3145 | time 465.9898 ms | tok/sec 1125106.1040
for step 16179 | loss 3.156940 | norm 0.2984 | time 465.0240 ms | tok/sec 1127442.8981
for step 16180 | loss 3.220784 | norm 0.3215 | time 465.6796 ms | tok/sec 1125855.5211
for step 16181 | loss 3.178102 | norm 0.3226 | time 465.4500 ms | tok/sec 1126410.8829
for step 16182 | loss 3.121540 | norm 0.3586 | time 465.5154 ms | tok/sec 1126252.8114
for step 16183 | loss 3.210172 | norm 0.3177 | time 465.1675 ms | tok/sec 1127095.0243
for step 16184 | loss 3.223511 | norm 0.3275 | time 465.6951 ms | tok/sec 1125818.0554
for step 16185 | loss 3.231283 | norm 0.3113 | time 464.5689 ms | tok/sec 1128547.4611
for step 16186 | loss 3.242927 | norm 0.3133 | time 465.3466 ms | tok/sec 1126661.3496
for step 16187 | loss 3.155638 | norm 0.3166 | time 465.6522 ms | tok/sec 1125921.8128
for step 16188 | loss 3.221661 | norm 0.3214 | time 465.1318 ms | tok/sec 1127181.6837
for step 16189 | loss 3.216553 | norm 0.3401 | time 466.0461 ms | tok/sec 1124970.2674
for step 16190 | loss 3.225627 | norm 0.3049 | time 465.1408 ms | tok/sec 1127159.7288
for step 16191 | loss 3.190694 | norm 0.3413 | time 465.0822 ms | tok/sec 1127301.8736
Will loading at 0 from edu_fineweb10B/edufineweb_train_000086.npy
for step 16192 | loss 3.179017 | norm 0.2890 | time 2522.0349 ms | tok/sec 207882.9295
for step 16193 | loss 3.198565 | norm 0.3129 | time 463.8627 ms | tok/sec 1130265.5893
for step 16194 | loss 3.199369 | norm 0.3125 | time 464.5662 ms | tok/sec 1128553.8321
for step 16195 | loss 3.166458 | norm 0.3127 | time 465.3664 ms | tok/sec 1126613.4407
for step 16196 | loss 3.169826 | norm 0.2970 | time 466.6348 ms | tok/sec 1123551.1277
for step 16197 | loss 3.165227 | norm 0.2883 | time 465.8952 ms | tok/sec 1125334.6827
for step 16198 | loss 3.191522 | norm 0.3088 | time 465.3478 ms | tok/sec 1126658.4635
for step 16199 | loss 3.166367 | norm 0.2926 | time 465.5259 ms | tok/sec 1126227.4318
for step 16200 | loss 3.156365 | norm 0.2832 | time 466.5549 ms | tok/sec 1123743.4701
for step 16201 | loss 3.133077 | norm 0.3044 | time 465.0426 ms | tok/sec 1127397.8127
for step 16202 | loss 3.158019 | norm 0.3150 | time 465.0970 ms | tok/sec 1127266.0451
for step 16203 | loss 3.157136 | norm 0.3033 | time 464.4890 ms | tok/sec 1128741.5181
for step 16204 | loss 3.172028 | norm 0.3082 | time 465.8701 ms | tok/sec 1125395.1536
for step 16205 | loss 3.169522 | norm 0.2924 | time 466.2673 ms | tok/sec 1124436.4483
for step 16206 | loss 3.169390 | norm 0.3481 | time 465.6982 ms | tok/sec 1125810.5625
for step 16207 | loss 3.215796 | norm 0.3217 | time 466.3618 ms | tok/sec 1124208.8090
for step 16208 | loss 3.182954 | norm 0.3296 | time 465.8215 ms | tok/sec 1125512.6585
for step 16209 | loss 3.225360 | norm 0.3285 | time 466.3486 ms | tok/sec 1124240.4201
for step 16210 | loss 3.218500 | norm 0.3734 | time 466.3200 ms | tok/sec 1124309.3958
for step 16211 | loss 3.172708 | norm 0.3265 | time 466.5244 ms | tok/sec 1123816.9795
for step 16212 | loss 3.260216 | norm 0.3290 | time 466.6908 ms | tok/sec 1123416.2402
for step 16213 | loss 3.260434 | norm 0.3692 | time 465.4491 ms | tok/sec 1126413.1908
for step 16214 | loss 3.166481 | norm 0.3186 | time 465.2534 ms | tok/sec 1126887.0960
for step 16215 | loss 3.201884 | norm 0.3591 | time 465.9266 ms | tok/sec 1125258.6714
for step 16216 | loss 3.219136 | norm 0.3359 | time 465.8909 ms | tok/sec 1125345.0487
for step 16217 | loss 3.127070 | norm 0.3212 | time 466.0134 ms | tok/sec 1125049.1177
for step 16218 | loss 3.186583 | norm 0.3141 | time 465.4076 ms | tok/sec 1126513.5954
for step 16219 | loss 3.193720 | norm 0.3038 | time 465.4481 ms | tok/sec 1126415.4988
for step 16220 | loss 3.213261 | norm 0.2992 | time 464.5357 ms | tok/sec 1128627.9722
for step 16221 | loss 3.142211 | norm 0.2956 | time 465.6627 ms | tok/sec 1125896.4481
for step 16222 | loss 3.219417 | norm 0.2936 | time 465.4179 ms | tok/sec 1126488.7811
for step 16223 | loss 3.139986 | norm 0.2765 | time 464.9503 ms | tok/sec 1127621.5414
for step 16224 | loss 3.227071 | norm 0.3097 | time 465.5972 ms | tok/sec 1126054.9961
for step 16225 | loss 3.184087 | norm 0.2619 | time 465.1923 ms | tok/sec 1127034.9482
for step 16226 | loss 3.195309 | norm 0.2740 | time 465.5020 ms | tok/sec 1126285.1144
for step 16227 | loss 3.180465 | norm 0.2803 | time 465.1504 ms | tok/sec 1127136.6192
for step 16228 | loss 3.144325 | norm 0.2859 | time 465.8175 ms | tok/sec 1125522.4516
for step 16229 | loss 3.161020 | norm 0.2967 | time 465.4241 ms | tok/sec 1126473.7776
for step 16230 | loss 3.093964 | norm 0.2857 | time 465.4489 ms | tok/sec 1126413.7678
for step 16231 | loss 3.111300 | norm 0.2850 | time 465.7280 ms | tok/sec 1125738.5210
for step 16232 | loss 3.108366 | norm 0.3285 | time 465.6343 ms | tok/sec 1125965.0507
for step 16233 | loss 3.160291 | norm 0.2806 | time 465.4305 ms | tok/sec 1126458.1975
for step 16234 | loss 3.145857 | norm 0.3054 | time 465.7159 ms | tok/sec 1125767.9128
for step 16235 | loss 3.121521 | norm 0.3058 | time 465.8864 ms | tok/sec 1125355.9908
for step 16236 | loss 3.146818 | norm 0.2762 | time 465.5199 ms | tok/sec 1126241.8519
for step 16237 | loss 3.083593 | norm 0.2814 | time 465.8957 ms | tok/sec 1125333.5310
for step 16238 | loss 3.180763 | norm 0.2939 | time 465.0464 ms | tok/sec 1127388.5648
for step 16239 | loss 3.185794 | norm 0.2729 | time 464.7119 ms | tok/sec 1128200.0631
for step 16240 | loss 3.157715 | norm 0.3087 | time 465.1473 ms | tok/sec 1127144.1297
for step 16241 | loss 3.189057 | norm 0.3227 | time 464.9475 ms | tok/sec 1127628.4801
for step 16242 | loss 3.259756 | norm 0.3200 | time 466.1348 ms | tok/sec 1124756.2187
for step 16243 | loss 3.257268 | norm 0.3341 | time 465.4830 ms | tok/sec 1126331.2647
for step 16244 | loss 3.213294 | norm 0.3586 | time 465.2801 ms | tok/sec 1126822.4228
for step 16245 | loss 3.190413 | norm 0.3440 | time 464.6521 ms | tok/sec 1128345.3652
for step 16246 | loss 3.210854 | norm 0.3312 | time 466.5308 ms | tok/sec 1123801.4728
for step 16247 | loss 3.246061 | norm 0.3448 | time 467.2327 ms | tok/sec 1122113.2325
for step 16248 | loss 3.187110 | norm 0.3101 | time 466.8524 ms | tok/sec 1123027.2570
for step 16249 | loss 3.161554 | norm 0.3593 | time 466.7408 ms | tok/sec 1123295.7299
validation loss 3.2317
HellaSwag accuracy: 2826/10042=0.2814
> Hello, I'm a language model, and there are many models out there looking at it and getting confused.
Let's get into more detail:
-
>>  Hello, I'm a language model, and I use it in several situations because what I have done is an interpreter that I've translated in the target language forHello, I'm a language model, which means I don't have to think about the model I'm about to learn, like there are no rules to follow

> Hello, I'm a language model, but before that, I'm going to talk about the first stage in a language model. It's the same with I> 
Hello, I'm a language model, so i'm not sure if i find a new "definition" to read and understand. Is this new definition what I
> Hello, I'm a language model, and I'm wondering what languages you want to learn. :)
Thank you so much for understanding this topic. It's> 
Hello, I'm a language model, so I haven't encountered one. If you like it, or have an idea for a language model, I would like
> Hello, I'm a language model, and here's my first lesson:
Theory: The key to success
Now, a brief discussion of what all
> Hello, I'm a language model, so I am going to talk about how my language models are built. You don't get one exact description or one specific
> Hello, I'm a language model, have no idea about what it represents, is just what it does. That means to have an idea and have at that
> Hello, I'm a language model, I take the problem as a whole and write it down. This means that the next part of the function is an exact
> >Hello, I'm a language model, and it needs that input as input instead of as input to work with to do it. I know the input can only
 Hello, I'm a language model, and we are building a model that uses a language model and an object-based model.
- I use a different
> > > Hello, I'm a language model, and I hope you enjoyed the article. Share your comments and suggestions in the comments!<|endoftext|>It is very common for a
Hello, I'm a language model, since that's how my system looks! I'm really not an effective model because I'm a language model. I used
Hello, I'm a language model, so I usually think of a language metaphor "language definition" rather than "language definition" as it refers to the process
> > > Hello, I'm a language model, especially if we're using the old SVCs you created earlier.
I tried to write a post for the following
Hello, I'm a language model, and I am a student. I have 3rd year students, and each student is assigned to a specific role. I
Hello, I'm a language model, so I can write it as I've drawn it.
Hello, i'm in German. How am i writing German
> > > Hello, I'm a language model, and I've designed it for languages like F#, C#, and C#, but I'm also not the
Hello, I'm a language model, but if you want to know the best way to use it in programming, and how to use it, this short guide
Hello, I'm a language model, but not an actual language model. For each language category, a language model uses three parameters ("conformations"), with
> > > > Hello, I'm a language model, and I'm a person of genius. I've always felt I had control over how others write. We've done all
Hello, I'm a language model, but there is a bunch of fancy words for this stuff, and some fun. So I'm using the term "C
Hello, I'm a language model, so you might have heard people say in that speech that "m" is a prefix in the form of "m-
Hello, I'm a language model, and I am now writing a project for a conference.
I've been using a project now for 7 and it would
> > Hello, I'm a language model, and this article is about
"Doing math for beginners" and "Doing math for
You're using

Hello, I'm a language model, and there's nothing but me in it. Can you tell me what you want. Can I guess you mean, I
> Hello, I'm a language model, and you've found every useful programming language model library for a number of years (for example, one that I recently read> 
Hello, I'm a language model, so you want the top-lefts to be the top-right. In other words, I'm a language model
> Hello, I'm a language model, and all the data is interpreted with the same semantics as that used to do the encoding. However, data models are only> 
Hello, I'm a language model, is the core idea of it. I've written an excellent introduction, and since it has a wide variety of topics,
for step 16250 | loss 3.204324 | norm 0.3011 | time 12409.6446 ms | tok/sec 42248.4299
for step 16251 | loss 3.228494 | norm 0.3733 | time 462.3644 ms | tok/sec 1133928.0448
for step 16252 | loss 3.223768 | norm 0.3213 | time 462.2738 ms | tok/sec 1134150.2785
for step 16253 | loss 3.264389 | norm 0.3345 | time 463.1462 ms | tok/sec 1132014.0142
for step 16254 | loss 3.199176 | norm 0.3332 | time 462.4627 ms | tok/sec 1133687.1951
for step 16255 | loss 3.185009 | norm 0.2794 | time 463.3079 ms | tok/sec 1131619.0552
for step 16256 | loss 3.154433 | norm 0.3258 | time 463.0132 ms | tok/sec 1132339.2757
for step 16257 | loss 3.242214 | norm 0.3383 | time 464.2916 ms | tok/sec 1129221.4440
for step 16258 | loss 3.189814 | norm 0.2955 | time 462.9700 ms | tok/sec 1132444.8219
for step 16259 | loss 3.186222 | norm 0.3213 | time 464.9005 ms | tok/sec 1127742.4034
for step 16260 | loss 3.227828 | norm 0.3004 | time 463.8634 ms | tok/sec 1130263.8465
for step 16261 | loss 3.221301 | norm 0.3092 | time 464.2096 ms | tok/sec 1129420.9535
for step 16262 | loss 3.225960 | norm 0.2816 | time 463.5921 ms | tok/sec 1130925.3410
for step 16263 | loss 3.143881 | norm 0.3146 | time 464.8366 ms | tok/sec 1127897.4222
for step 16264 | loss 3.206748 | norm 0.2776 | time 464.7012 ms | tok/sec 1128226.1105
for step 16265 | loss 3.125598 | norm 0.2852 | time 465.6494 ms | tok/sec 1125928.7306
for step 16266 | loss 3.228871 | norm 0.3066 | time 465.6017 ms | tok/sec 1126044.0405
for step 16267 | loss 3.103791 | norm 0.3030 | time 465.0493 ms | tok/sec 1127381.6290
for step 16268 | loss 3.179634 | norm 0.2955 | time 465.4369 ms | tok/sec 1126442.6179
for step 16269 | loss 3.195143 | norm 0.3029 | time 464.9212 ms | tok/sec 1127692.0892
for step 16270 | loss 3.204976 | norm 0.3263 | time 464.6528 ms | tok/sec 1128343.6283
for step 16271 | loss 3.155985 | norm 0.2988 | time 465.5788 ms | tok/sec 1126099.3976
for step 16272 | loss 3.166930 | norm 0.2864 | time 465.0285 ms | tok/sec 1127431.9154
for step 16273 | loss 3.205432 | norm 0.3166 | time 465.3380 ms | tok/sec 1126682.1307
for step 16274 | loss 3.176086 | norm 0.2861 | time 465.7679 ms | tok/sec 1125642.2878
for step 16275 | loss 3.194370 | norm 0.2873 | time 465.8751 ms | tok/sec 1125383.0589
for step 16276 | loss 3.182675 | norm 0.3554 | time 465.5712 ms | tok/sec 1126117.8512
for step 16277 | loss 3.279648 | norm 0.2901 | time 465.1537 ms | tok/sec 1127128.5310
for step 16278 | loss 3.262140 | norm 0.3032 | time 464.9556 ms | tok/sec 1127608.8206
for step 16279 | loss 3.207770 | norm 0.3255 | time 465.7331 ms | tok/sec 1125726.4189
for step 16280 | loss 3.199588 | norm 0.2972 | time 465.4508 ms | tok/sec 1126409.1519
for step 16281 | loss 3.250154 | norm 0.3205 | time 465.4887 ms | tok/sec 1126317.4192
for step 16282 | loss 3.206083 | norm 0.2994 | time 465.2224 ms | tok/sec 1126962.1724
for step 16283 | loss 3.255471 | norm 0.3043 | time 465.5175 ms | tok/sec 1126247.6200
for step 16284 | loss 3.189151 | norm 0.2729 | time 465.8306 ms | tok/sec 1125490.7684
for step 16285 | loss 3.232002 | norm 0.3005 | time 466.0335 ms | tok/sec 1125000.7702
for step 16286 | loss 3.235228 | norm 0.3140 | time 464.6826 ms | tok/sec 1128271.2623
for step 16287 | loss 3.178733 | norm 0.2964 | time 465.7784 ms | tok/sec 1125616.9357
for step 16288 | loss 3.212965 | norm 0.3058 | time 465.0908 ms | tok/sec 1127281.0696
for step 16289 | loss 3.202665 | norm 0.2771 | time 465.1556 ms | tok/sec 1127123.9093
for step 16290 | loss 3.199167 | norm 0.2890 | time 465.3285 ms | tok/sec 1126705.2217
for step 16291 | loss 3.216079 | norm 0.2906 | time 465.2879 ms | tok/sec 1126803.3688
for step 16292 | loss 3.202847 | norm 0.3238 | time 466.1124 ms | tok/sec 1124810.2986
for step 16293 | loss 3.191420 | norm 0.2962 | time 465.3285 ms | tok/sec 1126705.2217
for step 16294 | loss 3.219782 | norm 0.3126 | time 464.4659 ms | tok/sec 1128797.7202
for step 16295 | loss 3.316576 | norm 0.3562 | time 465.3618 ms | tok/sec 1126624.4075
for step 16296 | loss 3.216175 | norm 0.3270 | time 464.5195 ms | tok/sec 1128667.3631
for step 16297 | loss 3.229664 | norm 0.3129 | time 465.0102 ms | tok/sec 1127476.4256
for step 16298 | loss 3.205846 | norm 0.2898 | time 465.4257 ms | tok/sec 1126469.7383
for step 16299 | loss 3.151037 | norm 0.3159 | time 464.4802 ms | tok/sec 1128762.9554
for step 16300 | loss 3.237437 | norm 0.3234 | time 464.3359 ms | tok/sec 1129113.5990
for step 16301 | loss 3.205820 | norm 0.2931 | time 464.7119 ms | tok/sec 1128200.0631
for step 16302 | loss 3.164075 | norm 0.2880 | time 465.6379 ms | tok/sec 1125956.4028
for step 16303 | loss 3.172786 | norm 0.2820 | time 465.2410 ms | tok/sec 1126917.1253
for step 16304 | loss 3.192450 | norm 0.2960 | time 464.9067 ms | tok/sec 1127727.3665
for step 16305 | loss 3.145144 | norm 0.2852 | time 464.8921 ms | tok/sec 1127762.6459
for step 16306 | loss 3.155747 | norm 0.2903 | time 465.1754 ms | tok/sec 1127075.9610
for step 16307 | loss 3.117512 | norm 0.2969 | time 464.7851 ms | tok/sec 1128022.3938
for step 16308 | loss 3.171342 | norm 0.2929 | time 466.3053 ms | tok/sec 1124345.0366
for step 16309 | loss 3.110826 | norm 0.3075 | time 465.8682 ms | tok/sec 1125399.7612
for step 16310 | loss 3.220344 | norm 0.3057 | time 465.6096 ms | tok/sec 1126025.0127
for step 16311 | loss 3.172872 | norm 0.3235 | time 464.7205 ms | tok/sec 1128179.2260
for step 16312 | loss 3.199118 | norm 0.3072 | time 465.0071 ms | tok/sec 1127483.9406
for step 16313 | loss 3.255107 | norm 0.3429 | time 466.1479 ms | tok/sec 1124724.5787
for step 16314 | loss 3.284966 | norm 0.2978 | time 465.0495 ms | tok/sec 1127381.0510
for step 16315 | loss 3.210637 | norm 0.3284 | time 464.9296 ms | tok/sec 1127671.8492
for step 16316 | loss 3.266660 | norm 0.2925 | time 464.2706 ms | tok/sec 1129272.4746
for step 16317 | loss 3.175183 | norm 0.3032 | time 465.0710 ms | tok/sec 1127329.0354
for step 16318 | loss 3.246662 | norm 0.3042 | time 465.0462 ms | tok/sec 1127389.1428
for step 16319 | loss 3.189059 | norm 0.2904 | time 465.4126 ms | tok/sec 1126501.4767
for step 16320 | loss 3.228943 | norm 0.3321 | time 465.7702 ms | tok/sec 1125636.5259
for step 16321 | loss 3.222167 | norm 0.3086 | time 465.6847 ms | tok/sec 1125843.4165
for step 16322 | loss 3.199504 | norm 0.2878 | time 465.7500 ms | tok/sec 1125685.5043
for step 16323 | loss 3.206590 | norm 0.3242 | time 464.6456 ms | tok/sec 1128360.9976
for step 16324 | loss 3.244637 | norm 0.3439 | time 465.2364 ms | tok/sec 1126928.0980
for step 16325 | loss 3.280066 | norm 0.3206 | time 466.0289 ms | tok/sec 1125011.7056
for step 16326 | loss 3.230367 | norm 0.3481 | time 465.1155 ms | tok/sec 1127220.9738
for step 16327 | loss 3.214182 | norm 0.3288 | time 464.7684 ms | tok/sec 1128062.8998
for step 16328 | loss 3.220694 | norm 0.3452 | time 466.0521 ms | tok/sec 1124955.8799
for step 16329 | loss 3.183788 | norm 0.3056 | time 465.8432 ms | tok/sec 1125460.2390
for step 16330 | loss 3.201661 | norm 0.3166 | time 466.0444 ms | tok/sec 1124974.2960
for step 16331 | loss 3.147882 | norm 0.2921 | time 466.4688 ms | tok/sec 1123950.8143
for step 16332 | loss 3.185315 | norm 0.3143 | time 465.3957 ms | tok/sec 1126542.4506
for step 16333 | loss 3.182680 | norm 0.2934 | time 465.1084 ms | tok/sec 1127238.3084
for step 16334 | loss 3.204180 | norm 0.2926 | time 465.1196 ms | tok/sec 1127211.1510
for step 16335 | loss 3.137371 | norm 0.3022 | time 464.9372 ms | tok/sec 1127653.3446
for step 16336 | loss 3.144641 | norm 0.3049 | time 465.8220 ms | tok/sec 1125511.5063
for step 16337 | loss 3.085067 | norm 0.2931 | time 465.4038 ms | tok/sec 1126522.8289
for step 16338 | loss 3.137708 | norm 0.3225 | time 465.4255 ms | tok/sec 1126470.3154
for step 16339 | loss 3.138130 | norm 0.2990 | time 464.3667 ms | tok/sec 1129038.8154
for step 16340 | loss 3.186875 | norm 0.2794 | time 465.0204 ms | tok/sec 1127451.5688
for step 16341 | loss 3.190567 | norm 0.3054 | time 464.6730 ms | tok/sec 1128294.4184
for step 16342 | loss 3.164289 | norm 0.2878 | time 465.8713 ms | tok/sec 1125392.2739
for step 16343 | loss 3.147794 | norm 0.2993 | time 465.4496 ms | tok/sec 1126412.0368
for step 16344 | loss 3.210972 | norm 0.3002 | time 464.6780 ms | tok/sec 1128282.2613
for step 16345 | loss 3.242509 | norm 0.3260 | time 465.2445 ms | tok/sec 1126908.4629
for step 16346 | loss 3.204732 | norm 0.3235 | time 465.2197 ms | tok/sec 1126968.5254
for step 16347 | loss 3.212342 | norm 0.3144 | time 464.7355 ms | tok/sec 1128142.7630
for step 16348 | loss 3.171346 | norm 0.3383 | time 464.3285 ms | tok/sec 1129131.5717
for step 16349 | loss 3.192357 | norm 0.2905 | time 464.6020 ms | tok/sec 1128466.9615
for step 16350 | loss 3.094352 | norm 0.3422 | time 465.7276 ms | tok/sec 1125739.6736
for step 16351 | loss 3.222007 | norm 0.3360 | time 465.4708 ms | tok/sec 1126360.6875
for step 16352 | loss 3.200721 | norm 0.3013 | time 465.1318 ms | tok/sec 1127181.6837
for step 16353 | loss 3.189151 | norm 0.3236 | time 465.0974 ms | tok/sec 1127264.8894
for step 16354 | loss 3.167917 | norm 0.3013 | time 465.1005 ms | tok/sec 1127257.3772
for step 16355 | loss 3.195300 | norm 0.3160 | time 464.9763 ms | tok/sec 1127558.5184
for step 16356 | loss 3.209832 | norm 0.2959 | time 465.5359 ms | tok/sec 1126203.2069
for step 16357 | loss 3.225508 | norm 0.2736 | time 466.9604 ms | tok/sec 1122767.5112
for step 16358 | loss 3.215372 | norm 0.3105 | time 465.8172 ms | tok/sec 1125523.0277
for step 16359 | loss 3.250651 | norm 0.2989 | time 465.6632 ms | tok/sec 1125895.2952
for step 16360 | loss 3.238490 | norm 0.3034 | time 466.1729 ms | tok/sec 1124664.1798
for step 16361 | loss 3.220339 | norm 0.2874 | time 465.0893 ms | tok/sec 1127284.5369
for step 16362 | loss 3.209103 | norm 0.2853 | time 465.6985 ms | tok/sec 1125809.9861
for step 16363 | loss 3.234314 | norm 0.2698 | time 465.6439 ms | tok/sec 1125941.9901
for step 16364 | loss 3.176807 | norm 0.2941 | time 465.6086 ms | tok/sec 1126027.3191
for step 16365 | loss 3.167936 | norm 0.2867 | time 465.1415 ms | tok/sec 1127157.9955
for step 16366 | loss 3.187743 | norm 0.2995 | time 464.9746 ms | tok/sec 1127562.5655
for step 16367 | loss 3.151397 | norm 0.3059 | time 465.6689 ms | tok/sec 1125881.4604
for step 16368 | loss 3.201052 | norm 0.2998 | time 465.1189 ms | tok/sec 1127212.8844
for step 16369 | loss 3.154165 | norm 0.2846 | time 465.5118 ms | tok/sec 1126261.4638
for step 16370 | loss 3.148226 | norm 0.2748 | time 465.4088 ms | tok/sec 1126510.7100
for step 16371 | loss 3.149970 | norm 0.2748 | time 465.5166 ms | tok/sec 1126249.9273
for step 16372 | loss 3.190588 | norm 0.3062 | time 465.6525 ms | tok/sec 1125921.2363
for step 16373 | loss 3.181353 | norm 0.2998 | time 464.9274 ms | tok/sec 1127677.0537
for step 16374 | loss 3.140003 | norm 0.2858 | time 465.7657 ms | tok/sec 1125647.4736
for step 16375 | loss 3.164784 | norm 0.3150 | time 464.6618 ms | tok/sec 1128321.6281
for step 16376 | loss 3.174154 | norm 0.2902 | time 465.6367 ms | tok/sec 1125959.2854
for step 16377 | loss 3.203698 | norm 0.2811 | time 466.2864 ms | tok/sec 1124390.4531
for step 16378 | loss 3.199126 | norm 0.3016 | time 465.6227 ms | tok/sec 1125993.3012
for step 16379 | loss 3.196462 | norm 0.3339 | time 466.2094 ms | tok/sec 1124576.1817
for step 16380 | loss 3.270071 | norm 0.3391 | time 465.6019 ms | tok/sec 1126043.4639
for step 16381 | loss 3.235034 | norm 0.3328 | time 465.4610 ms | tok/sec 1126384.3422
Will loading at 0 from edu_fineweb10B/edufineweb_train_000087.npy
for step 16382 | loss 3.239014 | norm 0.3650 | time 2656.1701 ms | tok/sec 197384.9469
for step 16383 | loss 3.230190 | norm 0.3041 | time 482.5022 ms | tok/sec 1086602.2497
for step 16384 | loss 3.234616 | norm 0.3285 | time 463.6669 ms | tok/sec 1130742.7423
for step 16385 | loss 3.241340 | norm 0.3154 | time 464.0832 ms | tok/sec 1129728.4753
for step 16386 | loss 3.333086 | norm 0.3813 | time 464.5216 ms | tok/sec 1128662.1494
for step 16387 | loss 3.222873 | norm 0.3220 | time 465.0059 ms | tok/sec 1127486.8310
for step 16388 | loss 3.177549 | norm 0.3162 | time 464.9639 ms | tok/sec 1127588.5835
for step 16389 | loss 3.146416 | norm 0.3289 | time 465.1635 ms | tok/sec 1127104.8450
for step 16390 | loss 3.189872 | norm 0.3249 | time 465.5483 ms | tok/sec 1126173.2156
for step 16391 | loss 3.224712 | norm 0.3265 | time 465.3258 ms | tok/sec 1126711.5718
for step 16392 | loss 3.168276 | norm 0.3381 | time 464.4957 ms | tok/sec 1128725.2959
for step 16393 | loss 3.212227 | norm 0.3026 | time 464.6490 ms | tok/sec 1128352.8919
for step 16394 | loss 3.215326 | norm 0.3182 | time 465.0512 ms | tok/sec 1127377.0052
for step 16395 | loss 3.176196 | norm 0.3150 | time 465.1127 ms | tok/sec 1127227.9076
for step 16396 | loss 3.145579 | norm 0.3111 | time 465.2984 ms | tok/sec 1126777.9643
for step 16397 | loss 3.274491 | norm 0.3002 | time 465.4038 ms | tok/sec 1126522.8289
for step 16398 | loss 3.208293 | norm 0.2834 | time 465.1718 ms | tok/sec 1127084.6261
for step 16399 | loss 3.211197 | norm 0.3211 | time 464.9508 ms | tok/sec 1127620.3849
for step 16400 | loss 3.203443 | norm 0.3050 | time 465.3392 ms | tok/sec 1126679.2444
for step 16401 | loss 3.168690 | norm 0.3063 | time 465.3955 ms | tok/sec 1126543.0277
for step 16402 | loss 3.177273 | norm 0.3220 | time 466.0482 ms | tok/sec 1124965.0879
for step 16403 | loss 3.186594 | norm 0.2928 | time 464.9415 ms | tok/sec 1127642.9361
for step 16404 | loss 3.176008 | norm 0.2947 | time 465.7280 ms | tok/sec 1125738.5210
for step 16405 | loss 3.210867 | norm 0.2972 | time 464.7965 ms | tok/sec 1127994.6199
for step 16406 | loss 3.159546 | norm 0.2891 | time 464.6571 ms | tok/sec 1128333.2071
for step 16407 | loss 3.199372 | norm 0.2933 | time 464.7973 ms | tok/sec 1127992.8841
for step 16408 | loss 3.215968 | norm 0.3097 | time 465.0106 ms | tok/sec 1127475.2694
for step 16409 | loss 3.123457 | norm 0.2906 | time 465.4002 ms | tok/sec 1126531.4855
for step 16410 | loss 3.217129 | norm 0.2986 | time 465.1725 ms | tok/sec 1127082.8930
for step 16411 | loss 3.150279 | norm 0.2806 | time 465.0657 ms | tok/sec 1127341.7499
for step 16412 | loss 3.262703 | norm 0.3690 | time 465.0817 ms | tok/sec 1127303.0294
for step 16413 | loss 3.196191 | norm 0.3316 | time 465.4262 ms | tok/sec 1126468.5842
for step 16414 | loss 3.130808 | norm 0.3183 | time 466.4073 ms | tok/sec 1124099.0462
for step 16415 | loss 3.255629 | norm 0.3679 | time 466.8860 ms | tok/sec 1122946.3961
for step 16416 | loss 3.203215 | norm 0.6426 | time 466.2824 ms | tok/sec 1124400.2268
for step 16417 | loss 3.202526 | norm 0.3587 | time 465.3263 ms | tok/sec 1126710.4172
for step 16418 | loss 3.220151 | norm 0.3725 | time 465.5161 ms | tok/sec 1126251.0809
for step 16419 | loss 3.186752 | norm 0.3508 | time 465.5669 ms | tok/sec 1126128.2316
for step 16420 | loss 3.239995 | norm 0.3452 | time 465.5313 ms | tok/sec 1126214.1656
for step 16421 | loss 3.242351 | norm 0.3516 | time 465.2522 ms | tok/sec 1126889.9834
for step 16422 | loss 3.170814 | norm 0.3495 | time 465.2839 ms | tok/sec 1126813.1844
for step 16423 | loss 3.197564 | norm 0.3118 | time 465.5092 ms | tok/sec 1126267.8090
for step 16424 | loss 3.204690 | norm 0.3317 | time 465.2162 ms | tok/sec 1126977.1888
for step 16425 | loss 3.149256 | norm 0.3042 | time 465.2991 ms | tok/sec 1126776.2323
for step 16426 | loss 3.182443 | norm 0.2891 | time 465.3089 ms | tok/sec 1126752.5611
for step 16427 | loss 3.141387 | norm 0.3231 | time 466.4633 ms | tok/sec 1123964.0272
for step 16428 | loss 3.203860 | norm 0.3044 | time 465.1036 ms | tok/sec 1127249.8652
for step 16429 | loss 3.191468 | norm 0.3105 | time 466.1882 ms | tok/sec 1124627.3685
for step 16430 | loss 3.213199 | norm 0.3002 | time 466.5625 ms | tok/sec 1123725.0942
for step 16431 | loss 3.181470 | norm 0.2977 | time 465.4882 ms | tok/sec 1126318.5730
for step 16432 | loss 3.171046 | norm 0.3028 | time 464.8750 ms | tok/sec 1127804.2901
for step 16433 | loss 3.167959 | norm 0.3230 | time 465.8527 ms | tok/sec 1125437.1991
for step 16434 | loss 3.193075 | norm 0.2923 | time 466.2619 ms | tok/sec 1124449.6726
for step 16435 | loss 3.175155 | norm 0.2904 | time 466.1367 ms | tok/sec 1124751.6164
for step 16436 | loss 3.193061 | norm 0.2800 | time 465.0934 ms | tok/sec 1127274.7130
for step 16437 | loss 3.152707 | norm 0.2925 | time 465.9448 ms | tok/sec 1125214.9120
for step 16438 | loss 3.146703 | norm 0.2811 | time 465.5650 ms | tok/sec 1126132.8452
for step 16439 | loss 3.134699 | norm 0.2969 | time 469.5504 ms | tok/sec 1116574.5619
for step 16440 | loss 3.195803 | norm 0.3018 | time 465.2724 ms | tok/sec 1126840.9001
for step 16441 | loss 3.175653 | norm 0.2786 | time 464.8364 ms | tok/sec 1127898.0007
for step 16442 | loss 3.219892 | norm 0.3196 | time 465.4653 ms | tok/sec 1126373.9571
for step 16443 | loss 3.141757 | norm 0.2766 | time 465.6851 ms | tok/sec 1125842.2637
for step 16444 | loss 3.156221 | norm 0.2784 | time 465.6692 ms | tok/sec 1125880.8840
for step 16445 | loss 3.148091 | norm 0.3048 | time 465.0054 ms | tok/sec 1127487.9872
for step 16446 | loss 3.168433 | norm 0.2619 | time 465.8668 ms | tok/sec 1125403.2169
for step 16447 | loss 3.204969 | norm 0.3345 | time 465.8599 ms | tok/sec 1125419.9197
for step 16448 | loss 3.256443 | norm 0.3075 | time 466.0699 ms | tok/sec 1124912.7194
for step 16449 | loss 3.221657 | norm 0.3029 | time 464.6840 ms | tok/sec 1128267.7889
for step 16450 | loss 3.193525 | norm 0.3257 | time 466.2695 ms | tok/sec 1124431.2737
for step 16451 | loss 3.256016 | norm 0.2834 | time 465.3788 ms | tok/sec 1126583.4275
for step 16452 | loss 3.341226 | norm 0.4017 | time 466.5484 ms | tok/sec 1123758.9752
for step 16453 | loss 3.168577 | norm 0.3103 | time 465.6744 ms | tok/sec 1125868.2024
for step 16454 | loss 3.213376 | norm 0.3041 | time 466.2905 ms | tok/sec 1124380.6796
for step 16455 | loss 3.185462 | norm 0.3204 | time 466.3358 ms | tok/sec 1124271.4581
for step 16456 | loss 3.197269 | norm 0.3191 | time 465.4670 ms | tok/sec 1126369.9185
for step 16457 | loss 3.194627 | norm 0.2806 | time 465.9164 ms | tok/sec 1125283.4316
for step 16458 | loss 3.169502 | norm 0.2991 | time 464.5233 ms | tok/sec 1128658.0944
for step 16459 | loss 3.211682 | norm 0.2975 | time 465.6699 ms | tok/sec 1125879.1546
for step 16460 | loss 3.257325 | norm 0.2896 | time 464.8433 ms | tok/sec 1127881.2242
for step 16461 | loss 3.155716 | norm 0.3014 | time 465.0669 ms | tok/sec 1127338.8602
for step 16462 | loss 3.164121 | norm 0.2900 | time 464.9920 ms | tok/sec 1127520.3611
for step 16463 | loss 3.206124 | norm 0.2859 | time 465.8167 ms | tok/sec 1125524.1799
for step 16464 | loss 3.218467 | norm 0.2869 | time 465.8785 ms | tok/sec 1125374.9959
for step 16465 | loss 3.185386 | norm 0.2806 | time 464.9529 ms | tok/sec 1127615.1809
for step 16466 | loss 3.196889 | norm 0.4516 | time 465.0362 ms | tok/sec 1127413.4188
for step 16467 | loss 3.186186 | norm 0.2962 | time 465.8554 ms | tok/sec 1125430.8633
for step 16468 | loss 3.225707 | norm 0.2857 | time 465.2019 ms | tok/sec 1127011.8438
for step 16469 | loss 3.185649 | norm 0.3118 | time 465.1878 ms | tok/sec 1127045.9232
for step 16470 | loss 3.169541 | norm 0.2783 | time 465.1556 ms | tok/sec 1127123.9093
for step 16471 | loss 3.201607 | norm 0.2860 | time 464.8290 ms | tok/sec 1127915.9348
for step 16472 | loss 3.141225 | norm 0.2844 | time 465.2061 ms | tok/sec 1127001.4471
for step 16473 | loss 3.089018 | norm 0.3356 | time 465.5011 ms | tok/sec 1126287.4218
for step 16474 | loss 3.195104 | norm 0.2889 | time 465.3141 ms | tok/sec 1126739.8599
for step 16475 | loss 3.188474 | norm 0.2796 | time 465.4331 ms | tok/sec 1126451.8502
for step 16476 | loss 3.189808 | norm 0.3054 | time 465.1659 ms | tok/sec 1127099.0681
for step 16477 | loss 3.163345 | norm 0.2866 | time 465.7304 ms | tok/sec 1125732.7580
for step 16478 | loss 3.121973 | norm 0.3132 | time 465.2867 ms | tok/sec 1126806.2557
for step 16479 | loss 3.143778 | norm 0.2971 | time 465.2035 ms | tok/sec 1127007.8006
for step 16480 | loss 3.222348 | norm 0.2690 | time 465.1377 ms | tok/sec 1127167.2396
for step 16481 | loss 3.218578 | norm 0.3676 | time 466.0256 ms | tok/sec 1125019.7634
for step 16482 | loss 3.215539 | norm 0.3054 | time 465.9436 ms | tok/sec 1125217.7908
for step 16483 | loss 3.261893 | norm 0.3537 | time 465.9576 ms | tok/sec 1125183.8219
for step 16484 | loss 3.235162 | norm 0.3487 | time 465.0440 ms | tok/sec 1127394.3447
for step 16485 | loss 3.246408 | norm 0.3418 | time 465.7891 ms | tok/sec 1125591.0087
for step 16486 | loss 3.191953 | norm 0.3546 | time 465.2069 ms | tok/sec 1126999.7143
for step 16487 | loss 3.234217 | norm 0.3135 | time 465.3666 ms | tok/sec 1126612.8635
for step 16488 | loss 3.190638 | norm 0.3338 | time 465.6208 ms | tok/sec 1125997.9137
for step 16489 | loss 3.208899 | norm 0.3356 | time 465.4288 ms | tok/sec 1126462.2368
for step 16490 | loss 3.266581 | norm 0.3599 | time 465.5132 ms | tok/sec 1126258.0028
for step 16491 | loss 3.077486 | norm 0.3894 | time 465.7404 ms | tok/sec 1125708.5544
for step 16492 | loss 3.257438 | norm 0.3267 | time 465.9173 ms | tok/sec 1125281.1283
for step 16493 | loss 3.209200 | norm 0.3396 | time 466.1970 ms | tok/sec 1124606.0880
for step 16494 | loss 3.183489 | norm 0.3275 | time 466.3470 ms | tok/sec 1124244.4434
for step 16495 | loss 3.203106 | norm 0.3399 | time 466.6846 ms | tok/sec 1123431.1624
for step 16496 | loss 3.159359 | norm 0.3259 | time 465.3437 ms | tok/sec 1126668.2766
for step 16497 | loss 3.189997 | norm 0.3043 | time 465.0004 ms | tok/sec 1127500.1272
for step 16498 | loss 3.189310 | norm 0.3205 | time 465.5468 ms | tok/sec 1126176.6760
for step 16499 | loss 3.205881 | norm 0.3073 | time 465.5554 ms | tok/sec 1126155.9136
validation loss 3.2319
HellaSwag accuracy: 2816/10042=0.2804
> Hello, I'm a language model, and this is the only way to give the code for language families.
When you have two objects, one of them
> Hello, I'm a language model, which means I don't have to go through all the problems I would like to teach people, be able to explain the
> Hello, I'm a language model, but why should I use it?
I'm a person I've been learning, and I want to be like this
> Hello, I'm a language model, and I'll be a beginner with programming terminology - a list of different forms.
What is Python programming language?
> 
Hello, I'm a language model, and I'm working with that. That gave me a set of the languages like English, Math, and English. I
> Hello, I'm a language model, so when I'm trying to write more complex code such as "Hello world!" I can't understand your input, so
> Hello, I'm a language model, so I created my own template for you to print out. The goal is to be able to create a template that would
> Hello, I'm a language model, and for the last few years, I have begun to use the new Python library (as with the older Python libraries used
> Hello, I'm a language model, so I don't need to know how all this is going to work here.
This might be how it is done
> Hello, I'm a language model,
The language 'is' not true - just
As a result, we know this is true and it all is
> Hello, I'm a language model, I wanted to change that because I have no idea what I mean by "weep," but this should be understandable,
> > Hello, I'm a language model, as long as it exists because it is the language in question and a model is supposed to be a model, not a
Hello, I'm a language model, and it gets all the work done just as it looks, well. What's the point? There are two models of
> Hello, I'm a language model, and I want to explain the concepts and explain how you can use it."
The original article published on June 10,
> Hello, I'm a language model, and we are looking for objects. There is no one object in the universe. But our model of the Milky Way,>>
 Hello, I'm a language model, and my favorite is my friend and I love you.
- 1) When I type "Hello" on I type Hello, I'm a language model, and I understand that language modeling is a common topic. I mean, my point on the world how languages work is to> 

Hello, I'm a language model, because I code a language for a given piece of code. It takes into account the grammar rules and is an efficient programming
> > Hello, I'm a language model, and I want to talk about that in the first sentence.
[I want to write [I could start] my
> Hello, I'm a language model, thanks to our friends at the site; however, before sharing this article, I had to ask some questions, so this
Hello, I'm a language model, so I can write it.<|endoftext|>Cirrhosis is a common side effect of the liver injury from cirrhosis
> > > Hello, I'm a language model, and I'm a student and a research.
My students know the big
"If You Talk Here,
A
Hello, I'm a language model, and I have one on my wall too. I try to make sense of the world around me, and I see how
Hello, I'm a language model, but i feel like i need to write new words with my help.
When the language system stops working, it does
> > > Hello, I'm a language model, and this one is a super language model, so I just need a little help.
I'm not a great beginner
Hello, I'm a language model, but there is a limit to how long the code can run. There is no limit to how long the code can runHello, I'm a language model, so you don't have to worry about writing Python code. I've had a few good friends around to help me with>

 Hello, I'm a language model, and you've made a language model. Your project has to take care of itself and you know now that you'd just
> Hello, I'm a language model, with more than 1,800 different words in 20 languages. What an awesome website it will help me make friends and family> 
Hello, I'm a language model, and an app designer. Can you explain the language models of your company, or any other technology? Does it make sense
> Hello, I'm a language model, so you would have that. How cool is that?
Let's do a simple translation of the above sentence. What
> Hello, I'm a language model, isn't it?
We can't do anything without modeling. We can't get a good general understanding of what our
for step 16500 | loss 3.191668 | norm 0.3374 | time 12402.5249 ms | tok/sec 42272.6826
for step 16501 | loss 3.172758 | norm 0.3021 | time 461.9176 ms | tok/sec 1135024.8529
for step 16502 | loss 3.135454 | norm 0.2827 | time 462.9216 ms | tok/sec 1132563.2202
for step 16503 | loss 3.139038 | norm 0.2856 | time 463.3067 ms | tok/sec 1131621.9669
for step 16504 | loss 3.198928 | norm 0.2900 | time 463.3632 ms | tok/sec 1131483.9703
for step 16505 | loss 3.115288 | norm 0.3092 | time 463.0611 ms | tok/sec 1132222.0901
for step 16506 | loss 3.127034 | norm 0.2849 | time 463.6772 ms | tok/sec 1130717.7414
for step 16507 | loss 3.127253 | norm 0.2959 | time 464.1411 ms | tok/sec 1129587.4587
for step 16508 | loss 3.126991 | norm 0.3077 | time 463.5561 ms | tok/sec 1131013.1722
for step 16509 | loss 3.172134 | norm 0.3071 | time 464.3016 ms | tok/sec 1129197.0901
for step 16510 | loss 3.170721 | norm 0.3000 | time 464.7434 ms | tok/sec 1128123.6642
for step 16511 | loss 3.160571 | norm 0.3302 | time 465.0276 ms | tok/sec 1127434.2276
for step 16512 | loss 3.109573 | norm 0.3180 | time 464.5467 ms | tok/sec 1128601.3270
for step 16513 | loss 3.168173 | norm 0.3344 | time 464.4887 ms | tok/sec 1128742.0975
for step 16514 | loss 3.138827 | norm 0.3288 | time 464.1738 ms | tok/sec 1129507.9710
for step 16515 | loss 3.188530 | norm 0.3273 | time 464.6435 ms | tok/sec 1128366.2085
for step 16516 | loss 3.205093 | norm 0.2997 | time 464.8290 ms | tok/sec 1127915.9348
for step 16517 | loss 3.177248 | norm 0.3352 | time 464.7462 ms | tok/sec 1128116.7194
for step 16518 | loss 3.275832 | norm 0.3344 | time 464.6270 ms | tok/sec 1128406.1601
for step 16519 | loss 3.262547 | norm 0.3356 | time 465.6148 ms | tok/sec 1126012.3279
for step 16520 | loss 3.273915 | norm 0.2924 | time 465.3330 ms | tok/sec 1126694.2533
for step 16521 | loss 3.200865 | norm 0.3834 | time 465.4994 ms | tok/sec 1126291.4598
for step 16522 | loss 3.228781 | norm 0.3162 | time 464.8511 ms | tok/sec 1127862.1343
for step 16523 | loss 3.224420 | norm 0.3274 | time 465.0168 ms | tok/sec 1127460.2397
for step 16524 | loss 3.217279 | norm 0.3104 | time 465.4882 ms | tok/sec 1126318.5730
for step 16525 | loss 3.218133 | norm 0.3290 | time 465.3337 ms | tok/sec 1126692.5215
for step 16526 | loss 3.281359 | norm 0.3462 | time 465.1868 ms | tok/sec 1127048.2337
for step 16527 | loss 3.191359 | norm 0.3204 | time 465.6246 ms | tok/sec 1125988.6888
for step 16528 | loss 3.180892 | norm 0.3441 | time 464.9484 ms | tok/sec 1127626.1672
for step 16529 | loss 3.192776 | norm 0.3313 | time 465.3795 ms | tok/sec 1126581.6961
for step 16530 | loss 3.219254 | norm 0.3173 | time 465.5461 ms | tok/sec 1126178.4063
for step 16531 | loss 3.192054 | norm 0.3226 | time 466.1114 ms | tok/sec 1124812.6000
for step 16532 | loss 3.175649 | norm 0.2957 | time 465.1527 ms | tok/sec 1127130.8419
for step 16533 | loss 3.196761 | norm 0.3206 | time 465.5819 ms | tok/sec 1126091.9010
for step 16534 | loss 3.197108 | norm 0.3328 | time 466.2461 ms | tok/sec 1124487.6224
for step 16535 | loss 3.164018 | norm 0.2833 | time 465.5552 ms | tok/sec 1126156.4903
for step 16536 | loss 3.174401 | norm 0.3655 | time 465.1756 ms | tok/sec 1127075.3834
for step 16537 | loss 3.207576 | norm 0.2854 | time 465.8628 ms | tok/sec 1125413.0082
for step 16538 | loss 3.231621 | norm 0.3435 | time 465.6088 ms | tok/sec 1126026.7425
for step 16539 | loss 3.188381 | norm 0.3040 | time 464.9549 ms | tok/sec 1127610.5552
for step 16540 | loss 3.271467 | norm 0.4531 | time 465.5499 ms | tok/sec 1126169.1784
for step 16541 | loss 3.137517 | norm 0.4042 | time 465.4481 ms | tok/sec 1126415.4988
for step 16542 | loss 3.132457 | norm 0.3637 | time 465.4834 ms | tok/sec 1126330.1109
for step 16543 | loss 3.141659 | norm 0.4131 | time 465.5383 ms | tok/sec 1126197.4392
for step 16544 | loss 3.122605 | norm 0.3430 | time 465.8992 ms | tok/sec 1125324.8928
for step 16545 | loss 3.149771 | norm 0.4037 | time 465.1589 ms | tok/sec 1127115.8214
for step 16546 | loss 3.128819 | norm 0.3744 | time 465.1465 ms | tok/sec 1127145.8629
for step 16547 | loss 3.153885 | norm 0.3163 | time 465.8713 ms | tok/sec 1125392.2739
for step 16548 | loss 3.163553 | norm 0.3478 | time 466.2178 ms | tok/sec 1124556.0534
for step 16549 | loss 3.193290 | norm 0.3267 | time 465.8432 ms | tok/sec 1125460.2390
for step 16550 | loss 3.153247 | norm 0.3182 | time 466.2800 ms | tok/sec 1124405.9761
for step 16551 | loss 3.244103 | norm 0.3271 | time 464.3688 ms | tok/sec 1129033.5983
for step 16552 | loss 3.178537 | norm 0.3582 | time 465.6990 ms | tok/sec 1125808.8334
for step 16553 | loss 3.246356 | norm 0.3191 | time 464.7472 ms | tok/sec 1128114.4044
for step 16554 | loss 3.285516 | norm 0.3729 | time 465.5118 ms | tok/sec 1126261.4638
for step 16555 | loss 3.217736 | norm 0.3251 | time 465.3347 ms | tok/sec 1126690.2124
for step 16556 | loss 3.174479 | norm 0.3176 | time 465.5020 ms | tok/sec 1126285.1144
for step 16557 | loss 3.208543 | norm 0.3376 | time 465.4295 ms | tok/sec 1126460.5057
for step 16558 | loss 3.266688 | norm 0.3103 | time 465.2684 ms | tok/sec 1126850.7164
for step 16559 | loss 3.269776 | norm 0.3225 | time 467.5419 ms | tok/sec 1121371.0748
for step 16560 | loss 3.258827 | norm 0.2996 | time 466.7301 ms | tok/sec 1123321.5514
for step 16561 | loss 3.227241 | norm 0.3196 | time 465.2781 ms | tok/sec 1126827.0421
for step 16562 | loss 3.211068 | norm 0.3509 | time 464.5255 ms | tok/sec 1128652.8808
for step 16563 | loss 3.267682 | norm 0.3336 | time 465.2188 ms | tok/sec 1126970.8357
for step 16564 | loss 3.261034 | norm 0.3194 | time 465.5600 ms | tok/sec 1126144.9560
for step 16565 | loss 3.169373 | norm 0.3436 | time 464.8876 ms | tok/sec 1127773.6351
for step 16566 | loss 3.234772 | norm 0.2926 | time 465.5020 ms | tok/sec 1126285.1144
for step 16567 | loss 3.154456 | norm 0.3154 | time 464.9596 ms | tok/sec 1127598.9910
for step 16568 | loss 3.207005 | norm 0.3192 | time 465.3239 ms | tok/sec 1126716.1902
for step 16569 | loss 3.212807 | norm 0.2957 | time 465.8585 ms | tok/sec 1125423.3756
for step 16570 | loss 3.196487 | norm 0.3242 | time 464.8342 ms | tok/sec 1127903.2073
for step 16571 | loss 3.191465 | norm 0.3201 | time 465.1780 ms | tok/sec 1127069.6067
for step 16572 | loss 3.195663 | norm 0.3172 | time 465.3540 ms | tok/sec 1126643.4555
Will loading at 0 from edu_fineweb10B/edufineweb_train_000088.npy
for step 16573 | loss 3.187806 | norm 0.2900 | time 2592.8051 ms | tok/sec 202208.7933
for step 16574 | loss 3.272606 | norm 0.3046 | time 463.5384 ms | tok/sec 1131056.2202
for step 16575 | loss 3.202384 | norm 0.3094 | time 464.3812 ms | tok/sec 1129003.4561
for step 16576 | loss 3.114945 | norm 0.3013 | time 463.4168 ms | tok/sec 1131352.9921
for step 16577 | loss 3.152474 | norm 0.3130 | time 464.5207 ms | tok/sec 1128664.4666
for step 16578 | loss 3.138845 | norm 0.2990 | time 464.6904 ms | tok/sec 1128252.1592
for step 16579 | loss 3.133807 | norm 0.3009 | time 464.2670 ms | tok/sec 1129281.1734
for step 16580 | loss 3.106371 | norm 0.3013 | time 464.7639 ms | tok/sec 1128073.8948
for step 16581 | loss 3.177116 | norm 0.3014 | time 464.7102 ms | tok/sec 1128204.1148
for step 16582 | loss 3.157854 | norm 0.2819 | time 465.0991 ms | tok/sec 1127260.8444
for step 16583 | loss 3.120805 | norm 0.2924 | time 465.8670 ms | tok/sec 1125402.6409
for step 16584 | loss 3.128294 | norm 0.2833 | time 464.4430 ms | tok/sec 1128853.3484
for step 16585 | loss 3.118157 | norm 0.2745 | time 465.4574 ms | tok/sec 1126392.9966
for step 16586 | loss 3.269767 | norm 0.3257 | time 463.9235 ms | tok/sec 1130117.4691
for step 16587 | loss 3.182898 | norm 0.3137 | time 466.2433 ms | tok/sec 1124494.5226
for step 16588 | loss 3.261983 | norm 0.2898 | time 466.1114 ms | tok/sec 1124812.6000
for step 16589 | loss 3.161633 | norm 0.3127 | time 465.3451 ms | tok/sec 1126664.8131
for step 16590 | loss 3.217260 | norm 0.3294 | time 465.6088 ms | tok/sec 1126026.7425
for step 16591 | loss 3.229894 | norm 0.3347 | time 465.8608 ms | tok/sec 1125417.6159
for step 16592 | loss 3.269006 | norm 0.2959 | time 466.0418 ms | tok/sec 1124980.6267
for step 16593 | loss 3.209388 | norm 0.3562 | time 465.3146 ms | tok/sec 1126738.7052
for step 16594 | loss 3.168164 | norm 0.2956 | time 465.3304 ms | tok/sec 1126700.6034
for step 16595 | loss 3.225062 | norm 0.3349 | time 464.8204 ms | tok/sec 1127936.7621
for step 16596 | loss 3.223431 | norm 0.3361 | time 465.7662 ms | tok/sec 1125646.3212
for step 16597 | loss 3.220618 | norm 0.3181 | time 465.7168 ms | tok/sec 1125765.6075
for step 16598 | loss 3.222445 | norm 0.3352 | time 465.4396 ms | tok/sec 1126436.2707
for step 16599 | loss 3.176117 | norm 0.3245 | time 465.8937 ms | tok/sec 1125338.1380
for step 16600 | loss 3.157685 | norm 0.3032 | time 464.6013 ms | tok/sec 1128468.6988
for step 16601 | loss 3.222823 | norm 0.3413 | time 464.6077 ms | tok/sec 1128453.0635
for step 16602 | loss 3.236776 | norm 0.3235 | time 465.8761 ms | tok/sec 1125380.7552
for step 16603 | loss 3.100848 | norm 0.2717 | time 464.7028 ms | tok/sec 1128222.0586
for step 16604 | loss 3.179183 | norm 0.3183 | time 465.2524 ms | tok/sec 1126889.4059
for step 16605 | loss 3.229154 | norm 0.3113 | time 465.3516 ms | tok/sec 1126649.2277
for step 16606 | loss 3.177128 | norm 0.2911 | time 465.8036 ms | tok/sec 1125555.8649
for step 16607 | loss 3.160620 | norm 0.3130 | time 465.1399 ms | tok/sec 1127162.0398
for step 16608 | loss 3.187156 | norm 0.3087 | time 465.2932 ms | tok/sec 1126790.6664
for step 16609 | loss 3.209700 | norm 0.3129 | time 464.0396 ms | tok/sec 1129834.6963
for step 16610 | loss 3.176736 | norm 0.3132 | time 464.5278 ms | tok/sec 1128647.0880
for step 16611 | loss 3.106552 | norm 0.2944 | time 465.5883 ms | tok/sec 1126076.3315
for step 16612 | loss 3.111546 | norm 0.3138 | time 464.3769 ms | tok/sec 1129013.8897
for step 16613 | loss 3.116733 | norm 0.3109 | time 465.7161 ms | tok/sec 1125767.3365
for step 16614 | loss 3.155225 | norm 0.2994 | time 465.0872 ms | tok/sec 1127289.7379
for step 16615 | loss 3.129215 | norm 0.2993 | time 465.0505 ms | tok/sec 1127378.7391
for step 16616 | loss 3.161139 | norm 0.2830 | time 466.1391 ms | tok/sec 1124745.8636
for step 16617 | loss 3.163203 | norm 0.2761 | time 466.8360 ms | tok/sec 1123066.8315
for step 16618 | loss 3.119204 | norm 0.2907 | time 465.5144 ms | tok/sec 1126255.1187
for step 16619 | loss 3.122725 | norm 0.3047 | time 465.0242 ms | tok/sec 1127442.3201
for step 16620 | loss 3.197746 | norm 0.2663 | time 465.6763 ms | tok/sec 1125863.5910
for step 16621 | loss 3.155506 | norm 0.3088 | time 465.2593 ms | tok/sec 1126872.6594
for step 16622 | loss 3.264602 | norm 0.3018 | time 465.8897 ms | tok/sec 1125347.9282
for step 16623 | loss 3.243315 | norm 0.2869 | time 465.2693 ms | tok/sec 1126848.4067
for step 16624 | loss 3.245271 | norm 0.2748 | time 465.4291 ms | tok/sec 1126461.6598
for step 16625 | loss 3.261572 | norm 0.3138 | time 465.6847 ms | tok/sec 1125843.4165
for step 16626 | loss 3.221004 | norm 0.2910 | time 464.8616 ms | tok/sec 1127836.6821
for step 16627 | loss 3.179199 | norm 0.3182 | time 464.5185 ms | tok/sec 1128669.6803
for step 16628 | loss 3.255706 | norm 0.2891 | time 465.5397 ms | tok/sec 1126193.9786
for step 16629 | loss 3.221298 | norm 0.2838 | time 465.4398 ms | tok/sec 1126435.6937
for step 16630 | loss 3.233719 | norm 0.3172 | time 465.4233 ms | tok/sec 1126475.5088
for step 16631 | loss 3.221806 | norm 0.2927 | time 465.0848 ms | tok/sec 1127295.5167
for step 16632 | loss 3.307443 | norm 0.3093 | time 464.8025 ms | tok/sec 1127980.1549
for step 16633 | loss 3.189651 | norm 0.2909 | time 465.7977 ms | tok/sec 1125570.2678
for step 16634 | loss 3.204331 | norm 0.2959 | time 465.4613 ms | tok/sec 1126383.7653
for step 16635 | loss 3.203326 | norm 0.3238 | time 465.9159 ms | tok/sec 1125284.5832
for step 16636 | loss 3.167089 | norm 0.2908 | time 465.4703 ms | tok/sec 1126361.8414
for step 16637 | loss 3.207443 | norm 0.2897 | time 465.0006 ms | tok/sec 1127499.5491
for step 16638 | loss 3.184042 | norm 0.3065 | time 465.1752 ms | tok/sec 1127076.5387
for step 16639 | loss 3.181053 | norm 0.3225 | time 464.5250 ms | tok/sec 1128654.0394
for step 16640 | loss 3.193377 | norm 0.2813 | time 464.8442 ms | tok/sec 1127878.9102
for step 16641 | loss 3.208666 | norm 0.2953 | time 465.3168 ms | tok/sec 1126733.5094
for step 16642 | loss 3.206311 | norm 0.2958 | time 465.7960 ms | tok/sec 1125574.3007
for step 16643 | loss 3.182474 | norm 0.2844 | time 464.8392 ms | tok/sec 1127891.0586
for step 16644 | loss 3.144893 | norm 0.3058 | time 468.8160 ms | tok/sec 1118323.5066
for step 16645 | loss 3.224794 | norm 0.2994 | time 465.5914 ms | tok/sec 1126068.8352
for step 16646 | loss 3.153876 | norm 0.3133 | time 464.9909 ms | tok/sec 1127523.2517
for step 16647 | loss 3.106332 | norm 0.3118 | time 464.4458 ms | tok/sec 1128846.3946
for step 16648 | loss 3.111898 | norm 0.3082 | time 465.7588 ms | tok/sec 1125664.1838
for step 16649 | loss 3.126761 | norm 0.2991 | time 465.0116 ms | tok/sec 1127472.9571
for step 16650 | loss 3.186685 | norm 0.3079 | time 464.7074 ms | tok/sec 1128211.0607
for step 16651 | loss 3.100362 | norm 0.3286 | time 465.6343 ms | tok/sec 1125965.0507
for step 16652 | loss 3.215064 | norm 0.2909 | time 465.3242 ms | tok/sec 1126715.6129
for step 16653 | loss 3.140726 | norm 0.2928 | time 465.5478 ms | tok/sec 1126174.3691
for step 16654 | loss 3.142218 | norm 0.2977 | time 464.9878 ms | tok/sec 1127530.7673
for step 16655 | loss 3.130684 | norm 0.2708 | time 464.6854 ms | tok/sec 1128264.3156
for step 16656 | loss 3.145263 | norm 0.3182 | time 464.9448 ms | tok/sec 1127634.8407
for step 16657 | loss 3.260781 | norm 0.3143 | time 465.1995 ms | tok/sec 1127017.6198
for step 16658 | loss 3.230916 | norm 0.2983 | time 465.9753 ms | tok/sec 1125141.2196
for step 16659 | loss 3.286294 | norm 0.3050 | time 465.4114 ms | tok/sec 1126504.3620
for step 16660 | loss 3.215900 | norm 0.3071 | time 465.2255 ms | tok/sec 1126954.6643
for step 16661 | loss 3.277149 | norm 0.3064 | time 464.9765 ms | tok/sec 1127557.9402
for step 16662 | loss 3.176228 | norm 0.3194 | time 465.0161 ms | tok/sec 1127461.9738
for step 16663 | loss 3.239312 | norm 0.3058 | time 464.7534 ms | tok/sec 1128099.3576
for step 16664 | loss 3.278381 | norm 0.3192 | time 464.9017 ms | tok/sec 1127739.5116
for step 16665 | loss 3.192519 | norm 0.3187 | time 464.3278 ms | tok/sec 1129133.3111
for step 16666 | loss 3.266369 | norm 0.3156 | time 464.2985 ms | tok/sec 1129204.6281
for step 16667 | loss 3.187829 | norm 0.3187 | time 465.4915 ms | tok/sec 1126310.4966
for step 16668 | loss 3.167116 | norm 0.2947 | time 464.7419 ms | tok/sec 1128127.1367
for step 16669 | loss 3.240400 | norm 0.2880 | time 464.5762 ms | tok/sec 1128529.5070
for step 16670 | loss 3.190997 | norm 0.3045 | time 464.2897 ms | tok/sec 1129226.0829
for step 16671 | loss 3.211271 | norm 0.2914 | time 465.2138 ms | tok/sec 1126982.9645
for step 16672 | loss 3.250786 | norm 0.3275 | time 465.5132 ms | tok/sec 1126258.0028
for step 16673 | loss 3.234972 | norm 0.4869 | time 464.5517 ms | tok/sec 1128589.1633
for step 16674 | loss 3.210033 | norm 0.3974 | time 465.0764 ms | tok/sec 1127315.7433
for step 16675 | loss 3.142108 | norm 0.3482 | time 463.8307 ms | tok/sec 1130343.4406
for step 16676 | loss 3.227927 | norm 0.3255 | time 465.1189 ms | tok/sec 1127212.8844
for step 16677 | loss 3.209439 | norm 0.3141 | time 465.5597 ms | tok/sec 1126145.5327
for step 16678 | loss 3.207885 | norm 0.3130 | time 465.3931 ms | tok/sec 1126548.7990
for step 16679 | loss 3.176864 | norm 0.3457 | time 465.0371 ms | tok/sec 1127411.1067
for step 16680 | loss 3.130865 | norm 0.3248 | time 464.7927 ms | tok/sec 1128003.8777
for step 16681 | loss 3.147431 | norm 0.3349 | time 464.9541 ms | tok/sec 1127612.2898
for step 16682 | loss 3.159394 | norm 0.3116 | time 465.7602 ms | tok/sec 1125660.7265
for step 16683 | loss 3.166965 | norm 0.3276 | time 466.1891 ms | tok/sec 1124625.0679
for step 16684 | loss 3.120951 | norm 0.3039 | time 465.6043 ms | tok/sec 1126037.6978
for step 16685 | loss 3.170794 | norm 0.3078 | time 465.5390 ms | tok/sec 1126195.7089
for step 16686 | loss 3.175050 | norm 0.3075 | time 465.3721 ms | tok/sec 1126599.5883
for step 16687 | loss 3.166792 | norm 0.2784 | time 465.8744 ms | tok/sec 1125384.7867
for step 16688 | loss 3.095436 | norm 0.3953 | time 464.3855 ms | tok/sec 1128993.0226
for step 16689 | loss 3.103149 | norm 0.2801 | time 465.1251 ms | tok/sec 1127197.8617
for step 16690 | loss 3.114088 | norm 0.2831 | time 465.2238 ms | tok/sec 1126958.7071
for step 16691 | loss 3.162955 | norm 0.2774 | time 464.5672 ms | tok/sec 1128551.5154
for step 16692 | loss 3.218709 | norm 0.3159 | time 465.2686 ms | tok/sec 1126850.1390
for step 16693 | loss 3.265754 | norm 0.3004 | time 464.9849 ms | tok/sec 1127537.7050
for step 16694 | loss 3.204911 | norm 0.2961 | time 465.6887 ms | tok/sec 1125833.6178
for step 16695 | loss 3.172928 | norm 0.2996 | time 465.7156 ms | tok/sec 1125768.4891
for step 16696 | loss 3.228055 | norm 0.3002 | time 464.8774 ms | tok/sec 1127798.5060
for step 16697 | loss 3.212816 | norm 0.3134 | time 465.1248 ms | tok/sec 1127198.4394
for step 16698 | loss 3.227959 | norm 0.2815 | time 465.3382 ms | tok/sec 1126681.5534
for step 16699 | loss 3.309813 | norm 0.3195 | time 464.9093 ms | tok/sec 1127721.0049
for step 16700 | loss 3.225076 | norm 0.3125 | time 464.8650 ms | tok/sec 1127828.5839
for step 16701 | loss 3.319715 | norm 0.3129 | time 465.5983 ms | tok/sec 1126052.1131
for step 16702 | loss 3.192783 | norm 0.3108 | time 466.0909 ms | tok/sec 1124862.0821
for step 16703 | loss 3.225757 | norm 0.2990 | time 465.3265 ms | tok/sec 1126709.8400
for step 16704 | loss 3.216348 | norm 0.2799 | time 464.8793 ms | tok/sec 1127793.8788
for step 16705 | loss 3.182183 | norm 0.2899 | time 465.6885 ms | tok/sec 1125834.1942
for step 16706 | loss 3.203359 | norm 0.2833 | time 465.8065 ms | tok/sec 1125548.9517
for step 16707 | loss 3.206206 | norm 0.2921 | time 464.7064 ms | tok/sec 1128213.3761
for step 16708 | loss 3.263853 | norm 0.2788 | time 464.8762 ms | tok/sec 1127801.3980
for step 16709 | loss 3.170341 | norm 0.2800 | time 465.3044 ms | tok/sec 1126763.5305
for step 16710 | loss 3.186874 | norm 0.2866 | time 465.8513 ms | tok/sec 1125440.6550
for step 16711 | loss 3.204203 | norm 0.3136 | time 465.6014 ms | tok/sec 1126044.6171
for step 16712 | loss 3.163072 | norm 0.2836 | time 465.9257 ms | tok/sec 1125260.9747
for step 16713 | loss 3.229841 | norm 0.2936 | time 465.7290 ms | tok/sec 1125736.2158
for step 16714 | loss 3.296104 | norm 0.3701 | time 465.1544 ms | tok/sec 1127126.7979
for step 16715 | loss 3.167069 | norm 0.2988 | time 464.8046 ms | tok/sec 1127974.9476
for step 16716 | loss 3.164925 | norm 0.2934 | time 465.3373 ms | tok/sec 1126683.8625
for step 16717 | loss 3.166964 | norm 0.3142 | time 464.7012 ms | tok/sec 1128226.1105
for step 16718 | loss 3.164510 | norm 0.3003 | time 465.3997 ms | tok/sec 1126532.6397
for step 16719 | loss 3.202217 | norm 0.2993 | time 465.1113 ms | tok/sec 1127231.3745
for step 16720 | loss 3.202316 | norm 0.3068 | time 465.6868 ms | tok/sec 1125838.2289
for step 16721 | loss 3.153090 | norm 0.3041 | time 465.2493 ms | tok/sec 1126896.9131
for step 16722 | loss 3.163774 | norm 0.3078 | time 465.1911 ms | tok/sec 1127037.8364
for step 16723 | loss 3.152740 | norm 0.3195 | time 465.0509 ms | tok/sec 1127377.5832
for step 16724 | loss 3.187162 | norm 0.2800 | time 465.8487 ms | tok/sec 1125446.9910
for step 16725 | loss 3.183495 | norm 0.3451 | time 466.1217 ms | tok/sec 1124787.8606
for step 16726 | loss 3.099725 | norm 0.3246 | time 464.5834 ms | tok/sec 1128512.1326
for step 16727 | loss 3.214288 | norm 0.3017 | time 464.8311 ms | tok/sec 1127910.7280
for step 16728 | loss 3.205117 | norm 0.3088 | time 465.9538 ms | tok/sec 1125193.0336
for step 16729 | loss 3.273010 | norm 0.3444 | time 465.8775 ms | tok/sec 1125377.2996
for step 16730 | loss 3.296183 | norm 0.3887 | time 464.9813 ms | tok/sec 1127546.3771
for step 16731 | loss 3.199385 | norm 0.3038 | time 466.3181 ms | tok/sec 1124313.9945
for step 16732 | loss 3.268549 | norm 0.3191 | time 466.1124 ms | tok/sec 1124810.2986
for step 16733 | loss 3.188468 | norm 0.3207 | time 464.9339 ms | tok/sec 1127661.4403
for step 16734 | loss 3.284404 | norm 0.3600 | time 465.4758 ms | tok/sec 1126348.5720
for step 16735 | loss 3.217425 | norm 0.3156 | time 465.2090 ms | tok/sec 1126994.5160
for step 16736 | loss 3.213599 | norm 0.3748 | time 465.3251 ms | tok/sec 1126713.3037
for step 16737 | loss 3.235807 | norm 0.3120 | time 465.0531 ms | tok/sec 1127372.3814
for step 16738 | loss 3.226737 | norm 0.3304 | time 465.3213 ms | tok/sec 1126722.5405
for step 16739 | loss 3.189074 | norm 0.2907 | time 465.1134 ms | tok/sec 1127226.1741
for step 16740 | loss 3.223001 | norm 0.3073 | time 465.0283 ms | tok/sec 1127432.4935
for step 16741 | loss 3.207005 | norm 0.3037 | time 466.0044 ms | tok/sec 1125070.9905
for step 16742 | loss 3.257049 | norm 0.2974 | time 465.3959 ms | tok/sec 1126541.8735
for step 16743 | loss 3.185321 | norm 0.3056 | time 466.1705 ms | tok/sec 1124669.9318
for step 16744 | loss 3.249363 | norm 0.3231 | time 465.5643 ms | tok/sec 1126134.5753
for step 16745 | loss 3.209908 | norm 0.2987 | time 466.4485 ms | tok/sec 1123999.6461
for step 16746 | loss 3.162870 | norm 0.3361 | time 464.4229 ms | tok/sec 1128902.0276
for step 16747 | loss 3.216994 | norm 0.2974 | time 465.7483 ms | tok/sec 1125689.5380
for step 16748 | loss 3.217810 | norm 0.3032 | time 465.1642 ms | tok/sec 1127103.1120
for step 16749 | loss 3.215924 | norm 0.3001 | time 465.5948 ms | tok/sec 1126060.7624
validation loss 3.2285
HellaSwag accuracy: 2816/10042=0.2804
> Hello, I'm a language model, and i am a program engineer. Also a person. We would like to take a closer look on the web and see
> Hello, I'm a language model, not an artificial intelligence. I am a computational linguist so I believe that a paradigm which approximates a language is a
> Hello, I'm a language model, I had the most experience with it and I'm willing to explore more and more. I have learned how to program things
> Hello, I'm a language model, and I'm using the example given above without any additional discussion. To me, the two basic components of a language model
> Hello, I'm a language model, and I know that it's going to involve not only coding but an ability to interact with what is already around us and
> Hello, I'm a language model, so here's a simple example that just shows the three possible values in these variables: the x value here, the y
> Hello, I'm a language model, so I could only focus on being able to make sense of language as an object. I'm not a language model here
> Hello, I'm a language model, and how I learned it is really simple to demonstrate. I didn't get the project completed or the actual project. When
> Hello, I'm a language model, and that is to say, you do have a model. I'm not going to bother with the model but I will>
 Hello, I'm a language model, so I need to think about the way each of you is different. We might try it myself, but you don't
> Hello, I'm a language model, I'm mostly in the process of having no idea of what's going on. I'm trying to model something that isn> > 
>>Hello, I'm a language model, as is, I know exactly how much of the problem that is related with your current language. I'm a language theorist
Hello, I'm a language model, to use the information given by language model and you are saying, "That's really not a language model." Please tell
 Hello, I'm a language model, and I understand that there's a lot else to learn about it. As a post on wikipedia and a post about Hello, I'm a language model, so I'll give you a tutorial as to how to do it. Now you are going to know it's an object> > 

Hello, I'm a language model, and I want to give a very basic code for modeling and modeling a particular language, so that we can easily understand it
Hello, I'm a language model, but instead of getting my data from scratch, I'm really working with a machine learning model. And since I use machine
> > > > > Hello, I'm a language model, and I'm not even in a foreign language, but I am in "home." But my computer seems to get in
Hello, I'm a language model, let's do a simple web based presentation (I get at least 6 different languages each). It's also not difficult but
Hello, I'm a language model, I want to be the one who writes for the languages that is the most efficient language to develop here, I'm using
> Hello, I'm a language model, I use it all the time to represent different aspects of the same language and not just 'the'. In English I
Hello, I'm a language model, and you work out your best. Do that in practice, at home and at work. I'd love to hear about
> > Hello, I'm a language model, I like it because it's a very real thing. It's also fun, because it's an incredible tool for the
> Hello, I'm a language model, and this one is a more general classification of the language I'm teaching.
I'm a language model, in general
Hello, I'm a language model, and I've gotten your back the moment it's right.
"Well, I've got a lot of my head
> Hello, I'm a language model, and I want to show you, I hope, that you can implement it, that way . . . that way.
> > Hello, I'm a language model, learning new words is about having conversations and having people talk about their "words". When we are not doing grammar lessons,
Hello, I'm a language model, and it works perfectly!"
Now, with two completely different worlds, it looks like this:
I'm currently looking
Hello, I'm a language model, so it's not my choice. Maybe because I'm a math major, I'll try to keep that in my browser
> > Hello, I'm a language model, so I just need to do something simple.
I'm a linguist and so am going to start with the simplest
Hello, I'm a language model, and one of the most interesting, but very frustrating to have. I have a strong love of languages when it comes to
> Hello, I'm a language model, there is no equivalent to Python, so it's possible.
If you know nothing else about our programming, then why
for step 16750 | loss 3.196081 | norm 0.2822 | time 12364.0549 ms | tok/sec 42404.2115
for step 16751 | loss 3.147366 | norm 0.3098 | time 462.1234 ms | tok/sec 1134519.4959
for step 16752 | loss 3.142786 | norm 0.2954 | time 462.6336 ms | tok/sec 1133268.2903
for step 16753 | loss 3.147971 | norm 0.2910 | time 462.7516 ms | tok/sec 1132979.2689
for step 16754 | loss 3.115546 | norm 0.2859 | time 463.2547 ms | tok/sec 1131748.9302
for step 16755 | loss 3.083627 | norm 0.2809 | time 462.5781 ms | tok/sec 1133404.3858
for step 16756 | loss 3.106547 | norm 0.2711 | time 463.1438 ms | tok/sec 1132019.8416
for step 16757 | loss 3.138965 | norm 0.3082 | time 463.1641 ms | tok/sec 1131970.3104
for step 16758 | loss 3.143314 | norm 0.2857 | time 463.4600 ms | tok/sec 1131247.6493
for step 16759 | loss 3.141393 | norm 0.2791 | time 464.2057 ms | tok/sec 1129430.2347
for step 16760 | loss 3.164386 | norm 0.2902 | time 464.1526 ms | tok/sec 1129559.6077
for step 16761 | loss 3.165445 | norm 0.2996 | time 463.8629 ms | tok/sec 1130265.0084
for step 16762 | loss 3.233413 | norm 0.3352 | time 463.3691 ms | tok/sec 1131469.4157
Will loading at 0 from edu_fineweb10B/edufineweb_train_000089.npy
for step 16763 | loss 3.267401 | norm 0.3132 | time 2603.8086 ms | tok/sec 201354.2745
for step 16764 | loss 3.224716 | norm 0.3496 | time 481.0538 ms | tok/sec 1089873.8731
for step 16765 | loss 3.216563 | norm 0.2990 | time 462.7521 ms | tok/sec 1132978.1014
for step 16766 | loss 3.285959 | norm 0.3089 | time 464.1521 ms | tok/sec 1129560.7681
for step 16767 | loss 3.219155 | norm 0.3175 | time 464.0348 ms | tok/sec 1129846.3063
for step 16768 | loss 3.321943 | norm 0.3226 | time 464.9889 ms | tok/sec 1127527.8767
for step 16769 | loss 3.204411 | norm 0.3072 | time 463.5074 ms | tok/sec 1131131.8532
for step 16770 | loss 3.291958 | norm 0.3295 | time 464.5417 ms | tok/sec 1128613.4909
for step 16771 | loss 3.196194 | norm 0.3146 | time 464.9296 ms | tok/sec 1127671.8492
for step 16772 | loss 3.216741 | norm 0.3275 | time 465.5545 ms | tok/sec 1126158.2205
for step 16773 | loss 3.233625 | norm 0.3034 | time 465.0996 ms | tok/sec 1127259.6886
for step 16774 | loss 3.179883 | norm 0.3080 | time 465.8806 ms | tok/sec 1125369.8126
for step 16775 | loss 3.192107 | norm 0.3071 | time 465.5011 ms | tok/sec 1126287.4218
for step 16776 | loss 3.229046 | norm 0.2894 | time 466.0544 ms | tok/sec 1124950.1250
for step 16777 | loss 3.122598 | norm 0.2941 | time 465.4729 ms | tok/sec 1126355.4951
for step 16778 | loss 3.231708 | norm 0.2968 | time 465.1580 ms | tok/sec 1127118.1322
for step 16779 | loss 3.259799 | norm 0.3961 | time 465.7960 ms | tok/sec 1125574.3007
for step 16780 | loss 3.193041 | norm 0.3088 | time 465.7502 ms | tok/sec 1125684.9280
for step 16781 | loss 3.219065 | norm 0.3247 | time 465.5097 ms | tok/sec 1126266.6553
for step 16782 | loss 3.183714 | norm 0.3041 | time 465.9600 ms | tok/sec 1125178.0646
for step 16783 | loss 3.212686 | norm 0.2848 | time 465.5061 ms | tok/sec 1126275.3079
for step 16784 | loss 3.194229 | norm 0.3294 | time 464.9651 ms | tok/sec 1127585.6926
for step 16785 | loss 3.129843 | norm 0.2830 | time 464.9942 ms | tok/sec 1127515.1580
for step 16786 | loss 3.181411 | norm 0.2841 | time 465.7865 ms | tok/sec 1125597.3463
for step 16787 | loss 3.113939 | norm 0.3485 | time 465.9514 ms | tok/sec 1125198.7910
for step 16788 | loss 3.079586 | norm 0.2850 | time 465.8394 ms | tok/sec 1125469.4553
for step 16789 | loss 3.147851 | norm 0.3436 | time 466.7208 ms | tok/sec 1123343.9309
for step 16790 | loss 3.176286 | norm 0.3286 | time 465.8651 ms | tok/sec 1125407.2486
for step 16791 | loss 3.155612 | norm 0.3047 | time 466.7521 ms | tok/sec 1123268.7621
for step 16792 | loss 3.163329 | norm 0.3249 | time 465.5375 ms | tok/sec 1126199.1695
for step 16793 | loss 3.142993 | norm 0.3075 | time 465.0006 ms | tok/sec 1127499.5491
for step 16794 | loss 3.223341 | norm 0.4124 | time 466.0566 ms | tok/sec 1124944.9456
for step 16795 | loss 3.166420 | norm 0.3395 | time 466.2936 ms | tok/sec 1124373.2059
for step 16796 | loss 3.178120 | norm 0.3055 | time 465.6439 ms | tok/sec 1125941.9901
for step 16797 | loss 3.206415 | norm 0.3198 | time 465.7950 ms | tok/sec 1125576.6052
for step 16798 | loss 3.249711 | norm 0.3000 | time 465.9393 ms | tok/sec 1125228.1546
for step 16799 | loss 3.248554 | norm 0.3021 | time 465.7857 ms | tok/sec 1125599.0747
for step 16800 | loss 3.238886 | norm 0.3158 | time 465.6253 ms | tok/sec 1125986.9591
for step 16801 | loss 3.163913 | norm 0.3131 | time 465.4803 ms | tok/sec 1126337.6106
for step 16802 | loss 3.274484 | norm 0.3108 | time 465.8451 ms | tok/sec 1125455.6310
for step 16803 | loss 3.250447 | norm 0.3008 | time 466.3193 ms | tok/sec 1124311.1203
for step 16804 | loss 3.258310 | norm 0.2836 | time 465.6374 ms | tok/sec 1125957.5559
for step 16805 | loss 3.183758 | norm 0.3051 | time 465.9626 ms | tok/sec 1125171.7317
for step 16806 | loss 3.156997 | norm 0.3063 | time 465.9796 ms | tok/sec 1125130.8574
for step 16807 | loss 3.320316 | norm 0.3213 | time 465.1937 ms | tok/sec 1127031.4825
for step 16808 | loss 3.195169 | norm 0.2842 | time 465.6374 ms | tok/sec 1125957.5559
for step 16809 | loss 3.223794 | norm 0.2802 | time 465.3916 ms | tok/sec 1126552.2617
for step 16810 | loss 3.189381 | norm 0.2805 | time 465.5247 ms | tok/sec 1126230.3158
for step 16811 | loss 3.172076 | norm 0.2756 | time 465.1361 ms | tok/sec 1127171.2839
for step 16812 | loss 3.195733 | norm 0.3103 | time 465.3258 ms | tok/sec 1126711.5718
for step 16813 | loss 3.214105 | norm 0.3018 | time 465.9882 ms | tok/sec 1125110.1335
for step 16814 | loss 3.172126 | norm 0.2783 | time 465.1217 ms | tok/sec 1127205.9508
for step 16815 | loss 3.185553 | norm 0.2879 | time 465.1048 ms | tok/sec 1127246.9760
for step 16816 | loss 3.154911 | norm 0.2765 | time 465.3983 ms | tok/sec 1126536.1023
for step 16817 | loss 3.159693 | norm 0.2844 | time 465.5316 ms | tok/sec 1126213.5889
for step 16818 | loss 3.154097 | norm 0.2815 | time 465.2970 ms | tok/sec 1126781.4285
for step 16819 | loss 3.189821 | norm 0.2806 | time 466.3324 ms | tok/sec 1124279.5053
for step 16820 | loss 3.150988 | norm 0.2772 | time 465.7204 ms | tok/sec 1125756.9627
for step 16821 | loss 3.121822 | norm 0.2701 | time 465.6906 ms | tok/sec 1125829.0066
for step 16822 | loss 3.143762 | norm 0.2719 | time 464.4535 ms | tok/sec 1128827.8515
for step 16823 | loss 3.143432 | norm 0.2871 | time 464.3042 ms | tok/sec 1129190.7119
for step 16824 | loss 3.079595 | norm 0.2767 | time 465.2550 ms | tok/sec 1126883.0537
for step 16825 | loss 3.164268 | norm 0.2664 | time 464.7732 ms | tok/sec 1128051.3263
for step 16826 | loss 3.146122 | norm 0.2886 | time 464.9119 ms | tok/sec 1127714.6433
for step 16827 | loss 3.140626 | norm 0.2679 | time 464.3211 ms | tok/sec 1129149.5450
for step 16828 | loss 3.137581 | norm 0.2755 | time 465.3430 ms | tok/sec 1126670.0083
for step 16829 | loss 3.203674 | norm 0.2761 | time 464.5865 ms | tok/sec 1128504.6038
for step 16830 | loss 3.150964 | norm 0.2685 | time 465.6303 ms | tok/sec 1125974.8517
for step 16831 | loss 3.152332 | norm 0.2857 | time 465.5683 ms | tok/sec 1126124.7714
for step 16832 | loss 3.239632 | norm 0.2872 | time 465.7447 ms | tok/sec 1125698.1817
for step 16833 | loss 3.249945 | norm 0.2785 | time 464.4232 ms | tok/sec 1128901.4481
for step 16834 | loss 3.237050 | norm 0.3002 | time 465.3275 ms | tok/sec 1126707.5308
for step 16835 | loss 3.241723 | norm 0.3022 | time 467.0975 ms | tok/sec 1122437.9848
for step 16836 | loss 3.282716 | norm 0.3122 | time 465.5099 ms | tok/sec 1126266.0785
for step 16837 | loss 3.228830 | norm 0.2883 | time 465.6808 ms | tok/sec 1125852.6390
for step 16838 | loss 3.222097 | norm 0.3069 | time 466.4216 ms | tok/sec 1124064.5702
for step 16839 | loss 3.230666 | norm 0.2864 | time 465.3151 ms | tok/sec 1126737.5506
for step 16840 | loss 3.265420 | norm 0.3179 | time 464.7343 ms | tok/sec 1128145.6568
for step 16841 | loss 3.199622 | norm 0.2896 | time 466.0568 ms | tok/sec 1124944.3701
for step 16842 | loss 3.231677 | norm 0.2850 | time 465.1308 ms | tok/sec 1127183.9948
for step 16843 | loss 3.193997 | norm 0.2993 | time 465.9350 ms | tok/sec 1125238.5186
for step 16844 | loss 3.220307 | norm 0.2978 | time 464.4065 ms | tok/sec 1128942.0172
for step 16845 | loss 3.202367 | norm 0.2913 | time 465.9035 ms | tok/sec 1125314.5272
for step 16846 | loss 3.224473 | norm 0.2931 | time 465.4858 ms | tok/sec 1126324.3419
for step 16847 | loss 3.208008 | norm 0.3028 | time 465.7021 ms | tok/sec 1125801.3407
for step 16848 | loss 3.208853 | norm 0.3053 | time 465.1496 ms | tok/sec 1127138.3524
for step 16849 | loss 3.204619 | norm 0.2943 | time 465.5209 ms | tok/sec 1126239.5446
for step 16850 | loss 3.191688 | norm 0.2821 | time 466.8996 ms | tok/sec 1122913.7110
for step 16851 | loss 3.223739 | norm 0.2932 | time 466.7125 ms | tok/sec 1123364.0159
for step 16852 | loss 3.152658 | norm 0.2883 | time 465.5077 ms | tok/sec 1126271.2700
for step 16853 | loss 3.179164 | norm 0.2880 | time 465.5488 ms | tok/sec 1126172.0621
for step 16854 | loss 3.173442 | norm 0.2892 | time 466.4454 ms | tok/sec 1124007.1148
for step 16855 | loss 3.153184 | norm 0.2725 | time 466.0141 ms | tok/sec 1125047.3909
for step 16856 | loss 3.129899 | norm 0.2892 | time 464.4506 ms | tok/sec 1128834.8051
for step 16857 | loss 3.142845 | norm 0.2735 | time 466.4049 ms | tok/sec 1124104.7924
for step 16858 | loss 3.106776 | norm 0.2675 | time 466.0685 ms | tok/sec 1124916.1721
for step 16859 | loss 3.139769 | norm 0.2898 | time 465.0345 ms | tok/sec 1127417.4649
for step 16860 | loss 3.160925 | norm 0.2888 | time 465.8744 ms | tok/sec 1125384.7867
for step 16861 | loss 3.146878 | norm 0.2937 | time 464.8471 ms | tok/sec 1127871.9684
for step 16862 | loss 3.179784 | norm 0.3067 | time 465.6074 ms | tok/sec 1126030.2020
for step 16863 | loss 3.154419 | norm 0.3089 | time 465.1377 ms | tok/sec 1127167.2396
for step 16864 | loss 3.126859 | norm 0.3043 | time 466.4237 ms | tok/sec 1124059.3990
for step 16865 | loss 3.113995 | norm 0.3107 | time 465.7981 ms | tok/sec 1125569.1156
for step 16866 | loss 3.247315 | norm 0.3114 | time 466.2161 ms | tok/sec 1124560.0790
for step 16867 | loss 3.195530 | norm 0.3497 | time 465.8282 ms | tok/sec 1125496.5289
for step 16868 | loss 3.270702 | norm 0.2991 | time 466.1171 ms | tok/sec 1124798.7918
for step 16869 | loss 3.238658 | norm 0.3142 | time 465.6725 ms | tok/sec 1125872.8138
for step 16870 | loss 3.243901 | norm 0.3052 | time 465.2929 ms | tok/sec 1126791.2438
for step 16871 | loss 3.237492 | norm 0.3365 | time 464.6361 ms | tok/sec 1128384.1574
for step 16872 | loss 3.187041 | norm 0.3118 | time 464.9358 ms | tok/sec 1127656.8142
for step 16873 | loss 3.221277 | norm 0.3051 | time 465.4896 ms | tok/sec 1126315.1116
for step 16874 | loss 3.143904 | norm 0.2818 | time 464.8101 ms | tok/sec 1127961.6402
for step 16875 | loss 3.228068 | norm 0.3001 | time 464.4029 ms | tok/sec 1128950.7110
for step 16876 | loss 3.219121 | norm 0.3707 | time 465.0466 ms | tok/sec 1127387.9868
for step 16877 | loss 3.204938 | norm 0.3187 | time 464.5410 ms | tok/sec 1128615.2287
for step 16878 | loss 3.202863 | norm 0.3668 | time 464.8647 ms | tok/sec 1127829.1624
for step 16879 | loss 3.177416 | norm 0.3131 | time 465.3060 ms | tok/sec 1126759.4891
for step 16880 | loss 3.174468 | norm 0.3421 | time 464.7002 ms | tok/sec 1128228.4259
for step 16881 | loss 3.186934 | norm 0.3063 | time 465.3656 ms | tok/sec 1126615.1723
for step 16882 | loss 3.253114 | norm 0.3337 | time 465.2455 ms | tok/sec 1126906.1529
for step 16883 | loss 3.196380 | norm 0.3694 | time 465.5163 ms | tok/sec 1126250.5041
for step 16884 | loss 3.239167 | norm 0.3184 | time 465.0726 ms | tok/sec 1127324.9899
for step 16885 | loss 3.193900 | norm 0.3689 | time 465.6501 ms | tok/sec 1125927.0011
for step 16886 | loss 3.202135 | norm 0.2956 | time 465.3180 ms | tok/sec 1126730.6228
for step 16887 | loss 3.199962 | norm 0.3364 | time 465.7571 ms | tok/sec 1125668.2173
for step 16888 | loss 3.173153 | norm 0.3331 | time 465.5788 ms | tok/sec 1126099.3976
for step 16889 | loss 3.110040 | norm 0.3050 | time 466.2740 ms | tok/sec 1124420.3496
for step 16890 | loss 3.213859 | norm 0.3362 | time 465.7748 ms | tok/sec 1125625.5784
for step 16891 | loss 3.085377 | norm 0.3035 | time 466.2359 ms | tok/sec 1124512.3486
for step 16892 | loss 3.135709 | norm 0.3326 | time 464.9467 ms | tok/sec 1127630.2148
for step 16893 | loss 3.141891 | norm 0.3196 | time 466.1050 ms | tok/sec 1124828.1346
for step 16894 | loss 3.151381 | norm 0.3212 | time 466.6090 ms | tok/sec 1123613.1294
for step 16895 | loss 3.066759 | norm 0.3038 | time 465.7714 ms | tok/sec 1125633.6450
for step 16896 | loss 3.111230 | norm 0.3209 | time 465.6656 ms | tok/sec 1125889.5306
for step 16897 | loss 3.175086 | norm 0.3683 | time 465.7338 ms | tok/sec 1125724.6900
for step 16898 | loss 3.167490 | norm 0.2911 | time 465.6856 ms | tok/sec 1125841.1109
for step 16899 | loss 3.161118 | norm 0.3040 | time 465.7609 ms | tok/sec 1125658.9978
for step 16900 | loss 3.129940 | norm 0.3394 | time 466.0583 ms | tok/sec 1124940.9172
for step 16901 | loss 3.125141 | norm 0.3639 | time 465.6816 ms | tok/sec 1125850.9098
for step 16902 | loss 3.325282 | norm 0.3794 | time 464.9010 ms | tok/sec 1127741.2467
for step 16903 | loss 3.260523 | norm 0.3489 | time 465.5488 ms | tok/sec 1126172.0621
for step 16904 | loss 3.256421 | norm 0.2972 | time 465.2650 ms | tok/sec 1126858.8006
for step 16905 | loss 3.180290 | norm 0.3167 | time 465.1091 ms | tok/sec 1127236.5750
for step 16906 | loss 3.272767 | norm 0.3989 | time 466.5668 ms | tok/sec 1123714.7581
for step 16907 | loss 3.229518 | norm 0.3559 | time 466.5647 ms | tok/sec 1123719.9261
for step 16908 | loss 3.208076 | norm 0.3521 | time 465.2972 ms | tok/sec 1126780.8512
for step 16909 | loss 3.258801 | norm 0.3264 | time 465.0922 ms | tok/sec 1127277.6024
for step 16910 | loss 3.218285 | norm 0.3589 | time 465.1027 ms | tok/sec 1127252.1766
for step 16911 | loss 3.295409 | norm 0.3940 | time 466.4152 ms | tok/sec 1124080.0841
for step 16912 | loss 3.206328 | norm 0.3258 | time 466.0194 ms | tok/sec 1125034.7281
for step 16913 | loss 3.245519 | norm 0.3669 | time 466.2271 ms | tok/sec 1124533.6255
for step 16914 | loss 3.291684 | norm 0.3905 | time 466.0203 ms | tok/sec 1125032.4258
for step 16915 | loss 3.183535 | norm 0.2974 | time 465.2584 ms | tok/sec 1126874.9692
for step 16916 | loss 3.202798 | norm 0.3433 | time 466.1996 ms | tok/sec 1124599.7616
for step 16917 | loss 3.164361 | norm 0.3102 | time 464.9131 ms | tok/sec 1127711.7517
for step 16918 | loss 3.174467 | norm 0.3248 | time 466.1963 ms | tok/sec 1124607.8134
for step 16919 | loss 3.206244 | norm 0.3106 | time 465.7793 ms | tok/sec 1125614.6311
for step 16920 | loss 3.183203 | norm 0.2858 | time 465.4388 ms | tok/sec 1126438.0018
for step 16921 | loss 3.229971 | norm 0.3142 | time 465.8592 ms | tok/sec 1125421.6477
for step 16922 | loss 3.163876 | norm 0.3183 | time 464.8452 ms | tok/sec 1127876.5963
for step 16923 | loss 3.225038 | norm 0.3113 | time 465.3897 ms | tok/sec 1126556.8788
for step 16924 | loss 3.210573 | norm 0.2961 | time 465.5597 ms | tok/sec 1126145.5327
for step 16925 | loss 3.159216 | norm 0.3065 | time 466.4745 ms | tok/sec 1123937.0273
for step 16926 | loss 3.108174 | norm 0.3084 | time 465.9336 ms | tok/sec 1125241.9734
for step 16927 | loss 3.227298 | norm 0.3663 | time 465.5182 ms | tok/sec 1126245.8896
for step 16928 | loss 3.122686 | norm 0.3505 | time 465.7652 ms | tok/sec 1125648.6260
for step 16929 | loss 3.155998 | norm 0.3197 | time 465.8561 ms | tok/sec 1125429.1353
for step 16930 | loss 3.112912 | norm 0.2960 | time 466.2704 ms | tok/sec 1124428.9738
for step 16931 | loss 3.184715 | norm 0.3541 | time 465.4472 ms | tok/sec 1126417.8067
for step 16932 | loss 3.115084 | norm 0.3307 | time 465.7736 ms | tok/sec 1125628.4593
for step 16933 | loss 3.113042 | norm 0.3451 | time 465.4083 ms | tok/sec 1126511.8641
for step 16934 | loss 3.113321 | norm 0.3211 | time 466.1121 ms | tok/sec 1124810.8740
for step 16935 | loss 3.156704 | norm 0.2885 | time 466.0728 ms | tok/sec 1124905.8141
for step 16936 | loss 3.189445 | norm 0.2987 | time 465.6959 ms | tok/sec 1125816.3262
for step 16937 | loss 3.153207 | norm 0.2950 | time 465.5616 ms | tok/sec 1126140.9190
for step 16938 | loss 3.288909 | norm 0.3229 | time 465.8194 ms | tok/sec 1125517.8431
for step 16939 | loss 3.249912 | norm 0.3148 | time 466.7056 ms | tok/sec 1123380.6583
for step 16940 | loss 3.192745 | norm 0.3520 | time 464.9162 ms | tok/sec 1127704.2336
for step 16941 | loss 3.194762 | norm 0.3063 | time 466.0208 ms | tok/sec 1125031.2747
for step 16942 | loss 3.196470 | norm 0.3357 | time 465.4427 ms | tok/sec 1126428.7697
for step 16943 | loss 3.293520 | norm 0.3263 | time 465.8990 ms | tok/sec 1125325.4687
for step 16944 | loss 3.170323 | norm 0.3268 | time 464.6690 ms | tok/sec 1128304.2600
for step 16945 | loss 3.250306 | norm 0.3321 | time 465.6198 ms | tok/sec 1126000.2200
for step 16946 | loss 3.306470 | norm 0.3120 | time 466.0871 ms | tok/sec 1124871.2885
for step 16947 | loss 3.278966 | norm 0.3119 | time 464.6511 ms | tok/sec 1128347.6811
for step 16948 | loss 3.206251 | norm 0.3213 | time 465.0400 ms | tok/sec 1127404.1707
for step 16949 | loss 3.222086 | norm 0.2721 | time 464.9963 ms | tok/sec 1127509.9550
for step 16950 | loss 3.200791 | norm 0.2959 | time 466.3360 ms | tok/sec 1124270.8833
for step 16951 | loss 3.181206 | norm 0.3079 | time 465.5173 ms | tok/sec 1126248.1968
for step 16952 | loss 3.195155 | norm 0.2965 | time 465.2567 ms | tok/sec 1126879.0114
for step 16953 | loss 3.200788 | norm 0.2720 | time 466.1899 ms | tok/sec 1124623.3424
Will loading at 0 from edu_fineweb10B/edufineweb_train_000090.npy
for step 16954 | loss 3.183128 | norm 0.2810 | time 2597.8425 ms | tok/sec 201816.7033
for step 16955 | loss 3.217714 | norm 0.3057 | time 463.2246 ms | tok/sec 1131822.3256
for step 16956 | loss 3.186697 | norm 0.2852 | time 464.3807 ms | tok/sec 1129004.6153
for step 16957 | loss 3.132071 | norm 0.3138 | time 466.0206 ms | tok/sec 1125031.8503
for step 16958 | loss 3.223801 | norm 0.2903 | time 465.5745 ms | tok/sec 1126109.7777
for step 16959 | loss 3.218649 | norm 0.2980 | time 465.5402 ms | tok/sec 1126192.8251
for step 16960 | loss 3.168136 | norm 0.2782 | time 465.0199 ms | tok/sec 1127452.7249
for step 16961 | loss 3.193855 | norm 0.2917 | time 464.4327 ms | tok/sec 1128878.2670
for step 16962 | loss 3.102391 | norm 0.2778 | time 464.1356 ms | tok/sec 1129600.8044
for step 16963 | loss 3.127588 | norm 0.3038 | time 464.8497 ms | tok/sec 1127865.6052
for step 16964 | loss 3.140046 | norm 0.2962 | time 465.4260 ms | tok/sec 1126469.1613
for step 16965 | loss 3.139698 | norm 0.2761 | time 465.6031 ms | tok/sec 1126040.5808
for step 16966 | loss 3.149215 | norm 0.2855 | time 466.4569 ms | tok/sec 1123979.5383
for step 16967 | loss 3.146171 | norm 0.3050 | time 465.2755 ms | tok/sec 1126833.3937
for step 16968 | loss 3.121343 | norm 0.2774 | time 464.7768 ms | tok/sec 1128042.6464
for step 16969 | loss 3.192641 | norm 0.3186 | time 465.7767 ms | tok/sec 1125620.9690
for step 16970 | loss 3.139227 | norm 0.2986 | time 466.0988 ms | tok/sec 1124843.0943
for step 16971 | loss 3.133444 | norm 0.3084 | time 464.5000 ms | tok/sec 1128714.8675
for step 16972 | loss 3.185667 | norm 0.2939 | time 465.7366 ms | tok/sec 1125717.7747
for step 16973 | loss 3.259712 | norm 0.3222 | time 466.4230 ms | tok/sec 1124061.1227
for step 16974 | loss 3.257884 | norm 0.3088 | time 465.5995 ms | tok/sec 1126049.2300
for step 16975 | loss 3.228504 | norm 0.3464 | time 465.2641 ms | tok/sec 1126861.1104
for step 16976 | loss 3.298266 | norm 0.3381 | time 466.3944 ms | tok/sec 1124130.0765
for step 16977 | loss 3.206990 | norm 0.3693 | time 465.6389 ms | tok/sec 1125954.0968
for step 16978 | loss 3.211927 | norm 0.3207 | time 465.6522 ms | tok/sec 1125921.8128
for step 16979 | loss 3.215877 | norm 0.3018 | time 466.0192 ms | tok/sec 1125035.3037
for step 16980 | loss 3.295883 | norm 0.3293 | time 466.3501 ms | tok/sec 1124236.9715
for step 16981 | loss 3.231505 | norm 0.3277 | time 466.0449 ms | tok/sec 1124973.1450
for step 16982 | loss 3.200748 | norm 0.3097 | time 466.3365 ms | tok/sec 1124269.7337
for step 16983 | loss 3.232196 | norm 0.3143 | time 465.7559 ms | tok/sec 1125671.0984
for step 16984 | loss 3.123802 | norm 0.2907 | time 465.8329 ms | tok/sec 1125485.0080
for step 16985 | loss 3.182233 | norm 0.3040 | time 463.9192 ms | tok/sec 1130127.9233
for step 16986 | loss 3.222536 | norm 0.3100 | time 464.2696 ms | tok/sec 1129274.7943
for step 16987 | loss 3.208039 | norm 0.3049 | time 464.3993 ms | tok/sec 1128959.4049
for step 16988 | loss 3.194749 | norm 0.2999 | time 465.1413 ms | tok/sec 1127158.5733
for step 16989 | loss 3.204060 | norm 0.2899 | time 466.7556 ms | tok/sec 1123260.1556
for step 16990 | loss 3.210420 | norm 0.2880 | time 465.3468 ms | tok/sec 1126660.7724
for step 16991 | loss 3.202851 | norm 0.3029 | time 465.0669 ms | tok/sec 1127338.8602
for step 16992 | loss 3.176977 | norm 0.2840 | time 465.6842 ms | tok/sec 1125844.5693
for step 16993 | loss 3.263842 | norm 0.2788 | time 465.7257 ms | tok/sec 1125744.2840
for step 16994 | loss 3.193481 | norm 0.2957 | time 465.8124 ms | tok/sec 1125534.5493
for step 16995 | loss 3.181909 | norm 0.2857 | time 466.2404 ms | tok/sec 1124501.4229
for step 16996 | loss 3.137065 | norm 0.2910 | time 464.7121 ms | tok/sec 1128199.4843
for step 16997 | loss 3.145668 | norm 0.2927 | time 464.9098 ms | tok/sec 1127719.8482
for step 16998 | loss 3.144004 | norm 0.2756 | time 465.4696 ms | tok/sec 1126363.5722
for step 16999 | loss 3.144659 | norm 0.2783 | time 465.1756 ms | tok/sec 1127075.3834
validation loss 3.2271
HellaSwag accuracy: 2829/10042=0.2817
> Hello, I'm a language model, and am hoping to add a few pages of useful material to my site. There are several sections for each language, and
> Hello, I'm a language model, which is very special. I'm really enjoying it. For example, I love programming Languages, so it's not a
> Hello, I'm a language model, I teach it for a reason. I'm a language model who's teaching other languages. And the only way we're
> Hello, I'm a language model, and I'm just going to get back started.
At first, I only wanted to run it, but as I
> Hello, I'm a language model, and I can't help you if you wanna take a new job interview online and get to know more of them. But
> Hello, I'm a language model, so why not start off with some free resources for building good language models for my kids to start learning this language?

> Hello, I'm a language model, so I did this... you ask?
Hello, I hope so....
This is the first time I have created
> Hello, I'm a language model, and here's my first lesson on building the app.
Why use App?
When building apps, you'll typically
> Hello, I'm a language model, right?
You see, how much more language has a role in your daily life? To be able to understand one
> Hello, I'm a language model, not a real language.
In my training, my students are going to be using their native language to help them learn
> Hello, I'm a language model, I like to write sentences and paragraphs so I'll start writing the second paragraph for the first time. Since writing to a
> Hello, I'm a language model, and I'll leave you with all the knowledge and experience I've gained in language modeling and a lot of what I've
> Hello, I'm a language model, so I will be happy to help you make it easier for you. It's got it's how the English language works
> Hello, I'm a language model, especially one that relates to the human brain. That is...
2 posts. 1. The Human Brain. .

> Hello, I'm a language model, but is what every computer knows? How to choose the right type to use? For this section, lets look up a
> Hello, I'm a language model, and it fits so well to the Coding. For instance, here's an example of how each language model's code
>>  Hello, I'm a language model, and I use it every time I talk about it, so I'm only talking about my words and my sentences. IHello, I'm a language model, and I'm not the type I'm planning on learning it when I walk inside. You've actually tried it on several

> > > Hello, I'm a language model, I think it's hard to change my script. There's a couple different things to do, and I'll put the
> Hello, I'm a language model, and this isn't a particularly bad thing, as it uses a very formal language.
I'm not a beginner to
Hello, I'm a language model,
meaning it's a program to solve puzzles.
Because I'm a language model,
I was wondering what is
> Hello, I'm a language model, and we are only interested in how words and sentences are related, but we are only interested in how words in sentences relate
> > Hello, I'm a language model, one of the few of these things. I could easily write about English as well: "The meaning of any sentence is
Hello, I'm a language model, and you are just creating a story. To build it, I'm using Java on my phone -- and you should already
> Hello, I'm a language model, and I've taken the time to share it with you. I love modeling, so I'm going to show you the
> > Hello, I'm a language model, I'm fluent in my native language. After I passed the course, I was able to understand and interpret a lot of
> Hello, I'm a language model, so I'd love to see someone.
You're thinking, wow, there is always a tool for you to get
Hello, I'm a language model, and when I see a graph and see the number of points that make it up, I feel like no matter what the
Hello, I'm a language model, so it's not very well understood either. You can be any of three languages: English, English, and Spanish.
> > Hello, I'm a language model, so I've got a few problems where I need to define one more part for this example, one more part for example
Hello, I'm a language model, we have two objects, "I" and "We", and you can click to the "Find the Object". Then
> Hello, I'm a language model, I'll cover the basic syntax and semantics, and my next project will be making an interactive virtual map. And, this
for step 17000 | loss 3.152125 | norm 0.2806 | time 12002.0161 ms | tok/sec 43683.3276
for step 17001 | loss 3.131032 | norm 0.2636 | time 461.9834 ms | tok/sec 1134863.1834
for step 17002 | loss 3.119847 | norm 0.2654 | time 462.9469 ms | tok/sec 1132501.3934
for step 17003 | loss 3.154627 | norm 0.2872 | time 462.3268 ms | tok/sec 1134020.4367
for step 17004 | loss 3.125929 | norm 0.2754 | time 463.6736 ms | tok/sec 1130726.4625
for step 17005 | loss 3.127198 | norm 0.2748 | time 463.0196 ms | tok/sec 1132323.5330
for step 17006 | loss 3.192197 | norm 0.2946 | time 464.2406 ms | tok/sec 1129345.5491
for step 17007 | loss 3.192179 | norm 0.3225 | time 464.0200 ms | tok/sec 1129882.2990
for step 17008 | loss 3.222358 | norm 0.2896 | time 463.5615 ms | tok/sec 1130999.7930
for step 17009 | loss 3.209811 | norm 0.3250 | time 465.3745 ms | tok/sec 1126593.8165
for step 17010 | loss 3.201983 | norm 0.3104 | time 463.4144 ms | tok/sec 1131358.8127
for step 17011 | loss 3.218031 | norm 0.2988 | time 463.9888 ms | tok/sec 1129958.3557
for step 17012 | loss 3.247808 | norm 0.3011 | time 464.6463 ms | tok/sec 1128359.2606
for step 17013 | loss 3.216720 | norm 0.3021 | time 464.1833 ms | tok/sec 1129484.7650
for step 17014 | loss 3.217585 | norm 0.3224 | time 464.6382 ms | tok/sec 1128378.9464
for step 17015 | loss 3.205279 | norm 0.4130 | time 464.9048 ms | tok/sec 1127731.9932
for step 17016 | loss 3.177555 | norm 0.3119 | time 464.5143 ms | tok/sec 1128680.1078
for step 17017 | loss 3.196456 | norm 0.3560 | time 464.5402 ms | tok/sec 1128616.9664
for step 17018 | loss 3.160381 | norm 0.3130 | time 465.2648 ms | tok/sec 1126859.3780
for step 17019 | loss 3.132414 | norm 0.3482 | time 465.5664 ms | tok/sec 1126129.3850
for step 17020 | loss 3.133381 | norm 0.2999 | time 465.5895 ms | tok/sec 1126073.4483
for step 17021 | loss 3.234283 | norm 0.3071 | time 465.2283 ms | tok/sec 1126947.7338
for step 17022 | loss 3.251467 | norm 0.3577 | time 464.2863 ms | tok/sec 1129234.2012
for step 17023 | loss 3.143282 | norm 0.2844 | time 465.0991 ms | tok/sec 1127260.8444
for step 17024 | loss 3.193521 | norm 0.3269 | time 465.2584 ms | tok/sec 1126874.9692
for step 17025 | loss 3.224771 | norm 0.3046 | time 466.1334 ms | tok/sec 1124759.6705
for step 17026 | loss 3.201179 | norm 0.2980 | time 465.0605 ms | tok/sec 1127354.4647
for step 17027 | loss 3.176055 | norm 0.3048 | time 465.8337 ms | tok/sec 1125483.2799
for step 17028 | loss 3.175786 | norm 0.3000 | time 464.4575 ms | tok/sec 1128818.0007
for step 17029 | loss 3.253552 | norm 0.2953 | time 464.9534 ms | tok/sec 1127614.0245
for step 17030 | loss 3.167305 | norm 0.3082 | time 465.2455 ms | tok/sec 1126906.1529
for step 17031 | loss 3.183854 | norm 0.3048 | time 466.1090 ms | tok/sec 1124818.3535
for step 17032 | loss 3.136558 | norm 0.2916 | time 466.2039 ms | tok/sec 1124589.4093
for step 17033 | loss 3.173958 | norm 0.2939 | time 465.3068 ms | tok/sec 1126757.7571
for step 17034 | loss 3.132273 | norm 0.2953 | time 465.7648 ms | tok/sec 1125649.7785
for step 17035 | loss 3.167775 | norm 0.2923 | time 466.0113 ms | tok/sec 1125054.2980
for step 17036 | loss 3.094361 | norm 0.2847 | time 466.2542 ms | tok/sec 1124468.0722
for step 17037 | loss 3.112541 | norm 0.3216 | time 465.3966 ms | tok/sec 1126540.1421
for step 17038 | loss 3.103039 | norm 0.2822 | time 465.3354 ms | tok/sec 1126688.4806
for step 17039 | loss 3.162714 | norm 0.2887 | time 465.2412 ms | tok/sec 1126916.5478
for step 17040 | loss 3.125981 | norm 0.3169 | time 464.4766 ms | tok/sec 1128771.6464
for step 17041 | loss 3.154372 | norm 0.2889 | time 465.4121 ms | tok/sec 1126502.6308
for step 17042 | loss 3.149335 | norm 0.2951 | time 466.0721 ms | tok/sec 1124907.5404
for step 17043 | loss 3.287460 | norm 0.3042 | time 465.6541 ms | tok/sec 1125917.2009
for step 17044 | loss 3.152462 | norm 0.3100 | time 465.5616 ms | tok/sec 1126140.9190
for step 17045 | loss 3.279668 | norm 0.2842 | time 465.8058 ms | tok/sec 1125550.6800
for step 17046 | loss 3.216276 | norm 0.3022 | time 464.7291 ms | tok/sec 1128158.3897
for step 17047 | loss 3.217320 | norm 0.3038 | time 465.9524 ms | tok/sec 1125196.4880
for step 17048 | loss 3.240975 | norm 0.2985 | time 465.2989 ms | tok/sec 1126776.8096
for step 17049 | loss 3.298172 | norm 0.3063 | time 465.9901 ms | tok/sec 1125105.5283
for step 17050 | loss 3.233452 | norm 0.3025 | time 465.3158 ms | tok/sec 1126735.8186
for step 17051 | loss 3.210782 | norm 0.3145 | time 464.8085 ms | tok/sec 1127965.6903
for step 17052 | loss 3.246508 | norm 0.3084 | time 466.0935 ms | tok/sec 1124855.7527
for step 17053 | loss 3.127955 | norm 0.3127 | time 465.8668 ms | tok/sec 1125403.2169
for step 17054 | loss 3.149168 | norm 0.2891 | time 465.3049 ms | tok/sec 1126762.3758
for step 17055 | loss 3.153924 | norm 0.3113 | time 464.9651 ms | tok/sec 1127585.6926
for step 17056 | loss 3.309515 | norm 0.3377 | time 466.5277 ms | tok/sec 1123808.9389
for step 17057 | loss 3.213009 | norm 0.3099 | time 465.0464 ms | tok/sec 1127388.5648
for step 17058 | loss 3.192740 | norm 0.3432 | time 466.2912 ms | tok/sec 1124378.9549
for step 17059 | loss 3.184241 | norm 0.2858 | time 465.8747 ms | tok/sec 1125384.2108
for step 17060 | loss 3.162100 | norm 0.2939 | time 465.8720 ms | tok/sec 1125390.5461
for step 17061 | loss 3.152459 | norm 0.3230 | time 465.0726 ms | tok/sec 1127324.9899
for step 17062 | loss 3.178914 | norm 0.2946 | time 465.0507 ms | tok/sec 1127378.1612
for step 17063 | loss 3.190813 | norm 0.2986 | time 466.2216 ms | tok/sec 1124546.8521
for step 17064 | loss 3.216938 | norm 0.2923 | time 466.1171 ms | tok/sec 1124798.7918
for step 17065 | loss 3.192954 | norm 0.3193 | time 466.1589 ms | tok/sec 1124698.1174
for step 17066 | loss 3.160114 | norm 0.2981 | time 465.7204 ms | tok/sec 1125756.9627
for step 17067 | loss 3.131711 | norm 0.3154 | time 465.7035 ms | tok/sec 1125797.8825
for step 17068 | loss 3.156652 | norm 0.3072 | time 465.0319 ms | tok/sec 1127423.8231
for step 17069 | loss 3.167470 | norm 0.3276 | time 464.7167 ms | tok/sec 1128188.4868
for step 17070 | loss 3.181028 | norm 0.2927 | time 465.5695 ms | tok/sec 1126121.8880
for step 17071 | loss 3.137638 | norm 0.2856 | time 465.5011 ms | tok/sec 1126287.4218
for step 17072 | loss 3.154417 | norm 0.3164 | time 465.3325 ms | tok/sec 1126695.4079
for step 17073 | loss 3.112427 | norm 0.3107 | time 465.2216 ms | tok/sec 1126963.9050
for step 17074 | loss 3.141010 | norm 0.2891 | time 465.5776 ms | tok/sec 1126102.2809
for step 17075 | loss 3.114235 | norm 0.3025 | time 465.7288 ms | tok/sec 1125736.7921
for step 17076 | loss 3.135615 | norm 0.2963 | time 466.6717 ms | tok/sec 1123462.1557
for step 17077 | loss 3.169612 | norm 0.3184 | time 465.5113 ms | tok/sec 1126262.6175
for step 17078 | loss 3.180994 | norm 0.3541 | time 465.8158 ms | tok/sec 1125526.4842
for step 17079 | loss 3.198904 | norm 0.3177 | time 465.2228 ms | tok/sec 1126961.0173
for step 17080 | loss 3.256946 | norm 0.3246 | time 465.3599 ms | tok/sec 1126629.0251
for step 17081 | loss 3.204469 | norm 0.3460 | time 466.1987 ms | tok/sec 1124602.0621
for step 17082 | loss 3.206719 | norm 0.3004 | time 465.6115 ms | tok/sec 1126020.4000
for step 17083 | loss 3.273753 | norm 0.3285 | time 464.7810 ms | tok/sec 1128032.2307
for step 17084 | loss 3.216466 | norm 0.3020 | time 465.7311 ms | tok/sec 1125731.0292
for step 17085 | loss 3.162934 | norm 0.2993 | time 465.3473 ms | tok/sec 1126659.6179
for step 17086 | loss 3.231481 | norm 0.3371 | time 465.0004 ms | tok/sec 1127500.1272
for step 17087 | loss 3.348562 | norm 0.3155 | time 465.1077 ms | tok/sec 1127240.0419
for step 17088 | loss 3.215316 | norm 0.2992 | time 464.5014 ms | tok/sec 1128711.3915
for step 17089 | loss 3.223371 | norm 0.3260 | time 466.7885 ms | tok/sec 1123180.9822
for step 17090 | loss 3.170694 | norm 0.2903 | time 465.6074 ms | tok/sec 1126030.2020
for step 17091 | loss 3.174225 | norm 0.2980 | time 465.0636 ms | tok/sec 1127346.9514
for step 17092 | loss 3.195439 | norm 0.3056 | time 465.3966 ms | tok/sec 1126540.1421
for step 17093 | loss 3.093469 | norm 0.3002 | time 465.9746 ms | tok/sec 1125142.9467
for step 17094 | loss 3.174503 | norm 0.2925 | time 465.4930 ms | tok/sec 1126307.0353
for step 17095 | loss 3.190435 | norm 0.3100 | time 465.3356 ms | tok/sec 1126687.9033
for step 17096 | loss 3.156340 | norm 0.3242 | time 465.9610 ms | tok/sec 1125175.7617
for step 17097 | loss 3.180313 | norm 0.2921 | time 465.4841 ms | tok/sec 1126328.3802
for step 17098 | loss 3.193918 | norm 0.2976 | time 465.1160 ms | tok/sec 1127219.8181
for step 17099 | loss 3.231776 | norm 0.3168 | time 466.6061 ms | tok/sec 1123620.0189
for step 17100 | loss 3.214560 | norm 0.2911 | time 466.1727 ms | tok/sec 1124664.7550
for step 17101 | loss 3.156303 | norm 0.3085 | time 465.8957 ms | tok/sec 1125333.5310
for step 17102 | loss 3.129866 | norm 0.2806 | time 464.8418 ms | tok/sec 1127884.6952
for step 17103 | loss 3.149771 | norm 0.3039 | time 464.9353 ms | tok/sec 1127657.9707
for step 17104 | loss 3.138956 | norm 0.2774 | time 464.8542 ms | tok/sec 1127854.6142
for step 17105 | loss 3.100319 | norm 0.2960 | time 465.7578 ms | tok/sec 1125666.4886
for step 17106 | loss 3.122922 | norm 0.2679 | time 464.8349 ms | tok/sec 1127901.4718
for step 17107 | loss 3.171125 | norm 0.2891 | time 465.8844 ms | tok/sec 1125360.5980
for step 17108 | loss 3.159879 | norm 0.2956 | time 464.9150 ms | tok/sec 1127707.1252
for step 17109 | loss 3.173827 | norm 0.2648 | time 465.5058 ms | tok/sec 1126275.8848
for step 17110 | loss 3.126400 | norm 0.2994 | time 465.0517 ms | tok/sec 1127375.8493
for step 17111 | loss 3.111345 | norm 0.2718 | time 465.5275 ms | tok/sec 1126223.3942
for step 17112 | loss 3.110771 | norm 0.2955 | time 465.0784 ms | tok/sec 1127311.1200
for step 17113 | loss 3.251773 | norm 0.3211 | time 465.3072 ms | tok/sec 1126756.6024
for step 17114 | loss 3.305716 | norm 0.3838 | time 465.1663 ms | tok/sec 1127097.9127
for step 17115 | loss 3.203029 | norm 0.3731 | time 464.8945 ms | tok/sec 1127756.8623
for step 17116 | loss 3.241054 | norm 0.3255 | time 464.8848 ms | tok/sec 1127780.5757
for step 17117 | loss 3.225486 | norm 0.3344 | time 465.5809 ms | tok/sec 1126094.2076
for step 17118 | loss 3.244047 | norm 0.3227 | time 465.2452 ms | tok/sec 1126906.7304
for step 17119 | loss 3.272564 | norm 0.3673 | time 464.7341 ms | tok/sec 1128146.2355
for step 17120 | loss 3.242407 | norm 0.3050 | time 464.7815 ms | tok/sec 1128031.0734
for step 17121 | loss 3.226039 | norm 0.3421 | time 465.8458 ms | tok/sec 1125453.9030
for step 17122 | loss 3.195541 | norm 0.3613 | time 465.2965 ms | tok/sec 1126782.5832
for step 17123 | loss 3.217136 | norm 0.2934 | time 466.2342 ms | tok/sec 1124516.3739
for step 17124 | loss 3.217557 | norm 0.3129 | time 465.5502 ms | tok/sec 1126168.6017
for step 17125 | loss 3.221048 | norm 0.3090 | time 465.9116 ms | tok/sec 1125294.9483
for step 17126 | loss 3.266613 | norm 0.3129 | time 465.5566 ms | tok/sec 1126153.0300
for step 17127 | loss 3.197154 | norm 0.3323 | time 464.9687 ms | tok/sec 1127577.0198
for step 17128 | loss 3.207119 | norm 0.3178 | time 465.8716 ms | tok/sec 1125391.6980
for step 17129 | loss 3.254109 | norm 0.3156 | time 465.6284 ms | tok/sec 1125979.4640
for step 17130 | loss 3.216005 | norm 0.3275 | time 466.2886 ms | tok/sec 1124385.2789
for step 17131 | loss 3.186431 | norm 0.2990 | time 464.6249 ms | tok/sec 1128411.3714
for step 17132 | loss 3.166887 | norm 0.3145 | time 466.2571 ms | tok/sec 1124461.1723
for step 17133 | loss 3.172981 | norm 0.3034 | time 465.4534 ms | tok/sec 1126402.8051
for step 17134 | loss 3.184903 | norm 0.3007 | time 466.1593 ms | tok/sec 1124696.9670
for step 17135 | loss 3.195021 | norm 0.2842 | time 465.6243 ms | tok/sec 1125989.2654
for step 17136 | loss 3.129166 | norm 0.3148 | time 465.3265 ms | tok/sec 1126709.8400
for step 17137 | loss 3.233451 | norm 0.3165 | time 463.9781 ms | tok/sec 1129984.4844
for step 17138 | loss 3.141371 | norm 0.3113 | time 464.9057 ms | tok/sec 1127729.6798
for step 17139 | loss 3.168514 | norm 0.3125 | time 466.1858 ms | tok/sec 1124633.1201
for step 17140 | loss 3.072024 | norm 0.2775 | time 465.3549 ms | tok/sec 1126641.1466
for step 17141 | loss 3.226741 | norm 0.3233 | time 465.5037 ms | tok/sec 1126281.0764
for step 17142 | loss 3.144123 | norm 0.2930 | time 465.3907 ms | tok/sec 1126554.5702
for step 17143 | loss 3.138071 | norm 0.2811 | time 466.1808 ms | tok/sec 1124645.1987
Will loading at 0 from edu_fineweb10B/edufineweb_train_000091.npy
for step 17144 | loss 3.097423 | norm 0.2945 | time 2657.3944 ms | tok/sec 197294.0103
for step 17145 | loss 3.140985 | norm 0.2959 | time 475.0280 ms | tok/sec 1103699.0620
for step 17146 | loss 3.092837 | norm 0.2681 | time 464.1881 ms | tok/sec 1129473.1624
for step 17147 | loss 3.140553 | norm 0.3092 | time 465.0755 ms | tok/sec 1127318.0549
for step 17148 | loss 3.205224 | norm 0.3146 | time 464.3273 ms | tok/sec 1129134.4706
for step 17149 | loss 3.232224 | norm 0.3214 | time 465.5564 ms | tok/sec 1126153.6067
for step 17150 | loss 3.195437 | norm 0.2970 | time 466.1810 ms | tok/sec 1124644.6235
for step 17151 | loss 3.205738 | norm 0.3204 | time 465.5828 ms | tok/sec 1126089.5944
for step 17152 | loss 3.263710 | norm 0.2986 | time 466.7654 ms | tok/sec 1123236.6319
for step 17153 | loss 3.253406 | norm 0.3114 | time 465.2321 ms | tok/sec 1126938.4934
for step 17154 | loss 3.161542 | norm 0.2871 | time 465.7466 ms | tok/sec 1125693.5717
for step 17155 | loss 3.223014 | norm 0.3087 | time 464.9479 ms | tok/sec 1127627.3237
for step 17156 | loss 3.310078 | norm 0.3296 | time 464.4799 ms | tok/sec 1128763.5348
for step 17157 | loss 3.269724 | norm 0.3671 | time 464.7179 ms | tok/sec 1128185.5928
for step 17158 | loss 3.230212 | norm 0.3122 | time 465.1918 ms | tok/sec 1127036.1035
for step 17159 | loss 3.176804 | norm 0.2980 | time 465.7233 ms | tok/sec 1125750.0470
for step 17160 | loss 3.213205 | norm 0.3316 | time 465.5979 ms | tok/sec 1126053.2663
for step 17161 | loss 3.163262 | norm 0.3124 | time 465.0853 ms | tok/sec 1127294.3609
for step 17162 | loss 3.161666 | norm 0.3053 | time 465.4872 ms | tok/sec 1126320.8805
for step 17163 | loss 3.137339 | norm 0.3523 | time 465.7438 ms | tok/sec 1125700.4867
for step 17164 | loss 3.191789 | norm 0.2952 | time 465.8318 ms | tok/sec 1125487.8882
for step 17165 | loss 3.155087 | norm 0.3370 | time 465.3001 ms | tok/sec 1126773.9228
for step 17166 | loss 3.169038 | norm 0.3333 | time 464.8860 ms | tok/sec 1127777.6838
for step 17167 | loss 3.177675 | norm 0.2962 | time 465.7643 ms | tok/sec 1125650.9309
for step 17168 | loss 3.176870 | norm 0.3008 | time 465.0559 ms | tok/sec 1127365.4458
for step 17169 | loss 3.202015 | norm 0.3262 | time 465.9097 ms | tok/sec 1125299.5550
for step 17170 | loss 3.142354 | norm 0.2664 | time 465.4975 ms | tok/sec 1126296.0747
for step 17171 | loss 3.130609 | norm 0.3016 | time 466.1098 ms | tok/sec 1124816.6274
for step 17172 | loss 3.120712 | norm 0.2859 | time 465.1048 ms | tok/sec 1127246.9760
for step 17173 | loss 3.131969 | norm 0.3248 | time 466.0585 ms | tok/sec 1124940.3417
for step 17174 | loss 3.172761 | norm 0.3085 | time 465.7130 ms | tok/sec 1125774.8288
for step 17175 | loss 3.199028 | norm 0.3342 | time 465.3788 ms | tok/sec 1126583.4275
for step 17176 | loss 3.152183 | norm 0.3048 | time 465.0338 ms | tok/sec 1127419.1989
for step 17177 | loss 3.150950 | norm 0.2969 | time 465.9760 ms | tok/sec 1125139.4926
for step 17178 | loss 3.102485 | norm 0.2852 | time 465.5902 ms | tok/sec 1126071.7184
for step 17179 | loss 3.150717 | norm 0.3088 | time 465.9584 ms | tok/sec 1125182.0947
for step 17180 | loss 3.167553 | norm 0.3283 | time 465.4193 ms | tok/sec 1126485.3187
for step 17181 | loss 3.165442 | norm 0.2914 | time 465.7214 ms | tok/sec 1125754.6575
for step 17182 | loss 3.160521 | norm 0.3098 | time 466.4567 ms | tok/sec 1123980.1128
for step 17183 | loss 3.243171 | norm 0.3132 | time 465.6196 ms | tok/sec 1126000.7965
for step 17184 | loss 3.254618 | norm 0.2826 | time 465.8508 ms | tok/sec 1125441.8070
for step 17185 | loss 3.271623 | norm 0.3031 | time 464.8294 ms | tok/sec 1127914.7777
for step 17186 | loss 3.262174 | norm 0.3100 | time 466.1069 ms | tok/sec 1124823.5317
for step 17187 | loss 3.205736 | norm 0.2732 | time 466.1484 ms | tok/sec 1124723.4281
for step 17188 | loss 3.193715 | norm 0.3255 | time 466.4414 ms | tok/sec 1124016.8818
for step 17189 | loss 3.220945 | norm 0.2940 | time 465.6985 ms | tok/sec 1125809.9861
for step 17190 | loss 3.219151 | norm 0.2852 | time 465.7664 ms | tok/sec 1125645.7450
for step 17191 | loss 3.208098 | norm 0.3074 | time 465.3740 ms | tok/sec 1126594.9709
for step 17192 | loss 3.244600 | norm 0.2935 | time 465.5054 ms | tok/sec 1126277.0385
for step 17193 | loss 3.200586 | norm 0.3142 | time 465.8499 ms | tok/sec 1125444.1110
for step 17194 | loss 3.134689 | norm 0.2986 | time 465.5476 ms | tok/sec 1126174.9458
for step 17195 | loss 3.129986 | norm 0.2980 | time 465.4872 ms | tok/sec 1126320.8805
for step 17196 | loss 3.165590 | norm 0.3063 | time 466.5000 ms | tok/sec 1123875.5642
for step 17197 | loss 3.111459 | norm 0.2983 | time 465.1737 ms | tok/sec 1127080.0047
for step 17198 | loss 3.180513 | norm 0.3013 | time 465.9607 ms | tok/sec 1125176.3374
for step 17199 | loss 3.207497 | norm 0.3082 | time 465.1332 ms | tok/sec 1127178.2171
for step 17200 | loss 3.218719 | norm 0.2955 | time 466.1319 ms | tok/sec 1124763.1223
for step 17201 | loss 3.188201 | norm 0.2738 | time 465.8058 ms | tok/sec 1125550.6800
for step 17202 | loss 3.195728 | norm 0.2910 | time 465.2274 ms | tok/sec 1126950.0440
for step 17203 | loss 3.264813 | norm 0.3196 | time 465.6305 ms | tok/sec 1125974.2752
for step 17204 | loss 3.219072 | norm 0.2911 | time 465.6961 ms | tok/sec 1125815.7499
for step 17205 | loss 3.192760 | norm 0.2777 | time 464.8259 ms | tok/sec 1127923.4557
for step 17206 | loss 3.147964 | norm 0.3231 | time 465.4813 ms | tok/sec 1126335.3030
for step 17207 | loss 3.202522 | norm 0.2933 | time 464.5889 ms | tok/sec 1128498.8125
for step 17208 | loss 3.089839 | norm 0.2804 | time 465.2019 ms | tok/sec 1127011.8438
for step 17209 | loss 3.150774 | norm 0.3203 | time 465.5430 ms | tok/sec 1126185.9040
for step 17210 | loss 3.153095 | norm 0.2738 | time 465.7836 ms | tok/sec 1125604.2601
for step 17211 | loss 3.132480 | norm 0.2783 | time 465.3795 ms | tok/sec 1126581.6961
for step 17212 | loss 3.130819 | norm 0.2821 | time 465.4937 ms | tok/sec 1126305.3047
for step 17213 | loss 3.161383 | norm 0.2764 | time 465.9162 ms | tok/sec 1125284.0074
for step 17214 | loss 3.111433 | norm 0.2792 | time 465.2648 ms | tok/sec 1126859.3780
for step 17215 | loss 3.070588 | norm 0.2712 | time 465.4162 ms | tok/sec 1126492.8206
for step 17216 | loss 3.120143 | norm 0.2781 | time 465.7316 ms | tok/sec 1125729.8766
for step 17217 | loss 3.197336 | norm 0.2648 | time 465.3590 ms | tok/sec 1126631.3339
for step 17218 | loss 3.287614 | norm 0.3465 | time 465.2505 ms | tok/sec 1126894.0257
for step 17219 | loss 3.240943 | norm 0.3176 | time 465.6627 ms | tok/sec 1125896.4481
for step 17220 | loss 3.263715 | norm 0.3006 | time 466.4426 ms | tok/sec 1124014.0092
for step 17221 | loss 3.168262 | norm 0.2954 | time 465.5306 ms | tok/sec 1126215.8960
for step 17222 | loss 3.174548 | norm 0.2835 | time 465.1358 ms | tok/sec 1127171.8617
for step 17223 | loss 3.145815 | norm 0.2953 | time 464.9081 ms | tok/sec 1127723.8965
for step 17224 | loss 3.182207 | norm 0.2997 | time 465.5583 ms | tok/sec 1126148.9930
for step 17225 | loss 3.178243 | norm 0.2945 | time 465.8413 ms | tok/sec 1125464.8472
for step 17226 | loss 3.193182 | norm 0.2878 | time 465.6348 ms | tok/sec 1125963.8976
for step 17227 | loss 3.231891 | norm 0.3068 | time 465.5552 ms | tok/sec 1126156.4903
for step 17228 | loss 3.196727 | norm 0.2878 | time 465.6799 ms | tok/sec 1125854.9447
for step 17229 | loss 3.216771 | norm 0.2843 | time 465.2913 ms | tok/sec 1126795.2854
for step 17230 | loss 3.165490 | norm 0.2949 | time 465.6782 ms | tok/sec 1125858.9796
for step 17231 | loss 3.137072 | norm 0.2858 | time 465.1775 ms | tok/sec 1127070.7620
for step 17232 | loss 3.209793 | norm 0.3113 | time 465.7648 ms | tok/sec 1125649.7785
for step 17233 | loss 3.152629 | norm 0.2803 | time 465.1430 ms | tok/sec 1127154.5290
for step 17234 | loss 3.229131 | norm 0.3060 | time 465.7032 ms | tok/sec 1125798.4589
for step 17235 | loss 3.178784 | norm 0.2854 | time 464.6206 ms | tok/sec 1128421.7941
for step 17236 | loss 3.152454 | norm 0.3087 | time 465.1442 ms | tok/sec 1127151.6403
for step 17237 | loss 3.192148 | norm 0.2760 | time 465.3325 ms | tok/sec 1126695.4079
for step 17238 | loss 3.220953 | norm 0.2740 | time 465.7006 ms | tok/sec 1125804.7989
for step 17239 | loss 3.186064 | norm 0.3019 | time 465.4362 ms | tok/sec 1126444.3489
for step 17240 | loss 3.136194 | norm 0.2819 | time 465.8463 ms | tok/sec 1125452.7510
for step 17241 | loss 3.160157 | norm 0.2740 | time 466.2142 ms | tok/sec 1124564.6797
for step 17242 | loss 3.092223 | norm 0.2996 | time 465.6668 ms | tok/sec 1125886.6484
for step 17243 | loss 3.122643 | norm 0.2918 | time 465.5600 ms | tok/sec 1126144.9560
for step 17244 | loss 3.197602 | norm 0.3090 | time 465.2386 ms | tok/sec 1126922.9004
for step 17245 | loss 3.113275 | norm 0.3287 | time 465.8406 ms | tok/sec 1125466.5752
for step 17246 | loss 3.166734 | norm 0.3044 | time 465.6060 ms | tok/sec 1126033.6616
for step 17247 | loss 3.139011 | norm 0.3081 | time 465.4815 ms | tok/sec 1126334.7261
for step 17248 | loss 3.074146 | norm 0.3095 | time 465.3728 ms | tok/sec 1126597.8567
for step 17249 | loss 3.106137 | norm 0.2904 | time 465.3659 ms | tok/sec 1126614.5951
validation loss 3.2258
HellaSwag accuracy: 2838/10042=0.2826
> Hello, I'm a language model, and there's a lot of it outside. In my native language, I want to create the first language in the world
> Hello, I'm a language model, which is very intuitive. I am learning Japanese, but because I live in Japan and really enjoy trying new things, I
> Hello, I'm a language model, I hate learning English. I'm a language model, but learning to write has always been fun! So I started creating
> Hello, I'm a language model, and I'm using a bit of math a lot. Since I'm going to be my students, I'm going to
> Hello, I'm a language model, and I love to use modeling in various activities or projects.
Here are three examples to think about when you're trying
> Hello, I'm a language model, so why not start by taking this sample and trying...
- A sentence or a paragraph describing the concept of a language
> Hello, I'm a language model, so I do the task by filling out the assignment.
What do you think?
- I think that you use
> Hello, I'm a language model, and for the last few years, I have managed to do this all on the Linux-type system. I've created
> Hello, I'm a language model, so I will be creating a new model based on this.
I created a new project
At the bottom of this
> Hello, I'm a language model,
The language world is full of amazing ideas, every child is different. It may have some, but I get a
> > Hello, I'm a language model, as is the case for everyone, there's no point in using someone's names for a given language. I'm afraid
Hello, I'm a language model, but do you know that all languages and all languages are interwoven?
One of the biggest difference between Japanese,
> Hello, I'm a language model, and I can't remember the name of "nanny" because I didn't have the opportunity to do the grunt work> 
Hello, I'm a language model, and the real answer is yes, your program doesn't tell me anything... but because I can explain to you why it
> Hello, I'm a language model, I use it all the time to talk about it. I can explain the things that I'm trying to teach them,
> Hello, I'm a language model, I don't know about you, so I'll explain what I can learn from the game. If you have any information> > 
> Hello, I'm a language model, and I can now explain how and why, so I can explain to you. I'll add a nice definition, explain
Hello, I'm a language model, and I know that there's a lot which I'll add to this as well. It's something you have to get
> Hello, I'm a language model, and I'm not. But, since I am not a speaker, even those languages you talk to could have problems doing
>Hello, I'm a language model, isn't it? (The only problem is one of them is some other problem?)<|endoftext|>A large variety of health problems>>  Hello, I'm a language model, and my job is to explain how to use it. The goal is to explain things in a way that they can understand
 Hello, I'm a language model, and this article is about working together to make our language better. I'm a language model, and I'm the firstHello, I'm a language model, do I just need to look back then?
As our friend, I'm pretty sure we're just not a computer

> 
> > Hello, I'm a language model, so I would love to hear about more. I'm a language model, and it shows how to model a language from
> Hello, I'm a language model, and I have always spoken with and modeled my environment as a model system. I'm not a language model, as my
Hello, I'm a language model, I'm curious. I don't learn programming, but I can. When I learn programming, it helps to understand it
> Hello, I'm a language model, and you've got better accuracy data than if you went in, and I have data that will really help me determine where
> > Hello, I'm a language model, have a problem and I just want to make the program run faster and easier. What do I like? I'm pretty
> Hello, I'm a language model, so you are going to start with languages. For example, if I go to the main school at 10, I can
Hello, I'm a language model, so I can write about anything I understand, but I'm not. A great tool for helping beginners build the speed and
Hello, I'm a language model, and what I like to do is learn to read, so that when we learn to read we can feel more like we
> Hello, I'm a language model, I like going to be able to work on all I have with the language. The best person on my journey to make
for step 17250 | loss 3.168796 | norm 0.3039 | time 12673.0485 ms | tok/sec 41370.3143
for step 17251 | loss 3.113916 | norm 0.2732 | time 462.7244 ms | tok/sec 1133045.8183
for step 17252 | loss 3.183748 | norm 0.2981 | time 461.8964 ms | tok/sec 1135076.9953
for step 17253 | loss 3.232742 | norm 0.3272 | time 464.1225 ms | tok/sec 1129632.7194
for step 17254 | loss 3.133633 | norm 0.3330 | time 463.4058 ms | tok/sec 1131379.7674
for step 17255 | loss 3.200902 | norm 0.2890 | time 463.8710 ms | tok/sec 1130245.2568
for step 17256 | loss 3.156190 | norm 0.3258 | time 463.1379 ms | tok/sec 1132034.4104
for step 17257 | loss 3.225035 | norm 0.3260 | time 463.4705 ms | tok/sec 1131222.0441
for step 17258 | loss 3.289253 | norm 0.3447 | time 463.3229 ms | tok/sec 1131582.3695
for step 17259 | loss 3.177997 | norm 0.3186 | time 463.9850 ms | tok/sec 1129967.6457
for step 17260 | loss 3.242569 | norm 0.3246 | time 463.3133 ms | tok/sec 1131605.6618
for step 17261 | loss 3.215496 | norm 0.3194 | time 464.9768 ms | tok/sec 1127557.3620
for step 17262 | loss 3.235894 | norm 0.3000 | time 463.2251 ms | tok/sec 1131821.1605
for step 17263 | loss 3.245397 | norm 0.2920 | time 464.8776 ms | tok/sec 1127797.9276
for step 17264 | loss 3.211036 | norm 0.3114 | time 464.8550 ms | tok/sec 1127852.8788
for step 17265 | loss 3.197726 | norm 0.3158 | time 466.2306 ms | tok/sec 1124524.9996
for step 17266 | loss 3.190478 | norm 0.2916 | time 464.4969 ms | tok/sec 1128722.3991
for step 17267 | loss 3.160944 | norm 0.3346 | time 464.7634 ms | tok/sec 1128075.0521
for step 17268 | loss 3.184183 | norm 0.2792 | time 464.6285 ms | tok/sec 1128402.6859
for step 17269 | loss 3.208359 | norm 0.2823 | time 464.9723 ms | tok/sec 1127568.3472
for step 17270 | loss 3.174066 | norm 0.2965 | time 464.8530 ms | tok/sec 1127857.5066
for step 17271 | loss 3.155004 | norm 0.2730 | time 464.6587 ms | tok/sec 1128329.1544
for step 17272 | loss 3.198345 | norm 0.2698 | time 465.7815 ms | tok/sec 1125609.4456
for step 17273 | loss 3.221328 | norm 0.2789 | time 466.1984 ms | tok/sec 1124602.6372
for step 17274 | loss 3.176534 | norm 0.2765 | time 466.4893 ms | tok/sec 1123901.4123
for step 17275 | loss 3.183927 | norm 0.2948 | time 465.8282 ms | tok/sec 1125496.5289
for step 17276 | loss 3.119198 | norm 0.2725 | time 465.6444 ms | tok/sec 1125940.8370
for step 17277 | loss 3.137692 | norm 0.2819 | time 465.5306 ms | tok/sec 1126215.8960
for step 17278 | loss 3.101639 | norm 0.2759 | time 466.3622 ms | tok/sec 1124207.6595
for step 17279 | loss 3.115036 | norm 0.2852 | time 465.3666 ms | tok/sec 1126612.8635
for step 17280 | loss 3.171084 | norm 0.3009 | time 464.4866 ms | tok/sec 1128747.3119
for step 17281 | loss 3.179147 | norm 0.2860 | time 465.6491 ms | tok/sec 1125929.3071
for step 17282 | loss 3.147149 | norm 0.2881 | time 465.7884 ms | tok/sec 1125592.7371
for step 17283 | loss 3.146881 | norm 0.2716 | time 466.1608 ms | tok/sec 1124693.5156
for step 17284 | loss 3.097012 | norm 0.2886 | time 464.7775 ms | tok/sec 1128040.9104
for step 17285 | loss 3.173651 | norm 0.2928 | time 465.6167 ms | tok/sec 1126007.7153
for step 17286 | loss 3.137630 | norm 0.2799 | time 466.6240 ms | tok/sec 1123576.9609
for step 17287 | loss 3.202182 | norm 0.3214 | time 465.2491 ms | tok/sec 1126897.4906
for step 17288 | loss 3.279192 | norm 0.2964 | time 466.2054 ms | tok/sec 1124585.9586
for step 17289 | loss 3.196005 | norm 0.3073 | time 466.3465 ms | tok/sec 1124245.5929
for step 17290 | loss 3.259573 | norm 0.3160 | time 466.1598 ms | tok/sec 1124695.8165
for step 17291 | loss 3.255697 | norm 0.2893 | time 466.1682 ms | tok/sec 1124675.6839
for step 17292 | loss 3.230913 | norm 0.3304 | time 465.9307 ms | tok/sec 1125248.8828
for step 17293 | loss 3.253981 | norm 0.3168 | time 465.4112 ms | tok/sec 1126504.9391
for step 17294 | loss 3.290069 | norm 0.3164 | time 465.3335 ms | tok/sec 1126693.0988
for step 17295 | loss 3.239816 | norm 0.3476 | time 465.4574 ms | tok/sec 1126392.9966
for step 17296 | loss 3.246863 | norm 0.2856 | time 465.2588 ms | tok/sec 1126873.8143
for step 17297 | loss 3.218735 | norm 0.2984 | time 465.2770 ms | tok/sec 1126829.9292
for step 17298 | loss 3.317577 | norm 0.4175 | time 466.1527 ms | tok/sec 1124713.0736
for step 17299 | loss 3.184122 | norm 0.3234 | time 466.0473 ms | tok/sec 1124967.3899
for step 17300 | loss 3.170033 | norm 0.3031 | time 465.5249 ms | tok/sec 1126229.7390
for step 17301 | loss 3.188799 | norm 0.2892 | time 465.0235 ms | tok/sec 1127444.0542
for step 17302 | loss 3.214325 | norm 0.3002 | time 465.5702 ms | tok/sec 1126120.1579
for step 17303 | loss 3.176908 | norm 0.2896 | time 465.6291 ms | tok/sec 1125977.7344
for step 17304 | loss 3.194798 | norm 0.3091 | time 464.7329 ms | tok/sec 1128149.1293
for step 17305 | loss 3.240504 | norm 0.2842 | time 465.5428 ms | tok/sec 1126186.4808
for step 17306 | loss 3.156602 | norm 0.2854 | time 465.6482 ms | tok/sec 1125931.6131
for step 17307 | loss 3.282143 | norm 0.3160 | time 464.9324 ms | tok/sec 1127664.9099
for step 17308 | loss 3.216425 | norm 0.3173 | time 465.5857 ms | tok/sec 1126082.6746
for step 17309 | loss 3.151453 | norm 0.2853 | time 465.3740 ms | tok/sec 1126594.9709
for step 17310 | loss 3.171222 | norm 0.3201 | time 465.7919 ms | tok/sec 1125584.0950
for step 17311 | loss 3.109695 | norm 0.3037 | time 466.4435 ms | tok/sec 1124011.7110
for step 17312 | loss 3.153239 | norm 0.2978 | time 465.7369 ms | tok/sec 1125717.1984
for step 17313 | loss 3.137619 | norm 0.2927 | time 466.1314 ms | tok/sec 1124764.2728
for step 17314 | loss 3.141651 | norm 0.2967 | time 465.3647 ms | tok/sec 1126617.4811
for step 17315 | loss 3.138338 | norm 0.3362 | time 465.6956 ms | tok/sec 1125816.9026
for step 17316 | loss 3.162468 | norm 0.2687 | time 464.4520 ms | tok/sec 1128831.3283
for step 17317 | loss 3.130932 | norm 0.3356 | time 465.9953 ms | tok/sec 1125092.8642
for step 17318 | loss 3.194036 | norm 0.3093 | time 465.0779 ms | tok/sec 1127312.2758
for step 17319 | loss 3.164586 | norm 0.2944 | time 465.2543 ms | tok/sec 1126884.7861
for step 17320 | loss 3.155621 | norm 0.2956 | time 465.5516 ms | tok/sec 1126165.1413
for step 17321 | loss 3.190821 | norm 0.3249 | time 464.8337 ms | tok/sec 1127904.3643
for step 17322 | loss 3.146912 | norm 0.3175 | time 465.6014 ms | tok/sec 1126044.6171
for step 17323 | loss 3.236472 | norm 0.3181 | time 465.4505 ms | tok/sec 1126409.7289
for step 17324 | loss 3.265549 | norm 0.3158 | time 464.5207 ms | tok/sec 1128664.4666
for step 17325 | loss 3.223693 | norm 0.3081 | time 465.4369 ms | tok/sec 1126442.6179
for step 17326 | loss 3.347289 | norm 0.3599 | time 465.3406 ms | tok/sec 1126675.7809
for step 17327 | loss 3.217815 | norm 0.3542 | time 465.1875 ms | tok/sec 1127046.5008
for step 17328 | loss 3.242805 | norm 0.3249 | time 465.0805 ms | tok/sec 1127305.9189
for step 17329 | loss 3.210403 | norm 0.3234 | time 465.1003 ms | tok/sec 1127257.9551
for step 17330 | loss 3.227049 | norm 0.3215 | time 466.0094 ms | tok/sec 1125058.9028
for step 17331 | loss 3.272471 | norm 0.3194 | time 465.0431 ms | tok/sec 1127396.6567
for step 17332 | loss 3.225845 | norm 0.2821 | time 465.1439 ms | tok/sec 1127152.2180
for step 17333 | loss 3.217072 | norm 0.2941 | time 464.4849 ms | tok/sec 1128751.3676
for step 17334 | loss 3.235361 | norm 0.3018 | time 465.8778 ms | tok/sec 1125376.7237
Will loading at 0 from edu_fineweb10B/edufineweb_train_000092.npy
for step 17335 | loss 3.170959 | norm 0.3074 | time 2544.0106 ms | tok/sec 206087.1884
for step 17336 | loss 3.171411 | norm 0.3015 | time 463.3710 ms | tok/sec 1131464.7583
for step 17337 | loss 3.202078 | norm 0.3202 | time 466.5475 ms | tok/sec 1123761.2723
for step 17338 | loss 3.205733 | norm 0.3034 | time 465.5886 ms | tok/sec 1126075.7548
for step 17339 | loss 3.179934 | norm 0.3350 | time 466.7139 ms | tok/sec 1123360.5727
for step 17340 | loss 3.189886 | norm 0.3785 | time 466.2385 ms | tok/sec 1124506.0232
for step 17341 | loss 3.199682 | norm 0.3020 | time 465.4725 ms | tok/sec 1126356.6490
for step 17342 | loss 3.193700 | norm 0.3484 | time 465.2340 ms | tok/sec 1126933.8732
for step 17343 | loss 3.243237 | norm 0.3482 | time 466.2907 ms | tok/sec 1124380.1047
for step 17344 | loss 3.213144 | norm 0.3122 | time 465.2896 ms | tok/sec 1126799.3271
for step 17345 | loss 3.230820 | norm 0.3093 | time 465.7664 ms | tok/sec 1125645.7450
for step 17346 | loss 3.152967 | norm 0.3268 | time 465.8988 ms | tok/sec 1125326.0446
for step 17347 | loss 3.149312 | norm 0.3107 | time 465.0776 ms | tok/sec 1127312.8537
for step 17348 | loss 3.114468 | norm 0.2863 | time 465.4765 ms | tok/sec 1126346.8413
for step 17349 | loss 3.165405 | norm 0.3300 | time 464.7741 ms | tok/sec 1128049.0117
for step 17350 | loss 3.135682 | norm 0.3223 | time 464.4196 ms | tok/sec 1128910.1412
for step 17351 | loss 3.065933 | norm 0.3122 | time 464.7133 ms | tok/sec 1128196.5902
for step 17352 | loss 3.117356 | norm 0.3399 | time 466.5077 ms | tok/sec 1123857.1840
for step 17353 | loss 3.159474 | norm 0.3094 | time 465.3451 ms | tok/sec 1126664.8131
for step 17354 | loss 3.102964 | norm 0.2835 | time 465.8208 ms | tok/sec 1125514.3867
for step 17355 | loss 3.097131 | norm 0.3132 | time 465.8811 ms | tok/sec 1125368.6608
for step 17356 | loss 3.187268 | norm 0.3098 | time 466.3911 ms | tok/sec 1124138.1216
for step 17357 | loss 3.160004 | norm 0.2829 | time 466.2664 ms | tok/sec 1124438.7482
for step 17358 | loss 3.144828 | norm 0.3125 | time 465.6000 ms | tok/sec 1126048.0768
for step 17359 | loss 3.356637 | norm 0.3725 | time 465.2810 ms | tok/sec 1126820.1132
for step 17360 | loss 3.183171 | norm 0.3301 | time 466.2795 ms | tok/sec 1124407.1260
for step 17361 | loss 3.269404 | norm 0.3063 | time 465.4822 ms | tok/sec 1126332.9954
for step 17362 | loss 3.219600 | norm 0.3317 | time 466.3899 ms | tok/sec 1124140.9949
for step 17363 | loss 3.283787 | norm 0.3314 | time 466.4104 ms | tok/sec 1124091.5762
for step 17364 | loss 3.226546 | norm 0.2832 | time 466.3694 ms | tok/sec 1124190.4179
for step 17365 | loss 3.279479 | norm 0.2829 | time 466.0552 ms | tok/sec 1124948.3985
for step 17366 | loss 3.237948 | norm 0.3157 | time 465.9293 ms | tok/sec 1125252.3376
for step 17367 | loss 3.247410 | norm 0.3159 | time 465.9100 ms | tok/sec 1125298.9792
for step 17368 | loss 3.209238 | norm 0.3537 | time 466.0878 ms | tok/sec 1124869.5623
for step 17369 | loss 3.246324 | norm 0.3139 | time 465.7707 ms | tok/sec 1125635.3735
for step 17370 | loss 3.234966 | norm 0.2992 | time 465.5516 ms | tok/sec 1126165.1413
for step 17371 | loss 3.188310 | norm 0.3068 | time 468.3175 ms | tok/sec 1119513.9841
for step 17372 | loss 3.201507 | norm 0.3256 | time 465.9479 ms | tok/sec 1125207.4272
for step 17373 | loss 3.185414 | norm 0.2883 | time 465.8937 ms | tok/sec 1125338.1380
for step 17374 | loss 3.143961 | norm 0.2950 | time 466.7907 ms | tok/sec 1123175.8191
for step 17375 | loss 3.203377 | norm 0.3134 | time 466.8930 ms | tok/sec 1122929.7666
for step 17376 | loss 3.212717 | norm 0.2961 | time 466.8176 ms | tok/sec 1123110.9976
for step 17377 | loss 3.177320 | norm 0.2821 | time 466.1674 ms | tok/sec 1124677.4095
for step 17378 | loss 3.117423 | norm 0.2901 | time 464.8085 ms | tok/sec 1127965.6903
for step 17379 | loss 3.212409 | norm 0.2799 | time 465.9410 ms | tok/sec 1125224.1242
for step 17380 | loss 3.168123 | norm 0.3034 | time 466.5253 ms | tok/sec 1123814.6822
for step 17381 | loss 3.169326 | norm 0.2970 | time 465.9095 ms | tok/sec 1125300.1309
for step 17382 | loss 3.172731 | norm 0.3037 | time 466.5782 ms | tok/sec 1123687.1960
for step 17383 | loss 3.156794 | norm 0.3255 | time 464.8836 ms | tok/sec 1127783.4676
for step 17384 | loss 3.157866 | norm 0.3146 | time 465.5414 ms | tok/sec 1126189.9413
for step 17385 | loss 3.177197 | norm 0.3127 | time 466.3458 ms | tok/sec 1124247.3173
for step 17386 | loss 3.151646 | norm 0.3374 | time 466.7079 ms | tok/sec 1123374.9195
for step 17387 | loss 3.097540 | norm 0.2969 | time 464.3145 ms | tok/sec 1129165.7795
for step 17388 | loss 3.107917 | norm 0.2913 | time 465.4107 ms | tok/sec 1126506.0933
for step 17389 | loss 3.149902 | norm 0.2777 | time 466.3105 ms | tok/sec 1124332.3896
for step 17390 | loss 3.141404 | norm 0.2778 | time 465.4856 ms | tok/sec 1126324.9188
for step 17391 | loss 3.219527 | norm 0.3181 | time 465.7280 ms | tok/sec 1125738.5210
for step 17392 | loss 3.151954 | norm 0.3166 | time 465.6243 ms | tok/sec 1125989.2654
for step 17393 | loss 3.087454 | norm 0.2946 | time 467.0541 ms | tok/sec 1122542.2662
for step 17394 | loss 3.176546 | norm 0.3020 | time 466.3548 ms | tok/sec 1124225.4764
for step 17395 | loss 3.232379 | norm 0.3336 | time 466.1565 ms | tok/sec 1124703.8698
for step 17396 | loss 3.225917 | norm 0.3040 | time 466.0246 ms | tok/sec 1125022.0656
for step 17397 | loss 3.211682 | norm 0.3196 | time 465.9235 ms | tok/sec 1125266.1569
for step 17398 | loss 3.208771 | norm 0.3203 | time 466.5213 ms | tok/sec 1123824.4458
for step 17399 | loss 3.211111 | norm 0.3159 | time 465.6589 ms | tok/sec 1125905.6715
for step 17400 | loss 3.257096 | norm 0.3290 | time 466.0175 ms | tok/sec 1125039.3328
for step 17401 | loss 3.256645 | norm 0.3066 | time 465.6105 ms | tok/sec 1126022.7064
for step 17402 | loss 3.222714 | norm 0.3363 | time 466.2828 ms | tok/sec 1124399.0769
for step 17403 | loss 3.209896 | norm 0.3155 | time 465.7652 ms | tok/sec 1125648.6260
for step 17404 | loss 3.254028 | norm 0.3180 | time 466.0316 ms | tok/sec 1125005.3746
for step 17405 | loss 3.260510 | norm 0.3014 | time 465.7826 ms | tok/sec 1125606.5648
for step 17406 | loss 3.245680 | norm 0.3142 | time 465.3604 ms | tok/sec 1126627.8707
for step 17407 | loss 3.144547 | norm 0.2873 | time 466.4268 ms | tok/sec 1124051.9295
for step 17408 | loss 3.201684 | norm 0.2952 | time 465.8251 ms | tok/sec 1125504.0176
for step 17409 | loss 3.227840 | norm 0.2913 | time 465.2040 ms | tok/sec 1127006.6454
for step 17410 | loss 3.123674 | norm 0.2586 | time 467.2546 ms | tok/sec 1122060.5567
for step 17411 | loss 3.179882 | norm 0.3176 | time 465.2872 ms | tok/sec 1126805.1009
for step 17412 | loss 3.177633 | norm 0.2850 | time 466.7075 ms | tok/sec 1123376.0673
for step 17413 | loss 3.206077 | norm 0.3050 | time 465.6878 ms | tok/sec 1125835.9233
for step 17414 | loss 3.197989 | norm 0.2869 | time 465.3490 ms | tok/sec 1126655.5773
for step 17415 | loss 3.171418 | norm 0.2861 | time 465.9879 ms | tok/sec 1125110.7092
for step 17416 | loss 3.183944 | norm 0.2879 | time 465.8587 ms | tok/sec 1125422.7996
for step 17417 | loss 3.208162 | norm 0.3194 | time 465.8484 ms | tok/sec 1125447.5670
for step 17418 | loss 3.115687 | norm 0.2848 | time 466.0864 ms | tok/sec 1124873.0148
for step 17419 | loss 3.137877 | norm 0.3402 | time 465.6835 ms | tok/sec 1125846.2985
for step 17420 | loss 3.157802 | norm 0.2904 | time 465.2944 ms | tok/sec 1126787.7796
for step 17421 | loss 3.117228 | norm 0.3171 | time 465.9040 ms | tok/sec 1125313.3755
for step 17422 | loss 3.148360 | norm 0.3108 | time 466.6862 ms | tok/sec 1123427.1448
for step 17423 | loss 3.117243 | norm 0.3057 | time 465.9829 ms | tok/sec 1125122.7980
for step 17424 | loss 3.171302 | norm 0.2837 | time 465.7631 ms | tok/sec 1125653.8119
for step 17425 | loss 3.091249 | norm 0.2818 | time 465.5337 ms | tok/sec 1126208.3978
for step 17426 | loss 3.169116 | norm 0.3187 | time 466.0182 ms | tok/sec 1125037.6060
for step 17427 | loss 3.149863 | norm 0.3055 | time 466.6665 ms | tok/sec 1123474.7832
for step 17428 | loss 3.142652 | norm 0.2889 | time 465.9841 ms | tok/sec 1125119.9197
for step 17429 | loss 3.158256 | norm 0.3072 | time 465.3389 ms | tok/sec 1126679.8217
for step 17430 | loss 3.223966 | norm 0.3078 | time 465.6255 ms | tok/sec 1125986.3826
for step 17431 | loss 3.189101 | norm 0.2997 | time 465.4779 ms | tok/sec 1126343.3798
for step 17432 | loss 3.283189 | norm 0.3514 | time 465.0021 ms | tok/sec 1127496.0805
for step 17433 | loss 3.258296 | norm 0.2993 | time 466.4042 ms | tok/sec 1124106.5163
for step 17434 | loss 3.226624 | norm 0.3092 | time 465.9524 ms | tok/sec 1125196.4880
for step 17435 | loss 3.243586 | norm 0.2989 | time 465.4016 ms | tok/sec 1126528.0228
for step 17436 | loss 3.261362 | norm 0.2748 | time 465.7943 ms | tok/sec 1125578.3336
for step 17437 | loss 3.252124 | norm 0.2877 | time 465.6949 ms | tok/sec 1125818.6317
for step 17438 | loss 3.180959 | norm 0.3079 | time 466.2859 ms | tok/sec 1124391.6030
for step 17439 | loss 3.207390 | norm 0.2776 | time 465.7297 ms | tok/sec 1125734.4869
for step 17440 | loss 3.317778 | norm 0.3112 | time 465.8377 ms | tok/sec 1125473.4874
for step 17441 | loss 3.225764 | norm 0.3154 | time 466.1231 ms | tok/sec 1124784.4087
for step 17442 | loss 3.189646 | norm 0.3016 | time 466.1114 ms | tok/sec 1124812.6000
for step 17443 | loss 3.163771 | norm 0.3047 | time 465.4090 ms | tok/sec 1126510.1329
for step 17444 | loss 3.215338 | norm 0.3180 | time 466.2452 ms | tok/sec 1124489.9224
for step 17445 | loss 3.216360 | norm 0.3010 | time 464.8018 ms | tok/sec 1127981.8907
for step 17446 | loss 3.210595 | norm 0.3262 | time 465.5323 ms | tok/sec 1126211.8585
for step 17447 | loss 3.183707 | norm 0.2697 | time 464.6378 ms | tok/sec 1128380.1044
for step 17448 | loss 3.148788 | norm 0.3100 | time 466.0819 ms | tok/sec 1124883.9477
for step 17449 | loss 3.193793 | norm 0.3077 | time 465.4739 ms | tok/sec 1126353.1874
for step 17450 | loss 3.221967 | norm 0.2864 | time 465.8697 ms | tok/sec 1125396.3055
for step 17451 | loss 3.185516 | norm 0.2949 | time 466.1047 ms | tok/sec 1124828.7100
for step 17452 | loss 3.170735 | norm 0.2854 | time 466.0394 ms | tok/sec 1124986.3819
for step 17453 | loss 3.145947 | norm 0.2859 | time 465.8742 ms | tok/sec 1125385.3626
for step 17454 | loss 3.114360 | norm 0.2764 | time 465.7848 ms | tok/sec 1125601.3793
for step 17455 | loss 3.116064 | norm 0.2778 | time 465.7106 ms | tok/sec 1125780.5921
for step 17456 | loss 3.149721 | norm 0.2777 | time 465.4632 ms | tok/sec 1126379.1496
for step 17457 | loss 3.159658 | norm 0.3017 | time 465.3630 ms | tok/sec 1126621.5214
for step 17458 | loss 3.133135 | norm 0.2702 | time 465.1699 ms | tok/sec 1127089.2475
for step 17459 | loss 3.163917 | norm 0.3047 | time 465.3525 ms | tok/sec 1126646.9188
for step 17460 | loss 3.096199 | norm 0.3545 | time 467.4094 ms | tok/sec 1121689.1039
for step 17461 | loss 3.116978 | norm 0.3236 | time 465.0559 ms | tok/sec 1127365.4458
for step 17462 | loss 3.172390 | norm 0.3316 | time 464.5891 ms | tok/sec 1128498.2334
for step 17463 | loss 3.110041 | norm 0.3221 | time 464.6337 ms | tok/sec 1128389.9475
for step 17464 | loss 3.162663 | norm 0.3113 | time 465.2505 ms | tok/sec 1126894.0257
for step 17465 | loss 3.227496 | norm 0.3586 | time 465.4112 ms | tok/sec 1126504.9391
for step 17466 | loss 3.255911 | norm 0.3405 | time 465.4629 ms | tok/sec 1126379.7266
for step 17467 | loss 3.286376 | norm 0.3238 | time 465.8344 ms | tok/sec 1125481.5518
for step 17468 | loss 3.249648 | norm 0.3241 | time 465.1766 ms | tok/sec 1127073.0727
for step 17469 | loss 3.230474 | norm 0.3166 | time 465.3294 ms | tok/sec 1126702.9125
for step 17470 | loss 3.224932 | norm 0.2902 | time 465.1768 ms | tok/sec 1127072.4950
for step 17471 | loss 3.266293 | norm 0.3222 | time 465.5943 ms | tok/sec 1126061.9156
for step 17472 | loss 3.235049 | norm 0.3047 | time 467.0563 ms | tok/sec 1122537.1090
for step 17473 | loss 3.245565 | norm 0.2835 | time 464.8798 ms | tok/sec 1127792.7220
for step 17474 | loss 3.254620 | norm 0.3322 | time 466.2371 ms | tok/sec 1124509.4734
for step 17475 | loss 3.228218 | norm 0.3206 | time 465.1928 ms | tok/sec 1127033.7930
for step 17476 | loss 3.176410 | norm 0.3072 | time 465.2209 ms | tok/sec 1126965.6377
for step 17477 | loss 3.144902 | norm 0.3119 | time 465.0397 ms | tok/sec 1127404.7487
for step 17478 | loss 3.178791 | norm 0.2808 | time 464.9286 ms | tok/sec 1127674.1623
for step 17479 | loss 3.209878 | norm 0.3053 | time 465.5561 ms | tok/sec 1126154.1834
for step 17480 | loss 3.207190 | norm 0.3136 | time 464.7388 ms | tok/sec 1128134.6604
for step 17481 | loss 3.231607 | norm 0.2811 | time 465.6684 ms | tok/sec 1125882.6133
for step 17482 | loss 3.183311 | norm 0.2974 | time 465.2004 ms | tok/sec 1127015.3094
for step 17483 | loss 3.231740 | norm 0.2929 | time 466.0494 ms | tok/sec 1124962.2103
for step 17484 | loss 3.183688 | norm 0.3086 | time 465.2338 ms | tok/sec 1126934.4507
for step 17485 | loss 3.166995 | norm 0.3029 | time 465.8191 ms | tok/sec 1125518.4191
for step 17486 | loss 3.155853 | norm 0.2915 | time 465.7588 ms | tok/sec 1125664.1838
for step 17487 | loss 3.215494 | norm 0.3278 | time 465.0593 ms | tok/sec 1127357.3544
for step 17488 | loss 3.195424 | norm 0.2966 | time 466.2929 ms | tok/sec 1124374.9306
for step 17489 | loss 3.191730 | norm 0.2889 | time 465.7669 ms | tok/sec 1125644.5926
for step 17490 | loss 3.079888 | norm 0.2953 | time 465.6947 ms | tok/sec 1125819.2081
for step 17491 | loss 3.115703 | norm 0.2897 | time 466.2740 ms | tok/sec 1124420.3496
for step 17492 | loss 3.087989 | norm 0.2753 | time 465.5461 ms | tok/sec 1126178.4063
for step 17493 | loss 3.097027 | norm 0.2887 | time 465.3287 ms | tok/sec 1126704.6444
for step 17494 | loss 3.118507 | norm 0.2892 | time 465.4262 ms | tok/sec 1126468.5842
for step 17495 | loss 3.185751 | norm 0.3068 | time 465.6999 ms | tok/sec 1125806.5279
for step 17496 | loss 3.136045 | norm 0.3226 | time 464.9994 ms | tok/sec 1127502.4396
for step 17497 | loss 3.219486 | norm 0.3079 | time 465.2905 ms | tok/sec 1126797.0176
for step 17498 | loss 3.079955 | norm 0.2745 | time 465.7836 ms | tok/sec 1125604.2601
for step 17499 | loss 3.120922 | norm 0.3146 | time 465.6339 ms | tok/sec 1125966.2037
validation loss 3.2243
HellaSwag accuracy: 2811/10042=0.2799
> Hello, I'm a language model, and I don't care what you did, right? Yeah.
I did a post with you, and I'm
> Hello, I'm a language model, which means that if you want to make any kind of structure, then you'll have to specify it in the model.
> Hello, I'm a language model, I see that a lot of people don't understand the difference between languages and languages. I see language, language learning and
> Hello, I'm a language model, and I'm really excited to be involved in the technology change. Please be mindful of if you don't want to be
> Hello, I'm a language model, and I know that you know all too well but I don't yet know which language I'm working for, and the
> Hello, I'm a language model, so i'm not sure if i want to use java's library for I can use the library for both, but I
> Hello, I'm a language model, so I really love a variety of tools and features that I think can help you learn languages better.
I've included
> Hello, I'm a language model, and when I think of a model, it represents a model. You may be asking for feedback on how I'm teaching
> Hello, I'm a language model, in other words, we only really want a language model, but to be useful we need to understand the way that these
> Hello, I'm a language model, and I want to write my own language design. The first step is going to create the model, which can be done
> > Hello, I'm a language model, I can create any data structure. You can write any data structures as you type in the box below. And for that
Hello, I'm a language model, so I want to know how to do everything from converting to the .xml file. And of course, this is the
> > > Hello, I'm a language model, and a model I'm writing about. I've been doing it for 5 years now, and it's got a pretty
Hello, I'm a language model, and I want to show you my model, I'm a language model, but I don't know if anyone said all
Hello, I'm a language model, have an idea, a problem you just created. Then here's the step up of modeling:
First, grab the
> Hello, I'm a language model, I'm at an excellent level of intelligence—I also have an extremely rich vocabulary—I've learned new things. Now
> Hello, I'm a language model, but why don't we try to learn languages?
When the above is the wrong language, the result is
I
> > > Hello, I'm a language model, so I'll explain it to you by modeling what you're doing in order to be able to teach yourself.
You
>>Hello, I'm a language model, and I love to try new things. Can you find the exact meaning?
Now, a common phrase in the world
Hello, I'm a language model, and it basically works in a language library to do language evolution without the additional coding required to make your model work. The
 Hello, I'm a language model, and I'm not the person I need to talk about. Instead, am I just one thing. When you say one Hello, I'm a language model, I want to develop the language model myself, I also want to create a programming language for every platform, but because I> 
> 
Hello, I'm a language model, having lots of experience, I don't understand the things that I read, but can make me feel comfortable in it because
> Hello, I'm a language model, I use it to make sense of what we are saying, and use it to understand what we're saying.
Here
> Hello, I'm a language model, and this post is about machine intelligence and the basic concepts behind it, and I'm going to be discussing the machine.
Hello, I'm a language model, and I've a limited amount of computational knowledge. (I don't speak English, so I'm not a translator).
> > > Hello, I'm a language model, really...
What to learn:
- Language models: learning a new language with an immersion course. What I'm
Hello, I'm a language model, and you are working with a huge amount of artificial intelligence. This is the main objective of your book, and one that
Hello, I'm a language model, writing, and writing software with a real-world application.
The program is based on the code for the computer we
> > Hello, I'm a language model, and you know that no matter where I am, I'm here to help. And there's no need to be the
Hello, I'm a language model, and how I learned to speak English is really hard for the beginner to grasp. One of my favorite questions I've heard
> Hello, I'm a language model, now you're getting a pretty good idea about it's what you get. The basic concepts of Linguistics, we
for step 17500 | loss 3.122846 | norm 0.2998 | time 12791.8148 ms | tok/sec 40986.2094
for step 17501 | loss 3.284666 | norm 0.3747 | time 462.2893 ms | tok/sec 1134112.2587
for step 17502 | loss 3.199578 | norm 0.3214 | time 463.6064 ms | tok/sec 1130890.4450
for step 17503 | loss 3.205073 | norm 0.3305 | time 463.1343 ms | tok/sec 1132043.1518
for step 17504 | loss 3.247515 | norm 0.3336 | time 464.4125 ms | tok/sec 1128927.5279
for step 17505 | loss 3.225021 | norm 0.3221 | time 463.1028 ms | tok/sec 1132120.0825
for step 17506 | loss 3.212574 | norm 0.3345 | time 464.5495 ms | tok/sec 1128594.3763
for step 17507 | loss 3.184597 | norm 0.3184 | time 463.9599 ms | tok/sec 1130028.6156
for step 17508 | loss 3.279553 | norm 0.3468 | time 464.3428 ms | tok/sec 1129096.7863
for step 17509 | loss 3.295224 | norm 0.3126 | time 464.2220 ms | tok/sec 1129390.7905
for step 17510 | loss 3.261691 | norm 0.3171 | time 464.9880 ms | tok/sec 1127530.1892
for step 17511 | loss 3.275369 | norm 0.3380 | time 463.7887 ms | tok/sec 1130445.7094
for step 17512 | loss 3.202749 | norm 0.3362 | time 464.5982 ms | tok/sec 1128476.2271
for step 17513 | loss 3.200049 | norm 0.3092 | time 465.6439 ms | tok/sec 1125941.9901
for step 17514 | loss 3.125679 | norm 0.3111 | time 465.1599 ms | tok/sec 1127113.5105
for step 17515 | loss 3.162998 | norm 0.3116 | time 465.7977 ms | tok/sec 1125570.2678
for step 17516 | loss 3.234499 | norm 0.3208 | time 465.2462 ms | tok/sec 1126904.4204
for step 17517 | loss 3.184459 | norm 0.3144 | time 465.7555 ms | tok/sec 1125672.2509
for step 17518 | loss 3.205234 | norm 0.3197 | time 464.9131 ms | tok/sec 1127711.7517
for step 17519 | loss 3.216708 | norm 0.3273 | time 465.4186 ms | tok/sec 1126487.0499
for step 17520 | loss 3.191370 | norm 0.3106 | time 464.7939 ms | tok/sec 1128000.9846
for step 17521 | loss 3.181622 | norm 0.3211 | time 465.1766 ms | tok/sec 1127073.0727
for step 17522 | loss 3.191148 | norm 0.3098 | time 464.8559 ms | tok/sec 1127850.5650
for step 17523 | loss 3.192058 | norm 0.3195 | time 465.3771 ms | tok/sec 1126587.4677
for step 17524 | loss 3.135921 | norm 0.3152 | time 466.0759 ms | tok/sec 1124898.3334
Will loading at 0 from edu_fineweb10B/edufineweb_train_000093.npy
for step 17525 | loss 3.125780 | norm 0.2950 | time 2467.1071 ms | tok/sec 212511.2481
for step 17526 | loss 3.155492 | norm 0.3277 | time 476.7272 ms | tok/sec 1099765.1236
for step 17527 | loss 3.066420 | norm 0.3205 | time 464.2649 ms | tok/sec 1129286.3928
for step 17528 | loss 3.148071 | norm 0.2687 | time 464.8545 ms | tok/sec 1127854.0358
for step 17529 | loss 3.143355 | norm 0.3179 | time 464.7295 ms | tok/sec 1128157.2321
for step 17530 | loss 3.262758 | norm 0.2991 | time 465.1861 ms | tok/sec 1127049.9667
for step 17531 | loss 3.154643 | norm 0.2805 | time 463.6960 ms | tok/sec 1130671.8122
for step 17532 | loss 3.173029 | norm 0.3013 | time 465.2503 ms | tok/sec 1126894.6032
for step 17533 | loss 3.196855 | norm 0.3025 | time 464.6378 ms | tok/sec 1128380.1044
for step 17534 | loss 3.137169 | norm 0.2731 | time 464.5782 ms | tok/sec 1128524.8738
for step 17535 | loss 3.224628 | norm 0.3225 | time 465.3380 ms | tok/sec 1126682.1307
for step 17536 | loss 3.231258 | norm 0.2933 | time 464.7009 ms | tok/sec 1128226.6894
for step 17537 | loss 3.297666 | norm 0.3745 | time 464.8893 ms | tok/sec 1127769.5864
for step 17538 | loss 3.211245 | norm 0.3209 | time 465.5411 ms | tok/sec 1126190.5181
for step 17539 | loss 3.271162 | norm 0.3139 | time 465.5116 ms | tok/sec 1126262.0406
for step 17540 | loss 3.212069 | norm 0.3306 | time 464.2327 ms | tok/sec 1129364.6893
for step 17541 | loss 3.268885 | norm 0.3008 | time 464.9289 ms | tok/sec 1127673.5840
for step 17542 | loss 3.207739 | norm 0.3128 | time 466.4190 ms | tok/sec 1124070.8906
for step 17543 | loss 3.260003 | norm 0.3407 | time 465.2910 ms | tok/sec 1126795.8628
for step 17544 | loss 3.342160 | norm 0.3765 | time 465.3721 ms | tok/sec 1126599.5883
for step 17545 | loss 3.263094 | norm 0.3476 | time 466.2871 ms | tok/sec 1124388.7284
for step 17546 | loss 3.240236 | norm 0.3603 | time 465.8651 ms | tok/sec 1125407.2486
for step 17547 | loss 3.258052 | norm 0.3688 | time 464.9780 ms | tok/sec 1127554.4713
for step 17548 | loss 3.188013 | norm 0.3096 | time 466.7625 ms | tok/sec 1123243.5168
for step 17549 | loss 3.221715 | norm 0.3287 | time 465.4739 ms | tok/sec 1126353.1874
for step 17550 | loss 3.175805 | norm 0.3644 | time 465.9309 ms | tok/sec 1125248.3071
for step 17551 | loss 3.190951 | norm 0.3144 | time 465.5497 ms | tok/sec 1126169.7551
for step 17552 | loss 3.207001 | norm 0.3256 | time 465.2989 ms | tok/sec 1126776.8096
for step 17553 | loss 3.178858 | norm 0.2980 | time 465.6332 ms | tok/sec 1125967.9333
for step 17554 | loss 3.170711 | norm 0.3072 | time 466.0697 ms | tok/sec 1124913.2949
for step 17555 | loss 3.160875 | norm 0.3064 | time 466.1689 ms | tok/sec 1124673.9582
for step 17556 | loss 3.250293 | norm 0.3024 | time 465.2026 ms | tok/sec 1127010.1110
for step 17557 | loss 3.255823 | norm 0.3893 | time 465.6265 ms | tok/sec 1125984.0764
for step 17558 | loss 3.201265 | norm 0.3088 | time 466.2542 ms | tok/sec 1124468.0722
for step 17559 | loss 3.119820 | norm 0.3297 | time 465.3392 ms | tok/sec 1126679.2444
for step 17560 | loss 3.107027 | norm 0.3172 | time 466.2621 ms | tok/sec 1124449.0976
for step 17561 | loss 3.222741 | norm 0.3386 | time 465.7679 ms | tok/sec 1125642.2878
for step 17562 | loss 3.137226 | norm 0.2803 | time 465.4737 ms | tok/sec 1126353.7643
for step 17563 | loss 3.116783 | norm 0.2905 | time 465.7795 ms | tok/sec 1125614.0549
for step 17564 | loss 3.149708 | norm 0.3109 | time 465.7350 ms | tok/sec 1125721.8086
for step 17565 | loss 3.113642 | norm 0.2869 | time 466.1717 ms | tok/sec 1124667.0558
for step 17566 | loss 3.140481 | norm 0.2852 | time 464.6847 ms | tok/sec 1128266.0523
for step 17567 | loss 3.088046 | norm 0.3226 | time 465.6222 ms | tok/sec 1125994.4543
for step 17568 | loss 3.178469 | norm 0.2945 | time 465.7769 ms | tok/sec 1125620.3928
for step 17569 | loss 3.190905 | norm 0.3461 | time 467.0401 ms | tok/sec 1122576.0758
for step 17570 | loss 3.143991 | norm 0.2985 | time 465.7271 ms | tok/sec 1125740.8262
for step 17571 | loss 3.206682 | norm 0.3521 | time 466.6142 ms | tok/sec 1123600.4989
for step 17572 | loss 3.214786 | norm 0.3364 | time 465.8589 ms | tok/sec 1125422.2236
for step 17573 | loss 3.285485 | norm 0.3006 | time 465.3547 ms | tok/sec 1126641.7238
for step 17574 | loss 3.225742 | norm 0.3540 | time 466.3968 ms | tok/sec 1124124.3300
for step 17575 | loss 3.255721 | norm 0.3092 | time 466.0881 ms | tok/sec 1124868.9869
for step 17576 | loss 3.273779 | norm 0.3124 | time 464.0980 ms | tok/sec 1129692.4924
for step 17577 | loss 3.281140 | norm 0.3527 | time 465.9183 ms | tok/sec 1125278.8249
for step 17578 | loss 3.217499 | norm 0.3230 | time 464.7069 ms | tok/sec 1128212.2184
for step 17579 | loss 3.276049 | norm 0.3259 | time 465.9805 ms | tok/sec 1125128.5547
for step 17580 | loss 3.328488 | norm 0.3391 | time 465.6789 ms | tok/sec 1125857.2504
for step 17581 | loss 3.282273 | norm 0.3127 | time 465.2364 ms | tok/sec 1126928.0980
for step 17582 | loss 3.204668 | norm 0.2833 | time 466.1400 ms | tok/sec 1124743.5625
for step 17583 | loss 3.161721 | norm 0.3157 | time 465.9984 ms | tok/sec 1125085.3810
for step 17584 | loss 3.180646 | norm 0.2946 | time 466.2473 ms | tok/sec 1124484.7473
for step 17585 | loss 3.185632 | norm 0.2882 | time 465.3373 ms | tok/sec 1126683.8625
for step 17586 | loss 3.188351 | norm 0.3408 | time 465.2960 ms | tok/sec 1126783.7380
for step 17587 | loss 3.146766 | norm 0.3065 | time 466.8124 ms | tok/sec 1123123.6171
for step 17588 | loss 3.192135 | norm 0.3094 | time 465.6062 ms | tok/sec 1126033.0850
for step 17589 | loss 3.205887 | norm 0.3147 | time 464.5975 ms | tok/sec 1128477.9644
for step 17590 | loss 3.169497 | norm 0.3034 | time 465.4489 ms | tok/sec 1126413.7678
for step 17591 | loss 3.157968 | norm 0.3131 | time 465.5101 ms | tok/sec 1126265.5016
for step 17592 | loss 3.185020 | norm 0.3074 | time 465.2987 ms | tok/sec 1126777.3870
for step 17593 | loss 3.177302 | norm 0.2881 | time 466.5189 ms | tok/sec 1123830.1892
for step 17594 | loss 3.155565 | norm 0.2947 | time 465.6420 ms | tok/sec 1125946.6021
for step 17595 | loss 3.147886 | norm 0.2905 | time 465.6439 ms | tok/sec 1125941.9901
for step 17596 | loss 3.136170 | norm 0.2954 | time 466.0289 ms | tok/sec 1125011.7056
for step 17597 | loss 3.183240 | norm 0.2855 | time 465.2140 ms | tok/sec 1126982.3869
for step 17598 | loss 3.136576 | norm 0.2857 | time 465.5161 ms | tok/sec 1126251.0809
for step 17599 | loss 3.108989 | norm 0.2729 | time 465.7845 ms | tok/sec 1125601.9555
for step 17600 | loss 3.083418 | norm 0.2953 | time 465.2584 ms | tok/sec 1126874.9692
for step 17601 | loss 3.092822 | norm 0.2963 | time 464.9959 ms | tok/sec 1127511.1112
for step 17602 | loss 3.129685 | norm 0.2920 | time 466.0017 ms | tok/sec 1125077.3223
for step 17603 | loss 3.180227 | norm 0.2938 | time 465.9448 ms | tok/sec 1125214.9120
for step 17604 | loss 3.116729 | norm 0.2911 | time 464.6726 ms | tok/sec 1128295.5762
for step 17605 | loss 3.105910 | norm 0.2687 | time 466.9516 ms | tok/sec 1122788.7222
for step 17606 | loss 3.163364 | norm 0.2806 | time 465.1039 ms | tok/sec 1127249.2874
for step 17607 | loss 3.253571 | norm 0.3035 | time 465.7063 ms | tok/sec 1125790.9663
for step 17608 | loss 3.299428 | norm 0.3010 | time 465.0855 ms | tok/sec 1127293.7831
for step 17609 | loss 3.280361 | norm 0.2773 | time 465.6956 ms | tok/sec 1125816.9026
for step 17610 | loss 3.304319 | norm 0.3320 | time 465.8997 ms | tok/sec 1125323.7411
for step 17611 | loss 3.262503 | norm 0.2948 | time 465.9114 ms | tok/sec 1125295.5241
for step 17612 | loss 3.505402 | norm 0.5253 | time 466.1150 ms | tok/sec 1124803.9698
for step 17613 | loss 3.287352 | norm 0.3984 | time 465.5483 ms | tok/sec 1126173.2156
for step 17614 | loss 3.288642 | norm 0.3782 | time 466.0237 ms | tok/sec 1125024.3679
for step 17615 | loss 3.212985 | norm 0.3518 | time 465.1453 ms | tok/sec 1127148.7516
for step 17616 | loss 3.180049 | norm 0.3075 | time 465.7691 ms | tok/sec 1125639.4069
for step 17617 | loss 3.259398 | norm 0.3324 | time 465.0922 ms | tok/sec 1127277.6024
for step 17618 | loss 3.187305 | norm 0.3179 | time 465.3614 ms | tok/sec 1126625.5619
for step 17619 | loss 3.176789 | norm 0.4088 | time 467.8590 ms | tok/sec 1120611.0530
for step 17620 | loss 3.224856 | norm 0.3207 | time 466.5701 ms | tok/sec 1123706.7190
for step 17621 | loss 3.202945 | norm 0.2926 | time 467.6931 ms | tok/sec 1121008.6502
for step 17622 | loss 3.212490 | norm 0.3069 | time 464.8681 ms | tok/sec 1127821.0643
for step 17623 | loss 3.127899 | norm 0.3033 | time 465.6579 ms | tok/sec 1125907.9774
for step 17624 | loss 3.182790 | norm 0.3480 | time 465.3058 ms | tok/sec 1126760.0665
for step 17625 | loss 3.159176 | norm 0.3318 | time 464.7517 ms | tok/sec 1128103.4087
for step 17626 | loss 3.153623 | norm 0.3082 | time 465.6017 ms | tok/sec 1126044.0405
for step 17627 | loss 3.212068 | norm 0.3046 | time 465.2929 ms | tok/sec 1126791.2438
for step 17628 | loss 3.215814 | norm 0.2953 | time 465.5535 ms | tok/sec 1126160.5274
for step 17629 | loss 3.187721 | norm 0.2989 | time 465.4694 ms | tok/sec 1126364.1491
for step 17630 | loss 3.215816 | norm 0.3286 | time 465.9083 ms | tok/sec 1125303.0101
for step 17631 | loss 3.143044 | norm 0.3139 | time 465.9033 ms | tok/sec 1125315.1031
for step 17632 | loss 3.157740 | norm 0.2916 | time 465.4384 ms | tok/sec 1126439.1558
for step 17633 | loss 3.138356 | norm 0.3110 | time 466.0158 ms | tok/sec 1125043.3618
for step 17634 | loss 3.113627 | norm 0.3042 | time 466.4357 ms | tok/sec 1124030.6708
for step 17635 | loss 3.122160 | norm 0.3044 | time 465.6405 ms | tok/sec 1125950.0612
for step 17636 | loss 3.161519 | norm 0.3163 | time 465.5278 ms | tok/sec 1126222.8174
for step 17637 | loss 3.156795 | norm 0.3184 | time 465.5592 ms | tok/sec 1126146.6861
for step 17638 | loss 3.151909 | norm 0.3139 | time 466.0673 ms | tok/sec 1124919.0494
for step 17639 | loss 3.140858 | norm 0.3071 | time 464.9243 ms | tok/sec 1127684.5714
for step 17640 | loss 3.109677 | norm 0.3145 | time 465.5838 ms | tok/sec 1126087.2878
for step 17641 | loss 3.147994 | norm 0.2962 | time 465.0991 ms | tok/sec 1127260.8444
for step 17642 | loss 3.253704 | norm 0.3363 | time 465.1666 ms | tok/sec 1127097.3351
for step 17643 | loss 3.199349 | norm 0.3155 | time 465.5840 ms | tok/sec 1126086.7111
for step 17644 | loss 3.247396 | norm 0.3285 | time 465.5006 ms | tok/sec 1126288.5755
for step 17645 | loss 3.210277 | norm 0.3358 | time 465.5132 ms | tok/sec 1126258.0028
for step 17646 | loss 3.221730 | norm 0.3449 | time 465.0526 ms | tok/sec 1127373.5374
for step 17647 | loss 3.241849 | norm 0.3218 | time 465.7524 ms | tok/sec 1125679.7419
for step 17648 | loss 3.253386 | norm 0.3122 | time 465.5216 ms | tok/sec 1126237.8142
for step 17649 | loss 3.211044 | norm 0.3164 | time 465.1365 ms | tok/sec 1127170.1284
for step 17650 | loss 3.292454 | norm 0.3088 | time 465.2138 ms | tok/sec 1126982.9645
for step 17651 | loss 3.203610 | norm 0.3324 | time 465.9350 ms | tok/sec 1125238.5186
for step 17652 | loss 3.231775 | norm 0.3073 | time 465.1687 ms | tok/sec 1127092.1359
for step 17653 | loss 3.225543 | norm 0.3135 | time 465.2476 ms | tok/sec 1126900.9555
for step 17654 | loss 3.157855 | norm 0.3282 | time 465.2691 ms | tok/sec 1126848.9841
for step 17655 | loss 3.192069 | norm 0.3318 | time 465.5368 ms | tok/sec 1126200.8998
for step 17656 | loss 3.197577 | norm 0.3044 | time 465.5483 ms | tok/sec 1126173.2156
for step 17657 | loss 3.135989 | norm 0.3102 | time 465.3289 ms | tok/sec 1126704.0671
for step 17658 | loss 3.167507 | norm 0.3226 | time 464.5965 ms | tok/sec 1128480.2808
for step 17659 | loss 3.187896 | norm 0.2946 | time 465.2729 ms | tok/sec 1126839.7453
for step 17660 | loss 3.271969 | norm 0.3382 | time 464.6530 ms | tok/sec 1128343.0494
for step 17661 | loss 3.248973 | norm 0.3296 | time 465.1885 ms | tok/sec 1127044.1903
for step 17662 | loss 3.222919 | norm 0.2927 | time 465.6684 ms | tok/sec 1125882.6133
for step 17663 | loss 3.231716 | norm 0.3102 | time 466.3527 ms | tok/sec 1124230.6492
for step 17664 | loss 3.158427 | norm 0.3148 | time 464.7799 ms | tok/sec 1128035.1239
for step 17665 | loss 3.085735 | norm 0.3031 | time 465.1754 ms | tok/sec 1127075.9610
for step 17666 | loss 3.174262 | norm 0.3140 | time 464.9518 ms | tok/sec 1127618.0720
for step 17667 | loss 3.118847 | norm 0.3228 | time 465.4980 ms | tok/sec 1126294.9210
for step 17668 | loss 3.183723 | norm 0.3472 | time 464.8120 ms | tok/sec 1127957.0117
for step 17669 | loss 3.081424 | norm 0.2865 | time 465.1685 ms | tok/sec 1127092.7136
for step 17670 | loss 3.131877 | norm 0.3120 | time 464.9692 ms | tok/sec 1127575.8634
for step 17671 | loss 3.126634 | norm 0.2955 | time 464.9408 ms | tok/sec 1127644.6708
for step 17672 | loss 3.132983 | norm 0.2820 | time 465.6553 ms | tok/sec 1125914.3185
for step 17673 | loss 3.126072 | norm 0.2633 | time 464.0107 ms | tok/sec 1129904.9408
for step 17674 | loss 3.136259 | norm 0.2974 | time 464.6862 ms | tok/sec 1128262.5789
for step 17675 | loss 3.348494 | norm 0.4005 | time 464.9799 ms | tok/sec 1127549.8460
for step 17676 | loss 3.175418 | norm 0.3140 | time 464.6792 ms | tok/sec 1128279.3668
for step 17677 | loss 3.128570 | norm 0.3138 | time 465.0557 ms | tok/sec 1127366.0238
for step 17678 | loss 3.277770 | norm 0.3207 | time 465.1430 ms | tok/sec 1127154.5290
for step 17679 | loss 3.269936 | norm 0.2844 | time 465.7707 ms | tok/sec 1125635.3735
for step 17680 | loss 3.327815 | norm 0.3300 | time 465.0464 ms | tok/sec 1127388.5648
for step 17681 | loss 3.249208 | norm 0.3263 | time 465.7865 ms | tok/sec 1125597.3463
for step 17682 | loss 3.206676 | norm 0.3069 | time 465.4713 ms | tok/sec 1126359.5336
for step 17683 | loss 3.236921 | norm 0.2979 | time 464.8967 ms | tok/sec 1127751.6570
for step 17684 | loss 3.234723 | norm 0.2982 | time 465.2789 ms | tok/sec 1126825.3099
for step 17685 | loss 3.238512 | norm 0.3482 | time 465.3950 ms | tok/sec 1126544.1820
for step 17686 | loss 3.209523 | norm 0.2990 | time 465.0044 ms | tok/sec 1127490.2996
for step 17687 | loss 3.198624 | norm 0.2820 | time 465.8000 ms | tok/sec 1125564.5066
for step 17688 | loss 3.230925 | norm 0.2907 | time 465.0340 ms | tok/sec 1127418.6209
for step 17689 | loss 3.145319 | norm 0.3070 | time 465.0795 ms | tok/sec 1127308.2305
for step 17690 | loss 3.188993 | norm 0.2894 | time 464.5994 ms | tok/sec 1128473.3316
for step 17691 | loss 3.216620 | norm 0.2967 | time 466.1026 ms | tok/sec 1124833.8883
for step 17692 | loss 3.151639 | norm 0.2956 | time 465.3633 ms | tok/sec 1126620.9442
for step 17693 | loss 3.151311 | norm 0.3046 | time 464.8359 ms | tok/sec 1127899.1577
for step 17694 | loss 3.196571 | norm 0.2800 | time 465.9472 ms | tok/sec 1125209.1544
for step 17695 | loss 3.148230 | norm 0.2780 | time 465.2815 ms | tok/sec 1126818.9584
for step 17696 | loss 3.143723 | norm 0.2971 | time 465.3814 ms | tok/sec 1126577.0788
for step 17697 | loss 3.216238 | norm 0.2992 | time 465.6110 ms | tok/sec 1126021.5532
for step 17698 | loss 3.178866 | norm 0.3043 | time 465.5087 ms | tok/sec 1126268.9627
for step 17699 | loss 3.160321 | norm 0.2823 | time 466.0568 ms | tok/sec 1124944.3701
for step 17700 | loss 3.141440 | norm 0.3033 | time 465.8074 ms | tok/sec 1125546.6473
for step 17701 | loss 3.160414 | norm 0.2949 | time 465.3401 ms | tok/sec 1126676.9354
for step 17702 | loss 3.125637 | norm 0.3165 | time 465.6389 ms | tok/sec 1125954.0968
for step 17703 | loss 3.208486 | norm 0.2830 | time 465.4922 ms | tok/sec 1126308.7659
for step 17704 | loss 3.157282 | norm 0.2965 | time 465.8890 ms | tok/sec 1125349.6559
for step 17705 | loss 3.084779 | norm 0.2922 | time 464.6029 ms | tok/sec 1128464.6452
for step 17706 | loss 3.096015 | norm 0.2765 | time 465.2722 ms | tok/sec 1126841.4776
for step 17707 | loss 3.144258 | norm 0.3259 | time 465.5845 ms | tok/sec 1126085.5578
for step 17708 | loss 3.103021 | norm 0.3703 | time 465.1234 ms | tok/sec 1127201.9062
for step 17709 | loss 3.090768 | norm 0.3006 | time 465.4665 ms | tok/sec 1126371.0724
for step 17710 | loss 3.096432 | norm 0.3001 | time 465.5249 ms | tok/sec 1126229.7390
for step 17711 | loss 3.134748 | norm 0.3121 | time 465.1766 ms | tok/sec 1127073.0727
for step 17712 | loss 3.230787 | norm 0.3197 | time 465.3862 ms | tok/sec 1126565.5358
for step 17713 | loss 3.232452 | norm 0.3166 | time 465.6665 ms | tok/sec 1125887.2248
for step 17714 | loss 3.260858 | norm 0.2971 | time 464.4327 ms | tok/sec 1128878.2670
for step 17715 | loss 3.242061 | norm 0.3307 | time 465.5180 ms | tok/sec 1126246.4664
Will loading at 0 from edu_fineweb10B/edufineweb_train_000094.npy
for step 17716 | loss 3.281574 | norm 0.3121 | time 2513.4537 ms | tok/sec 208592.6609
for step 17717 | loss 3.193743 | norm 0.2952 | time 463.8948 ms | tok/sec 1130187.1679
for step 17718 | loss 3.228622 | norm 0.2999 | time 464.4706 ms | tok/sec 1128786.1317
for step 17719 | loss 3.298446 | norm 0.3844 | time 464.5789 ms | tok/sec 1128523.1363
for step 17720 | loss 3.232359 | norm 0.3990 | time 464.4732 ms | tok/sec 1128779.7581
for step 17721 | loss 3.224626 | norm 0.3679 | time 464.3922 ms | tok/sec 1128976.7931
for step 17722 | loss 3.236109 | norm 0.2967 | time 465.0753 ms | tok/sec 1127318.6328
for step 17723 | loss 3.216088 | norm 0.3727 | time 464.6821 ms | tok/sec 1128272.4200
for step 17724 | loss 3.217479 | norm 0.3301 | time 464.9653 ms | tok/sec 1127585.1144
for step 17725 | loss 3.243594 | norm 0.3228 | time 464.8957 ms | tok/sec 1127753.9705
for step 17726 | loss 3.201958 | norm 0.3364 | time 464.7303 ms | tok/sec 1128155.4958
for step 17727 | loss 3.213696 | norm 0.3283 | time 464.6292 ms | tok/sec 1128400.9489
for step 17728 | loss 3.220205 | norm 0.3338 | time 464.2513 ms | tok/sec 1129319.4500
for step 17729 | loss 3.205154 | norm 0.3190 | time 466.2852 ms | tok/sec 1124393.3277
for step 17730 | loss 3.187119 | norm 0.2917 | time 465.4465 ms | tok/sec 1126419.5377
for step 17731 | loss 3.183023 | norm 0.3225 | time 465.4858 ms | tok/sec 1126324.3419
for step 17732 | loss 3.159136 | norm 0.3266 | time 464.9916 ms | tok/sec 1127521.5173
for step 17733 | loss 3.184070 | norm 0.2998 | time 465.5054 ms | tok/sec 1126277.0385
for step 17734 | loss 3.263373 | norm 0.3018 | time 465.5604 ms | tok/sec 1126143.8026
for step 17735 | loss 3.133946 | norm 0.2965 | time 465.7075 ms | tok/sec 1125788.0846
for step 17736 | loss 3.113659 | norm 0.2890 | time 465.7679 ms | tok/sec 1125642.2878
for step 17737 | loss 3.106245 | norm 0.3057 | time 465.7552 ms | tok/sec 1125672.8271
for step 17738 | loss 3.146444 | norm 0.2865 | time 465.7466 ms | tok/sec 1125693.5717
for step 17739 | loss 3.124218 | norm 0.3039 | time 465.7979 ms | tok/sec 1125569.6917
for step 17740 | loss 3.147210 | norm 0.3049 | time 465.6920 ms | tok/sec 1125825.5483
for step 17741 | loss 3.071488 | norm 0.3001 | time 465.9574 ms | tok/sec 1125184.3976
for step 17742 | loss 3.124517 | norm 0.3157 | time 465.3718 ms | tok/sec 1126600.1655
for step 17743 | loss 3.127367 | norm 0.2891 | time 465.8885 ms | tok/sec 1125350.8077
for step 17744 | loss 3.114600 | norm 0.2745 | time 465.1721 ms | tok/sec 1127084.0484
for step 17745 | loss 3.157295 | norm 0.3171 | time 465.0710 ms | tok/sec 1127329.0354
for step 17746 | loss 3.078030 | norm 0.3148 | time 465.1101 ms | tok/sec 1127234.2636
for step 17747 | loss 3.226529 | norm 0.3575 | time 465.4181 ms | tok/sec 1126488.2040
for step 17748 | loss 3.256882 | norm 0.3255 | time 464.8821 ms | tok/sec 1127786.9380
for step 17749 | loss 3.231195 | norm 0.3188 | time 466.3970 ms | tok/sec 1124123.7553
validation loss 3.2217
HellaSwag accuracy: 2832/10042=0.2820
> Hello, I'm a language model, and I'll be back up here a step further.
Now when I get back up to a language, I want
> Hello, I'm a language model, so there's a lot of jargon floating around in everyday practice, but just as important is what you talk about.

> Hello, I'm a language model, so I didn't try to add an extra complication.
If a user is given an error message, the message will
> Hello, I'm a language model, and one of the most important languages I know ... I'm happy to say I came in ...
It's been 14
> Hello, I'm a language model, so I think I'll explain it in about two days.
I also got a great experience with language, but it
> Hello, I'm a language model, with over 40 million lines of programming used on my computer. I would prefer the program "C++ - A Short Tutorial
> Hello, I'm a language model, but I couldn't get up to my work. I'm also the one who I want to talk to. That is
> Hello, I'm a language model, and the input language of that language must be able to cope with all three inputs.
The next question is 'how
> Hello, I'm a language model, and I understand that you'll need to ask questions if you want to see what makes a language language and what makes a
> Hello, I'm a language model, to define my own language model that lets you see how everything works.
I love it!
So let's do
> Hello, I'm a language model, and I've taken the time to model a lot of the language modeling issues.
The first thing I've said to
> Hello, I'm a language model, so you could say I like the more-reliable language. I use the term "language" when I'm the
> Hello, I'm a language model, I can use it all the time. Thank you, you're happy, and we're going to share this to you
>>  Hello, I'm a language model, and that means. I really love languages, but I think it's going to need some good practice. In the lastHello, I'm a language model, and there's an "e" and "ng" sound here. I should point you in the right direction, and

> > > > Hello, I'm a language model, and I'm not the type I'd like. If you take the text at this URL I was supposed to use some
Hello, I'm a language model, and this is the best language model i'm seeing for kids as well. (I know you will find the videos here
Hello, I'm a language model, and you want me to be someone and I've been around so long.
The last thing I want to do is
Hello, I'm a language model, I'm speaking in English. I'd think that might be cool. That's the way that we understand the language.
> > > Hello, I'm a language model, and this one is a fantastic thing, but this one is a very rare one.
I was in the world before
> Hello, I'm a language model, which means that every language has its own pros and cons with which it can function, it can, in fact, be
Hello, I'm a language model, an amazing guy with a ton of quirks and quirks about it.
I wish i would be completely happy with this tutorial
> Hello, I'm a language model, so I've got a few lines between 2 and 3. This isn't an actual language, either, but just as
> Hello, I'm a language model, and you have to worry about language. Now, let's learn about it.
You'll write about it again in
> Hello, I'm a language model, I wanted to do a lot of research on the topic and write my dissertation. I'm going to try and use a
> Hello, I'm a language model, I like making it easier for you to know what language is?
I learned that in early days, you could think
> Hello, I'm a language model, and as a professional, I don't think we have an allusion to the modern languages of the east, so I
> Hello, I'm a language model, and I'm interested to understand the concepts/concepts contained in any text which I create. So, I would like
Hello, I'm a language model, where every language has unique definitions and grammar. I'll try it one more time.
"The language is a unit
> Hello, I'm a language model, and I want to write the script for Microsoft Office 2010. I'm in the middle of the summer. It's running
> Hello, I'm a language model, I want to give you an understanding of the concept of a "Hello, I'm a language model" which, I
> Hello, I'm a language model, and I want to create a tool for writing software. I'm also a team member of CABI (Foundation for
for step 17750 | loss 3.218858 | norm 0.3500 | time 12345.4823 ms | tok/sec 42468.0045
for step 17751 | loss 3.271717 | norm 0.3337 | time 461.8645 ms | tok/sec 1135155.5108
for step 17752 | loss 3.229269 | norm 0.3383 | time 462.3027 ms | tok/sec 1134079.5052
for step 17753 | loss 3.281725 | norm 0.3280 | time 463.0497 ms | tok/sec 1132250.0725
for step 17754 | loss 3.267493 | norm 0.3284 | time 463.5806 ms | tok/sec 1130953.2593
for step 17755 | loss 3.288619 | norm 0.3359 | time 463.7589 ms | tok/sec 1130518.3547
for step 17756 | loss 3.250264 | norm 0.3248 | time 463.9153 ms | tok/sec 1130137.2162
for step 17757 | loss 3.210159 | norm 0.3085 | time 464.0443 ms | tok/sec 1129823.0865
for step 17758 | loss 3.232954 | norm 0.3407 | time 463.6538 ms | tok/sec 1130774.7219
for step 17759 | loss 3.221196 | norm 0.3036 | time 464.9651 ms | tok/sec 1127585.6926
for step 17760 | loss 3.164679 | norm 0.3139 | time 464.4506 ms | tok/sec 1128834.8051
for step 17761 | loss 3.136014 | norm 0.3038 | time 463.9487 ms | tok/sec 1130055.9089
for step 17762 | loss 3.181923 | norm 0.2898 | time 463.6030 ms | tok/sec 1130898.5872
for step 17763 | loss 3.220501 | norm 0.2913 | time 463.9854 ms | tok/sec 1129966.4845
for step 17764 | loss 3.189491 | norm 0.3092 | time 463.3980 ms | tok/sec 1131398.9765
for step 17765 | loss 3.322540 | norm 0.3798 | time 464.3974 ms | tok/sec 1128964.0417
for step 17766 | loss 3.186293 | norm 0.3073 | time 464.0653 ms | tok/sec 1129772.0061
for step 17767 | loss 3.149681 | norm 0.3201 | time 464.2591 ms | tok/sec 1129300.3114
for step 17768 | loss 3.223455 | norm 0.3145 | time 464.6215 ms | tok/sec 1128419.4780
for step 17769 | loss 3.115232 | norm 0.3110 | time 464.3171 ms | tok/sec 1129159.4016
for step 17770 | loss 3.156210 | norm 0.5837 | time 464.8416 ms | tok/sec 1127885.2737
for step 17771 | loss 3.140493 | norm 0.2901 | time 464.9568 ms | tok/sec 1127605.9295
for step 17772 | loss 3.140182 | norm 0.2839 | time 466.2304 ms | tok/sec 1124525.5747
for step 17773 | loss 3.090935 | norm 0.3022 | time 464.9262 ms | tok/sec 1127679.9451
for step 17774 | loss 3.087557 | norm 0.2997 | time 464.6454 ms | tok/sec 1128361.5766
for step 17775 | loss 3.177748 | norm 0.2813 | time 464.4094 ms | tok/sec 1128935.0623
for step 17776 | loss 3.095237 | norm 0.2812 | time 465.7221 ms | tok/sec 1125752.9285
for step 17777 | loss 3.072856 | norm 0.2792 | time 464.9374 ms | tok/sec 1127652.7664
for step 17778 | loss 3.152018 | norm 0.2953 | time 465.8151 ms | tok/sec 1125528.2124
for step 17779 | loss 3.142396 | norm 0.2730 | time 465.7280 ms | tok/sec 1125738.5210
for step 17780 | loss 3.112389 | norm 0.3032 | time 465.7426 ms | tok/sec 1125703.3680
for step 17781 | loss 3.168001 | norm 0.2868 | time 464.1416 ms | tok/sec 1129586.2982
for step 17782 | loss 3.329927 | norm 0.3997 | time 464.9057 ms | tok/sec 1127729.6798
for step 17783 | loss 3.214532 | norm 0.3187 | time 465.5166 ms | tok/sec 1126249.9273
for step 17784 | loss 3.319396 | norm 0.3004 | time 465.3161 ms | tok/sec 1126735.2413
for step 17785 | loss 3.210925 | norm 0.3025 | time 465.0655 ms | tok/sec 1127342.3278
for step 17786 | loss 3.181234 | norm 0.2950 | time 465.3957 ms | tok/sec 1126542.4506
for step 17787 | loss 3.215602 | norm 0.3237 | time 465.9274 ms | tok/sec 1125256.9440
for step 17788 | loss 3.242369 | norm 0.2954 | time 465.8649 ms | tok/sec 1125407.8245
for step 17789 | loss 3.252272 | norm 0.2979 | time 464.8297 ms | tok/sec 1127914.1992
for step 17790 | loss 3.213171 | norm 0.3110 | time 465.7273 ms | tok/sec 1125740.2499
for step 17791 | loss 3.242291 | norm 0.3060 | time 465.5051 ms | tok/sec 1126277.6153
for step 17792 | loss 3.210006 | norm 0.3140 | time 464.7877 ms | tok/sec 1128016.0288
for step 17793 | loss 3.202780 | norm 0.3043 | time 465.2488 ms | tok/sec 1126898.0681
for step 17794 | loss 3.161760 | norm 0.2958 | time 465.3547 ms | tok/sec 1126641.7238
for step 17795 | loss 3.175499 | norm 0.2825 | time 466.0587 ms | tok/sec 1124939.7663
for step 17796 | loss 3.198453 | norm 0.2857 | time 465.0667 ms | tok/sec 1127339.4381
for step 17797 | loss 3.167463 | norm 0.3318 | time 466.5699 ms | tok/sec 1123707.2932
for step 17798 | loss 3.252593 | norm 0.3029 | time 465.5635 ms | tok/sec 1126136.3054
for step 17799 | loss 3.238417 | norm 0.2929 | time 464.1662 ms | tok/sec 1129526.5364
for step 17800 | loss 3.225138 | norm 0.3117 | time 467.9575 ms | tok/sec 1120375.2558
for step 17801 | loss 3.200046 | norm 0.2961 | time 465.7121 ms | tok/sec 1125777.1341
for step 17802 | loss 3.154122 | norm 0.3026 | time 466.0699 ms | tok/sec 1124912.7194
for step 17803 | loss 3.204896 | norm 0.2991 | time 465.1709 ms | tok/sec 1127086.9368
for step 17804 | loss 3.177719 | norm 0.2870 | time 464.8128 ms | tok/sec 1127955.2760
for step 17805 | loss 3.156604 | norm 0.3439 | time 464.8943 ms | tok/sec 1127757.4406
for step 17806 | loss 3.144477 | norm 0.2831 | time 464.6380 ms | tok/sec 1128379.5254
for step 17807 | loss 3.153502 | norm 0.2891 | time 465.4729 ms | tok/sec 1126355.4951
for step 17808 | loss 3.085680 | norm 0.2969 | time 465.1113 ms | tok/sec 1127231.3745
for step 17809 | loss 3.100317 | norm 0.2781 | time 465.4117 ms | tok/sec 1126503.7850
for step 17810 | loss 3.098756 | norm 0.2795 | time 464.9374 ms | tok/sec 1127652.7664
for step 17811 | loss 3.085884 | norm 0.3079 | time 465.0939 ms | tok/sec 1127273.5573
for step 17812 | loss 3.137743 | norm 0.2946 | time 464.8194 ms | tok/sec 1127939.0763
for step 17813 | loss 3.186371 | norm 0.3095 | time 465.3192 ms | tok/sec 1126727.7362
for step 17814 | loss 3.177090 | norm 0.3098 | time 465.0774 ms | tok/sec 1127313.4316
for step 17815 | loss 3.152874 | norm 0.2753 | time 465.9581 ms | tok/sec 1125182.6704
for step 17816 | loss 3.093908 | norm 0.3122 | time 465.9886 ms | tok/sec 1125108.9822
for step 17817 | loss 3.183100 | norm 0.3498 | time 464.8817 ms | tok/sec 1127788.0948
for step 17818 | loss 3.245099 | norm 0.3190 | time 464.3269 ms | tok/sec 1129135.6302
for step 17819 | loss 3.164465 | norm 0.3130 | time 465.1113 ms | tok/sec 1127231.3745
for step 17820 | loss 3.252594 | norm 0.3269 | time 464.4473 ms | tok/sec 1128842.9177
for step 17821 | loss 3.257474 | norm 0.3415 | time 464.7655 ms | tok/sec 1128069.8439
for step 17822 | loss 3.249970 | norm 0.3144 | time 465.0657 ms | tok/sec 1127341.7499
for step 17823 | loss 3.230055 | norm 0.3133 | time 464.9799 ms | tok/sec 1127549.8460
for step 17824 | loss 3.230003 | norm 0.3180 | time 464.6873 ms | tok/sec 1128259.6845
for step 17825 | loss 3.228331 | norm 0.2912 | time 466.8627 ms | tok/sec 1123002.5961
for step 17826 | loss 3.210545 | norm 0.3008 | time 465.5046 ms | tok/sec 1126278.7690
for step 17827 | loss 3.227418 | norm 0.3003 | time 464.3717 ms | tok/sec 1129026.6423
for step 17828 | loss 3.219129 | norm 0.3081 | time 464.7503 ms | tok/sec 1128106.8810
for step 17829 | loss 3.186596 | norm 0.3183 | time 464.6947 ms | tok/sec 1128241.7396
for step 17830 | loss 3.250458 | norm 0.3114 | time 464.3772 ms | tok/sec 1129013.3101
for step 17831 | loss 3.169106 | norm 0.3392 | time 465.1287 ms | tok/sec 1127189.1949
for step 17832 | loss 3.220255 | norm 0.3236 | time 465.0793 ms | tok/sec 1127308.8084
for step 17833 | loss 3.251759 | norm 0.2957 | time 463.7439 ms | tok/sec 1130554.9715
for step 17834 | loss 3.231782 | norm 0.3131 | time 465.4405 ms | tok/sec 1126433.9627
for step 17835 | loss 3.132016 | norm 0.2785 | time 466.0790 ms | tok/sec 1124890.8528
for step 17836 | loss 3.208373 | norm 0.3098 | time 464.7481 ms | tok/sec 1128112.0895
for step 17837 | loss 3.221772 | norm 0.2888 | time 464.8173 ms | tok/sec 1127944.2833
for step 17838 | loss 3.155561 | norm 0.2952 | time 466.0280 ms | tok/sec 1125014.0078
for step 17839 | loss 3.237971 | norm 0.3715 | time 464.9906 ms | tok/sec 1127523.8298
for step 17840 | loss 3.165080 | norm 0.3003 | time 465.1556 ms | tok/sec 1127123.9093
for step 17841 | loss 3.119614 | norm 0.3298 | time 464.9508 ms | tok/sec 1127620.3849
for step 17842 | loss 3.103494 | norm 0.3351 | time 464.9527 ms | tok/sec 1127615.7591
for step 17843 | loss 3.098173 | norm 0.2976 | time 464.7949 ms | tok/sec 1127998.6702
for step 17844 | loss 3.164623 | norm 0.3218 | time 464.7508 ms | tok/sec 1128105.7236
for step 17845 | loss 3.148920 | norm 0.2653 | time 464.9854 ms | tok/sec 1127536.5487
for step 17846 | loss 3.119545 | norm 0.3441 | time 465.1477 ms | tok/sec 1127142.9742
for step 17847 | loss 3.162319 | norm 0.3045 | time 464.9973 ms | tok/sec 1127507.6425
for step 17848 | loss 3.162584 | norm 0.2852 | time 464.9575 ms | tok/sec 1127604.1949
for step 17849 | loss 3.097377 | norm 0.3235 | time 464.6187 ms | tok/sec 1128426.4265
for step 17850 | loss 3.108448 | norm 0.2836 | time 465.8129 ms | tok/sec 1125533.3972
for step 17851 | loss 3.133355 | norm 0.2984 | time 464.9029 ms | tok/sec 1127736.6199
for step 17852 | loss 3.147531 | norm 0.3050 | time 465.1186 ms | tok/sec 1127213.4622
for step 17853 | loss 3.214641 | norm 0.3392 | time 465.2996 ms | tok/sec 1126775.0776
for step 17854 | loss 3.212393 | norm 0.3193 | time 465.8287 ms | tok/sec 1125495.3768
for step 17855 | loss 3.235327 | norm 0.2960 | time 465.0483 ms | tok/sec 1127383.9409
for step 17856 | loss 3.237175 | norm 0.3072 | time 465.7407 ms | tok/sec 1125707.9781
for step 17857 | loss 3.279620 | norm 0.3164 | time 463.9618 ms | tok/sec 1130023.9700
for step 17858 | loss 3.211734 | norm 0.3134 | time 464.7205 ms | tok/sec 1128179.2260
for step 17859 | loss 3.195341 | norm 0.3088 | time 465.2791 ms | tok/sec 1126824.7325
for step 17860 | loss 3.265489 | norm 0.3286 | time 465.9519 ms | tok/sec 1125197.6395
for step 17861 | loss 3.285545 | norm 0.3263 | time 465.4481 ms | tok/sec 1126415.4988
for step 17862 | loss 3.186985 | norm 0.3213 | time 466.0542 ms | tok/sec 1124950.7004
for step 17863 | loss 3.189931 | norm 0.3528 | time 465.6558 ms | tok/sec 1125913.1656
for step 17864 | loss 3.150985 | norm 0.2983 | time 464.6208 ms | tok/sec 1128421.2151
for step 17865 | loss 3.158543 | norm 0.3123 | time 465.2772 ms | tok/sec 1126829.3518
for step 17866 | loss 3.255843 | norm 0.3765 | time 466.2821 ms | tok/sec 1124400.8017
for step 17867 | loss 3.245119 | norm 0.3094 | time 464.1266 ms | tok/sec 1129622.8546
for step 17868 | loss 3.166929 | norm 0.3505 | time 465.4317 ms | tok/sec 1126455.3124
for step 17869 | loss 3.179028 | norm 0.3182 | time 464.6790 ms | tok/sec 1128279.9457
for step 17870 | loss 3.188581 | norm 0.5620 | time 465.0884 ms | tok/sec 1127286.8484
for step 17871 | loss 3.197177 | norm 0.3087 | time 465.0209 ms | tok/sec 1127450.4127
for step 17872 | loss 3.164030 | norm 0.3107 | time 465.7283 ms | tok/sec 1125737.9447
for step 17873 | loss 3.170469 | norm 0.3334 | time 465.1601 ms | tok/sec 1127112.9328
for step 17874 | loss 3.143715 | norm 0.2997 | time 464.9189 ms | tok/sec 1127697.8722
for step 17875 | loss 3.181469 | norm 0.3209 | time 465.1608 ms | tok/sec 1127111.1997
for step 17876 | loss 3.117469 | norm 0.3340 | time 465.1864 ms | tok/sec 1127049.3890
for step 17877 | loss 3.138499 | norm 0.2965 | time 464.9396 ms | tok/sec 1127647.5621
for step 17878 | loss 3.159400 | norm 0.3234 | time 465.0993 ms | tok/sec 1127260.2665
for step 17879 | loss 3.101316 | norm 0.3112 | time 465.1415 ms | tok/sec 1127157.9955
for step 17880 | loss 3.116180 | norm 0.2985 | time 465.3318 ms | tok/sec 1126697.1397
for step 17881 | loss 3.072022 | norm 0.3072 | time 466.0103 ms | tok/sec 1125056.6004
for step 17882 | loss 3.128950 | norm 0.2982 | time 464.7667 ms | tok/sec 1128066.9505
for step 17883 | loss 3.143689 | norm 0.3064 | time 464.5712 ms | tok/sec 1128541.6694
for step 17884 | loss 3.101341 | norm 0.2827 | time 464.4403 ms | tok/sec 1128859.7229
for step 17885 | loss 3.126029 | norm 0.3066 | time 466.0506 ms | tok/sec 1124959.3328
for step 17886 | loss 3.106951 | norm 0.2987 | time 465.6377 ms | tok/sec 1125956.9793
for step 17887 | loss 3.130391 | norm 0.2886 | time 465.2050 ms | tok/sec 1127004.3350
for step 17888 | loss 3.233882 | norm 0.3441 | time 464.7439 ms | tok/sec 1128122.5067
for step 17889 | loss 3.296186 | norm 0.3020 | time 466.0814 ms | tok/sec 1124885.0985
for step 17890 | loss 3.219249 | norm 0.3109 | time 464.7551 ms | tok/sec 1128095.3067
for step 17891 | loss 3.183837 | norm 0.3665 | time 465.6880 ms | tok/sec 1125835.3469
for step 17892 | loss 3.274812 | norm 0.3901 | time 465.1420 ms | tok/sec 1127156.8400
for step 17893 | loss 3.248052 | norm 0.3027 | time 465.4603 ms | tok/sec 1126386.0731
for step 17894 | loss 3.267309 | norm 0.3515 | time 465.2464 ms | tok/sec 1126903.8429
for step 17895 | loss 3.237216 | norm 0.3133 | time 465.1365 ms | tok/sec 1127170.1284
for step 17896 | loss 3.228154 | norm 0.3066 | time 465.3177 ms | tok/sec 1126731.2001
for step 17897 | loss 3.257210 | norm 0.3265 | time 465.3194 ms | tok/sec 1126727.1589
for step 17898 | loss 3.254371 | norm 0.3228 | time 465.2212 ms | tok/sec 1126965.0601
for step 17899 | loss 3.157013 | norm 0.3213 | time 465.1711 ms | tok/sec 1127086.3591
for step 17900 | loss 3.208650 | norm 0.3142 | time 466.2180 ms | tok/sec 1124555.4783
for step 17901 | loss 3.178795 | norm 0.3146 | time 465.4491 ms | tok/sec 1126413.1908
for step 17902 | loss 3.176798 | norm 0.3287 | time 465.1804 ms | tok/sec 1127063.8302
for step 17903 | loss 3.170078 | norm 0.3126 | time 465.3597 ms | tok/sec 1126629.6023
for step 17904 | loss 3.170592 | norm 0.2928 | time 465.1506 ms | tok/sec 1127136.0414
for step 17905 | loss 3.183992 | norm 0.3242 | time 464.7799 ms | tok/sec 1128035.1239
Will loading at 0 from edu_fineweb10B/edufineweb_train_000095.npy
for step 17906 | loss 3.188832 | norm 0.3050 | time 2451.5824 ms | tok/sec 213856.9738
for step 17907 | loss 3.250899 | norm 0.2978 | time 474.5500 ms | tok/sec 1104810.8530
for step 17908 | loss 3.180800 | norm 0.2892 | time 463.1493 ms | tok/sec 1132006.4386
for step 17909 | loss 3.190487 | norm 0.2897 | time 463.9528 ms | tok/sec 1130046.0367
for step 17910 | loss 3.208486 | norm 0.2860 | time 463.0597 ms | tok/sec 1132225.5878
for step 17911 | loss 3.145069 | norm 0.2668 | time 462.8024 ms | tok/sec 1132854.9474
for step 17912 | loss 3.151615 | norm 0.3107 | time 464.0448 ms | tok/sec 1129821.9255
for step 17913 | loss 3.138247 | norm 0.2664 | time 464.1876 ms | tok/sec 1129474.3226
for step 17914 | loss 3.146847 | norm 0.2882 | time 464.0987 ms | tok/sec 1129690.7514
for step 17915 | loss 3.148254 | norm 0.2725 | time 464.6454 ms | tok/sec 1128361.5766
for step 17916 | loss 3.134567 | norm 0.2727 | time 465.5106 ms | tok/sec 1126264.3480
for step 17917 | loss 3.134608 | norm 0.2919 | time 465.2634 ms | tok/sec 1126862.8427
for step 17918 | loss 3.092282 | norm 0.2833 | time 464.9611 ms | tok/sec 1127595.5218
for step 17919 | loss 3.103191 | norm 0.2724 | time 465.8456 ms | tok/sec 1125454.4790
for step 17920 | loss 3.081382 | norm 0.2691 | time 466.0761 ms | tok/sec 1124897.7579
for step 17921 | loss 3.120299 | norm 0.2764 | time 466.3217 ms | tok/sec 1124305.3720
for step 17922 | loss 3.128366 | norm 0.2730 | time 464.4353 ms | tok/sec 1128871.8924
for step 17923 | loss 3.254161 | norm 0.3153 | time 465.8351 ms | tok/sec 1125479.8237
for step 17924 | loss 3.208324 | norm 0.3062 | time 464.7300 ms | tok/sec 1128156.0746
for step 17925 | loss 3.273390 | norm 0.3049 | time 465.9529 ms | tok/sec 1125195.3365
for step 17926 | loss 3.277345 | norm 0.2992 | time 465.0476 ms | tok/sec 1127385.6749
for step 17927 | loss 3.262962 | norm 0.3125 | time 464.3409 ms | tok/sec 1129101.4243
for step 17928 | loss 3.282430 | norm 0.3009 | time 464.6339 ms | tok/sec 1128389.3685
for step 17929 | loss 3.239934 | norm 0.2801 | time 465.0109 ms | tok/sec 1127474.6913
for step 17930 | loss 3.235796 | norm 0.3063 | time 465.2946 ms | tok/sec 1126787.2022
for step 17931 | loss 3.227280 | norm 0.2881 | time 464.8118 ms | tok/sec 1127957.5902
for step 17932 | loss 3.280344 | norm 0.3144 | time 465.8246 ms | tok/sec 1125505.1697
for step 17933 | loss 3.183733 | norm 0.3183 | time 465.2667 ms | tok/sec 1126854.7585
for step 17934 | loss 3.182920 | norm 0.3169 | time 465.9543 ms | tok/sec 1125191.8821
for step 17935 | loss 3.172784 | norm 0.3177 | time 466.3022 ms | tok/sec 1124352.5100
for step 17936 | loss 3.192396 | norm 0.3064 | time 465.5225 ms | tok/sec 1126235.5070
for step 17937 | loss 3.212350 | norm 0.3080 | time 466.4819 ms | tok/sec 1123919.2195
for step 17938 | loss 3.190606 | norm 0.3108 | time 465.5509 ms | tok/sec 1126166.8715
for step 17939 | loss 3.205858 | norm 0.2964 | time 465.6894 ms | tok/sec 1125831.8886
for step 17940 | loss 3.201374 | norm 0.2831 | time 465.4694 ms | tok/sec 1126364.1491
for step 17941 | loss 3.186327 | norm 0.2927 | time 465.9228 ms | tok/sec 1125267.8844
for step 17942 | loss 3.149920 | norm 0.3016 | time 465.7063 ms | tok/sec 1125790.9663
for step 17943 | loss 3.179396 | norm 0.2762 | time 465.3442 ms | tok/sec 1126667.1221
for step 17944 | loss 3.127643 | norm 0.2908 | time 465.7111 ms | tok/sec 1125779.4394
for step 17945 | loss 3.119284 | norm 0.2995 | time 465.6484 ms | tok/sec 1125931.0366
for step 17946 | loss 3.133857 | norm 0.2777 | time 465.1241 ms | tok/sec 1127200.1728
for step 17947 | loss 3.111518 | norm 0.2936 | time 466.1520 ms | tok/sec 1124714.7994
for step 17948 | loss 3.174785 | norm 0.2928 | time 465.7178 ms | tok/sec 1125763.3022
for step 17949 | loss 3.128315 | norm 0.2904 | time 465.9994 ms | tok/sec 1125083.0785
for step 17950 | loss 3.099713 | norm 0.2822 | time 465.2534 ms | tok/sec 1126887.0960
for step 17951 | loss 3.127222 | norm 0.2803 | time 466.1939 ms | tok/sec 1124613.5649
for step 17952 | loss 3.090642 | norm 0.2846 | time 465.2827 ms | tok/sec 1126816.0714
for step 17953 | loss 3.121941 | norm 0.3038 | time 465.0371 ms | tok/sec 1127411.1067
for step 17954 | loss 3.067302 | norm 0.3140 | time 465.0018 ms | tok/sec 1127496.6586
for step 17955 | loss 3.139027 | norm 0.2873 | time 464.9825 ms | tok/sec 1127543.4864
for step 17956 | loss 3.129841 | norm 0.2694 | time 465.7094 ms | tok/sec 1125783.4738
for step 17957 | loss 3.161940 | norm 0.3052 | time 465.3988 ms | tok/sec 1126534.9481
for step 17958 | loss 3.243298 | norm 0.3178 | time 465.0679 ms | tok/sec 1127336.5485
for step 17959 | loss 3.215098 | norm 0.2920 | time 464.6831 ms | tok/sec 1128270.1045
for step 17960 | loss 3.283240 | norm 0.3250 | time 466.2545 ms | tok/sec 1124467.4972
for step 17961 | loss 3.274045 | norm 0.3180 | time 464.9889 ms | tok/sec 1127527.8767
for step 17962 | loss 3.277362 | norm 0.3134 | time 465.4193 ms | tok/sec 1126485.3187
for step 17963 | loss 3.275838 | norm 0.3101 | time 465.8859 ms | tok/sec 1125357.1426
for step 17964 | loss 3.174960 | norm 0.3084 | time 466.7168 ms | tok/sec 1123353.6864
for step 17965 | loss 3.208343 | norm 0.3097 | time 465.9662 ms | tok/sec 1125163.0960
for step 17966 | loss 3.313074 | norm 0.3136 | time 465.5271 ms | tok/sec 1126224.5478
for step 17967 | loss 3.297058 | norm 0.3204 | time 465.5793 ms | tok/sec 1126098.2443
for step 17968 | loss 3.212743 | norm 0.2819 | time 466.0280 ms | tok/sec 1125014.0078
for step 17969 | loss 3.278924 | norm 0.3543 | time 465.6613 ms | tok/sec 1125899.9068
for step 17970 | loss 3.182301 | norm 0.3279 | time 466.9249 ms | tok/sec 1122852.9331
for step 17971 | loss 3.205774 | norm 0.3210 | time 465.8346 ms | tok/sec 1125480.9758
for step 17972 | loss 3.173334 | norm 0.3237 | time 464.8776 ms | tok/sec 1127797.9276
for step 17973 | loss 3.208302 | norm 0.3291 | time 465.2188 ms | tok/sec 1126970.8357
for step 17974 | loss 3.145500 | norm 0.3120 | time 465.3461 ms | tok/sec 1126662.5041
for step 17975 | loss 3.144048 | norm 0.3295 | time 464.9949 ms | tok/sec 1127513.4237
for step 17976 | loss 3.182517 | norm 0.3380 | time 465.4982 ms | tok/sec 1126294.3441
for step 17977 | loss 3.212983 | norm 0.3249 | time 465.0674 ms | tok/sec 1127337.7043
for step 17978 | loss 3.208015 | norm 0.3317 | time 465.8992 ms | tok/sec 1125324.8928
for step 17979 | loss 3.203278 | norm 0.2903 | time 464.8428 ms | tok/sec 1127882.3812
for step 17980 | loss 3.168257 | norm 0.2972 | time 465.6630 ms | tok/sec 1125895.8716
for step 17981 | loss 3.106227 | norm 0.3540 | time 465.2972 ms | tok/sec 1126780.8512
for step 17982 | loss 3.123122 | norm 0.2824 | time 465.8737 ms | tok/sec 1125386.5145
for step 17983 | loss 3.070689 | norm 0.3169 | time 464.7045 ms | tok/sec 1128218.0067
for step 17984 | loss 3.117728 | norm 0.3004 | time 465.6641 ms | tok/sec 1125892.9894
for step 17985 | loss 3.084440 | norm 0.2770 | time 465.7578 ms | tok/sec 1125666.4886
for step 17986 | loss 3.102204 | norm 0.2831 | time 465.1198 ms | tok/sec 1127210.5732
for step 17987 | loss 3.143698 | norm 0.2805 | time 466.2259 ms | tok/sec 1124536.5008
for step 17988 | loss 3.098683 | norm 0.2808 | time 464.8244 ms | tok/sec 1127926.9269
for step 17989 | loss 3.106490 | norm 0.2805 | time 465.7121 ms | tok/sec 1125777.1341
for step 17990 | loss 3.164304 | norm 0.2800 | time 465.5590 ms | tok/sec 1126147.2628
for step 17991 | loss 3.150178 | norm 0.3681 | time 465.7941 ms | tok/sec 1125578.9097
for step 17992 | loss 3.083723 | norm 0.3049 | time 465.3540 ms | tok/sec 1126643.4555
for step 17993 | loss 3.353734 | norm 0.3584 | time 466.0504 ms | tok/sec 1124959.9083
for step 17994 | loss 3.228670 | norm 0.3180 | time 465.4059 ms | tok/sec 1126517.6350
for step 17995 | loss 3.218354 | norm 0.3122 | time 464.6657 ms | tok/sec 1128312.3651
for step 17996 | loss 3.312743 | norm 0.3186 | time 465.3792 ms | tok/sec 1126582.2732
for step 17997 | loss 3.294018 | norm 0.3860 | time 464.7768 ms | tok/sec 1128042.6464
for step 17998 | loss 3.242205 | norm 0.3170 | time 465.5888 ms | tok/sec 1126075.1782
for step 17999 | loss 3.233935 | norm 0.3218 | time 466.4733 ms | tok/sec 1123939.8995
validation loss 3.2202
HellaSwag accuracy: 2857/10042=0.2845
> Hello, I'm a language model, and this is my only explanation. Today's answer was right answer.
But I have another answer.
I'm
> Hello, I'm a language model, which means that if you want to be familiar with the system, you can create classes for the specific class you want to> 
Hello, I'm a language model, and I use it in this project. A number of kids, just so.
You use your model for a while
> Hello, I'm a language model, I code a program, I code a program, I code how many people code, I code the same thing for this> 
Hello, I'm a language model, so when I'm working on this issue, you say in a very interesting way, that "the two of you are
> Hello, I'm a language model, and I'm interested to understand the different grammar rules. Some of the basic grammar rules aren't quite as clear as I> 
Hello, I'm a language model, so I did some reading my articles for the BBC and I did this in the first semester of my PhD.
"
> Hello, I'm a language model, and how I think about it. For me, it's a kind of "brainwashing" issue. I've thought
> Hello, I'm a language model, so I think that's my first language experience, but I also have been trying to find it elsewhere.<|endoftext|>In this
> Hello, I'm a language model, now it's working in a really important way, it's not exactly accurate or complete, it's actually very concise and
> Hello, I'm a language model, but I'm getting frustrated trying to translate this message into English to keep it going. And I know for a bit of
> Hello, I'm a language model, and it describes things I know. Can a good understanding of one language allow me to understand the nuances and nuances of other
> Hello, I'm a language model, I use it to talk about how I read a program, and you have to use it to tell me about some special
> Hello, I'm a language model, why it's not going to help you with most of my projects.
LINK TO MY
Now when you're
> Hello, I'm a language model, so I just need to figure out how to implement the model. You will not get started with the model, but in
> Hello, I'm a language model, since I'm using the Python programming language, so my job is to write it! It is used for many things,
> Hello, I'm a language model, and I'm not the type I do (or, rather, the user must do this for me; it isn't> 
> Hello, I'm a language model, and that is to say, you would say that we can write a language that looks like a computer, in a program
> Hello, I'm a language model, and I love to teach a language. Each of us has a different method of choosing a strategy to teach. I also
> Hello, I'm a language model, and this was a great start in learning about. I want to make sure that I understand the language. I will also
> Hello, I'm a language model, I'm able to teach a language I used to learn, I'm really good at it! I like the way they
> > Hello, I'm a language model, that you can use and what you learned. I can do it easily. So, I guess I'm a language model
> Hello, I'm a language model, for non-native speakers, which explains how to draw that 'native' sound of the language. That means, that
Hello, I'm a language model, and you've tried it. Well, first of all, how do I write and understand these algorithms? I was going
Hello, I'm a language model, so I've got a few suggestions I may want to put to the best of my ability, because in a class where
> > > > Hello, I'm a language model, not a real language.
In my early days at the University of Minnesota, I taught at a university where I was
Hello, I'm a language model, and I've worked with a lot of different languages! I've got you covered, but I'm not sure you know
Hello, I'm a language model, and when I get to talk about a model that has to be applied in a large part of the way that we talk
Hello, I'm a language model, I want to provide the base for a script. We use some of the code and some new documentation to write our script
> > Hello, I'm a language model, I can make things go a long way. If you want, do you wanna be a language model? It looks like
Hello, I'm a language model, no one. I don't understand "rules" or "situation" but I can express all sorts of thought processes
> Hello, I'm a language model, and I want to understand what languages are, what they are, and how you can make different languages possible."
At
for step 18000 | loss 3.222719 | norm 0.2995 | time 12423.7568 ms | tok/sec 42200.4396
for step 18001 | loss 3.284304 | norm 0.3188 | time 463.8224 ms | tok/sec 1130363.7767
for step 18002 | loss 3.277717 | norm 0.3225 | time 463.3582 ms | tok/sec 1131496.1965
for step 18003 | loss 3.241713 | norm 0.2858 | time 465.8954 ms | tok/sec 1125334.1069
for step 18004 | loss 3.179167 | norm 0.2951 | time 463.3038 ms | tok/sec 1131628.9550
for step 18005 | loss 3.324610 | norm 0.5542 | time 463.8627 ms | tok/sec 1130265.5893
for step 18006 | loss 3.184349 | norm 0.3198 | time 463.7582 ms | tok/sec 1130520.0983
for step 18007 | loss 3.142377 | norm 0.3509 | time 464.8678 ms | tok/sec 1127821.6427
for step 18008 | loss 3.198167 | norm 0.3321 | time 464.8154 ms | tok/sec 1127948.9118
for step 18009 | loss 3.151532 | norm 0.3374 | time 464.1063 ms | tok/sec 1129672.1805
for step 18010 | loss 3.176561 | norm 0.3023 | time 463.2130 ms | tok/sec 1131850.8709
for step 18011 | loss 3.169361 | norm 0.2834 | time 464.4029 ms | tok/sec 1128950.7110
for step 18012 | loss 3.190741 | norm 0.3020 | time 464.1526 ms | tok/sec 1129559.6077
for step 18013 | loss 3.171004 | norm 0.3113 | time 464.8061 ms | tok/sec 1127971.4761
for step 18014 | loss 3.136275 | norm 0.2767 | time 464.7787 ms | tok/sec 1128038.0172
for step 18015 | loss 3.165625 | norm 0.4008 | time 463.9294 ms | tok/sec 1130102.9496
for step 18016 | loss 3.134081 | norm 0.3011 | time 464.1764 ms | tok/sec 1129501.5892
for step 18017 | loss 3.133312 | norm 0.3147 | time 464.8671 ms | tok/sec 1127823.3780
for step 18018 | loss 3.195113 | norm 0.3167 | time 465.1663 ms | tok/sec 1127097.9127
for step 18019 | loss 3.135279 | norm 0.2874 | time 464.9205 ms | tok/sec 1127693.8241
for step 18020 | loss 3.140413 | norm 0.2990 | time 465.3072 ms | tok/sec 1126756.6024
for step 18021 | loss 3.148035 | norm 0.2901 | time 465.3499 ms | tok/sec 1126653.2683
for step 18022 | loss 3.104643 | norm 0.2932 | time 465.1525 ms | tok/sec 1127131.4196
for step 18023 | loss 3.166009 | norm 0.2918 | time 464.8447 ms | tok/sec 1127877.7533
for step 18024 | loss 3.101069 | norm 0.2913 | time 465.3094 ms | tok/sec 1126751.4064
for step 18025 | loss 3.085579 | norm 0.2710 | time 466.0752 ms | tok/sec 1124900.0597
for step 18026 | loss 3.117869 | norm 0.2794 | time 465.5952 ms | tok/sec 1126059.6091
for step 18027 | loss 3.114805 | norm 0.2766 | time 464.6492 ms | tok/sec 1128352.3129
for step 18028 | loss 3.198855 | norm 0.2944 | time 465.2653 ms | tok/sec 1126858.2231
for step 18029 | loss 3.197687 | norm 0.3080 | time 465.8403 ms | tok/sec 1125467.1512
for step 18030 | loss 3.249888 | norm 0.2818 | time 465.7111 ms | tok/sec 1125779.4394
for step 18031 | loss 3.258870 | norm 0.3076 | time 465.3373 ms | tok/sec 1126683.8625
for step 18032 | loss 3.274854 | norm 0.3472 | time 464.8018 ms | tok/sec 1127981.8907
for step 18033 | loss 3.261707 | norm 0.3119 | time 465.3065 ms | tok/sec 1126758.3344
for step 18034 | loss 3.203248 | norm 0.3048 | time 465.4150 ms | tok/sec 1126495.7059
for step 18035 | loss 3.239071 | norm 0.3451 | time 464.2105 ms | tok/sec 1129418.6332
for step 18036 | loss 3.328845 | norm 0.4010 | time 464.9425 ms | tok/sec 1127640.6231
for step 18037 | loss 3.186987 | norm 0.3312 | time 465.1089 ms | tok/sec 1127237.1528
for step 18038 | loss 3.244352 | norm 0.3226 | time 465.0369 ms | tok/sec 1127411.6847
for step 18039 | loss 3.174687 | norm 0.3226 | time 467.5369 ms | tok/sec 1121383.0834
for step 18040 | loss 3.158463 | norm 0.3720 | time 465.0216 ms | tok/sec 1127448.6786
for step 18041 | loss 3.191447 | norm 0.3066 | time 464.7555 ms | tok/sec 1128094.1492
for step 18042 | loss 3.182624 | norm 0.3116 | time 465.2269 ms | tok/sec 1126951.1990
for step 18043 | loss 3.175599 | norm 0.3294 | time 464.5910 ms | tok/sec 1128493.6004
for step 18044 | loss 3.231040 | norm 0.2973 | time 466.1720 ms | tok/sec 1124666.4806
for step 18045 | loss 3.228067 | norm 0.3146 | time 465.2035 ms | tok/sec 1127007.8006
for step 18046 | loss 3.186966 | norm 0.2963 | time 465.0469 ms | tok/sec 1127387.4088
for step 18047 | loss 3.141096 | norm 0.3013 | time 465.0149 ms | tok/sec 1127464.8642
for step 18048 | loss 3.193333 | norm 0.2783 | time 465.3409 ms | tok/sec 1126675.2036
for step 18049 | loss 3.176409 | norm 0.3220 | time 464.8471 ms | tok/sec 1127871.9684
for step 18050 | loss 3.195957 | norm 0.2879 | time 465.1575 ms | tok/sec 1127119.2876
for step 18051 | loss 3.120347 | norm 0.3056 | time 464.5622 ms | tok/sec 1128563.6783
for step 18052 | loss 3.122361 | norm 0.3070 | time 465.2660 ms | tok/sec 1126856.4908
for step 18053 | loss 3.097884 | norm 0.2816 | time 465.4534 ms | tok/sec 1126402.8051
for step 18054 | loss 3.109205 | norm 0.2751 | time 465.5731 ms | tok/sec 1126113.2377
for step 18055 | loss 3.106011 | norm 0.2863 | time 465.5745 ms | tok/sec 1126109.7777
for step 18056 | loss 3.093531 | norm 0.2861 | time 465.6258 ms | tok/sec 1125985.8060
for step 18057 | loss 3.216672 | norm 0.2762 | time 464.9887 ms | tok/sec 1127528.4548
for step 18058 | loss 3.170236 | norm 0.2980 | time 465.3599 ms | tok/sec 1126629.0251
for step 18059 | loss 3.125593 | norm 0.3080 | time 465.4946 ms | tok/sec 1126302.9972
for step 18060 | loss 3.075494 | norm 0.2905 | time 465.9030 ms | tok/sec 1125315.6790
for step 18061 | loss 3.141761 | norm 0.3266 | time 465.8363 ms | tok/sec 1125476.9436
for step 18062 | loss 3.094426 | norm 0.3009 | time 466.2743 ms | tok/sec 1124419.7746
for step 18063 | loss 3.119730 | norm 0.3142 | time 465.1413 ms | tok/sec 1127158.5733
for step 18064 | loss 3.201145 | norm 0.2954 | time 465.6343 ms | tok/sec 1125965.0507
for step 18065 | loss 3.261761 | norm 0.2924 | time 465.5232 ms | tok/sec 1126233.7766
for step 18066 | loss 3.253447 | norm 0.3823 | time 466.3684 ms | tok/sec 1124192.7168
for step 18067 | loss 3.227506 | norm 0.2960 | time 464.9730 ms | tok/sec 1127566.6127
for step 18068 | loss 3.191342 | norm 0.3795 | time 465.9276 ms | tok/sec 1125256.3682
for step 18069 | loss 3.181489 | norm 0.3130 | time 465.5917 ms | tok/sec 1126068.2586
for step 18070 | loss 3.222997 | norm 0.2975 | time 465.5697 ms | tok/sec 1126121.3113
for step 18071 | loss 3.168523 | norm 0.3131 | time 465.1122 ms | tok/sec 1127229.0632
for step 18072 | loss 3.202952 | norm 0.3037 | time 465.5228 ms | tok/sec 1126234.9302
for step 18073 | loss 3.249567 | norm 0.3009 | time 465.6997 ms | tok/sec 1125807.1043
for step 18074 | loss 3.160938 | norm 0.2867 | time 465.9061 ms | tok/sec 1125308.1928
for step 18075 | loss 3.212373 | norm 0.2643 | time 466.4798 ms | tok/sec 1123924.3894
for step 18076 | loss 3.159839 | norm 0.2895 | time 465.4136 ms | tok/sec 1126499.1684
for step 18077 | loss 3.189951 | norm 0.2911 | time 465.9951 ms | tok/sec 1125093.4399
for step 18078 | loss 3.186196 | norm 0.2767 | time 466.0332 ms | tok/sec 1125001.3458
for step 18079 | loss 3.182131 | norm 0.2831 | time 466.3756 ms | tok/sec 1124175.4756
for step 18080 | loss 3.161556 | norm 0.2960 | time 465.1155 ms | tok/sec 1127220.9738
for step 18081 | loss 3.165882 | norm 0.2939 | time 465.7326 ms | tok/sec 1125727.5715
for step 18082 | loss 3.228199 | norm 0.3078 | time 465.9390 ms | tok/sec 1125228.7304
for step 18083 | loss 3.212758 | norm 0.3008 | time 464.8666 ms | tok/sec 1127824.5349
for step 18084 | loss 3.188433 | norm 0.2857 | time 465.5449 ms | tok/sec 1126181.2900
for step 18085 | loss 3.249434 | norm 0.3122 | time 465.5375 ms | tok/sec 1126199.1695
for step 18086 | loss 3.191031 | norm 0.3145 | time 465.3730 ms | tok/sec 1126597.2796
for step 18087 | loss 3.144802 | norm 0.2916 | time 465.5190 ms | tok/sec 1126244.1591
for step 18088 | loss 3.167198 | norm 0.3146 | time 465.4529 ms | tok/sec 1126403.9591
for step 18089 | loss 3.171936 | norm 0.3014 | time 465.3647 ms | tok/sec 1126617.4811
for step 18090 | loss 3.081084 | norm 0.2997 | time 465.1284 ms | tok/sec 1127189.7726
for step 18091 | loss 3.152768 | norm 0.3102 | time 465.0712 ms | tok/sec 1127328.4575
for step 18092 | loss 3.128280 | norm 0.2742 | time 465.5709 ms | tok/sec 1126118.4279
for step 18093 | loss 3.118011 | norm 0.2934 | time 465.4927 ms | tok/sec 1126307.6122
for step 18094 | loss 3.120409 | norm 0.2896 | time 465.8897 ms | tok/sec 1125347.9282
for step 18095 | loss 3.074281 | norm 0.2808 | time 465.7025 ms | tok/sec 1125800.1880
for step 18096 | loss 3.144633 | norm 0.3100 | time 466.3148 ms | tok/sec 1124322.0423
Will loading at 0 from edu_fineweb10B/edufineweb_train_000096.npy
for step 18097 | loss 3.171664 | norm 0.3041 | time 2466.1937 ms | tok/sec 212589.9539
for step 18098 | loss 3.053760 | norm 0.2845 | time 463.4943 ms | tok/sec 1131163.8548
for step 18099 | loss 3.187521 | norm 0.3139 | time 463.8398 ms | tok/sec 1130321.3623
for step 18100 | loss 3.192815 | norm 0.3195 | time 465.8303 ms | tok/sec 1125491.3445
for step 18101 | loss 3.243664 | norm 0.3129 | time 465.3549 ms | tok/sec 1126641.1466
for step 18102 | loss 3.213212 | norm 0.2901 | time 465.6250 ms | tok/sec 1125987.5357
for step 18103 | loss 3.210806 | norm 0.3060 | time 465.2798 ms | tok/sec 1126823.0002
for step 18104 | loss 3.196487 | norm 0.2954 | time 465.2388 ms | tok/sec 1126922.3229
for step 18105 | loss 3.218123 | norm 0.2856 | time 465.6203 ms | tok/sec 1125999.0668
for step 18106 | loss 3.096242 | norm 0.3385 | time 465.5449 ms | tok/sec 1126181.2900
for step 18107 | loss 3.196222 | norm 0.4318 | time 465.1842 ms | tok/sec 1127054.5878
for step 18108 | loss 3.259771 | norm 0.3221 | time 464.6363 ms | tok/sec 1128383.5784
for step 18109 | loss 3.166589 | norm 0.3625 | time 464.6435 ms | tok/sec 1128366.2085
for step 18110 | loss 3.191939 | norm 0.3244 | time 465.0230 ms | tok/sec 1127445.2103
for step 18111 | loss 3.202481 | norm 0.3195 | time 465.5986 ms | tok/sec 1126051.5364
for step 18112 | loss 3.149110 | norm 0.3237 | time 465.5654 ms | tok/sec 1126131.6918
for step 18113 | loss 3.172214 | norm 0.3051 | time 465.8964 ms | tok/sec 1125331.8033
for step 18114 | loss 3.188445 | norm 0.3162 | time 465.3974 ms | tok/sec 1126538.4108
for step 18115 | loss 3.157280 | norm 0.3191 | time 465.8525 ms | tok/sec 1125437.7751
for step 18116 | loss 3.165389 | norm 0.2952 | time 464.6595 ms | tok/sec 1128327.4175
for step 18117 | loss 3.153111 | norm 0.3035 | time 466.1851 ms | tok/sec 1124634.8456
for step 18118 | loss 3.185164 | norm 0.3130 | time 465.9767 ms | tok/sec 1125137.7655
for step 18119 | loss 3.178560 | norm 0.3003 | time 465.5647 ms | tok/sec 1126133.4219
for step 18120 | loss 3.154777 | norm 0.2746 | time 465.0807 ms | tok/sec 1127305.3410
for step 18121 | loss 3.181190 | norm 0.2985 | time 466.3401 ms | tok/sec 1124261.1119
for step 18122 | loss 3.141359 | norm 0.3203 | time 465.8625 ms | tok/sec 1125413.5841
for step 18123 | loss 3.166917 | norm 0.3046 | time 465.7412 ms | tok/sec 1125706.8256
for step 18124 | loss 3.119846 | norm 0.2795 | time 465.1887 ms | tok/sec 1127043.6127
for step 18125 | loss 3.133231 | norm 0.2961 | time 468.9083 ms | tok/sec 1118103.4520
for step 18126 | loss 3.047819 | norm 0.3087 | time 466.1980 ms | tok/sec 1124603.7875
for step 18127 | loss 3.126774 | norm 0.2970 | time 465.8277 ms | tok/sec 1125497.6810
for step 18128 | loss 3.102005 | norm 0.3132 | time 465.7404 ms | tok/sec 1125708.5544
for step 18129 | loss 3.126479 | norm 0.3015 | time 466.3203 ms | tok/sec 1124308.8210
for step 18130 | loss 3.145374 | norm 0.2977 | time 465.6417 ms | tok/sec 1125947.1786
for step 18131 | loss 3.168710 | norm 0.2958 | time 466.2941 ms | tok/sec 1124372.0561
for step 18132 | loss 3.138006 | norm 0.2845 | time 465.3027 ms | tok/sec 1126767.5720
for step 18133 | loss 3.073370 | norm 0.2987 | time 465.5170 ms | tok/sec 1126248.7737
for step 18134 | loss 3.099631 | norm 0.2872 | time 465.8208 ms | tok/sec 1125514.3867
for step 18135 | loss 3.247597 | norm 0.3408 | time 464.6044 ms | tok/sec 1128461.1706
for step 18136 | loss 3.281228 | norm 0.3181 | time 465.7421 ms | tok/sec 1125704.5206
for step 18137 | loss 3.227049 | norm 0.3205 | time 464.9408 ms | tok/sec 1127644.6708
for step 18138 | loss 3.216489 | norm 0.2957 | time 466.0373 ms | tok/sec 1124991.5617
for step 18139 | loss 3.179813 | norm 0.3179 | time 465.4262 ms | tok/sec 1126468.5842
for step 18140 | loss 3.201500 | norm 0.3151 | time 464.7753 ms | tok/sec 1128046.1184
for step 18141 | loss 3.203528 | norm 0.2823 | time 465.7407 ms | tok/sec 1125707.9781
for step 18142 | loss 3.209020 | norm 0.3244 | time 465.6065 ms | tok/sec 1126032.5084
for step 18143 | loss 3.244769 | norm 0.3469 | time 465.6093 ms | tok/sec 1126025.5893
for step 18144 | loss 3.183793 | norm 0.3024 | time 464.9649 ms | tok/sec 1127586.2707
for step 18145 | loss 3.212159 | norm 0.3033 | time 465.5547 ms | tok/sec 1126157.6438
for step 18146 | loss 3.184535 | norm 0.3285 | time 465.8079 ms | tok/sec 1125545.4951
for step 18147 | loss 3.196323 | norm 0.3145 | time 464.8609 ms | tok/sec 1127838.4175
for step 18148 | loss 3.139314 | norm 0.2910 | time 465.3456 ms | tok/sec 1126663.6586
for step 18149 | loss 3.165841 | norm 0.3238 | time 465.7378 ms | tok/sec 1125714.8933
for step 18150 | loss 3.166945 | norm 0.3077 | time 466.4943 ms | tok/sec 1123889.3497
for step 18151 | loss 3.159183 | norm 0.3263 | time 465.2629 ms | tok/sec 1126863.9976
for step 18152 | loss 3.150910 | norm 0.2899 | time 466.5802 ms | tok/sec 1123682.6024
for step 18153 | loss 3.188463 | norm 0.3012 | time 465.6100 ms | tok/sec 1126023.8595
for step 18154 | loss 3.253407 | norm 0.2907 | time 465.0986 ms | tok/sec 1127262.0001
for step 18155 | loss 3.220152 | norm 0.2760 | time 465.5783 ms | tok/sec 1126100.5509
for step 18156 | loss 3.208112 | norm 0.2864 | time 465.1301 ms | tok/sec 1127185.7282
for step 18157 | loss 3.208097 | norm 0.2767 | time 465.2646 ms | tok/sec 1126859.9555
for step 18158 | loss 3.136821 | norm 0.2947 | time 465.1947 ms | tok/sec 1127029.1720
for step 18159 | loss 3.098249 | norm 0.2847 | time 464.9696 ms | tok/sec 1127574.7071
for step 18160 | loss 3.085326 | norm 0.2911 | time 465.4427 ms | tok/sec 1126428.7697
for step 18161 | loss 3.148952 | norm 0.2831 | time 466.2516 ms | tok/sec 1124474.3972
for step 18162 | loss 3.093285 | norm 0.2980 | time 466.1098 ms | tok/sec 1124816.6274
for step 18163 | loss 3.101976 | norm 0.3068 | time 465.0035 ms | tok/sec 1127492.6119
for step 18164 | loss 3.112411 | norm 0.3178 | time 465.6270 ms | tok/sec 1125982.9233
for step 18165 | loss 3.127861 | norm 0.2961 | time 465.2553 ms | tok/sec 1126882.4762
for step 18166 | loss 3.081239 | norm 0.2928 | time 465.6837 ms | tok/sec 1125845.7221
for step 18167 | loss 3.143532 | norm 0.2999 | time 465.5528 ms | tok/sec 1126162.2576
for step 18168 | loss 3.024129 | norm 0.3139 | time 465.0493 ms | tok/sec 1127381.6290
for step 18169 | loss 3.108063 | norm 0.2784 | time 465.7235 ms | tok/sec 1125749.4707
for step 18170 | loss 3.103922 | norm 0.2998 | time 465.7407 ms | tok/sec 1125707.9781
for step 18171 | loss 3.232058 | norm 0.3041 | time 465.4210 ms | tok/sec 1126481.2793
for step 18172 | loss 3.171173 | norm 0.3103 | time 465.4655 ms | tok/sec 1126373.3801
for step 18173 | loss 3.182301 | norm 0.3137 | time 465.5600 ms | tok/sec 1126144.9560
for step 18174 | loss 3.143186 | norm 0.3205 | time 465.6718 ms | tok/sec 1125874.5431
for step 18175 | loss 3.241657 | norm 0.2834 | time 465.9605 ms | tok/sec 1125176.9132
for step 18176 | loss 3.184970 | norm 0.3184 | time 465.5087 ms | tok/sec 1126268.9627
for step 18177 | loss 3.188393 | norm 0.3161 | time 466.4478 ms | tok/sec 1124001.3696
for step 18178 | loss 3.183177 | norm 0.3143 | time 465.1423 ms | tok/sec 1127156.2623
for step 18179 | loss 3.132432 | norm 0.2979 | time 465.8854 ms | tok/sec 1125358.2944
for step 18180 | loss 3.236441 | norm 0.2989 | time 465.5659 ms | tok/sec 1126130.5384
for step 18181 | loss 3.153983 | norm 0.3050 | time 465.1325 ms | tok/sec 1127179.9504
for step 18182 | loss 3.165661 | norm 0.4655 | time 464.0691 ms | tok/sec 1129762.7192
for step 18183 | loss 3.139686 | norm 0.3080 | time 465.3325 ms | tok/sec 1126695.4079
for step 18184 | loss 3.203071 | norm 0.2947 | time 465.5030 ms | tok/sec 1126282.8070
for step 18185 | loss 3.170781 | norm 0.3104 | time 465.3704 ms | tok/sec 1126603.6285
for step 18186 | loss 3.194335 | norm 0.3066 | time 465.3313 ms | tok/sec 1126698.2943
for step 18187 | loss 3.158505 | norm 0.2928 | time 465.9209 ms | tok/sec 1125272.4909
for step 18188 | loss 3.213106 | norm 0.3060 | time 464.8995 ms | tok/sec 1127744.7168
for step 18189 | loss 3.187688 | norm 0.2849 | time 465.1279 ms | tok/sec 1127190.9282
for step 18190 | loss 3.227469 | norm 0.3095 | time 465.7054 ms | tok/sec 1125793.2717
for step 18191 | loss 3.149852 | norm 0.2927 | time 464.6685 ms | tok/sec 1128305.4179
for step 18192 | loss 3.173204 | norm 0.3092 | time 465.2717 ms | tok/sec 1126842.6324
for step 18193 | loss 3.175729 | norm 0.3308 | time 465.2994 ms | tok/sec 1126775.6549
for step 18194 | loss 3.071943 | norm 0.2958 | time 465.4734 ms | tok/sec 1126354.3413
for step 18195 | loss 3.115416 | norm 0.3120 | time 465.0383 ms | tok/sec 1127408.2167
for step 18196 | loss 3.098723 | norm 0.3143 | time 465.9109 ms | tok/sec 1125296.6758
for step 18197 | loss 3.122411 | norm 0.2839 | time 465.6386 ms | tok/sec 1125954.6733
for step 18198 | loss 3.174416 | norm 0.2989 | time 466.1484 ms | tok/sec 1124723.4281
for step 18199 | loss 3.172858 | norm 0.3305 | time 465.8942 ms | tok/sec 1125336.9863
for step 18200 | loss 3.089524 | norm 0.2777 | time 465.6563 ms | tok/sec 1125912.0126
for step 18201 | loss 3.130305 | norm 0.3275 | time 465.9772 ms | tok/sec 1125136.6142
for step 18202 | loss 3.119337 | norm 0.3122 | time 466.2867 ms | tok/sec 1124389.8782
for step 18203 | loss 3.103601 | norm 0.2795 | time 465.2617 ms | tok/sec 1126866.8848
for step 18204 | loss 3.121688 | norm 0.3056 | time 465.8217 ms | tok/sec 1125512.0824
for step 18205 | loss 3.119501 | norm 0.3029 | time 465.7731 ms | tok/sec 1125629.6117
for step 18206 | loss 3.178272 | norm 0.3450 | time 465.6081 ms | tok/sec 1126028.4723
for step 18207 | loss 3.184177 | norm 0.3292 | time 465.5390 ms | tok/sec 1126195.7089
for step 18208 | loss 3.341480 | norm 0.4391 | time 464.7222 ms | tok/sec 1128175.1744
for step 18209 | loss 3.222455 | norm 0.3273 | time 465.0965 ms | tok/sec 1127267.2008
for step 18210 | loss 3.178130 | norm 0.3682 | time 465.0352 ms | tok/sec 1127415.7308
for step 18211 | loss 3.296652 | norm 0.3734 | time 465.2512 ms | tok/sec 1126892.2933
for step 18212 | loss 3.162298 | norm 0.3525 | time 464.5429 ms | tok/sec 1128610.5947
for step 18213 | loss 3.186678 | norm 0.3379 | time 464.9525 ms | tok/sec 1127616.3374
for step 18214 | loss 3.191227 | norm 0.3383 | time 465.6448 ms | tok/sec 1125939.6840
for step 18215 | loss 3.190999 | norm 0.3458 | time 464.6604 ms | tok/sec 1128325.1017
for step 18216 | loss 3.191144 | norm 0.3408 | time 465.1377 ms | tok/sec 1127167.2396
for step 18217 | loss 3.225979 | norm 0.3121 | time 465.1096 ms | tok/sec 1127235.4193
for step 18218 | loss 3.202804 | norm 0.3315 | time 465.4541 ms | tok/sec 1126401.0742
for step 18219 | loss 3.270666 | norm 0.3441 | time 465.9977 ms | tok/sec 1125087.1079
for step 18220 | loss 3.212610 | norm 0.4019 | time 464.8108 ms | tok/sec 1127959.9045
for step 18221 | loss 3.183931 | norm 0.3343 | time 465.7836 ms | tok/sec 1125604.2601
for step 18222 | loss 3.203131 | norm 0.3297 | time 466.4652 ms | tok/sec 1123959.4313
for step 18223 | loss 3.219798 | norm 0.2947 | time 465.8933 ms | tok/sec 1125339.2898
for step 18224 | loss 3.153444 | norm 0.2972 | time 465.1294 ms | tok/sec 1127187.4615
for step 18225 | loss 3.162704 | norm 0.3954 | time 465.4717 ms | tok/sec 1126358.3798
for step 18226 | loss 3.231676 | norm 0.3601 | time 466.3088 ms | tok/sec 1124336.4136
for step 18227 | loss 3.168694 | norm 0.3133 | time 465.3363 ms | tok/sec 1126686.1715
for step 18228 | loss 3.183161 | norm 0.3199 | time 465.4377 ms | tok/sec 1126440.8868
for step 18229 | loss 3.161155 | norm 0.3180 | time 465.6024 ms | tok/sec 1126042.3107
for step 18230 | loss 3.154978 | norm 0.3411 | time 465.7099 ms | tok/sec 1125782.3211
for step 18231 | loss 3.118414 | norm 0.3012 | time 464.8604 ms | tok/sec 1127839.5744
for step 18232 | loss 3.087917 | norm 0.2906 | time 465.5678 ms | tok/sec 1126125.9248
for step 18233 | loss 3.122861 | norm 0.3023 | time 466.0356 ms | tok/sec 1124995.5904
for step 18234 | loss 3.106887 | norm 0.3246 | time 464.7470 ms | tok/sec 1128114.9832
for step 18235 | loss 3.159356 | norm 0.2931 | time 466.2709 ms | tok/sec 1124427.8239
for step 18236 | loss 3.116809 | norm 0.2714 | time 465.2166 ms | tok/sec 1126976.0337
for step 18237 | loss 3.167101 | norm 0.3016 | time 465.7853 ms | tok/sec 1125600.2270
for step 18238 | loss 3.148385 | norm 0.3066 | time 466.7265 ms | tok/sec 1123330.1588
for step 18239 | loss 3.174839 | norm 0.2888 | time 465.3406 ms | tok/sec 1126675.7809
for step 18240 | loss 3.120376 | norm 0.2857 | time 465.4915 ms | tok/sec 1126310.4966
for step 18241 | loss 3.108140 | norm 0.2855 | time 465.6520 ms | tok/sec 1125922.3893
for step 18242 | loss 3.206542 | norm 0.3136 | time 465.5612 ms | tok/sec 1126142.0724
for step 18243 | loss 3.248822 | norm 0.3324 | time 466.2580 ms | tok/sec 1124458.8723
for step 18244 | loss 3.172711 | norm 0.3130 | time 465.7371 ms | tok/sec 1125716.6222
for step 18245 | loss 3.241569 | norm 0.3291 | time 465.6639 ms | tok/sec 1125893.5658
for step 18246 | loss 3.220074 | norm 0.3019 | time 465.2333 ms | tok/sec 1126935.6057
for step 18247 | loss 3.216276 | norm 0.3199 | time 466.5051 ms | tok/sec 1123863.5021
for step 18248 | loss 3.201590 | norm 0.2951 | time 465.6339 ms | tok/sec 1125966.2037
for step 18249 | loss 3.242506 | norm 0.3032 | time 465.8122 ms | tok/sec 1125535.1254
validation loss 3.2182
HellaSwag accuracy: 2837/10042=0.2825
> Hello, I'm a language model, and this is my last sentence. Therefore it seems like something different.
A few weeks ago my friend asked me to
> Hello, I'm a language model, which means that everything I'm writing has its own way...
To solve this, lets take the following steps:

> Hello, I'm a language model, I never knew I was going to be a language model in 3rd person. I'm an amazing linguistics and lingu
> Hello, I'm a language model, and I'm using a standard language of instruction. I thought about whether there were any grammar rules, and I think that
> Hello, I'm a language model, so I think that's where the problem in this post is.
It's quite hard to implement any model, and
> > Hello, I'm a language model, do you know the rules of speech?
A lot of people (who don't know what a phrase means) think
Hello, I'm a language model, and I love to write this book as a writing program.
When I write this book, I want to make this
> Hello, I'm a language model, but after using X and Y, that's I'm not doing the same thing on Y. It took me an awful
> Hello, I'm a language model, so i'm not sure if i used the "chr" symbol... i'm just gonna be asking the question:
> Hello, I'm a language model, and it worked well enough for me to work at Microsoft, since it means I'm a computer software developer. My first> 
Hello, I'm a language model, so I haven't spoken, but if you were to write something you can do and think of it, you would start
> Hello, I'm a language model, and how I can use it is in the .NET Framework. I think I'd like to help!
And of
> Hello, I'm a language model, I can do this because I'm trying to make my language model's sound better than the language models I have. I
> Hello, I'm a language model, that will be interesting when I explain the concept of what a "Bilingual" is. My first question was, �> 
> Hello, I'm a language model, isn't it? Do any of us want to have the model ready? Where should you go? Are it my own
> Hello, I'm a language model, and I'm a teacher in a way that's pretty straightforward, and i did that because this is why it was cool
Hello, I'm a language model, and I want to model it, and see how to do it. That's what I did on the test.

> > > Hello, I'm a language model, so this time,
I thought if you are going to model and say 'this,'
you're going to need
Hello, I'm a language model, and this was my first talk at the time, and there's no way to tell if I'm going to translate that
> Hello, I'm a language model, I like it because I think the things I have to do in one day really matter.
I just have three different
> > Hello, I'm a language model, and I use it as part of my first language program. I'm used to building languages to handle complex problems.

> Hello, I'm a language model,
You just look at some of the things
The things that you can change the world, see what
you can
Hello, I'm a language model, and you are familiar with the different types of people who speak Spanish. I can be a native speaker, and when you
> Hello, I'm a language model, and I want to talk to you about your project. I'm planning to create an application framework for PHP (such as
Hello, I'm a language model, or set of models for teaching purposes -- i.e., the same thing that defines a language model or language model?
> Hello, I'm a language model, and here's an example:
$ echo "I've got some great ideas here! We're sharing these ideas here> 
Hello, I'm a language model, and I've also talked about writing an entire language, and I know language models are a great way to learn, so
> Hello, I'm a language model, doesn't it? I do it but I did it so that it might be easy to understand."
The program described
> Hello, I'm a language model, and that means "I'm not in a hurry to do anything." I've heard that word many times lately, but
> Hello, I'm a language model, this is sort of like the language itself for a different reason: The program is a system of instructions. It's basically
> Hello, I'm a language model, so I've been doing it since 2002, and I'm very pleased with this one. I like the fact that we
> Hello, I'm a language model, I use words that have meaning, I read them by looking to the dictionary, and the book answers my questions, and
for step 18250 | loss 3.176061 | norm 0.3147 | time 12440.3310 ms | tok/sec 42144.2163
for step 18251 | loss 3.200464 | norm 0.3180 | time 462.2238 ms | tok/sec 1134273.1292
for step 18252 | loss 3.234202 | norm 0.3254 | time 462.8873 ms | tok/sec 1132647.2223
for step 18253 | loss 3.105691 | norm 0.3108 | time 463.6035 ms | tok/sec 1130897.4240
for step 18254 | loss 3.187567 | norm 0.3085 | time 462.9331 ms | tok/sec 1132535.2223
for step 18255 | loss 3.207804 | norm 0.3334 | time 463.4852 ms | tok/sec 1131185.9660
for step 18256 | loss 3.160647 | norm 0.3075 | time 463.2821 ms | tok/sec 1131681.9506
for step 18257 | loss 3.200241 | norm 0.2872 | time 463.9947 ms | tok/sec 1129943.8403
for step 18258 | loss 3.234053 | norm 0.2985 | time 463.7406 ms | tok/sec 1130563.1088
for step 18259 | loss 3.182964 | norm 0.3179 | time 464.5703 ms | tok/sec 1128543.9861
for step 18260 | loss 3.190738 | norm 0.2957 | time 463.4247 ms | tok/sec 1131333.7845
for step 18261 | loss 3.120255 | norm 0.2899 | time 463.8343 ms | tok/sec 1130334.7254
for step 18262 | loss 3.176355 | norm 0.3096 | time 464.4198 ms | tok/sec 1128909.5617
for step 18263 | loss 3.170312 | norm 0.3100 | time 464.5469 ms | tok/sec 1128600.7477
for step 18264 | loss 3.145257 | norm 0.2822 | time 465.8780 ms | tok/sec 1125376.1478
for step 18265 | loss 3.168370 | norm 0.2828 | time 464.4935 ms | tok/sec 1128730.5101
for step 18266 | loss 3.090912 | norm 0.3091 | time 465.2572 ms | tok/sec 1126877.8565
for step 18267 | loss 3.152095 | norm 0.3043 | time 465.6460 ms | tok/sec 1125936.8015
for step 18268 | loss 3.196064 | norm 0.2947 | time 464.6776 ms | tok/sec 1128283.4191
for step 18269 | loss 3.120685 | norm 0.2819 | time 465.1027 ms | tok/sec 1127252.1766
for step 18270 | loss 3.137912 | norm 0.2902 | time 465.1632 ms | tok/sec 1127105.4227
for step 18271 | loss 3.118452 | norm 0.2901 | time 466.3320 ms | tok/sec 1124280.6549
for step 18272 | loss 3.087051 | norm 0.2880 | time 467.3035 ms | tok/sec 1121943.1991
for step 18273 | loss 3.109248 | norm 0.2568 | time 465.3671 ms | tok/sec 1126611.7091
for step 18274 | loss 3.162620 | norm 0.2937 | time 464.6978 ms | tok/sec 1128234.2144
for step 18275 | loss 3.125763 | norm 0.2710 | time 465.1766 ms | tok/sec 1127073.0727
for step 18276 | loss 3.162022 | norm 0.2922 | time 464.9715 ms | tok/sec 1127570.0817
for step 18277 | loss 3.198334 | norm 0.3119 | time 464.7636 ms | tok/sec 1128074.4734
for step 18278 | loss 3.231344 | norm 0.3051 | time 465.4393 ms | tok/sec 1126436.8477
for step 18279 | loss 3.196043 | norm 0.2844 | time 464.7915 ms | tok/sec 1128006.7708
for step 18280 | loss 3.168959 | norm 0.3179 | time 466.0928 ms | tok/sec 1124857.4789
for step 18281 | loss 3.244219 | norm 0.3233 | time 465.3487 ms | tok/sec 1126656.1545
for step 18282 | loss 3.230297 | norm 0.3143 | time 466.3053 ms | tok/sec 1124345.0366
for step 18283 | loss 3.190487 | norm 0.3678 | time 466.1951 ms | tok/sec 1124610.6891
for step 18284 | loss 3.230918 | norm 0.2890 | time 465.7059 ms | tok/sec 1125792.1190
for step 18285 | loss 3.173875 | norm 0.3287 | time 465.2104 ms | tok/sec 1126991.0506
for step 18286 | loss 3.239104 | norm 0.3336 | time 466.4323 ms | tok/sec 1124038.7145
Will loading at 0 from edu_fineweb10B/edufineweb_train_000097.npy
for step 18287 | loss 3.239709 | norm 0.3558 | time 2461.3767 ms | tok/sec 213006.0007
for step 18288 | loss 3.192873 | norm 0.3636 | time 473.3989 ms | tok/sec 1107497.2366
for step 18289 | loss 3.228772 | norm 0.3496 | time 463.6438 ms | tok/sec 1130799.1439
for step 18290 | loss 3.161868 | norm 0.3474 | time 464.7081 ms | tok/sec 1128209.3243
for step 18291 | loss 3.203571 | norm 0.3411 | time 464.7014 ms | tok/sec 1128225.5317
for step 18292 | loss 3.117713 | norm 0.3065 | time 464.9899 ms | tok/sec 1127525.5642
for step 18293 | loss 3.245254 | norm 0.3644 | time 465.3821 ms | tok/sec 1126575.3473
for step 18294 | loss 3.128105 | norm 0.3525 | time 464.1619 ms | tok/sec 1129536.9798
for step 18295 | loss 3.227962 | norm 0.3096 | time 464.9892 ms | tok/sec 1127527.2986
for step 18296 | loss 3.200282 | norm 0.3373 | time 464.8952 ms | tok/sec 1127755.1272
for step 18297 | loss 3.161216 | norm 0.3227 | time 465.7407 ms | tok/sec 1125707.9781
for step 18298 | loss 3.198684 | norm 0.3020 | time 464.6888 ms | tok/sec 1128256.2113
for step 18299 | loss 3.175717 | norm 0.3387 | time 465.2641 ms | tok/sec 1126861.1104
for step 18300 | loss 3.171264 | norm 0.3384 | time 464.8342 ms | tok/sec 1127903.2073
for step 18301 | loss 3.128646 | norm 0.2912 | time 464.7613 ms | tok/sec 1128080.2604
for step 18302 | loss 3.101989 | norm 0.3236 | time 465.7881 ms | tok/sec 1125593.3132
for step 18303 | loss 3.131908 | norm 0.3243 | time 465.2689 ms | tok/sec 1126849.5616
for step 18304 | loss 3.119958 | norm 0.3403 | time 465.0886 ms | tok/sec 1127286.2706
for step 18305 | loss 3.125732 | norm 0.3046 | time 464.9670 ms | tok/sec 1127581.0671
for step 18306 | loss 3.122110 | norm 0.3272 | time 465.7624 ms | tok/sec 1125655.5405
for step 18307 | loss 3.087861 | norm 0.2984 | time 464.3760 ms | tok/sec 1129016.2083
for step 18308 | loss 3.236037 | norm 0.3351 | time 464.7214 ms | tok/sec 1128176.9108
for step 18309 | loss 3.083304 | norm 0.2954 | time 465.7753 ms | tok/sec 1125624.4260
for step 18310 | loss 3.109561 | norm 0.2930 | time 465.6312 ms | tok/sec 1125972.5456
for step 18311 | loss 3.115969 | norm 0.2968 | time 465.0538 ms | tok/sec 1127370.6475
for step 18312 | loss 3.106334 | norm 0.3169 | time 466.4438 ms | tok/sec 1124011.1365
for step 18313 | loss 3.197844 | norm 0.2991 | time 464.9897 ms | tok/sec 1127526.1423
for step 18314 | loss 3.203932 | norm 0.3017 | time 464.7329 ms | tok/sec 1128149.1293
for step 18315 | loss 3.212514 | norm 0.3030 | time 464.9632 ms | tok/sec 1127590.3181
for step 18316 | loss 3.232996 | norm 0.3001 | time 465.8277 ms | tok/sec 1125497.6810
for step 18317 | loss 3.181932 | norm 0.3008 | time 465.0431 ms | tok/sec 1127396.6567
for step 18318 | loss 3.234607 | norm 0.3029 | time 464.9258 ms | tok/sec 1127681.1017
for step 18319 | loss 3.204899 | norm 0.3190 | time 465.5681 ms | tok/sec 1126125.3481
for step 18320 | loss 3.284953 | norm 0.3285 | time 465.3709 ms | tok/sec 1126602.4742
for step 18321 | loss 3.192663 | norm 0.3114 | time 465.7314 ms | tok/sec 1125730.4529
for step 18322 | loss 3.238907 | norm 0.3185 | time 467.3820 ms | tok/sec 1121754.9058
for step 18323 | loss 3.204086 | norm 0.3239 | time 466.0447 ms | tok/sec 1124973.7205
for step 18324 | loss 3.233062 | norm 0.3186 | time 465.5170 ms | tok/sec 1126248.7737
for step 18325 | loss 3.217519 | norm 0.3144 | time 465.6072 ms | tok/sec 1126030.7786
for step 18326 | loss 3.147457 | norm 0.3186 | time 465.6608 ms | tok/sec 1125901.0598
for step 18327 | loss 3.209446 | norm 0.2929 | time 465.2088 ms | tok/sec 1126995.0936
for step 18328 | loss 3.192830 | norm 0.3249 | time 465.5066 ms | tok/sec 1126274.1542
for step 18329 | loss 3.194844 | norm 0.3340 | time 464.4842 ms | tok/sec 1128753.1057
for step 18330 | loss 3.165104 | norm 0.3130 | time 465.0903 ms | tok/sec 1127282.2254
for step 18331 | loss 3.186705 | norm 0.3113 | time 464.7050 ms | tok/sec 1128216.8491
for step 18332 | loss 3.191728 | norm 0.3447 | time 465.8091 ms | tok/sec 1125542.6146
for step 18333 | loss 3.191677 | norm 0.2935 | time 465.8096 ms | tok/sec 1125541.4624
for step 18334 | loss 3.165591 | norm 0.2966 | time 465.7114 ms | tok/sec 1125778.8631
for step 18335 | loss 3.161211 | norm 0.3187 | time 465.0810 ms | tok/sec 1127304.7631
for step 18336 | loss 3.182540 | norm 0.2910 | time 466.1009 ms | tok/sec 1124837.9159
for step 18337 | loss 3.141624 | norm 0.3053 | time 465.2181 ms | tok/sec 1126972.5683
for step 18338 | loss 3.152122 | norm 0.2993 | time 464.8921 ms | tok/sec 1127762.6459
for step 18339 | loss 3.117005 | norm 0.2946 | time 465.0223 ms | tok/sec 1127446.9444
for step 18340 | loss 3.076295 | norm 0.3105 | time 465.2112 ms | tok/sec 1126989.3178
for step 18341 | loss 3.138304 | norm 0.3458 | time 465.7242 ms | tok/sec 1125747.7418
for step 18342 | loss 3.122357 | norm 0.2835 | time 465.8227 ms | tok/sec 1125509.7781
for step 18343 | loss 3.114510 | norm 0.3064 | time 465.1330 ms | tok/sec 1127178.7949
for step 18344 | loss 3.099771 | norm 0.3183 | time 465.3301 ms | tok/sec 1126701.1807
for step 18345 | loss 3.113937 | norm 0.2914 | time 465.9584 ms | tok/sec 1125182.0947
for step 18346 | loss 3.123206 | norm 0.3038 | time 465.2803 ms | tok/sec 1126821.8454
for step 18347 | loss 3.164258 | norm 0.3932 | time 465.5161 ms | tok/sec 1126251.0809
for step 18348 | loss 3.178789 | norm 0.3146 | time 465.1165 ms | tok/sec 1127218.6625
for step 18349 | loss 3.196442 | norm 0.3492 | time 465.9114 ms | tok/sec 1125295.5241
for step 18350 | loss 3.193582 | norm 0.3360 | time 465.2317 ms | tok/sec 1126939.6484
for step 18351 | loss 3.165211 | norm 0.3088 | time 465.1115 ms | tok/sec 1127230.7967
for step 18352 | loss 3.184195 | norm 0.3414 | time 465.4102 ms | tok/sec 1126507.2475
for step 18353 | loss 3.272506 | norm 0.3183 | time 464.9873 ms | tok/sec 1127531.9236
for step 18354 | loss 3.252262 | norm 0.3443 | time 464.8659 ms | tok/sec 1127826.2702
for step 18355 | loss 3.185908 | norm 0.3113 | time 466.4640 ms | tok/sec 1123962.3037
for step 18356 | loss 3.253275 | norm 0.3606 | time 465.1773 ms | tok/sec 1127071.3397
for step 18357 | loss 3.156207 | norm 0.3165 | time 464.7529 ms | tok/sec 1128100.5151
for step 18358 | loss 3.280308 | norm 0.3545 | time 465.6117 ms | tok/sec 1126019.8234
for step 18359 | loss 3.156497 | norm 0.3299 | time 464.8187 ms | tok/sec 1127940.8120
for step 18360 | loss 3.190097 | norm 0.3008 | time 464.6909 ms | tok/sec 1128251.0014
for step 18361 | loss 3.247516 | norm 0.2992 | time 465.2863 ms | tok/sec 1126807.4105
for step 18362 | loss 3.198942 | norm 0.3355 | time 465.2917 ms | tok/sec 1126794.1307
for step 18363 | loss 3.196000 | norm 0.2870 | time 465.0474 ms | tok/sec 1127386.2529
for step 18364 | loss 3.245315 | norm 0.2962 | time 464.7844 ms | tok/sec 1128024.1297
for step 18365 | loss 3.163477 | norm 0.3082 | time 465.2622 ms | tok/sec 1126865.7299
for step 18366 | loss 3.209907 | norm 0.3033 | time 465.4436 ms | tok/sec 1126426.4617
for step 18367 | loss 3.157379 | norm 0.2932 | time 465.3497 ms | tok/sec 1126653.8456
for step 18368 | loss 3.188291 | norm 0.3082 | time 465.2364 ms | tok/sec 1126928.0980
for step 18369 | loss 3.207602 | norm 0.2918 | time 465.4198 ms | tok/sec 1126484.1646
for step 18370 | loss 3.177314 | norm 0.3026 | time 465.8570 ms | tok/sec 1125426.8314
for step 18371 | loss 3.090819 | norm 0.3167 | time 464.7167 ms | tok/sec 1128188.4868
for step 18372 | loss 3.119706 | norm 0.2848 | time 465.4958 ms | tok/sec 1126300.1128
for step 18373 | loss 3.150355 | norm 0.3193 | time 464.7961 ms | tok/sec 1127995.7771
for step 18374 | loss 3.053253 | norm 0.2981 | time 465.6315 ms | tok/sec 1125971.9690
for step 18375 | loss 3.136536 | norm 0.3160 | time 466.6784 ms | tok/sec 1123446.0849
for step 18376 | loss 3.086241 | norm 0.3142 | time 464.8938 ms | tok/sec 1127758.5974
for step 18377 | loss 3.118370 | norm 0.3109 | time 464.6695 ms | tok/sec 1128303.1022
for step 18378 | loss 3.137664 | norm 0.2911 | time 464.5393 ms | tok/sec 1128619.2834
for step 18379 | loss 3.123097 | norm 0.3189 | time 464.8590 ms | tok/sec 1127843.0451
for step 18380 | loss 3.130858 | norm 0.3160 | time 465.6224 ms | tok/sec 1125993.8778
for step 18381 | loss 3.114521 | norm 0.3048 | time 466.3646 ms | tok/sec 1124201.9123
for step 18382 | loss 3.186276 | norm 0.3340 | time 465.5533 ms | tok/sec 1126161.1041
for step 18383 | loss 3.177326 | norm 0.3011 | time 465.8799 ms | tok/sec 1125371.5404
for step 18384 | loss 3.163943 | norm 0.2912 | time 466.0490 ms | tok/sec 1124963.3613
for step 18385 | loss 3.159091 | norm 0.3258 | time 465.9534 ms | tok/sec 1125194.1850
for step 18386 | loss 3.201819 | norm 0.3030 | time 465.9088 ms | tok/sec 1125301.8584
for step 18387 | loss 3.184748 | norm 0.2758 | time 466.4335 ms | tok/sec 1124035.8418
for step 18388 | loss 3.202155 | norm 0.3143 | time 465.2023 ms | tok/sec 1127010.6886
for step 18389 | loss 3.213034 | norm 0.2967 | time 465.8005 ms | tok/sec 1125563.3544
for step 18390 | loss 3.223705 | norm 0.2909 | time 465.9514 ms | tok/sec 1125198.7910
for step 18391 | loss 3.210639 | norm 0.3085 | time 465.3089 ms | tok/sec 1126752.5611
for step 18392 | loss 3.185333 | norm 0.2797 | time 465.6804 ms | tok/sec 1125853.7919
for step 18393 | loss 3.179392 | norm 0.2710 | time 465.5321 ms | tok/sec 1126212.4353
for step 18394 | loss 3.194717 | norm 0.2835 | time 464.6018 ms | tok/sec 1128467.5406
for step 18395 | loss 3.155350 | norm 0.2787 | time 465.1666 ms | tok/sec 1127097.3351
for step 18396 | loss 3.188654 | norm 0.2878 | time 466.4142 ms | tok/sec 1124082.3825
for step 18397 | loss 3.194937 | norm 0.2943 | time 464.7021 ms | tok/sec 1128223.7951
for step 18398 | loss 3.141290 | norm 0.3017 | time 465.3990 ms | tok/sec 1126534.3710
for step 18399 | loss 3.151413 | norm 0.2772 | time 465.6978 ms | tok/sec 1125811.7153
for step 18400 | loss 3.165239 | norm 0.2760 | time 466.1229 ms | tok/sec 1124784.9840
for step 18401 | loss 3.163704 | norm 0.3030 | time 465.6026 ms | tok/sec 1126041.7341
for step 18402 | loss 3.209465 | norm 0.2849 | time 465.2135 ms | tok/sec 1126983.5421
for step 18403 | loss 3.194764 | norm 0.2794 | time 466.1074 ms | tok/sec 1124822.3810
for step 18404 | loss 3.213133 | norm 0.2830 | time 466.2368 ms | tok/sec 1124510.0484
for step 18405 | loss 3.201882 | norm 0.2748 | time 466.0447 ms | tok/sec 1124973.7205
for step 18406 | loss 3.152895 | norm 0.2806 | time 465.2274 ms | tok/sec 1126950.0440
for step 18407 | loss 3.096191 | norm 0.2779 | time 465.2812 ms | tok/sec 1126819.5358
for step 18408 | loss 3.167020 | norm 0.2821 | time 465.7204 ms | tok/sec 1125756.9627
for step 18409 | loss 3.106947 | norm 0.2838 | time 464.5889 ms | tok/sec 1128498.8125
for step 18410 | loss 3.147914 | norm 0.2864 | time 465.3528 ms | tok/sec 1126646.3416
for step 18411 | loss 3.128383 | norm 0.2757 | time 464.9572 ms | tok/sec 1127604.7731
for step 18412 | loss 3.131136 | norm 0.2964 | time 465.7373 ms | tok/sec 1125716.0459
for step 18413 | loss 3.123558 | norm 0.2808 | time 465.4377 ms | tok/sec 1126440.8868
for step 18414 | loss 3.102099 | norm 0.2894 | time 465.5933 ms | tok/sec 1126064.2221
for step 18415 | loss 3.112842 | norm 0.2814 | time 465.7199 ms | tok/sec 1125758.1154
for step 18416 | loss 3.093383 | norm 0.3090 | time 466.0394 ms | tok/sec 1124986.3819
for step 18417 | loss 3.152583 | norm 0.3046 | time 465.4706 ms | tok/sec 1126361.2644
for step 18418 | loss 3.271077 | norm 0.3305 | time 466.1613 ms | tok/sec 1124692.3651
for step 18419 | loss 3.231594 | norm 0.2898 | time 465.7712 ms | tok/sec 1125634.2211
for step 18420 | loss 3.176279 | norm 0.3271 | time 465.8735 ms | tok/sec 1125387.0905
for step 18421 | loss 3.264692 | norm 0.3621 | time 465.0970 ms | tok/sec 1127266.0451
for step 18422 | loss 3.210050 | norm 0.3433 | time 465.7345 ms | tok/sec 1125722.9612
for step 18423 | loss 3.254160 | norm 0.3301 | time 465.7164 ms | tok/sec 1125766.7602
for step 18424 | loss 3.270471 | norm 0.3287 | time 465.1079 ms | tok/sec 1127239.4641
for step 18425 | loss 3.256974 | norm 0.3514 | time 465.1108 ms | tok/sec 1127232.5302
for step 18426 | loss 3.174978 | norm 0.3339 | time 465.5583 ms | tok/sec 1126148.9930
for step 18427 | loss 3.247144 | norm 0.3033 | time 465.4405 ms | tok/sec 1126433.9627
for step 18428 | loss 3.208953 | norm 0.3360 | time 465.4484 ms | tok/sec 1126414.9218
for step 18429 | loss 3.178418 | norm 0.3127 | time 465.3642 ms | tok/sec 1126618.6355
for step 18430 | loss 3.168399 | norm 0.3078 | time 465.1077 ms | tok/sec 1127240.0419
for step 18431 | loss 3.167912 | norm 0.3277 | time 464.9599 ms | tok/sec 1127598.4128
for step 18432 | loss 3.195638 | norm 0.3122 | time 465.5650 ms | tok/sec 1126132.8452
for step 18433 | loss 3.152317 | norm 0.2986 | time 465.6596 ms | tok/sec 1125903.9421
for step 18434 | loss 3.159024 | norm 0.3065 | time 465.3192 ms | tok/sec 1126727.7362
for step 18435 | loss 3.180044 | norm 0.3045 | time 465.9417 ms | tok/sec 1125222.3969
for step 18436 | loss 3.168350 | norm 0.2838 | time 465.2486 ms | tok/sec 1126898.6456
for step 18437 | loss 3.113908 | norm 0.3027 | time 465.5154 ms | tok/sec 1126252.8114
for step 18438 | loss 3.241827 | norm 0.3011 | time 465.3740 ms | tok/sec 1126594.9709
for step 18439 | loss 3.152352 | norm 0.2857 | time 465.0521 ms | tok/sec 1127374.6933
for step 18440 | loss 3.173385 | norm 0.3710 | time 464.5514 ms | tok/sec 1128589.7425
for step 18441 | loss 3.136297 | norm 0.2886 | time 465.8585 ms | tok/sec 1125423.3756
for step 18442 | loss 3.115362 | norm 0.3042 | time 465.1363 ms | tok/sec 1127170.7061
for step 18443 | loss 3.180359 | norm 0.3158 | time 465.3668 ms | tok/sec 1126612.2863
for step 18444 | loss 3.117754 | norm 0.2829 | time 465.6420 ms | tok/sec 1125946.6021
for step 18445 | loss 3.178372 | norm 0.3175 | time 466.2910 ms | tok/sec 1124379.5298
for step 18446 | loss 3.134655 | norm 0.2939 | time 465.1349 ms | tok/sec 1127174.1727
for step 18447 | loss 3.120976 | norm 0.2788 | time 465.1625 ms | tok/sec 1127107.1558
for step 18448 | loss 3.054162 | norm 0.2776 | time 465.4069 ms | tok/sec 1126515.3267
for step 18449 | loss 3.137384 | norm 0.2759 | time 466.4798 ms | tok/sec 1123924.3894
for step 18450 | loss 3.117440 | norm 0.2798 | time 465.4329 ms | tok/sec 1126452.4273
for step 18451 | loss 3.131375 | norm 0.2893 | time 465.3254 ms | tok/sec 1126712.7264
for step 18452 | loss 3.111003 | norm 0.2775 | time 466.4683 ms | tok/sec 1123951.9632
for step 18453 | loss 3.109325 | norm 0.2799 | time 465.5488 ms | tok/sec 1126172.0621
for step 18454 | loss 3.217936 | norm 0.3044 | time 465.9166 ms | tok/sec 1125282.8557
for step 18455 | loss 3.257230 | norm 0.4460 | time 465.4443 ms | tok/sec 1126424.7307
for step 18456 | loss 3.208465 | norm 0.3324 | time 465.6727 ms | tok/sec 1125872.2374
for step 18457 | loss 3.236341 | norm 0.3206 | time 464.6194 ms | tok/sec 1128424.6894
for step 18458 | loss 3.310486 | norm 0.3753 | time 466.3208 ms | tok/sec 1124307.6713
for step 18459 | loss 3.170218 | norm 0.3289 | time 466.4502 ms | tok/sec 1123995.6245
for step 18460 | loss 3.100043 | norm 0.2948 | time 465.7481 ms | tok/sec 1125690.1142
for step 18461 | loss 3.191145 | norm 0.3145 | time 465.1330 ms | tok/sec 1127178.7949
for step 18462 | loss 3.200884 | norm 0.2989 | time 466.0850 ms | tok/sec 1124876.4672
for step 18463 | loss 3.311405 | norm 0.4158 | time 465.7395 ms | tok/sec 1125710.8595
for step 18464 | loss 3.181681 | norm 0.3251 | time 466.5222 ms | tok/sec 1123822.1485
for step 18465 | loss 3.198283 | norm 0.3336 | time 465.5693 ms | tok/sec 1126122.4647
for step 18466 | loss 3.211473 | norm 0.3077 | time 465.4102 ms | tok/sec 1126507.2475
for step 18467 | loss 3.186858 | norm 0.3136 | time 466.0106 ms | tok/sec 1125056.0248
for step 18468 | loss 3.217972 | norm 0.2991 | time 465.1175 ms | tok/sec 1127216.3513
for step 18469 | loss 3.148569 | norm 0.3198 | time 466.1245 ms | tok/sec 1124780.9568
for step 18470 | loss 3.172297 | norm 0.3819 | time 465.3661 ms | tok/sec 1126614.0179
for step 18471 | loss 3.160279 | norm 0.3238 | time 465.9708 ms | tok/sec 1125152.1577
for step 18472 | loss 3.141133 | norm 0.2954 | time 466.4075 ms | tok/sec 1124098.4716
for step 18473 | loss 3.221021 | norm 0.3009 | time 464.9363 ms | tok/sec 1127655.6577
for step 18474 | loss 3.176758 | norm 0.3120 | time 466.7954 ms | tok/sec 1123164.3457
for step 18475 | loss 3.238796 | norm 0.3012 | time 465.0655 ms | tok/sec 1127342.3278
for step 18476 | loss 3.174652 | norm 0.3090 | time 465.6293 ms | tok/sec 1125977.1579
for step 18477 | loss 3.160523 | norm 0.3083 | time 465.8980 ms | tok/sec 1125327.7722
Will loading at 0 from edu_fineweb10B/edufineweb_train_000098.npy
for step 18478 | loss 3.205298 | norm 0.2866 | time 2531.3363 ms | tok/sec 207119.0614
for step 18479 | loss 3.113060 | norm 0.3110 | time 464.5777 ms | tok/sec 1128526.0321
for step 18480 | loss 3.120849 | norm 0.3233 | time 465.1158 ms | tok/sec 1127220.3959
for step 18481 | loss 3.165052 | norm 0.2810 | time 464.0801 ms | tok/sec 1129736.0204
for step 18482 | loss 3.148049 | norm 0.2858 | time 464.1774 ms | tok/sec 1129499.2686
for step 18483 | loss 3.121296 | norm 0.2937 | time 464.1607 ms | tok/sec 1129539.8807
for step 18484 | loss 3.049941 | norm 0.2998 | time 464.0915 ms | tok/sec 1129708.1621
for step 18485 | loss 3.128053 | norm 0.3039 | time 464.4008 ms | tok/sec 1128955.9273
for step 18486 | loss 3.105662 | norm 0.2773 | time 464.7539 ms | tok/sec 1128098.2002
for step 18487 | loss 3.082712 | norm 0.3103 | time 463.7980 ms | tok/sec 1130423.0460
for step 18488 | loss 3.130180 | norm 0.3084 | time 465.4584 ms | tok/sec 1126390.6888
for step 18489 | loss 3.140315 | norm 0.3278 | time 464.9920 ms | tok/sec 1127520.3611
for step 18490 | loss 3.184649 | norm 0.3168 | time 465.3969 ms | tok/sec 1126539.5650
for step 18491 | loss 3.161077 | norm 0.3169 | time 464.8302 ms | tok/sec 1127913.0421
for step 18492 | loss 3.285430 | norm 0.3453 | time 464.4489 ms | tok/sec 1128838.8614
for step 18493 | loss 3.185499 | norm 0.3145 | time 465.3211 ms | tok/sec 1126723.1178
for step 18494 | loss 3.190409 | norm 0.3135 | time 465.5266 ms | tok/sec 1126225.7014
for step 18495 | loss 3.162345 | norm 0.3089 | time 466.0757 ms | tok/sec 1124898.9088
for step 18496 | loss 3.183174 | norm 0.3914 | time 465.2426 ms | tok/sec 1126913.0828
for step 18497 | loss 3.166759 | norm 0.3644 | time 465.3115 ms | tok/sec 1126746.2104
for step 18498 | loss 3.192485 | norm 0.3279 | time 465.2534 ms | tok/sec 1126887.0960
for step 18499 | loss 3.258163 | norm 0.3156 | time 467.3479 ms | tok/sec 1121836.7398
validation loss 3.2180
HellaSwag accuracy: 2854/10042=0.2842
> Hello, I'm a language model, and this is the one that I work in when I try to understand the other languages in a specific way.
I
> Hello, I'm a language model, not an artificial intelligence. I can learn an object that contains a unique feature (from all descriptions I can find). I
> Hello, I'm a language model, I usually write in a language model, I'm a computer code model and I'm a programmer. And that sounds pretty
> Hello, I'm a language model, and I'm interested to look at things it's not particularly nice about. Now, do you want to compare these two
> Hello, I'm a language model, and I love to learn the fundamentals of artificial intelligence. I love knowing if the language of the brain has evolved to handle
> Hello, I'm a language model, so why don't I talk to friends?<|endoftext|>Taken by the public in a public library or private school, this
> Hello, I'm a language model, has the ability to model real world.
The most interesting aspect of the language model is that it is used to simulate> 
Hello, I'm a language model, so I could only refer to things you know already.
You could be a computer programmer, a programmer, or even
> > > Hello, I'm a language model, I'm planning to go to the computer after I spend a couple of hours on the computer.
What is the most
Hello, I'm a language model, that can be used to visualize models.
I don't think I'm in the position to do this.
We
Hello, I'm a language model, and like most people, I've read the title of the book, I don't want to read it. But a
> > > Hello, I'm a language model, so I've got to do something differently.
I'm looking to translate your document into English if I need to

Hello, I'm a language model, a language analysis tool, a language analysis analysis tool! I'm a beginner, so I've created this article to help
Hello, I'm a language model, and I use it in an interesting way where I can explain grammar and lexical topics and ideas clearly without any background in
> > > > Hello, I'm a language model, I want to add the letters to the English alphabet:
Now, I'm looking at different functions, the functions which
Hello, I'm a language model, I like to think of a "tongue" as the basic building blocks of a language. We start a language
Hello, I'm a language model, so I think that if you're interested this is going to be a challenge.
And one of the other things I
> Hello, I'm a language model, to play, I'm not even. I'm on a mission to be a citizen scientist. I'm an artist I
> > Hello, I'm a language model, I am an author. I'm also a language learner, as an author and as a professional. I read lots
> Hello, I'm a language model, and I want to create a world of the future. I'm also a business analyst and as a former manager of accounting
Hello, I'm a language model, here I'm based on the way our mind is wired when I'm listening to our speakers, and we use more complex
Hello, I'm a language model, and I've taken the time for you to discuss it with your classmates."
"The fact that you can teach the
> > > > Hello, I'm a language model, doesn't it? How are you going to talk about the world this way? Let's start with a description.

Hello, I'm a language model, and I'm a beginner with the "English language".
That's awesome? (Okay, I should say I haven
Hello, I'm a language model, but instead of getting my work done from my point of view as well as my place in the world through my research and
Hello, I'm a language model, so you've got a really nice interface you can use to explore and analyze the different kinds of input devices.
How
> > > Hello, I'm a language model, so you don’t know this. I'm not a person.
So why do you think that? The
Hello, I'm a language model, and this one is a fantastic, wonderful example of how I can make it work.
I'm a language translator (
Hello, I'm a language model, and it didn't really get that well when I started back in school.
One of my primary goals is to teach
> > Hello, I'm a language model, since I'm using the concept of a "conversion".
If you think what's the deal, I'll talk
Hello, I'm a language model, and you have to show me why. When you finish that step, I've put together a project for you who would
> Hello, I'm a language model, and how I learned it. But I don't think the students have a lot of trouble understanding the math, or how
for step 18500 | loss 3.225077 | norm 0.3109 | time 12503.7098 ms | tok/sec 41930.5957
for step 18501 | loss 3.200952 | norm 0.3464 | time 462.2207 ms | tok/sec 1134280.7352
for step 18502 | loss 3.204447 | norm 0.3106 | time 462.9509 ms | tok/sec 1132491.4784
for step 18503 | loss 3.165583 | norm 0.2866 | time 463.5546 ms | tok/sec 1131016.6624
for step 18504 | loss 3.169428 | norm 0.2914 | time 464.0429 ms | tok/sec 1129826.5694
for step 18505 | loss 3.199083 | norm 0.3228 | time 463.5816 ms | tok/sec 1130950.9328
for step 18506 | loss 3.150309 | norm 0.3026 | time 462.8708 ms | tok/sec 1132687.4777
for step 18507 | loss 3.195555 | norm 0.3134 | time 463.2144 ms | tok/sec 1131847.3754
for step 18508 | loss 3.150480 | norm 0.3559 | time 462.5838 ms | tok/sec 1133390.3659
for step 18509 | loss 3.201552 | norm 0.3136 | time 463.6002 ms | tok/sec 1130905.5663
for step 18510 | loss 3.220104 | norm 0.3045 | time 463.1610 ms | tok/sec 1131977.8855
for step 18511 | loss 3.189231 | norm 0.2995 | time 463.7194 ms | tok/sec 1130614.8421
for step 18512 | loss 3.188344 | norm 0.3202 | time 464.5455 ms | tok/sec 1128604.2231
for step 18513 | loss 3.165781 | norm 0.2979 | time 464.5779 ms | tok/sec 1128525.4529
for step 18514 | loss 3.094174 | norm 0.3376 | time 464.3044 ms | tok/sec 1129190.1320
for step 18515 | loss 3.138794 | norm 0.3114 | time 464.2527 ms | tok/sec 1129315.9702
for step 18516 | loss 3.118678 | norm 0.3026 | time 464.3238 ms | tok/sec 1129143.1673
for step 18517 | loss 3.103277 | norm 0.3182 | time 464.9265 ms | tok/sec 1127679.3668
for step 18518 | loss 3.133535 | norm 0.3476 | time 464.9589 ms | tok/sec 1127600.7257
for step 18519 | loss 3.131484 | norm 0.3375 | time 464.8211 ms | tok/sec 1127935.0265
for step 18520 | loss 3.120213 | norm 0.2813 | time 464.6506 ms | tok/sec 1128348.8391
for step 18521 | loss 3.132422 | norm 0.3070 | time 464.4973 ms | tok/sec 1128721.2404
for step 18522 | loss 3.173295 | norm 0.3239 | time 466.4299 ms | tok/sec 1124044.4601
for step 18523 | loss 3.079370 | norm 0.2838 | time 465.4181 ms | tok/sec 1126488.2040
for step 18524 | loss 3.093319 | norm 0.3099 | time 465.3955 ms | tok/sec 1126543.0277
for step 18525 | loss 3.122825 | norm 0.2898 | time 466.2278 ms | tok/sec 1124531.9003
for step 18526 | loss 3.211800 | norm 0.2991 | time 464.7968 ms | tok/sec 1127994.0413
for step 18527 | loss 3.246669 | norm 0.3119 | time 466.0337 ms | tok/sec 1125000.1947
for step 18528 | loss 3.194396 | norm 0.2956 | time 464.9215 ms | tok/sec 1127691.5109
for step 18529 | loss 3.225030 | norm 0.3114 | time 465.1155 ms | tok/sec 1127220.9738
for step 18530 | loss 3.246334 | norm 0.3105 | time 465.6909 ms | tok/sec 1125828.4302
for step 18531 | loss 3.222407 | norm 0.3259 | time 464.6618 ms | tok/sec 1128321.6281
for step 18532 | loss 3.214704 | norm 0.2831 | time 465.5035 ms | tok/sec 1126281.6533
for step 18533 | loss 3.264146 | norm 0.3425 | time 465.4639 ms | tok/sec 1126377.4188
for step 18534 | loss 3.192794 | norm 0.3460 | time 465.0865 ms | tok/sec 1127291.4715
for step 18535 | loss 3.252438 | norm 0.3186 | time 465.6959 ms | tok/sec 1125816.3262
for step 18536 | loss 3.190854 | norm 0.3444 | time 465.8422 ms | tok/sec 1125462.5431
for step 18537 | loss 3.222096 | norm 0.3222 | time 465.6603 ms | tok/sec 1125902.2127
for step 18538 | loss 3.196790 | norm 0.3162 | time 465.3828 ms | tok/sec 1126573.6159
for step 18539 | loss 3.171540 | norm 0.3263 | time 466.6948 ms | tok/sec 1123406.4837
for step 18540 | loss 3.199537 | norm 0.3004 | time 465.1427 ms | tok/sec 1127155.1068
for step 18541 | loss 3.227708 | norm 0.3087 | time 465.5128 ms | tok/sec 1126259.1565
for step 18542 | loss 3.138122 | norm 0.3339 | time 464.9625 ms | tok/sec 1127592.0527
for step 18543 | loss 3.192686 | norm 0.3083 | time 465.8296 ms | tok/sec 1125493.0726
for step 18544 | loss 3.140512 | norm 0.2825 | time 465.0207 ms | tok/sec 1127450.9908
for step 18545 | loss 3.168242 | norm 0.2852 | time 465.6894 ms | tok/sec 1125831.8886
for step 18546 | loss 3.167862 | norm 0.2952 | time 465.1179 ms | tok/sec 1127215.1956
for step 18547 | loss 3.209151 | norm 0.2933 | time 465.7288 ms | tok/sec 1125736.7921
for step 18548 | loss 3.131531 | norm 0.3024 | time 465.6036 ms | tok/sec 1126039.4276
for step 18549 | loss 3.152302 | norm 0.2809 | time 464.9560 ms | tok/sec 1127607.6641
for step 18550 | loss 3.123855 | norm 0.2795 | time 464.9422 ms | tok/sec 1127641.2014
for step 18551 | loss 3.122155 | norm 0.2720 | time 466.2328 ms | tok/sec 1124519.8241
for step 18552 | loss 3.105923 | norm 0.3133 | time 465.2684 ms | tok/sec 1126850.7164
for step 18553 | loss 3.168646 | norm 0.3001 | time 465.5502 ms | tok/sec 1126168.6017
for step 18554 | loss 3.107183 | norm 0.2993 | time 465.9061 ms | tok/sec 1125308.1928
for step 18555 | loss 3.100646 | norm 0.3015 | time 465.0712 ms | tok/sec 1127328.4575
for step 18556 | loss 3.127190 | norm 0.2907 | time 465.8761 ms | tok/sec 1125380.7552
for step 18557 | loss 3.114884 | norm 0.2922 | time 465.7812 ms | tok/sec 1125610.0217
for step 18558 | loss 3.064321 | norm 0.2802 | time 465.3456 ms | tok/sec 1126663.6586
for step 18559 | loss 3.085638 | norm 0.2859 | time 464.9513 ms | tok/sec 1127619.2285
for step 18560 | loss 3.071360 | norm 0.2788 | time 466.3212 ms | tok/sec 1124306.5217
for step 18561 | loss 3.234634 | norm 0.3348 | time 465.4260 ms | tok/sec 1126469.1613
for step 18562 | loss 3.184398 | norm 0.2980 | time 465.3211 ms | tok/sec 1126723.1178
for step 18563 | loss 3.202617 | norm 0.3164 | time 466.0187 ms | tok/sec 1125036.4549
for step 18564 | loss 3.221456 | norm 0.3218 | time 466.3534 ms | tok/sec 1124228.9249
for step 18565 | loss 3.178308 | norm 0.3175 | time 465.5528 ms | tok/sec 1126162.2576
for step 18566 | loss 3.165113 | norm 0.3031 | time 465.7075 ms | tok/sec 1125788.0846
for step 18567 | loss 3.201478 | norm 0.3533 | time 465.1763 ms | tok/sec 1127073.6504
for step 18568 | loss 3.205344 | norm 0.2966 | time 465.7187 ms | tok/sec 1125760.9969
for step 18569 | loss 3.287637 | norm 0.3428 | time 464.9003 ms | tok/sec 1127742.9817
for step 18570 | loss 3.259281 | norm 0.3572 | time 466.1531 ms | tok/sec 1124711.9231
for step 18571 | loss 3.196116 | norm 0.2959 | time 465.3988 ms | tok/sec 1126534.9481
for step 18572 | loss 3.196738 | norm 0.3380 | time 465.6694 ms | tok/sec 1125880.3075
for step 18573 | loss 3.201464 | norm 0.3135 | time 465.5583 ms | tok/sec 1126148.9930
for step 18574 | loss 3.227850 | norm 0.3123 | time 465.7385 ms | tok/sec 1125713.1645
for step 18575 | loss 3.161395 | norm 0.3154 | time 465.0657 ms | tok/sec 1127341.7499
for step 18576 | loss 3.236012 | norm 0.3275 | time 466.3460 ms | tok/sec 1124246.7425
for step 18577 | loss 3.198443 | norm 0.3091 | time 465.5547 ms | tok/sec 1126157.6438
for step 18578 | loss 3.161561 | norm 0.3022 | time 465.5557 ms | tok/sec 1126155.3369
for step 18579 | loss 3.271461 | norm 0.3289 | time 465.3127 ms | tok/sec 1126743.3238
for step 18580 | loss 3.221884 | norm 0.3375 | time 465.1895 ms | tok/sec 1127041.8798
for step 18581 | loss 3.206556 | norm 0.3039 | time 465.6036 ms | tok/sec 1126039.4276
for step 18582 | loss 3.218601 | norm 0.3429 | time 465.2710 ms | tok/sec 1126844.3647
for step 18583 | loss 3.162137 | norm 0.3062 | time 466.4068 ms | tok/sec 1124100.1955
for step 18584 | loss 3.128433 | norm 0.2925 | time 465.0550 ms | tok/sec 1127367.7577
for step 18585 | loss 3.160574 | norm 0.2938 | time 465.5910 ms | tok/sec 1126069.9885
for step 18586 | loss 3.111226 | norm 0.2905 | time 465.5237 ms | tok/sec 1126232.6230
for step 18587 | loss 3.192500 | norm 0.2833 | time 466.0292 ms | tok/sec 1125011.1300
for step 18588 | loss 3.098163 | norm 0.3179 | time 465.8051 ms | tok/sec 1125552.4083
for step 18589 | loss 3.134017 | norm 0.2955 | time 465.3273 ms | tok/sec 1126708.1081
for step 18590 | loss 3.087807 | norm 0.3433 | time 465.4067 ms | tok/sec 1126515.9038
for step 18591 | loss 3.141379 | norm 0.3158 | time 465.6544 ms | tok/sec 1125916.6245
for step 18592 | loss 3.106433 | norm 0.3094 | time 465.8535 ms | tok/sec 1125435.4711
for step 18593 | loss 3.130323 | norm 0.3186 | time 465.3161 ms | tok/sec 1126735.2413
for step 18594 | loss 3.134633 | norm 0.2950 | time 465.2348 ms | tok/sec 1126932.1406
for step 18595 | loss 3.133539 | norm 0.3438 | time 465.6427 ms | tok/sec 1125944.8726
for step 18596 | loss 3.170887 | norm 0.3619 | time 465.3022 ms | tok/sec 1126768.7267
for step 18597 | loss 3.212679 | norm 0.3092 | time 465.0798 ms | tok/sec 1127307.6526
for step 18598 | loss 3.362514 | norm 0.3916 | time 464.9200 ms | tok/sec 1127694.9807
for step 18599 | loss 3.261775 | norm 0.3585 | time 465.5523 ms | tok/sec 1126163.4111
for step 18600 | loss 3.204185 | norm 0.3457 | time 465.1108 ms | tok/sec 1127232.5302
for step 18601 | loss 3.227153 | norm 0.3289 | time 465.7173 ms | tok/sec 1125764.4549
for step 18602 | loss 3.223547 | norm 0.3453 | time 465.2753 ms | tok/sec 1126833.9711
for step 18603 | loss 3.188479 | norm 0.3609 | time 466.2051 ms | tok/sec 1124586.5337
for step 18604 | loss 3.175742 | norm 0.3086 | time 465.2078 ms | tok/sec 1126997.4040
for step 18605 | loss 3.217368 | norm 0.3711 | time 466.5179 ms | tok/sec 1123832.4866
for step 18606 | loss 3.294910 | norm 0.3233 | time 465.7762 ms | tok/sec 1125622.1213
for step 18607 | loss 3.208721 | norm 0.3674 | time 465.3983 ms | tok/sec 1126536.1023
for step 18608 | loss 3.160858 | norm 0.3488 | time 465.2181 ms | tok/sec 1126972.5683
for step 18609 | loss 3.205224 | norm 0.3011 | time 466.1732 ms | tok/sec 1124663.6046
for step 18610 | loss 3.212135 | norm 0.3237 | time 467.4284 ms | tok/sec 1121643.3332
for step 18611 | loss 3.150345 | norm 0.3095 | time 465.4171 ms | tok/sec 1126490.5123
for step 18612 | loss 3.196004 | norm 0.2955 | time 464.8888 ms | tok/sec 1127770.7432
for step 18613 | loss 3.126460 | norm 0.2890 | time 465.7185 ms | tok/sec 1125761.5733
for step 18614 | loss 3.216120 | norm 0.3291 | time 465.1687 ms | tok/sec 1127092.1359
for step 18615 | loss 3.196228 | norm 0.3016 | time 465.7881 ms | tok/sec 1125593.3132
for step 18616 | loss 3.206750 | norm 0.2962 | time 465.1823 ms | tok/sec 1127059.2090
for step 18617 | loss 3.195614 | norm 0.3086 | time 465.6568 ms | tok/sec 1125910.8597
for step 18618 | loss 3.181205 | norm 0.2916 | time 465.9269 ms | tok/sec 1125258.0956
for step 18619 | loss 3.069609 | norm 0.2869 | time 465.7114 ms | tok/sec 1125778.8631
for step 18620 | loss 3.100034 | norm 0.3117 | time 465.8618 ms | tok/sec 1125415.3120
for step 18621 | loss 3.084599 | norm 0.2963 | time 465.4987 ms | tok/sec 1126293.1904
for step 18622 | loss 3.092273 | norm 0.2907 | time 465.4849 ms | tok/sec 1126326.6495
for step 18623 | loss 3.095754 | norm 0.3068 | time 465.4901 ms | tok/sec 1126313.9579
for step 18624 | loss 3.052977 | norm 0.2986 | time 465.0857 ms | tok/sec 1127293.2052
for step 18625 | loss 3.136091 | norm 0.2728 | time 465.0180 ms | tok/sec 1127457.3494
for step 18626 | loss 3.124607 | norm 0.3110 | time 464.8423 ms | tok/sec 1127883.5382
for step 18627 | loss 3.108716 | norm 0.3058 | time 464.3583 ms | tok/sec 1129059.1045
for step 18628 | loss 3.025727 | norm 0.2852 | time 465.5080 ms | tok/sec 1126270.6932
for step 18629 | loss 3.042539 | norm 0.3094 | time 465.9429 ms | tok/sec 1125219.5181
for step 18630 | loss 3.143193 | norm 0.2859 | time 465.0795 ms | tok/sec 1127308.2305
for step 18631 | loss 3.217519 | norm 0.3138 | time 465.5213 ms | tok/sec 1126238.3910
for step 18632 | loss 3.174912 | norm 0.2759 | time 465.9622 ms | tok/sec 1125172.8831
for step 18633 | loss 3.166661 | norm 0.3073 | time 464.8926 ms | tok/sec 1127761.4892
for step 18634 | loss 3.184206 | norm 0.3331 | time 465.6410 ms | tok/sec 1125948.9081
for step 18635 | loss 3.179188 | norm 0.3001 | time 464.9973 ms | tok/sec 1127507.6425
for step 18636 | loss 3.207154 | norm 0.2953 | time 464.9007 ms | tok/sec 1127741.8250
for step 18637 | loss 3.171639 | norm 0.2997 | time 465.2560 ms | tok/sec 1126880.7438
for step 18638 | loss 3.226812 | norm 0.3012 | time 466.9077 ms | tok/sec 1122894.2155
for step 18639 | loss 3.239078 | norm 0.3349 | time 465.5116 ms | tok/sec 1126262.0406
for step 18640 | loss 3.236907 | norm 0.2836 | time 465.5774 ms | tok/sec 1126102.8576
for step 18641 | loss 3.141059 | norm 0.3143 | time 465.1265 ms | tok/sec 1127194.3949
for step 18642 | loss 3.197620 | norm 0.2995 | time 465.0788 ms | tok/sec 1127309.9642
for step 18643 | loss 3.173441 | norm 0.2958 | time 466.3165 ms | tok/sec 1124318.0184
for step 18644 | loss 3.185612 | norm 0.3017 | time 465.4016 ms | tok/sec 1126528.0228
for step 18645 | loss 3.175196 | norm 0.2849 | time 464.3922 ms | tok/sec 1128976.7931
for step 18646 | loss 3.155024 | norm 0.2602 | time 464.2305 ms | tok/sec 1129369.9094
for step 18647 | loss 3.165384 | norm 0.2728 | time 464.6244 ms | tok/sec 1128412.5295
for step 18648 | loss 3.191676 | norm 0.2808 | time 464.4945 ms | tok/sec 1128728.1927
for step 18649 | loss 3.161540 | norm 0.2836 | time 464.9220 ms | tok/sec 1127690.3544
for step 18650 | loss 3.201699 | norm 0.3133 | time 465.3316 ms | tok/sec 1126697.7170
for step 18651 | loss 3.146458 | norm 0.3061 | time 464.6051 ms | tok/sec 1128459.4334
for step 18652 | loss 3.247414 | norm 0.2890 | time 465.1270 ms | tok/sec 1127193.2393
for step 18653 | loss 3.146222 | norm 0.2995 | time 465.2531 ms | tok/sec 1126887.6735
for step 18654 | loss 3.073357 | norm 0.2989 | time 465.7869 ms | tok/sec 1125596.1940
for step 18655 | loss 3.145872 | norm 0.2876 | time 465.2376 ms | tok/sec 1126925.2104
for step 18656 | loss 3.107780 | norm 0.3422 | time 465.0350 ms | tok/sec 1127416.3088
for step 18657 | loss 3.009073 | norm 0.3353 | time 465.0562 ms | tok/sec 1127364.8679
for step 18658 | loss 3.081593 | norm 0.3271 | time 464.7849 ms | tok/sec 1128022.9724
for step 18659 | loss 3.112394 | norm 0.3216 | time 466.4721 ms | tok/sec 1123942.7718
for step 18660 | loss 3.115033 | norm 0.2941 | time 465.2691 ms | tok/sec 1126848.9841
for step 18661 | loss 3.137437 | norm 0.3263 | time 465.2207 ms | tok/sec 1126966.2152
for step 18662 | loss 3.118616 | norm 0.2965 | time 465.5917 ms | tok/sec 1126068.2586
for step 18663 | loss 3.116472 | norm 0.3400 | time 465.8761 ms | tok/sec 1125380.7552
for step 18664 | loss 3.154929 | norm 0.3254 | time 465.3852 ms | tok/sec 1126567.8444
for step 18665 | loss 3.250333 | norm 0.3421 | time 464.2699 ms | tok/sec 1129274.2143
for step 18666 | loss 3.123760 | norm 0.3224 | time 464.8564 ms | tok/sec 1127849.4081
for step 18667 | loss 3.216003 | norm 0.3190 | time 464.6013 ms | tok/sec 1128468.6988
Will loading at 0 from edu_fineweb10B/edufineweb_train_000099.npy
for step 18668 | loss 3.109990 | norm 0.3534 | time 1567.4803 ms | tok/sec 334478.2014
for step 18669 | loss 3.170504 | norm 0.3408 | time 463.8693 ms | tok/sec 1130249.3232
for step 18670 | loss 3.238059 | norm 0.3033 | time 465.1067 ms | tok/sec 1127242.3533
for step 18671 | loss 3.203632 | norm 0.3350 | time 465.4245 ms | tok/sec 1126472.6235
for step 18672 | loss 3.204800 | norm 0.3393 | time 464.3066 ms | tok/sec 1129184.9135
for step 18673 | loss 3.187733 | norm 0.3096 | time 464.3462 ms | tok/sec 1129088.6700
for step 18674 | loss 3.205554 | norm 0.3088 | time 466.2497 ms | tok/sec 1124478.9972
for step 18675 | loss 3.218349 | norm 0.3126 | time 465.8735 ms | tok/sec 1125387.0905
for step 18676 | loss 3.131454 | norm 0.3165 | time 464.2684 ms | tok/sec 1129277.6939
for step 18677 | loss 3.170024 | norm 0.2932 | time 464.9148 ms | tok/sec 1127707.7035
for step 18678 | loss 3.146257 | norm 0.2992 | time 465.0190 ms | tok/sec 1127455.0371
for step 18679 | loss 3.189954 | norm 0.3247 | time 464.9117 ms | tok/sec 1127715.2216
for step 18680 | loss 3.185380 | norm 0.2992 | time 464.5529 ms | tok/sec 1128586.2672
for step 18681 | loss 3.188538 | norm 0.3310 | time 465.6477 ms | tok/sec 1125932.7661
for step 18682 | loss 3.192723 | norm 0.3434 | time 465.0807 ms | tok/sec 1127305.3410
for step 18683 | loss 3.160201 | norm 0.3048 | time 464.9982 ms | tok/sec 1127505.3301
for step 18684 | loss 3.187776 | norm 0.3910 | time 464.7555 ms | tok/sec 1128094.1492
for step 18685 | loss 3.271467 | norm 0.3654 | time 465.3020 ms | tok/sec 1126769.3040
for step 18686 | loss 3.171430 | norm 0.3708 | time 465.6653 ms | tok/sec 1125890.1071
for step 18687 | loss 3.217126 | norm 0.3590 | time 465.2989 ms | tok/sec 1126776.8096
for step 18688 | loss 3.212009 | norm 0.3434 | time 465.1458 ms | tok/sec 1127147.5961
for step 18689 | loss 3.142071 | norm 0.3121 | time 465.2228 ms | tok/sec 1126961.0173
for step 18690 | loss 3.085232 | norm 0.3251 | time 465.1191 ms | tok/sec 1127212.3066
for step 18691 | loss 3.092546 | norm 0.2986 | time 464.1409 ms | tok/sec 1129588.0389
for step 18692 | loss 3.063781 | norm 0.3254 | time 464.7892 ms | tok/sec 1128012.5570
for step 18693 | loss 3.085511 | norm 0.3533 | time 465.4498 ms | tok/sec 1126411.4598
for step 18694 | loss 3.161315 | norm 0.3005 | time 465.7152 ms | tok/sec 1125769.6418
for step 18695 | loss 3.081106 | norm 0.2988 | time 464.8509 ms | tok/sec 1127862.7128
for step 18696 | loss 3.121945 | norm 0.3080 | time 465.1442 ms | tok/sec 1127151.6403
for step 18697 | loss 3.089262 | norm 0.2930 | time 465.0629 ms | tok/sec 1127348.6852
for step 18698 | loss 3.110111 | norm 0.2838 | time 465.4078 ms | tok/sec 1126513.0183
for step 18699 | loss 3.120285 | norm 0.3193 | time 464.9041 ms | tok/sec 1127733.7282
for step 18700 | loss 3.093461 | norm 0.3027 | time 465.1401 ms | tok/sec 1127161.4620
for step 18701 | loss 3.255681 | norm 0.3495 | time 465.4677 ms | tok/sec 1126368.1877
for step 18702 | loss 3.250129 | norm 0.3169 | time 465.4889 ms | tok/sec 1126316.8423
for step 18703 | loss 3.236104 | norm 0.2920 | time 465.1935 ms | tok/sec 1127032.0601
for step 18704 | loss 3.169517 | norm 0.3826 | time 464.8397 ms | tok/sec 1127889.9016
for step 18705 | loss 3.208643 | norm 0.3429 | time 465.9457 ms | tok/sec 1125212.6090
for step 18706 | loss 3.259239 | norm 0.3358 | time 465.4617 ms | tok/sec 1126382.6114
for step 18707 | loss 3.182827 | norm 0.3147 | time 465.1480 ms | tok/sec 1127142.3965
for step 18708 | loss 3.175739 | norm 0.3037 | time 464.4685 ms | tok/sec 1128791.3465
for step 18709 | loss 3.195227 | norm 0.3150 | time 465.1151 ms | tok/sec 1127222.1294
for step 18710 | loss 3.203761 | norm 0.3109 | time 464.7412 ms | tok/sec 1128128.8729
for step 18711 | loss 3.238233 | norm 0.3421 | time 466.4733 ms | tok/sec 1123939.8995
for step 18712 | loss 3.185564 | norm 0.3176 | time 465.0736 ms | tok/sec 1127322.6782
for step 18713 | loss 3.181042 | norm 0.3092 | time 464.6442 ms | tok/sec 1128364.4715
for step 18714 | loss 3.105293 | norm 0.3038 | time 464.8244 ms | tok/sec 1127926.9269
for step 18715 | loss 3.168910 | norm 0.3339 | time 466.0854 ms | tok/sec 1124875.3164
for step 18716 | loss 3.161616 | norm 0.2906 | time 465.3599 ms | tok/sec 1126629.0251
for step 18717 | loss 3.120920 | norm 0.3061 | time 465.4620 ms | tok/sec 1126382.0344
for step 18718 | loss 3.170722 | norm 0.3127 | time 465.1134 ms | tok/sec 1127226.1741
for step 18719 | loss 3.343246 | norm 0.3279 | time 464.4177 ms | tok/sec 1128914.7776
for step 18720 | loss 3.230007 | norm 0.3518 | time 465.5530 ms | tok/sec 1126161.6809
for step 18721 | loss 3.189716 | norm 0.3091 | time 465.6022 ms | tok/sec 1126042.8873
for step 18722 | loss 3.168296 | norm 0.3317 | time 464.4043 ms | tok/sec 1128947.2335
for step 18723 | loss 3.210476 | norm 0.3587 | time 465.2622 ms | tok/sec 1126865.7299
for step 18724 | loss 3.239491 | norm 0.2791 | time 465.5240 ms | tok/sec 1126232.0462
for step 18725 | loss 3.080750 | norm 0.3563 | time 465.4734 ms | tok/sec 1126354.3413
for step 18726 | loss 3.114533 | norm 0.3590 | time 464.9928 ms | tok/sec 1127518.6267
for step 18727 | loss 3.099126 | norm 0.3006 | time 464.9625 ms | tok/sec 1127592.0527
for step 18728 | loss 3.132593 | norm 0.3615 | time 465.1947 ms | tok/sec 1127029.1720
for step 18729 | loss 3.114824 | norm 0.3012 | time 465.3804 ms | tok/sec 1126579.3874
for step 18730 | loss 3.081892 | norm 0.3354 | time 464.8278 ms | tok/sec 1127918.8274
for step 18731 | loss 3.107360 | norm 0.3065 | time 464.7796 ms | tok/sec 1128035.7026
for step 18732 | loss 3.112289 | norm 0.2854 | time 465.7013 ms | tok/sec 1125803.0698
for step 18733 | loss 3.138259 | norm 0.3220 | time 466.3401 ms | tok/sec 1124261.1119
for step 18734 | loss 3.101114 | norm 0.3040 | time 465.2565 ms | tok/sec 1126879.5889
for step 18735 | loss 3.099229 | norm 0.2933 | time 465.1415 ms | tok/sec 1127157.9955
for step 18736 | loss 3.121392 | norm 0.3484 | time 465.4419 ms | tok/sec 1126430.5007
for step 18737 | loss 3.251291 | norm 0.2920 | time 465.5633 ms | tok/sec 1126136.8821
for step 18738 | loss 3.209797 | norm 0.3316 | time 465.3642 ms | tok/sec 1126618.6355
for step 18739 | loss 3.200247 | norm 0.2881 | time 465.6456 ms | tok/sec 1125937.9545
for step 18740 | loss 3.266018 | norm 0.3007 | time 465.2953 ms | tok/sec 1126785.4701
for step 18741 | loss 3.195067 | norm 0.3055 | time 465.6496 ms | tok/sec 1125928.1541
for step 18742 | loss 3.275775 | norm 0.3060 | time 465.4262 ms | tok/sec 1126468.5842
for step 18743 | loss 3.201417 | norm 0.3156 | time 465.9715 ms | tok/sec 1125150.4306
for step 18744 | loss 3.210877 | norm 0.2970 | time 465.7905 ms | tok/sec 1125587.5518
for step 18745 | loss 3.125836 | norm 0.3030 | time 466.1431 ms | tok/sec 1124736.0839
for step 18746 | loss 3.228019 | norm 0.3102 | time 465.1744 ms | tok/sec 1127078.2717
for step 18747 | loss 3.195081 | norm 0.3060 | time 465.6782 ms | tok/sec 1125858.9796
for step 18748 | loss 3.205114 | norm 0.2912 | time 465.5659 ms | tok/sec 1126130.5384
for step 18749 | loss 3.134562 | norm 0.3184 | time 465.0233 ms | tok/sec 1127444.6323
validation loss 3.2163
HellaSwag accuracy: 2841/10042=0.2829
> Hello, I'm a language model, and this is the only way to approach this type of issue because I'm referring to "modes" or "m
> Hello, I'm a language model, not an artificial intelligence. I can see if I can't think of any more like Turing. One thing I can't
> Hello, I'm a language model, I also love that you can use it to create websites that don't need to be updated or have the ability to automatically
> Hello, I'm a language model, and I'm really excited to see my school and learning system take that learning about the concept this semester.
What's
> Hello, I'm a language model, and I love to write a book! A perfect book for beginners - they'll love learning so much, it's fun
> Hello, I'm a language model, so, no, it's a simulation. It might actually be called a programming language or a language translation.
In
> Hello, I'm a language model, so I didn't just take a very deep history of it all in. I started out by looking at the history and
> Hello, I'm a language model, and when I think of a model, it appears to be a "machine" structure, just like in nature
> Hello, I'm a language model, in order to use your notes, and to make this easier, i'll have to write it in a different way;> 
> > Hello, I'm a language model, I want the model to be a collection of words and sentences. You just have to type in each word. Now let
> Hello, I'm a language model, and I know that's good, but isn't that worth it? Do you consider a language translator or language model as
Hello, I'm a language model, so I will be going to the second post in this series.
Hello, I need to introduce your topic of "
> Hello, I'm a language model, and I am a student. I have several language models, and I know that they all have some kind of big theme
> > > Hello, I'm a language model, or you just need to use some more powerful computer language, I bet you'll see more and more new tools and techniques
> > Hello, I'm a language model, trying to understand some of the language, but it's really difficult to define what exactly is a language. This is one
Hello, I'm a language model, and I'm a person of character! In my personal life, I’m interested in building myself, my world
Hello, I'm a language model, here is an example of how an article, a conversation or a report can be a conversation between two people.
Here
> Hello, I'm a language model, there's a standard definition of English. There are some "rules" or "categories", but in most cases I
Hello, I'm a language model, I think it's like this one. If a language model can learn from scratch, it can learn from your native speaker
> > > Hello, I'm a language model, so I need a word processor, i'm going to go to language model. So how do I write a language model
> > Hello, I'm a language model, I'm starting a new project,
That's over on a little research. I'm not going to do any other
Hello, I'm a language model, and this isn't a challenge; it's just a language model that's been around for a while.
Now you
Hello, I'm a language model, and I have several features that do well in a C# environment. And I have some features that help you learn C
> Hello, I'm a language model, but... I hope it does. The language model is a collection of syntax rules that is used to solve problems using a
Hello, I'm a language model, and I am talking about a programming language, so I'm going to talk about the different constructs that we'll want to
> > > Hello, I'm a language model, is a machine, and how I created the problem and then I applied the rule
The first challenge I faced in class
> Hello, I'm a language model, so I need to be able to think about this.
To understand our program you need a few rules to know that
Hello, I'm a language model, and it looks like any other
language would be accurate.<|endoftext|>The following text is from the British National Archives of Art
Hello, I'm a language model, to model. I want to think my friends tell me that they're so cool that I can see them. I thought
Hello, I'm a language model, and the algorithm to create an algorithm does what's normal, well. In fact, I think what I'm doing in
> > Hello, I'm a language model, I like trying to make sense of what that means from my research. I wanted to do these analyses and make sure that
Hello, I'm a language model, and for the best of reasons -- I don't have enough data or so I could write and get code to write it
for step 18750 | loss 3.132397 | norm 0.2797 | time 12508.7404 ms | tok/sec 41913.7325
for step 18751 | loss 3.156202 | norm 0.2871 | time 461.4899 ms | tok/sec 1136076.8285
for step 18752 | loss 3.180529 | norm 0.3137 | time 462.4560 ms | tok/sec 1133703.5603
for step 18753 | loss 3.125643 | norm 0.3227 | time 463.7034 ms | tok/sec 1130653.7905
for step 18754 | loss 3.173987 | norm 0.2998 | time 465.8670 ms | tok/sec 1125402.6409
for step 18755 | loss 3.169200 | norm 0.3228 | time 463.4976 ms | tok/sec 1131155.7087
for step 18756 | loss 3.145772 | norm 0.2838 | time 463.4192 ms | tok/sec 1131347.1715
for step 18757 | loss 3.194856 | norm 0.3245 | time 463.9680 ms | tok/sec 1130008.8722
for step 18758 | loss 3.179327 | norm 0.3244 | time 465.0023 ms | tok/sec 1127495.5024
for step 18759 | loss 3.185597 | norm 0.2923 | time 463.8727 ms | tok/sec 1130241.1904
for step 18760 | loss 3.140886 | norm 0.3114 | time 464.1557 ms | tok/sec 1129552.0649
for step 18761 | loss 3.117662 | norm 0.3168 | time 464.5288 ms | tok/sec 1128644.7709
for step 18762 | loss 3.120621 | norm 0.2859 | time 464.5252 ms | tok/sec 1128653.4601
for step 18763 | loss 3.132121 | norm 0.3146 | time 464.3366 ms | tok/sec 1129111.8597
for step 18764 | loss 3.088863 | norm 0.3158 | time 464.0923 ms | tok/sec 1129706.4210
for step 18765 | loss 3.085433 | norm 0.2932 | time 464.5140 ms | tok/sec 1128680.6871
for step 18766 | loss 3.167563 | norm 0.3070 | time 464.0746 ms | tok/sec 1129749.3697
for step 18767 | loss 3.089872 | norm 0.2958 | time 465.3056 ms | tok/sec 1126760.6438
for step 18768 | loss 3.147329 | norm 0.3050 | time 464.8688 ms | tok/sec 1127819.3290
for step 18769 | loss 3.096971 | norm 0.2884 | time 464.5267 ms | tok/sec 1128649.9844
for step 18770 | loss 3.098507 | norm 0.3007 | time 464.5998 ms | tok/sec 1128472.1734
Will loading at 0 from edu_fineweb10B/edufineweb_train_000001.npy
for step 18771 | loss 3.154615 | norm 0.2948 | time 1398.8457 ms | tok/sec 374800.4589
for step 18772 | loss 3.186866 | norm 0.3127 | time 464.7002 ms | tok/sec 1128228.4259
for step 18773 | loss 3.293472 | norm 0.3169 | time 463.5952 ms | tok/sec 1130917.7800
for step 18774 | loss 3.293107 | norm 0.3459 | time 464.6330 ms | tok/sec 1128391.6846
for step 18775 | loss 3.298661 | norm 0.3045 | time 465.2863 ms | tok/sec 1126807.4105
for step 18776 | loss 3.225394 | norm 0.3159 | time 465.2998 ms | tok/sec 1126774.5002
for step 18777 | loss 3.354780 | norm 0.3578 | time 465.2009 ms | tok/sec 1127014.1542
for step 18778 | loss 3.260557 | norm 0.3218 | time 463.6734 ms | tok/sec 1130727.0439
for step 18779 | loss 3.290087 | norm 0.3485 | time 465.0521 ms | tok/sec 1127374.6933
for step 18780 | loss 3.289026 | norm 0.3397 | time 464.3900 ms | tok/sec 1128982.0097
for step 18781 | loss 3.260017 | norm 0.3101 | time 464.7322 ms | tok/sec 1128150.8657
for step 18782 | loss 3.236153 | norm 0.3269 | time 465.2612 ms | tok/sec 1126868.0397
for step 18783 | loss 3.229151 | norm 0.3006 | time 465.0590 ms | tok/sec 1127357.9324
for step 18784 | loss 3.193829 | norm 0.3246 | time 465.1623 ms | tok/sec 1127107.7335
for step 18785 | loss 3.190980 | norm 0.3312 | time 464.8871 ms | tok/sec 1127774.7918
for step 18786 | loss 3.225142 | norm 0.3342 | time 464.5605 ms | tok/sec 1128567.7326
for step 18787 | loss 3.300340 | norm 0.3715 | time 464.6602 ms | tok/sec 1128325.6807
for step 18788 | loss 3.182895 | norm 0.3568 | time 465.8720 ms | tok/sec 1125390.5461
for step 18789 | loss 3.200278 | norm 0.3404 | time 465.8563 ms | tok/sec 1125428.5594
for step 18790 | loss 3.228714 | norm 0.4437 | time 464.5970 ms | tok/sec 1128479.1226
for step 18791 | loss 3.107129 | norm 0.3837 | time 464.9019 ms | tok/sec 1127738.9333
for step 18792 | loss 3.173357 | norm 0.3286 | time 465.2016 ms | tok/sec 1127012.4214
for step 18793 | loss 3.194675 | norm 0.3077 | time 465.8377 ms | tok/sec 1125473.4874
for step 18794 | loss 3.255201 | norm 0.3189 | time 465.3814 ms | tok/sec 1126577.0788
for step 18795 | loss 3.123999 | norm 0.3553 | time 465.2774 ms | tok/sec 1126828.7743
for step 18796 | loss 3.232848 | norm 0.3254 | time 465.4751 ms | tok/sec 1126350.3028
for step 18797 | loss 3.149451 | norm 0.3239 | time 464.5555 ms | tok/sec 1128579.8959
for step 18798 | loss 3.152488 | norm 0.3267 | time 465.5237 ms | tok/sec 1126232.6230
for step 18799 | loss 3.177572 | norm 0.2883 | time 465.0068 ms | tok/sec 1127484.5187
for step 18800 | loss 3.172878 | norm 0.3248 | time 465.4982 ms | tok/sec 1126294.3441
for step 18801 | loss 3.157476 | norm 0.3139 | time 465.8759 ms | tok/sec 1125381.3311
for step 18802 | loss 3.160535 | norm 0.2867 | time 464.9811 ms | tok/sec 1127546.9553
for step 18803 | loss 3.193020 | norm 0.3323 | time 465.2936 ms | tok/sec 1126789.5117
for step 18804 | loss 3.253739 | norm 0.3591 | time 465.0724 ms | tok/sec 1127325.5678
for step 18805 | loss 3.183155 | norm 0.2805 | time 465.2469 ms | tok/sec 1126902.6880
for step 18806 | loss 3.146630 | norm 0.3030 | time 465.3742 ms | tok/sec 1126594.3937
for step 18807 | loss 3.090858 | norm 0.2995 | time 465.1005 ms | tok/sec 1127257.3772
for step 18808 | loss 3.143173 | norm 0.2844 | time 465.1542 ms | tok/sec 1127127.3756
for step 18809 | loss 3.082076 | norm 0.3271 | time 466.4946 ms | tok/sec 1123888.7753
for step 18810 | loss 3.121512 | norm 0.3052 | time 465.4222 ms | tok/sec 1126478.3941
for step 18811 | loss 3.106992 | norm 0.2992 | time 464.2365 ms | tok/sec 1129355.4091
for step 18812 | loss 3.088715 | norm 0.2702 | time 465.0638 ms | tok/sec 1127346.3734
for step 18813 | loss 3.173880 | norm 0.3175 | time 465.1186 ms | tok/sec 1127213.4622
for step 18814 | loss 3.135021 | norm 0.2804 | time 465.4036 ms | tok/sec 1126523.4060
for step 18815 | loss 3.089501 | norm 0.3491 | time 465.0028 ms | tok/sec 1127494.3462
for step 18816 | loss 3.077154 | norm 0.3103 | time 464.6671 ms | tok/sec 1128308.8915
for step 18817 | loss 3.119232 | norm 0.3123 | time 464.5889 ms | tok/sec 1128498.8125
for step 18818 | loss 3.188639 | norm 0.3153 | time 465.0352 ms | tok/sec 1127415.7308
for step 18819 | loss 3.262565 | norm 0.3409 | time 465.8108 ms | tok/sec 1125538.5819
for step 18820 | loss 3.284979 | norm 0.3331 | time 465.4150 ms | tok/sec 1126495.7059
for step 18821 | loss 3.237364 | norm 0.3384 | time 465.6489 ms | tok/sec 1125929.8836
for step 18822 | loss 3.223658 | norm 0.3431 | time 465.3172 ms | tok/sec 1126732.3547
for step 18823 | loss 3.302527 | norm 0.3321 | time 465.2944 ms | tok/sec 1126787.7796
for step 18824 | loss 3.277166 | norm 0.3211 | time 465.2631 ms | tok/sec 1126863.4201
for step 18825 | loss 3.233421 | norm 0.3131 | time 465.1513 ms | tok/sec 1127134.3083
for step 18826 | loss 3.201085 | norm 0.2994 | time 466.0850 ms | tok/sec 1124876.4672
for step 18827 | loss 3.216744 | norm 0.3157 | time 464.5877 ms | tok/sec 1128501.7082
for step 18828 | loss 3.275941 | norm 0.3083 | time 465.3640 ms | tok/sec 1126619.2127
for step 18829 | loss 3.214339 | norm 0.3008 | time 465.3709 ms | tok/sec 1126602.4742
for step 18830 | loss 3.193179 | norm 0.3069 | time 465.0619 ms | tok/sec 1127350.9970
for step 18831 | loss 3.147246 | norm 0.3187 | time 465.4839 ms | tok/sec 1126328.9571
for step 18832 | loss 3.182727 | norm 0.3101 | time 465.0989 ms | tok/sec 1127261.4222
for step 18833 | loss 3.128383 | norm 0.3183 | time 464.4904 ms | tok/sec 1128738.0419
for step 18834 | loss 3.219793 | norm 0.2992 | time 464.5741 ms | tok/sec 1128534.7194
for step 18835 | loss 3.221822 | norm 0.2985 | time 465.5187 ms | tok/sec 1126244.7360
for step 18836 | loss 3.150530 | norm 0.2757 | time 465.4648 ms | tok/sec 1126375.1110
for step 18837 | loss 3.320066 | norm 0.2980 | time 465.7176 ms | tok/sec 1125763.8785
for step 18838 | loss 3.205203 | norm 0.3074 | time 465.2762 ms | tok/sec 1126831.6614
for step 18839 | loss 3.294734 | norm 0.3147 | time 464.1843 ms | tok/sec 1129482.4444
for step 18840 | loss 3.224380 | norm 0.3102 | time 464.2978 ms | tok/sec 1129206.3676
for step 18841 | loss 3.163625 | norm 0.3033 | time 464.8542 ms | tok/sec 1127854.6142
for step 18842 | loss 3.205031 | norm 0.3118 | time 465.2176 ms | tok/sec 1126973.7235
for step 18843 | loss 3.175443 | norm 0.2873 | time 464.9372 ms | tok/sec 1127653.3446
for step 18844 | loss 3.193975 | norm 0.2805 | time 464.6289 ms | tok/sec 1128401.5279
for step 18845 | loss 3.233745 | norm 0.3044 | time 464.8778 ms | tok/sec 1127797.3492
for step 18846 | loss 3.191753 | norm 0.2880 | time 465.2476 ms | tok/sec 1126900.9555
for step 18847 | loss 3.154851 | norm 0.2873 | time 465.5907 ms | tok/sec 1126070.5651
for step 18848 | loss 3.212206 | norm 0.2858 | time 469.2361 ms | tok/sec 1117322.3040
for step 18849 | loss 3.173009 | norm 0.2982 | time 465.3816 ms | tok/sec 1126576.5017
for step 18850 | loss 3.179670 | norm 0.2816 | time 464.9129 ms | tok/sec 1127712.3300
for step 18851 | loss 3.177631 | norm 0.2951 | time 464.9529 ms | tok/sec 1127615.1809
for step 18852 | loss 3.146452 | norm 0.2896 | time 465.6839 ms | tok/sec 1125845.1457
for step 18853 | loss 3.180097 | norm 0.2991 | time 465.7090 ms | tok/sec 1125784.6265
for step 18854 | loss 3.135273 | norm 0.2925 | time 465.1775 ms | tok/sec 1127070.7620
for step 18855 | loss 3.116662 | norm 0.2912 | time 465.0233 ms | tok/sec 1127444.6323
for step 18856 | loss 3.098486 | norm 0.2979 | time 465.4970 ms | tok/sec 1126297.2285
for step 18857 | loss 3.067363 | norm 0.2806 | time 465.1914 ms | tok/sec 1127037.2587
for step 18858 | loss 3.114340 | norm 0.3064 | time 465.9357 ms | tok/sec 1125236.7913
for step 18859 | loss 3.113919 | norm 0.2940 | time 465.8310 ms | tok/sec 1125489.6164
for step 18860 | loss 3.123986 | norm 0.2953 | time 464.7639 ms | tok/sec 1128073.8948
for step 18861 | loss 3.069427 | norm 0.2850 | time 465.7044 ms | tok/sec 1125795.5771
for step 18862 | loss 3.113397 | norm 0.2900 | time 464.7522 ms | tok/sec 1128102.2512
for step 18863 | loss 3.167263 | norm 0.3023 | time 465.4055 ms | tok/sec 1126518.7892
for step 18864 | loss 3.177486 | norm 0.3305 | time 466.1610 ms | tok/sec 1124692.9404
for step 18865 | loss 3.231344 | norm 0.3124 | time 465.0364 ms | tok/sec 1127412.8408
for step 18866 | loss 3.260914 | norm 0.3332 | time 464.7791 ms | tok/sec 1128036.8599
for step 18867 | loss 3.246958 | norm 0.3557 | time 465.3444 ms | tok/sec 1126666.5448
for step 18868 | loss 3.277498 | norm 0.3283 | time 465.9343 ms | tok/sec 1125240.2460
for step 18869 | loss 3.240603 | norm 0.3850 | time 465.4043 ms | tok/sec 1126521.6747
for step 18870 | loss 3.185780 | norm 0.3300 | time 465.6610 ms | tok/sec 1125900.4833
for step 18871 | loss 3.290776 | norm 0.3739 | time 465.4114 ms | tok/sec 1126504.3620
for step 18872 | loss 3.225510 | norm 0.3362 | time 465.6858 ms | tok/sec 1125840.5345
for step 18873 | loss 3.264073 | norm 0.3084 | time 465.3778 ms | tok/sec 1126585.7362
for step 18874 | loss 3.284374 | norm 0.2949 | time 465.2257 ms | tok/sec 1126954.0867
for step 18875 | loss 3.252090 | norm 0.3600 | time 465.8229 ms | tok/sec 1125509.2021
for step 18876 | loss 3.228245 | norm 0.3220 | time 466.3472 ms | tok/sec 1124243.8686
for step 18877 | loss 3.222676 | norm 0.3005 | time 465.7314 ms | tok/sec 1125730.4529
for step 18878 | loss 3.175164 | norm 0.3166 | time 465.5685 ms | tok/sec 1126124.1947
for step 18879 | loss 3.238455 | norm 0.3252 | time 465.2367 ms | tok/sec 1126927.5205
for step 18880 | loss 3.156146 | norm 0.2928 | time 465.7962 ms | tok/sec 1125573.7246
for step 18881 | loss 3.145461 | norm 0.3139 | time 465.0631 ms | tok/sec 1127348.1072
for step 18882 | loss 3.183424 | norm 0.3069 | time 465.3463 ms | tok/sec 1126661.9269
for step 18883 | loss 3.193109 | norm 0.2963 | time 465.2994 ms | tok/sec 1126775.6549
for step 18884 | loss 3.152524 | norm 0.2756 | time 465.3029 ms | tok/sec 1126766.9946
for step 18885 | loss 3.194015 | norm 0.3300 | time 465.5569 ms | tok/sec 1126152.4533
for step 18886 | loss 3.204472 | norm 0.3326 | time 464.6895 ms | tok/sec 1128254.4746
for step 18887 | loss 3.175256 | norm 0.3129 | time 465.1585 ms | tok/sec 1127116.9768
for step 18888 | loss 3.180009 | norm 0.3432 | time 464.9467 ms | tok/sec 1127630.2148
for step 18889 | loss 3.194718 | norm 0.3038 | time 465.7412 ms | tok/sec 1125706.8256
for step 18890 | loss 3.205033 | norm 0.3173 | time 465.9433 ms | tok/sec 1125218.3666
for step 18891 | loss 3.168895 | norm 0.3286 | time 465.5826 ms | tok/sec 1126090.1710
for step 18892 | loss 3.213997 | norm 0.3193 | time 465.5924 ms | tok/sec 1126066.5287
for step 18893 | loss 3.155531 | norm 0.3091 | time 466.0208 ms | tok/sec 1125031.2747
for step 18894 | loss 3.216389 | norm 0.3097 | time 465.9865 ms | tok/sec 1125114.1631
for step 18895 | loss 3.176142 | norm 0.3157 | time 465.0879 ms | tok/sec 1127288.0042
for step 18896 | loss 3.159917 | norm 0.2934 | time 465.7664 ms | tok/sec 1125645.7450
for step 18897 | loss 3.173635 | norm 0.2895 | time 465.2245 ms | tok/sec 1126956.9745
for step 18898 | loss 3.157725 | norm 0.2988 | time 465.9269 ms | tok/sec 1125258.0956
for step 18899 | loss 3.193050 | norm 0.3149 | time 465.8680 ms | tok/sec 1125400.3371
for step 18900 | loss 3.118385 | norm 0.2902 | time 465.2846 ms | tok/sec 1126811.4522
for step 18901 | loss 3.078905 | norm 0.3189 | time 465.1971 ms | tok/sec 1127023.3959
for step 18902 | loss 3.095736 | norm 0.3019 | time 464.9634 ms | tok/sec 1127589.7399
for step 18903 | loss 3.075022 | norm 0.3036 | time 465.5991 ms | tok/sec 1126050.3832
for step 18904 | loss 3.188054 | norm 0.3657 | time 465.8334 ms | tok/sec 1125483.8560
for step 18905 | loss 3.141873 | norm 0.3097 | time 465.6289 ms | tok/sec 1125978.3110
for step 18906 | loss 3.160529 | norm 0.2959 | time 466.3959 ms | tok/sec 1124126.6286
for step 18907 | loss 3.164510 | norm 0.3386 | time 465.4477 ms | tok/sec 1126416.6527
for step 18908 | loss 3.081852 | norm 0.3116 | time 464.9029 ms | tok/sec 1127736.6199
for step 18909 | loss 3.119122 | norm 0.3012 | time 465.7159 ms | tok/sec 1125767.9128
for step 18910 | loss 3.150448 | norm 0.3424 | time 465.0285 ms | tok/sec 1127431.9154
for step 18911 | loss 3.265554 | norm 0.3665 | time 465.3199 ms | tok/sec 1126726.0043
for step 18912 | loss 3.272202 | norm 0.3283 | time 465.9493 ms | tok/sec 1125203.9727
for step 18913 | loss 3.288745 | norm 0.3422 | time 465.7357 ms | tok/sec 1125720.0798
for step 18914 | loss 3.231748 | norm 0.3270 | time 465.3962 ms | tok/sec 1126541.2964
for step 18915 | loss 3.273008 | norm 0.3153 | time 465.5244 ms | tok/sec 1126230.8926
for step 18916 | loss 3.189025 | norm 0.2984 | time 466.2707 ms | tok/sec 1124428.3989
for step 18917 | loss 3.202597 | norm 0.3142 | time 465.8742 ms | tok/sec 1125385.3626
for step 18918 | loss 3.256126 | norm 0.3239 | time 466.1639 ms | tok/sec 1124686.0377
for step 18919 | loss 3.267959 | norm 0.3239 | time 464.8845 ms | tok/sec 1127781.1541
for step 18920 | loss 3.311029 | norm 0.3281 | time 465.7576 ms | tok/sec 1125667.0649
for step 18921 | loss 3.241397 | norm 0.3141 | time 466.1975 ms | tok/sec 1124604.9378
for step 18922 | loss 3.226885 | norm 0.3346 | time 467.0086 ms | tok/sec 1122651.7251
for step 18923 | loss 3.157451 | norm 0.3179 | time 466.3169 ms | tok/sec 1124316.8687
for step 18924 | loss 3.221301 | norm 0.3153 | time 465.9128 ms | tok/sec 1125292.0691
for step 18925 | loss 3.198066 | norm 0.3162 | time 465.0486 ms | tok/sec 1127383.3630
for step 18926 | loss 3.149266 | norm 0.3242 | time 466.4063 ms | tok/sec 1124101.3447
for step 18927 | loss 3.202486 | norm 0.3282 | time 464.9982 ms | tok/sec 1127505.3301
for step 18928 | loss 3.210816 | norm 0.3129 | time 463.9325 ms | tok/sec 1130095.3996
for step 18929 | loss 3.308193 | norm 0.3326 | time 464.6208 ms | tok/sec 1128421.2151
for step 18930 | loss 3.236016 | norm 0.3267 | time 465.3854 ms | tok/sec 1126567.2673
for step 18931 | loss 3.187869 | norm 0.2979 | time 464.9749 ms | tok/sec 1127561.9873
for step 18932 | loss 3.185161 | norm 0.3140 | time 466.0187 ms | tok/sec 1125036.4549
for step 18933 | loss 3.261896 | norm 0.3266 | time 466.0518 ms | tok/sec 1124956.4554
for step 18934 | loss 3.200897 | norm 0.3005 | time 465.8616 ms | tok/sec 1125415.8880
for step 18935 | loss 3.146343 | norm 0.3034 | time 465.3084 ms | tok/sec 1126753.7157
for step 18936 | loss 3.162591 | norm 0.3259 | time 465.9483 ms | tok/sec 1125206.2757
for step 18937 | loss 3.176641 | norm 0.2860 | time 465.8360 ms | tok/sec 1125477.5196
for step 18938 | loss 3.218714 | norm 0.3374 | time 465.6451 ms | tok/sec 1125939.1075
for step 18939 | loss 3.191653 | norm 0.3067 | time 466.0861 ms | tok/sec 1124873.5902
for step 18940 | loss 3.197819 | norm 0.2975 | time 466.8920 ms | tok/sec 1122932.0603
for step 18941 | loss 3.149815 | norm 0.2872 | time 465.7278 ms | tok/sec 1125739.0973
for step 18942 | loss 3.140299 | norm 0.3070 | time 465.5328 ms | tok/sec 1126210.7050
for step 18943 | loss 3.162745 | norm 0.3078 | time 467.3545 ms | tok/sec 1121820.7154
for step 18944 | loss 3.157792 | norm 0.2819 | time 464.2341 ms | tok/sec 1129361.2092
for step 18945 | loss 3.116595 | norm 0.2913 | time 465.8706 ms | tok/sec 1125394.0017
for step 18946 | loss 3.104377 | norm 0.2881 | time 464.4742 ms | tok/sec 1128777.4405
for step 18947 | loss 3.111599 | norm 0.2753 | time 466.0656 ms | tok/sec 1124923.0776
for step 18948 | loss 3.115682 | norm 0.2797 | time 465.2531 ms | tok/sec 1126887.6735
for step 18949 | loss 3.195692 | norm 0.2919 | time 464.9320 ms | tok/sec 1127666.0665
for step 18950 | loss 3.083035 | norm 0.2942 | time 464.8514 ms | tok/sec 1127861.5558
for step 18951 | loss 3.072357 | norm 0.2913 | time 465.1389 ms | tok/sec 1127164.3508
for step 18952 | loss 3.052577 | norm 0.2739 | time 464.2549 ms | tok/sec 1129310.7506
for step 18953 | loss 3.107115 | norm 0.3040 | time 465.8134 ms | tok/sec 1125532.2450
for step 18954 | loss 3.104288 | norm 0.2850 | time 465.5490 ms | tok/sec 1126171.4853
for step 18955 | loss 3.149254 | norm 0.2766 | time 465.8799 ms | tok/sec 1125371.5404
for step 18956 | loss 3.107492 | norm 0.3093 | time 465.1718 ms | tok/sec 1127084.6261
for step 18957 | loss 3.185837 | norm 0.3600 | time 465.4095 ms | tok/sec 1126508.9787
for step 18958 | loss 3.245340 | norm 0.3219 | time 465.1568 ms | tok/sec 1127121.0207
for step 18959 | loss 3.219198 | norm 0.3418 | time 465.5602 ms | tok/sec 1126144.3793
for step 18960 | loss 3.236010 | norm 0.3396 | time 465.2426 ms | tok/sec 1126913.0828
for step 18961 | loss 3.253581 | norm 0.3348 | time 464.9601 ms | tok/sec 1127597.8346
Will loading at 0 from edu_fineweb10B/edufineweb_train_000002.npy
for step 18962 | loss 3.230810 | norm 0.3392 | time 2925.5533 ms | tok/sec 179209.8596
for step 18963 | loss 3.277096 | norm 0.3096 | time 464.0427 ms | tok/sec 1129827.1499
for step 18964 | loss 3.241021 | norm 0.3100 | time 464.1690 ms | tok/sec 1129519.5743
for step 18965 | loss 3.203699 | norm 0.3250 | time 465.0056 ms | tok/sec 1127487.4091
for step 18966 | loss 3.267614 | norm 0.3109 | time 464.5965 ms | tok/sec 1128480.2808
for step 18967 | loss 3.230360 | norm 0.3177 | time 464.9019 ms | tok/sec 1127738.9333
for step 18968 | loss 3.198641 | norm 0.3326 | time 465.6875 ms | tok/sec 1125836.4997
for step 18969 | loss 3.211457 | norm 0.2985 | time 465.4849 ms | tok/sec 1126326.6495
for step 18970 | loss 3.139034 | norm 0.3285 | time 464.3018 ms | tok/sec 1129196.5102
for step 18971 | loss 3.207036 | norm 0.3314 | time 464.6108 ms | tok/sec 1128445.5355
for step 18972 | loss 3.221728 | norm 0.3118 | time 465.0753 ms | tok/sec 1127318.6328
for step 18973 | loss 3.237996 | norm 0.3231 | time 465.0269 ms | tok/sec 1127435.9617
for step 18974 | loss 3.181592 | norm 0.3178 | time 465.4377 ms | tok/sec 1126440.8868
for step 18975 | loss 3.180538 | norm 0.3423 | time 465.9879 ms | tok/sec 1125110.7092
for step 18976 | loss 3.246356 | norm 0.3043 | time 465.0645 ms | tok/sec 1127344.6396
for step 18977 | loss 3.204338 | norm 0.3358 | time 467.0727 ms | tok/sec 1122497.5718
for step 18978 | loss 3.270598 | norm 0.3316 | time 465.5976 ms | tok/sec 1126053.8429
for step 18979 | loss 3.209052 | norm 0.2957 | time 464.5503 ms | tok/sec 1128592.6386
for step 18980 | loss 3.135429 | norm 0.3383 | time 465.2588 ms | tok/sec 1126873.8143
for step 18981 | loss 3.248619 | norm 0.3402 | time 465.8229 ms | tok/sec 1125509.2021
for step 18982 | loss 3.186754 | norm 0.3100 | time 466.0146 ms | tok/sec 1125046.2398
for step 18983 | loss 3.192509 | norm 0.3434 | time 464.8972 ms | tok/sec 1127750.5003
for step 18984 | loss 3.214710 | norm 0.3380 | time 465.0078 ms | tok/sec 1127482.2064
for step 18985 | loss 3.161269 | norm 0.3009 | time 466.4867 ms | tok/sec 1123907.7309
for step 18986 | loss 3.232450 | norm 0.3402 | time 465.1284 ms | tok/sec 1127189.7726
for step 18987 | loss 3.136518 | norm 0.3188 | time 465.2503 ms | tok/sec 1126894.6032
for step 18988 | loss 3.197833 | norm 0.3641 | time 465.9929 ms | tok/sec 1125098.6206
for step 18989 | loss 3.212718 | norm 0.3378 | time 466.4779 ms | tok/sec 1123928.9850
for step 18990 | loss 3.197236 | norm 0.3515 | time 466.4662 ms | tok/sec 1123957.1334
for step 18991 | loss 3.169018 | norm 0.3239 | time 465.0340 ms | tok/sec 1127418.6209
for step 18992 | loss 3.097712 | norm 0.3138 | time 466.6009 ms | tok/sec 1123632.6499
for step 18993 | loss 3.164894 | norm 0.3671 | time 465.5747 ms | tok/sec 1126109.2010
for step 18994 | loss 3.092186 | norm 0.2930 | time 466.2309 ms | tok/sec 1124524.4246
for step 18995 | loss 3.138554 | norm 0.3329 | time 465.9557 ms | tok/sec 1125188.4277
for step 18996 | loss 3.170133 | norm 0.3230 | time 465.8384 ms | tok/sec 1125471.7594
for step 18997 | loss 3.119876 | norm 0.2664 | time 465.8480 ms | tok/sec 1125448.7190
for step 18998 | loss 3.160390 | norm 0.3001 | time 466.0962 ms | tok/sec 1124849.4235
for step 18999 | loss 3.098430 | norm 0.3278 | time 465.7369 ms | tok/sec 1125717.1984
validation loss 3.2124
HellaSwag accuracy: 2834/10042=0.2822
> Hello, I'm a language model, and this is the reason I created here: It is what was the best possible language. The way I think about it
> Hello, I'm a language model, which is very cool. I can see things from a video, but I need a program that just makes it work.
> Hello, I'm a language model, I write languages as a set of rules, I write the words, the rules, the language code.
What I
> Hello, I'm a language model, and I'm really excited to do things."
After returning to the game over the dinner spot, the three team members
> Hello, I'm a language model, and I can't understand a sentence unless I get it right.<|endoftext|>Methadone is one of the most common
> Hello, I'm a language model, so there's a lot of stuff to learn about those stuff, like finding and writing code for the web, and learning
> Hello, I'm a language model, so I had to define each person. I did that by having my class create a model, and then I had my
> Hello, I'm a language model, and when I think about it I really don't understand what I'm doing here too. For instance, I can find
> Hello, I'm a language model, as opposed to a modeling paradigm, where you can see what the task of an artist is and how it's done.
> Hello, I'm a language model, and I am a linguist. I teach, learn, and communicate the language of a large group of people. I
> > > Hello, I'm a language model, and I'll be doing an interview in it. I'm actually a real language observer, and after you're done with
Hello, I'm a language model, I can explain it well enough in some of the other languages I can understand, but I'm not talking about anything.
Hello, I'm a language model, so I thought I'd explain the basics as I do. So, for example, "Sine of Sine".
> > > > > Hello, I'm a language model, I like it. If you want to show how much I am writing to the language model, I am not in charge
Hello, I'm a language model, to look at the way I learn an alien language. Then I go to the place I want to find out about how
Hello, I'm a language model, and I'm a person of the process.
This is what you’re supposed to do before you actually speak
Hello, I'm a language model, and i'm from the language lab, so the first thing I did was stop the class trying to start class that were
Hello, I'm a language model, who wants to come up with new ways to learn? For me, as the author of this book, I realized how
> > > > > Hello, I'm a language model, and that means "I'm not in a language model". So, as a native speaker, you can think of you
Hello, I'm a language model, since i'm an awesome way to write. What i can do is I could also do a lot more math. I
Hello, I'm a language model, and I've only included a word tree in the program.
What kind of language do you use to describe how your
Hello, I'm a language model, and this blog is about developing it.
It's got a bunch of ways to look at a language, you could
Hello, I'm a language model, but... I could do things differently!<|endoftext|>Romeo and Juliet
Romeo and Juliet as a child were
> > > > > Hello, I'm a language model, I'm having an assignment in my field code, and I've seen so many examples of the model in the real world
Hello, I'm a language model, so I use it this way because it's easier to understand. When you want to be a model, you have to
Hello, I'm a language model, so this is a really, really tricky piece of work. I've spent a lot of time studying it, and what
Hello, I'm a language model, and it sounds like many languages will be made this way. However, it sounds good, because one language will change as
Hello, I'm a language model, and you ask about your model, no worries; your mind really does its job, and it really doesn't really work
> > > Hello, I'm a language model, so I'll explain it to you and point out the differences between languages we are interested in, such as:
|
Hello, I'm a language model, one of the areas I learned to use. So it has been so helpful to share my thoughts now, so that I
Hello, I'm a language model, and for the purpose of this, I'll start from this:
The first component of a language sentence, and then
> Hello, I'm a language model, I use modeling languages, and I can talk about everything from my own language to how I create languages, how to translate
for step 19000 | loss 3.099109 | norm 0.3002 | time 12388.4051 ms | tok/sec 42320.8635
for step 19001 | loss 3.089502 | norm 0.3039 | time 462.7454 ms | tok/sec 1132994.4461
for step 19002 | loss 3.080132 | norm 0.3032 | time 463.4259 ms | tok/sec 1131330.8743
for step 19003 | loss 3.170348 | norm 0.3025 | time 463.3884 ms | tok/sec 1131422.2612
for step 19004 | loss 3.278500 | norm 0.3546 | time 463.6598 ms | tok/sec 1130760.1855
for step 19005 | loss 3.231362 | norm 0.3032 | time 463.2006 ms | tok/sec 1131881.1653
for step 19006 | loss 3.247351 | norm 0.3360 | time 464.7036 ms | tok/sec 1128220.3221
for step 19007 | loss 3.280998 | norm 0.3348 | time 464.5102 ms | tok/sec 1128689.9562
for step 19008 | loss 3.271745 | norm 0.3187 | time 464.4616 ms | tok/sec 1128808.1501
for step 19009 | loss 3.217035 | norm 0.3003 | time 464.3254 ms | tok/sec 1129139.1089
for step 19010 | loss 3.262920 | norm 0.2960 | time 464.6699 ms | tok/sec 1128301.9443
for step 19011 | loss 3.223110 | norm 0.2972 | time 464.4361 ms | tok/sec 1128870.1539
for step 19012 | loss 3.314668 | norm 0.3163 | time 463.3672 ms | tok/sec 1131474.0731
for step 19013 | loss 3.231315 | norm 0.3209 | time 463.7640 ms | tok/sec 1130506.1496
for step 19014 | loss 3.262229 | norm 0.3073 | time 464.2835 ms | tok/sec 1129241.1598
for step 19015 | loss 3.215183 | norm 0.3155 | time 465.0819 ms | tok/sec 1127302.4515
for step 19016 | loss 3.201780 | norm 0.3082 | time 464.5743 ms | tok/sec 1128534.1403
for step 19017 | loss 3.185193 | norm 0.3282 | time 465.0452 ms | tok/sec 1127391.4548
for step 19018 | loss 3.204003 | norm 0.3096 | time 464.6347 ms | tok/sec 1128387.6315
for step 19019 | loss 3.205678 | norm 0.3269 | time 465.1752 ms | tok/sec 1127076.5387
for step 19020 | loss 3.116303 | norm 0.3113 | time 465.0555 ms | tok/sec 1127366.6018
for step 19021 | loss 3.186572 | norm 0.2979 | time 465.4651 ms | tok/sec 1126374.5340
for step 19022 | loss 3.247990 | norm 0.3265 | time 464.9940 ms | tok/sec 1127515.7361
for step 19023 | loss 3.232328 | norm 0.2968 | time 465.4264 ms | tok/sec 1126468.0072
for step 19024 | loss 3.191064 | norm 0.2953 | time 464.2265 ms | tok/sec 1129379.7699
for step 19025 | loss 3.295275 | norm 0.3298 | time 464.3474 ms | tok/sec 1129085.7714
for step 19026 | loss 3.200683 | norm 0.2965 | time 465.7183 ms | tok/sec 1125762.1496
for step 19027 | loss 3.156119 | norm 0.2919 | time 465.1649 ms | tok/sec 1127101.3789
for step 19028 | loss 3.188766 | norm 0.3074 | time 464.8089 ms | tok/sec 1127964.5331
for step 19029 | loss 3.161147 | norm 0.2770 | time 465.8442 ms | tok/sec 1125457.9350
for step 19030 | loss 3.183752 | norm 0.2924 | time 464.7193 ms | tok/sec 1128182.1200
for step 19031 | loss 3.212888 | norm 0.3809 | time 464.8085 ms | tok/sec 1127965.6903
for step 19032 | loss 3.171074 | norm 0.2837 | time 467.1795 ms | tok/sec 1122240.9345
for step 19033 | loss 3.186870 | norm 0.3484 | time 466.1329 ms | tok/sec 1124760.8211
for step 19034 | loss 3.227818 | norm 0.3041 | time 465.7657 ms | tok/sec 1125647.4736
for step 19035 | loss 3.162848 | norm 0.3089 | time 465.3988 ms | tok/sec 1126534.9481
for step 19036 | loss 3.224037 | norm 0.3038 | time 465.6281 ms | tok/sec 1125980.0406
for step 19037 | loss 3.197542 | norm 0.3074 | time 465.5831 ms | tok/sec 1126089.0177
for step 19038 | loss 3.158456 | norm 0.2946 | time 465.5581 ms | tok/sec 1126149.5697
for step 19039 | loss 3.072780 | norm 0.2989 | time 464.9818 ms | tok/sec 1127545.2208
for step 19040 | loss 3.108842 | norm 0.3004 | time 466.0850 ms | tok/sec 1124876.4672
for step 19041 | loss 3.133221 | norm 0.3574 | time 465.4083 ms | tok/sec 1126511.8641
for step 19042 | loss 3.104988 | norm 0.3541 | time 465.1570 ms | tok/sec 1127120.4430
for step 19043 | loss 3.107277 | norm 0.3059 | time 465.5612 ms | tok/sec 1126142.0724
for step 19044 | loss 3.128924 | norm 0.3008 | time 465.4369 ms | tok/sec 1126442.6179
for step 19045 | loss 3.107885 | norm 0.2894 | time 465.9374 ms | tok/sec 1125232.7608
for step 19046 | loss 3.107294 | norm 0.2978 | time 467.0935 ms | tok/sec 1122447.7246
for step 19047 | loss 3.121630 | norm 0.3021 | time 466.5883 ms | tok/sec 1123663.0802
for step 19048 | loss 3.112663 | norm 0.2988 | time 464.8397 ms | tok/sec 1127889.9016
for step 19049 | loss 3.170233 | norm 0.3312 | time 464.4990 ms | tok/sec 1128717.1849
for step 19050 | loss 3.271607 | norm 0.3594 | time 464.8993 ms | tok/sec 1127745.2951
for step 19051 | loss 3.269236 | norm 0.3145 | time 464.3667 ms | tok/sec 1129038.8154
for step 19052 | loss 3.248904 | norm 0.3410 | time 465.4214 ms | tok/sec 1126480.1252
for step 19053 | loss 3.262752 | norm 0.3135 | time 465.3296 ms | tok/sec 1126702.3352
for step 19054 | loss 3.219589 | norm 0.3201 | time 465.2743 ms | tok/sec 1126836.2807
for step 19055 | loss 3.267986 | norm 0.3105 | time 465.3714 ms | tok/sec 1126601.3198
for step 19056 | loss 3.216958 | norm 0.3198 | time 464.9951 ms | tok/sec 1127512.8455
for step 19057 | loss 3.203998 | norm 0.3571 | time 466.1212 ms | tok/sec 1124789.0112
for step 19058 | loss 3.241058 | norm 0.3542 | time 465.4663 ms | tok/sec 1126371.6493
for step 19059 | loss 3.208915 | norm 0.3237 | time 465.2011 ms | tok/sec 1127013.5766
for step 19060 | loss 3.202224 | norm 0.3274 | time 465.0867 ms | tok/sec 1127290.8936
for step 19061 | loss 3.252404 | norm 0.3451 | time 465.4527 ms | tok/sec 1126404.5361
for step 19062 | loss 3.128184 | norm 0.3209 | time 465.1945 ms | tok/sec 1127029.7497
for step 19063 | loss 3.155510 | norm 0.3202 | time 465.0769 ms | tok/sec 1127314.5874
for step 19064 | loss 3.211594 | norm 0.3257 | time 464.8068 ms | tok/sec 1127969.7403
for step 19065 | loss 3.187248 | norm 0.3484 | time 464.8850 ms | tok/sec 1127779.9973
for step 19066 | loss 3.244376 | norm 0.3139 | time 465.6827 ms | tok/sec 1125848.0278
for step 19067 | loss 3.185708 | norm 0.3216 | time 465.4098 ms | tok/sec 1126508.4016
for step 19068 | loss 3.207934 | norm 0.3307 | time 464.9613 ms | tok/sec 1127594.9436
for step 19069 | loss 3.228099 | norm 0.2992 | time 465.6208 ms | tok/sec 1125997.9137
for step 19070 | loss 3.147832 | norm 0.3374 | time 465.7443 ms | tok/sec 1125699.3342
for step 19071 | loss 3.142967 | norm 0.3107 | time 467.3500 ms | tok/sec 1121831.5891
validation loss 3.2094
HellaSwag accuracy: 2839/10042=0.2827
> Hello, I'm a language model, and this is the same thing. No one says, "If you are really good at this job, it's not
> Hello, I'm a language model, not an artificial intelligence. I don't mean to explain in words, but you make people listen and think.
I
> Hello, I'm a language model, but did you know that you can create a model of an infinite set of events?
A computer system that contains its
> Hello, I'm a language model, and I'm using the IJE platform to understand concepts of learning to become a multilingual. I'm interested in
> Hello, I'm a language model, and I know that you need help when translating something in a sentence."
He said: "There is no need here
> Hello, I'm a language model, so there's a lot of work like this, how long does it take you to get a language model that's correct
> Hello, I'm a language model, so I hope to create that future as well - even if my code has a slightly different syntax.
I hope to
> Hello, I'm a language model, and how I've been to this post is all about the concepts of what a parser is, how it is structured?
> > Hello, I'm a language model, and I'm a beginner and am getting stuck on how to type in E really, REALLY cool. Here is my take
Hello, I'm a language model, but that is not how I structure the work, that is, if you're going to be modeling you have to look
> > > Hello, I'm a language model, this blog is about learning at a fast rate. In other language schools I think there are some things that are not to>Hello, I'm a language model, and this was a great piece in my head. I would like you to help me out.
A common feature I
Hello, I'm a language model, one of the languages from language school that I studied. It's made of mostly words. It's really difficult. I

 Hello, I'm a language model, so I thought I'd try to do as much as possible with this tutorial.
What should I do when I am> > > > > 
Hello, I'm a language model, and that is just my language. My goal is to get to the bottom, not the top, but not the top
Hello, I'm a language model, and I use it for writing. I always wrote an HTML document, called a TELTS guide.
I would
Hello, I'm a language model, and it sounds like no problem; I hope you've come up with a little experiment to solve it, but how many
Hello, I'm a language model, so I use it not only as an example of what I'm writing, but as a way to help me understand some
Hello, I'm a language model, and I don't need a PhD to tell you exactly how to do one. I know that, and so I'm
> > > > > > Hello, I'm a language model, we use a process for describing languages, like anaphony, grammars, morse, a list of languages
Hello, I'm a language model, but I mean a language model. My primary goal in this article is to show you how to make the best use of
Hello, I'm a language model, to add something to a file called Python. I could look at the file and understand it as a list of objects (
Hello, I'm a language model, and like I've never heard of it, it was very limited and limited to 3 levels of interpretation — I was limited
Hello, I'm a language model, now that you are already here. I am trying to figure out what is in English that you get, what are the
Hello, I'm a language model, but there is no point at it. There may be a way to get rid of the grammar, but not as a
> > > > Hello, I'm a language model, I wanted to go with some of it's syntax. I was inspired by the two terms language, lexical structure,
Hello, I'm a language model, so I've got a few tips...that I can use to understand why. And here are ten helpful suggestions and suggestions
Hello, I'm a language model, and I have just spoken to everyone who calls me with a little trouble on my part, and I think that you have
Hello, I'm a language model, and my mother's mom is fluent in Spanish and English. I was born to a Spanish immersion in Portugal...a first
> > > Hello, I'm a language model, and you aren't supposed to know just that? No problem, you'll have to have some really great ideas about what
Hello, I'm a language model, but my initial thought is to write code for a particular language but I'm having an itch where something like the word "
Hello, I'm a language model, writing code that is easy to understand? So if you're interested, we've compiled a list of most popular languages.
for step 19072 | loss 3.249376 | norm 0.3226 | time 12367.2063 ms | tok/sec 42393.4061
